{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab94131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:44:00,410 - \u001b[1;32mINFO\u001b[1;0m - Loading facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "100%|███████████████████████████████| 578733/578733 [00:03<00:00, 176059.13it/s]\n",
      "2023-07-12 17:44:09,032 - \u001b[1;32mINFO\u001b[1;0m - Found 771124 unique observations\n",
      "2023-07-12 17:44:11,448 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to those with at least 2 words\n",
      "2023-07-12 17:44:12,197 - \u001b[1;32mINFO\u001b[1;0m - Found 769122 observations with at least 2 words\n",
      "2023-07-12 17:44:12,197 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to the 2-th of every 4 sentences\n",
      "2023-07-12 17:44:12,430 - \u001b[1;32mINFO\u001b[1;0m - Found 192280 observations that are the 2-th of every 4 sentences\n",
      "2023-07-12 17:44:12,430 - \u001b[1;32mINFO\u001b[1;0m - Sampling 15000 equally spaced sentences\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - Total number of sentences to paraphrase: 15000\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - Example sentences to paraphrase:\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 1. CT 2\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 1667. likely a pericardial drain\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 3334. chronic anterior wedge deformity\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 5000. patient received the top of catheter\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 6667. resolved bilateral parenchymal opacities\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 8333. small loculated right effusion has decreased\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 10000. increased opacity projecting over the lower spine\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 11666. similar appearance of bibasilar opacities on the right\n",
      "2023-07-12 17:44:12,439 - \u001b[1;32mINFO\u001b[1;0m - 13333. continuous improvement in opacification of the right lower lobe\n",
      "2023-07-12 17:44:12,440 - \u001b[1;32mINFO\u001b[1;0m - 15000. area of consolidation in the left mid lung at the fourth left anterior rib level is more dense and larger in size than any corresponding opacities in this region on the recent CT scan\n",
      "2023-07-12 17:44:12,491 - \u001b[1;32mINFO\u001b[1;0m - Saving API requests to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_requests_20230712_174412.jsonl\n",
      "2023-07-12 17:44:12,491 - \u001b[1;32mINFO\u001b[1;0m - Saving API responses to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174412.jsonl\n",
      "2023-07-12 17:44:13,507 - \u001b[1;32mINFO\u001b[1;0m - Starting request #0\n",
      "2023-07-12 17:44:13,647 - \u001b[1;32mINFO\u001b[1;0m - Starting request #50\n",
      "2023-07-12 17:44:13,781 - \u001b[1;32mINFO\u001b[1;0m - Starting request #100\n",
      "2023-07-12 17:44:15,661 - \u001b[1;32mINFO\u001b[1;0m - Starting request #150\n",
      "2023-07-12 17:44:36,246 - \u001b[1;32mINFO\u001b[1;0m - Starting request #200\n",
      "2023-07-12 17:44:44,357 - \u001b[1;33mWARNING\u001b[1;0m - Request 142 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e174ebf77d4e1bebcdeda69a23ffb514 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:44:56,838 - \u001b[1;32mINFO\u001b[1;0m - Starting request #249\n",
      "2023-07-12 17:45:17,449 - \u001b[1;32mINFO\u001b[1;0m - Starting request #299\n",
      "2023-07-12 17:45:29,090 - \u001b[1;33mWARNING\u001b[1;0m - Request 252 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a004187ef642202b10dce504f528fba8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:45:38,058 - \u001b[1;32mINFO\u001b[1;0m - Starting request #348\n",
      "2023-07-12 17:45:58,664 - \u001b[1;32mINFO\u001b[1;0m - Starting request #398\n",
      "2023-07-12 17:46:19,276 - \u001b[1;32mINFO\u001b[1;0m - Starting request #448\n",
      "2023-07-12 17:46:26,265 - \u001b[1;33mWARNING\u001b[1;0m - Request 391 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c2ad35b2b4633d5fbac291281a63f99a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:46:39,905 - \u001b[1;32mINFO\u001b[1;0m - Starting request #497\n",
      "2023-07-12 17:47:00,522 - \u001b[1;32mINFO\u001b[1;0m - Starting request #547\n",
      "2023-07-12 17:47:21,146 - \u001b[1;32mINFO\u001b[1;0m - Starting request #597\n",
      "2023-07-12 17:47:41,767 - \u001b[1;32mINFO\u001b[1;0m - Starting request #647\n",
      "2023-07-12 17:48:02,403 - \u001b[1;32mINFO\u001b[1;0m - Starting request #697\n",
      "2023-07-12 17:48:15,722 - \u001b[1;33mWARNING\u001b[1;0m - Request 656 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0209b3bc2dbca493caa30ac37193ec93 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:48:23,041 - \u001b[1;32mINFO\u001b[1;0m - Starting request #746\n",
      "2023-07-12 17:48:35,212 - \u001b[1;33mWARNING\u001b[1;0m - Request 703 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 50e4bc756293a83f451809e67f80aaf5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:48:43,665 - \u001b[1;32mINFO\u001b[1;0m - Starting request #795\n",
      "2023-07-12 17:49:04,299 - \u001b[1;32mINFO\u001b[1;0m - Starting request #845\n",
      "2023-07-12 17:49:16,877 - \u001b[1;33mWARNING\u001b[1;0m - Request 802 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ce99211c8d1fc5c1cc1f08945ef78f9f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:49:24,933 - \u001b[1;32mINFO\u001b[1;0m - Starting request #894\n",
      "2023-07-12 17:49:45,565 - \u001b[1;32mINFO\u001b[1;0m - Starting request #944\n",
      "2023-07-12 17:50:01,763 - \u001b[1;33mWARNING\u001b[1;0m - Request 910 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5c9be8ab553aa7fad355175cbd7bad5c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:50:06,213 - \u001b[1;32mINFO\u001b[1;0m - Starting request #993\n",
      "2023-07-12 17:50:26,852 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1043\n",
      "2023-07-12 17:50:35,793 - \u001b[1;33mWARNING\u001b[1;0m - Request 990 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 377d420279d0e02da8cff5a3832542ef in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:50:47,496 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1092\n",
      "2023-07-12 17:50:47,996 - \u001b[1;33mWARNING\u001b[1;0m - Request 1020 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ae16df8e39e83aa36aabc52b3f0b4331 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:51:08,144 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1141\n",
      "2023-07-12 17:51:28,800 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1191\n",
      "2023-07-12 17:51:49,457 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1241\n",
      "2023-07-12 17:52:10,120 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1291\n",
      "2023-07-12 17:52:11,675 - \u001b[1;33mWARNING\u001b[1;0m - Request 1199 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 26c65f03e2861684dd8f565d2ad216bb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:52:30,771 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:52:51,437 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1390\n",
      "2023-07-12 17:53:12,096 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1440\n",
      "2023-07-12 17:53:32,760 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1490\n",
      "2023-07-12 17:53:34,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 1421 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d61733345aa28e78324cc012e98f2f97 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:53:53,421 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1539\n",
      "2023-07-12 17:54:14,091 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1589\n",
      "2023-07-12 17:54:34,758 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1639\n",
      "2023-07-12 17:54:55,425 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1689\n",
      "2023-07-12 17:55:00,570 - \u001b[1;33mWARNING\u001b[1;0m - Request 1628 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b5ceef7a3563bc9710f1ae7e8e982a47 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:55:16,077 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1738\n",
      "2023-07-12 17:55:29,791 - \u001b[1;33mWARNING\u001b[1;0m - Request 1699 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a2f21dd968d665ccaab781ceebe3abcb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:55:36,740 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1787\n",
      "2023-07-12 17:55:57,427 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1837\n",
      "2023-07-12 17:56:18,093 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1887\n",
      "2023-07-12 17:56:38,775 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1937\n",
      "2023-07-12 17:56:45,141 - \u001b[1;33mWARNING\u001b[1;0m - Request 1879 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6c286f34ec8c768208ae7b27b66340d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:56:59,429 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1986\n",
      "2023-07-12 17:57:20,094 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2036\n",
      "2023-07-12 17:57:40,794 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2086\n",
      "2023-07-12 17:57:51,834 - \u001b[1;33mWARNING\u001b[1;0m - Request 1802 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 11108758607fa82baa225b4ee2f2b3df in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:58:01,482 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2135\n",
      "2023-07-12 17:58:22,154 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2185\n",
      "2023-07-12 17:58:26,009 - \u001b[1;33mWARNING\u001b[1;0m - Request 2121 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7dd1f567683672cc775ee8523d14d2a0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:58:42,825 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2234\n",
      "2023-07-12 17:59:03,509 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2284\n",
      "2023-07-12 17:59:24,190 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2334\n",
      "2023-07-12 17:59:24,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 1613 failed with Exception \n",
      "2023-07-12 17:59:44,893 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2383\n",
      "2023-07-12 18:00:05,579 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2433\n",
      "2023-07-12 18:00:15,552 - \u001b[1;33mWARNING\u001b[1;0m - Request 2384 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3b54b152ff6d43db33d2b63a6387ac99 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:00:20,526 - \u001b[1;33mWARNING\u001b[1;0m - Request 2396 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 59f83d1e4f245e8592a1388d81b66d62 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:00:26,257 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2481\n",
      "2023-07-12 18:00:46,933 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2531\n",
      "2023-07-12 18:01:07,623 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2581\n",
      "2023-07-12 18:01:28,314 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2631\n",
      "2023-07-12 18:01:49,034 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2681\n",
      "2023-07-12 18:02:00,309 - \u001b[1;33mWARNING\u001b[1;0m - Request 2635 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a8e32d1c3fadd1bcb0b869644a119fa5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:02:01,048 - \u001b[1;33mWARNING\u001b[1;0m - Request 2637 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 28cc7e675e5e5181b603092a251ed10c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:02:09,735 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2729\n",
      "2023-07-12 18:02:30,430 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2779\n",
      "2023-07-12 18:02:51,121 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2829\n",
      "2023-07-12 18:03:11,820 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2879\n",
      "2023-07-12 18:03:32,506 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2929\n",
      "2023-07-12 18:03:53,226 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2979\n",
      "2023-07-12 18:04:13,923 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3029\n",
      "2023-07-12 18:04:34,623 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3079\n",
      "2023-07-12 18:04:36,390 - \u001b[1;33mWARNING\u001b[1;0m - Request 3010 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID db2c405f05f1ebe954e7095dd777735a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:04:55,331 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3128\n",
      "2023-07-12 18:05:01,958 - \u001b[1;33mWARNING\u001b[1;0m - Request 3072 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ac8f6106f0b4916f97236b20ecfd12f4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:05:16,012 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3177\n",
      "2023-07-12 18:05:35,498 - \u001b[1;33mWARNING\u001b[1;0m - Request 3151 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8984db1555f9c467002e353efb739179 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:05:36,707 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3226\n",
      "2023-07-12 18:05:53,301 - \u001b[1;33mWARNING\u001b[1;0m - Request 3194 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9c918ba05a4f5b9c7ab54f6b6ff8e6ac in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:05:57,411 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3275\n",
      "2023-07-12 18:06:18,140 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3325\n",
      "2023-07-12 18:06:38,847 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3375\n",
      "2023-07-12 18:06:59,555 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3425\n",
      "2023-07-12 18:07:20,264 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3475\n",
      "2023-07-12 18:07:40,981 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3525\n",
      "2023-07-12 18:08:01,671 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3575\n",
      "2023-07-12 18:08:22,372 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3625\n",
      "2023-07-12 18:08:43,110 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3675\n",
      "2023-07-12 18:09:03,830 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3725\n",
      "2023-07-12 18:09:24,544 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3775\n",
      "2023-07-12 18:09:45,269 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3825\n",
      "2023-07-12 18:10:05,973 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3875\n",
      "2023-07-12 18:10:26,690 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3925\n",
      "2023-07-12 18:10:47,395 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3975\n",
      "2023-07-12 18:11:02,387 - \u001b[1;33mWARNING\u001b[1;0m - Request 3938 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5c1d008e7cab93951f0f1b1ace788f19 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:11:08,126 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4024\n",
      "2023-07-12 18:11:28,845 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4074\n",
      "2023-07-12 18:11:49,565 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4124\n",
      "2023-07-12 18:12:10,298 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4174\n",
      "2023-07-12 18:12:31,007 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4224\n",
      "2023-07-12 18:12:38,970 - \u001b[1;33mWARNING\u001b[1;0m - Request 4170 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5e887d44fc76fdd34402e29df52cdd79 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:12:51,708 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4273\n",
      "2023-07-12 18:13:12,420 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4323\n",
      "2023-07-12 18:13:24,012 - \u001b[1;33mWARNING\u001b[1;0m - Request 4278 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4013f20aaba1cf9735154dbd87aefd4f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:13:33,159 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4372\n",
      "2023-07-12 18:13:53,878 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4422\n",
      "2023-07-12 18:14:02,669 - \u001b[1;33mWARNING\u001b[1;0m - Request 4370 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 01ca0028b0f353e0941879ac4f01a56d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:14:14,599 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4471\n",
      "2023-07-12 18:14:35,325 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4521\n",
      "2023-07-12 18:14:56,052 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4571\n",
      "2023-07-12 18:15:13,046 - \u001b[1;33mWARNING\u001b[1;0m - Request 4539 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9bd42328236b4e16fb535c83c53b24d8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:15:16,773 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4620\n",
      "2023-07-12 18:15:26,715 - \u001b[1;33mWARNING\u001b[1;0m - Request 4572 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0496ae15ab3406166326df74e1c63439 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:15:29,300 - \u001b[1;33mWARNING\u001b[1;0m - Request 4578 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ab561d90bb370b205836fdb7a8815625 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:15:37,504 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4668\n",
      "2023-07-12 18:15:58,235 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4718\n",
      "2023-07-12 18:16:10,733 - \u001b[1;33mWARNING\u001b[1;0m - Request 4675 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3b0979e31ffcfe6cf37ea5ead1454bc2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:16:18,989 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4767\n",
      "2023-07-12 18:16:30,546 - \u001b[1;33mWARNING\u001b[1;0m - Request 4723 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 30b659344d51ffbc0c20f01333b947c2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:16:39,721 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4816\n",
      "2023-07-12 18:17:00,457 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4866\n",
      "2023-07-12 18:17:21,193 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4916\n",
      "2023-07-12 18:17:41,944 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4966\n",
      "2023-07-12 18:18:02,668 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5016\n",
      "2023-07-12 18:18:23,395 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5066\n",
      "2023-07-12 18:18:44,125 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5116\n",
      "2023-07-12 18:19:04,866 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5166\n",
      "2023-07-12 18:19:25,602 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5216\n",
      "2023-07-12 18:19:46,346 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5266\n",
      "2023-07-12 18:20:07,090 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5316\n",
      "2023-07-12 18:20:26,229 - \u001b[1;33mWARNING\u001b[1;0m - Request 5289 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ae668b4bea88dccac4a576cec333d9bc in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:20:27,817 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5365\n",
      "2023-07-12 18:20:35,249 - \u001b[1;33mWARNING\u001b[1;0m - Request 5311 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c66f1c3a6786ec4dbacbc0a1bcde7e78 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:20:48,546 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5414\n",
      "2023-07-12 18:21:09,280 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5464\n",
      "2023-07-12 18:21:30,030 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5514\n",
      "2023-07-12 18:21:50,796 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5564\n",
      "2023-07-12 18:22:11,549 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5614\n",
      "2023-07-12 18:22:13,151 - \u001b[1;33mWARNING\u001b[1;0m - Request 5545 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 81fcd140c0b9c83d09b3fa9e39cbce35 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:22:32,301 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5663\n",
      "2023-07-12 18:22:32,682 - \u001b[1;33mWARNING\u001b[1;0m - Request 5592 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2a11b1a829f1f825b682f9921ae58768 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:22:38,963 - \u001b[1;33mWARNING\u001b[1;0m - Request 5607 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6f067407582c4d2e6a75cd01ba8bc566 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:22:53,051 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5711\n",
      "2023-07-12 18:23:13,810 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5761\n",
      "2023-07-12 18:23:34,545 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5811\n",
      "2023-07-12 18:23:54,496 - \u001b[1;33mWARNING\u001b[1;0m - Request 5786 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 40f600d4a8deafaadfdbb00d2b41cdca in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:23:55,279 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5786\n",
      "2023-07-12 18:24:00,721 - \u001b[1;33mWARNING\u001b[1;0m - Request 5801 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1be5cae590a9c42b5564e6a2217b0ceb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:24:16,020 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5909\n",
      "2023-07-12 18:24:36,789 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5959\n",
      "2023-07-12 18:24:36,803 - \u001b[1;33mWARNING\u001b[1;0m - Request 5886 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d5646280562b88f4f6ca8c762e3b25f4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:24:57,544 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6008\n",
      "2023-07-12 18:25:18,291 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6058\n",
      "2023-07-12 18:25:27,870 - \u001b[1;33mWARNING\u001b[1;0m - Request 6008 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5078208cb5453d6ae0d66dd4d9f2feb1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:25:39,039 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6107\n",
      "2023-07-12 18:25:59,801 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6157\n",
      "2023-07-12 18:26:20,543 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6207\n",
      "2023-07-12 18:26:41,287 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6257\n",
      "2023-07-12 18:27:02,042 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6307\n",
      "2023-07-12 18:27:22,817 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6357\n",
      "2023-07-12 18:27:43,588 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6407\n",
      "2023-07-12 18:27:47,740 - \u001b[1;33mWARNING\u001b[1;0m - Request 6344 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3fccb9ef2d6ccd664ccb8e8ae5d7432c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:28:04,351 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6456\n",
      "2023-07-12 18:28:15,498 - \u001b[1;33mWARNING\u001b[1;0m - Request 6411 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e847e6035daa28474e5bd0f28185c866 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:28:25,115 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6505\n",
      "2023-07-12 18:28:33,892 - \u001b[1;33mWARNING\u001b[1;0m - Request 6454 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 97ba6a5f369a3cfaf1e8f7be38a21c99 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:28:45,878 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6554\n",
      "2023-07-12 18:29:06,628 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6604\n",
      "2023-07-12 18:29:27,372 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6654\n",
      "2023-07-12 18:29:48,135 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6704\n",
      "2023-07-12 18:30:08,904 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6754\n",
      "2023-07-12 18:30:26,691 - \u001b[1;33mWARNING\u001b[1;0m - Request 6724 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5f2b60d09e27cf95b65fe087b07bb2f8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:30:29,684 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6803\n",
      "2023-07-12 18:30:50,457 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6853\n",
      "2023-07-12 18:31:00,759 - \u001b[1;33mWARNING\u001b[1;0m - Request 6805 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7f32b4fe23b4524b34603fdb7e1cacf5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:31:11,232 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6902\n",
      "2023-07-12 18:31:31,997 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6952\n",
      "2023-07-12 18:31:52,769 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7002\n",
      "2023-07-12 18:32:13,538 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7052\n",
      "2023-07-12 18:32:34,299 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7102\n",
      "2023-07-12 18:32:55,082 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7152\n",
      "2023-07-12 18:33:15,856 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7202\n",
      "2023-07-12 18:33:36,630 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7252\n",
      "2023-07-12 18:33:57,410 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7302\n",
      "2023-07-12 18:34:18,175 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7352\n",
      "2023-07-12 18:34:38,951 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7402\n",
      "2023-07-12 18:34:49,748 - \u001b[1;33mWARNING\u001b[1;0m - Request 7355 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e1d6339dcb1e43845f732003b8fe3f90 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:34:51,731 - \u001b[1;33mWARNING\u001b[1;0m - Request 7360 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0d331e92f9d75a761fb53417ff98744b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:34:59,713 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7450\n",
      "2023-07-12 18:35:20,482 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7500\n",
      "2023-07-12 18:35:41,257 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7550\n",
      "2023-07-12 18:36:02,059 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7600\n",
      "2023-07-12 18:36:04,930 - \u001b[1;33mWARNING\u001b[1;0m - Request 7534 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 526aa0dc29a285ac860901a60ef297ab in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:36:22,833 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7649\n",
      "2023-07-12 18:36:43,617 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7699\n",
      "2023-07-12 18:36:44,438 - \u001b[1;33mWARNING\u001b[1;0m - Request 7628 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bf93daf5fec877cd47f17f725d08d8d0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:37:04,396 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7748\n",
      "2023-07-12 18:37:09,376 - \u001b[1;33mWARNING\u001b[1;0m - Request 7688 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fa1b074b287cee11c760ed1bea20cb36 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:37:25,188 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7797\n",
      "2023-07-12 18:37:45,964 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7847\n",
      "2023-07-12 18:38:06,733 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7897\n",
      "2023-07-12 18:38:27,529 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7947\n",
      "2023-07-12 18:38:48,333 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7997\n",
      "2023-07-12 18:39:09,132 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8047\n",
      "2023-07-12 18:39:29,918 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8097\n",
      "2023-07-12 18:39:50,707 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8147\n",
      "2023-07-12 18:40:11,493 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8197\n",
      "2023-07-12 18:40:32,270 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8247\n",
      "2023-07-12 18:40:37,179 - \u001b[1;33mWARNING\u001b[1;0m - Request 8186 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dfcfa1395b18fca825367958efc507cd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:40:53,046 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8296\n",
      "2023-07-12 18:40:56,780 - \u001b[1;33mWARNING\u001b[1;0m - Request 8233 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8f0d5c6eebaaf342b860bcc9712e01e0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:41:13,832 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8345\n",
      "2023-07-12 18:41:34,632 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8395\n",
      "2023-07-12 18:41:43,241 - \u001b[1;33mWARNING\u001b[1;0m - Request 8343 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 06d589e96dd9fafb709df9513ba20e26 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:41:55,441 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8444\n",
      "2023-07-12 18:42:16,258 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8494\n",
      "2023-07-12 18:42:37,057 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8544\n",
      "2023-07-12 18:42:41,588 - \u001b[1;33mWARNING\u001b[1;0m - Request 8482 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9fd6cb97499ca6336353a047b7f3bf53 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:42:57,853 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8593\n",
      "2023-07-12 18:43:18,642 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8643\n",
      "2023-07-12 18:43:39,428 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8693\n",
      "2023-07-12 18:44:00,218 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8743\n",
      "2023-07-12 18:44:21,030 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8793\n",
      "2023-07-12 18:44:41,839 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8843\n",
      "2023-07-12 18:45:02,667 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8893\n",
      "2023-07-12 18:45:23,462 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8943\n",
      "2023-07-12 18:45:44,262 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8993\n",
      "2023-07-12 18:46:05,054 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9043\n",
      "2023-07-12 18:46:23,338 - \u001b[1;33mWARNING\u001b[1;0m - Request 9014 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0b48406017a9b499512d9ea56a315193 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:46:25,852 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9092\n",
      "2023-07-12 18:46:42,450 - \u001b[1;33mWARNING\u001b[1;0m - Request 9060 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f397cd0c9809eb0fccb5c414208bc064 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:46:43,614 - \u001b[1;33mWARNING\u001b[1;0m - Request 9063 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1fe99c3dabb98f5c6b77eb8f686bb0fd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:46:46,666 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9140\n",
      "2023-07-12 18:47:07,484 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9190\n",
      "2023-07-12 18:47:28,294 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9240\n",
      "2023-07-12 18:47:49,115 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9290\n",
      "2023-07-12 18:48:09,924 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9340\n",
      "2023-07-12 18:48:30,721 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9390\n",
      "2023-07-12 18:48:43,166 - \u001b[1;33mWARNING\u001b[1;0m - Request 9347 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 76f225ae12b12fab7021aaf19f88acb7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:48:51,518 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9439\n",
      "2023-07-12 18:49:12,336 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9489\n",
      "2023-07-12 18:49:13,126 - \u001b[1;33mWARNING\u001b[1;0m - Request 9419 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID db4764bcf5510ff8b3a7be845f162812 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:49:24,250 - \u001b[1;33mWARNING\u001b[1;0m - Request 9445 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5b0ec6470734f8036f3ca893eeae6588 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:49:33,167 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9537\n",
      "2023-07-12 18:49:53,987 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9587\n",
      "2023-07-12 18:50:14,818 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9637\n",
      "2023-07-12 18:50:35,633 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9687\n",
      "2023-07-12 18:50:56,437 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9737\n",
      "2023-07-12 18:51:05,463 - \u001b[1;33mWARNING\u001b[1;0m - Request 9686 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0d5cbcda38c2101924f3d429b8b235d4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:51:17,231 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9786\n",
      "2023-07-12 18:51:38,044 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9836\n",
      "2023-07-12 18:51:43,306 - \u001b[1;33mWARNING\u001b[1;0m - Request 9776 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 418099408fa1c7959a86744e2fabf456 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:51:58,867 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9885\n",
      "2023-07-12 18:52:19,694 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:52:40,525 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9985\n",
      "2023-07-12 18:53:01,347 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10035\n",
      "2023-07-12 18:53:22,175 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10085\n",
      "2023-07-12 18:53:42,991 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10135\n",
      "2023-07-12 18:53:46,642 - \u001b[1;33mWARNING\u001b[1;0m - Request 10071 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f03eb2efae2d274c2c3b4e429d0d0ab4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:54:03,809 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10184\n",
      "2023-07-12 18:54:09,983 - \u001b[1;33mWARNING\u001b[1;0m - Request 10127 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID df3a0aba23b3d51dce6131501b99138d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:54:24,639 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10233\n",
      "2023-07-12 18:54:45,463 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10283\n",
      "2023-07-12 18:55:00,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9601 failed with Exception \n",
      "2023-07-12 18:55:06,292 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10332\n",
      "2023-07-12 18:55:27,117 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10382\n",
      "2023-07-12 18:55:47,940 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10432\n",
      "2023-07-12 18:56:08,761 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10482\n",
      "2023-07-12 18:56:29,597 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10532\n",
      "2023-07-12 18:56:50,443 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10582\n",
      "2023-07-12 18:57:08,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9906 failed with Exception \n",
      "2023-07-12 18:57:11,279 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10631\n",
      "2023-07-12 18:57:32,119 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10681\n",
      "2023-07-12 18:57:38,980 - \u001b[1;33mWARNING\u001b[1;0m - Request 10626 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 742c8f64f6eae44fd404baca981893a3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:57:52,938 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10730\n",
      "2023-07-12 18:58:13,748 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10780\n",
      "2023-07-12 18:58:23,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10088 failed with Exception \n",
      "2023-07-12 18:58:34,574 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10829\n",
      "2023-07-12 18:58:55,437 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10879\n",
      "2023-07-12 18:59:16,283 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10929\n",
      "2023-07-12 18:59:37,134 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10979\n",
      "2023-07-12 18:59:57,977 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11029\n",
      "2023-07-12 19:00:18,800 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11079\n",
      "2023-07-12 19:00:27,020 - \u001b[1;33mWARNING\u001b[1;0m - Request 11026 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e3684ee0c27592ae52c5c2433ecce057 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:00:39,637 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11128\n",
      "2023-07-12 19:01:00,510 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11178\n",
      "2023-07-12 19:01:21,369 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11228\n",
      "2023-07-12 19:01:42,227 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11278\n",
      "2023-07-12 19:01:56,542 - \u001b[1;33mWARNING\u001b[1;0m - Request 10875 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 778c22944e5f13338f1bcce921140606 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:01:58,499 - \u001b[1;33mWARNING\u001b[1;0m - Request 11244 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9223bef88cabfbd54b5728c30c53c507 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:02:03,080 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11326\n",
      "2023-07-12 19:02:23,917 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11376\n",
      "2023-07-12 19:02:44,767 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11426\n",
      "2023-07-12 19:03:05,645 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11476\n",
      "2023-07-12 19:03:26,518 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11526\n",
      "2023-07-12 19:03:36,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10833 failed with Exception \n",
      "2023-07-12 19:03:47,360 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11575\n",
      "2023-07-12 19:04:05,142 - \u001b[1;33mWARNING\u001b[1;0m - Request 11546 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 153646f8ceed3992d7fddfb82a02fb20 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:04:08,229 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11624\n",
      "2023-07-12 19:04:13,897 - \u001b[1;33mWARNING\u001b[1;0m - Request 11566 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 929e70dcdbe21a07e70f8ed2260dbb89 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:04:20,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10937 failed with Exception \n",
      "2023-07-12 19:04:29,078 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11672\n",
      "2023-07-12 19:04:40,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10985 failed with Exception \n",
      "2023-07-12 19:04:43,012 - \u001b[1;33mWARNING\u001b[1;0m - Request 11635 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3d8cc72c67a78ddc6d35b02f8b1ac9fb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:04:49,947 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11720\n",
      "2023-07-12 19:05:10,822 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11770\n",
      "2023-07-12 19:05:31,670 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11820\n",
      "2023-07-12 19:05:52,532 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11870\n",
      "2023-07-12 19:06:13,376 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11920\n",
      "2023-07-12 19:06:34,256 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11970\n",
      "2023-07-12 19:06:55,121 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12020\n",
      "2023-07-12 19:07:15,992 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12070\n",
      "2023-07-12 19:07:36,855 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12120\n",
      "2023-07-12 19:07:57,722 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12170\n",
      "2023-07-12 19:08:18,613 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12220\n",
      "2023-07-12 19:08:39,495 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12270\n",
      "2023-07-12 19:09:00,364 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12320\n",
      "2023-07-12 19:09:10,203 - \u001b[1;33mWARNING\u001b[1;0m - Request 12271 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 37fb7116f9088512998e4f2c20c0962d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:09:21,239 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12369\n",
      "2023-07-12 19:09:41,507 - \u001b[1;33mWARNING\u001b[1;0m - Request 12345 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0d34e464faf450224d9e618887b4ec31 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:09:42,135 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12345\n",
      "2023-07-12 19:10:03,042 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12468\n",
      "2023-07-12 19:10:23,920 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12518\n",
      "2023-07-12 19:10:38,677 - \u001b[1;33mWARNING\u001b[1;0m - Request 12481 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f3e702015b30a76f68ee8640bc9705c0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:10:44,785 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12567\n",
      "2023-07-12 19:11:05,671 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12617\n",
      "2023-07-12 19:11:26,585 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12667\n",
      "2023-07-12 19:11:40,060 - \u001b[1;33mWARNING\u001b[1;0m - Request 12627 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 56a4a8900321d391e5f6b581c4cd381b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:11:47,474 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12716\n",
      "2023-07-12 19:12:00,990 - \u001b[1;33mWARNING\u001b[1;0m - Request 12677 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9977ae2125c8a238443c9897703a3ab6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:12:08,342 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12765\n",
      "2023-07-12 19:12:29,250 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12815\n",
      "2023-07-12 19:12:50,156 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12865\n",
      "2023-07-12 19:12:53,166 - \u001b[1;33mWARNING\u001b[1;0m - Request 12800 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6844fbc967cdb78e7410787d4dc50a68 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:13:11,052 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12914\n",
      "2023-07-12 19:13:31,937 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12964\n",
      "2023-07-12 19:13:40,207 - \u001b[1;33mWARNING\u001b[1;0m - Request 12911 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d1b3123f5fb4893dd7620bd3743fa1c7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:13:52,852 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13013\n",
      "2023-07-12 19:14:13,757 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13063\n",
      "2023-07-12 19:14:34,652 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13113\n",
      "2023-07-12 19:14:55,572 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13163\n",
      "2023-07-12 19:15:16,486 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13213\n",
      "2023-07-12 19:15:37,380 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13263\n",
      "2023-07-12 19:15:39,190 - \u001b[1;33mWARNING\u001b[1;0m - Request 13195 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e151efadd16b6544062f2447329a306f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:15:57,576 - \u001b[1;33mWARNING\u001b[1;0m - Request 13239 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f2b985a13e854350a9c6fe6cee173a2d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:15:58,295 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13239\n",
      "2023-07-12 19:16:19,211 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13361\n",
      "2023-07-12 19:16:40,111 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13411\n",
      "2023-07-12 19:17:01,032 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13461\n",
      "2023-07-12 19:17:21,959 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13511\n",
      "2023-07-12 19:17:42,865 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13561\n",
      "2023-07-12 19:18:03,826 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13611\n",
      "2023-07-12 19:18:24,753 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13661\n",
      "2023-07-12 19:18:39,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 12982 failed with Exception \n",
      "2023-07-12 19:18:45,693 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13710\n",
      "2023-07-12 19:19:06,642 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13760\n",
      "2023-07-12 19:19:27,577 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13810\n",
      "2023-07-12 19:19:48,521 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13860\n",
      "2023-07-12 19:19:59,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 13172 failed with Exception \n",
      "2023-07-12 19:20:09,453 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13909\n",
      "2023-07-12 19:20:30,408 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13959\n",
      "2023-07-12 19:20:51,361 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14009\n",
      "2023-07-12 19:21:12,321 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14059\n",
      "2023-07-12 19:21:33,269 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14109\n",
      "2023-07-12 19:21:54,238 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14159\n",
      "2023-07-12 19:22:15,216 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14209\n",
      "2023-07-12 19:22:36,210 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14259\n",
      "2023-07-12 19:22:57,184 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14309\n",
      "2023-07-12 19:23:18,176 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14359\n",
      "2023-07-12 19:23:39,160 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14409\n",
      "2023-07-12 19:24:00,145 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14459\n",
      "2023-07-12 19:24:21,153 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14509\n",
      "2023-07-12 19:24:42,176 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14559\n",
      "2023-07-12 19:25:03,181 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14609\n",
      "2023-07-12 19:25:06,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13901 failed with Exception \n",
      "2023-07-12 19:25:09,027 - \u001b[1;33mWARNING\u001b[1;0m - Request 14551 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d8201cc3b7bc0e74edb92f2067a2df74 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:25:24,200 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14657\n",
      "2023-07-12 19:25:45,237 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14707\n",
      "2023-07-12 19:26:06,274 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14757\n",
      "2023-07-12 19:26:27,329 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14807\n",
      "2023-07-12 19:26:48,425 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14857\n",
      "2023-07-12 19:27:09,526 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14907\n",
      "2023-07-12 19:27:30,665 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14957\n",
      "2023-07-12 19:27:53,710 - \u001b[1;33mWARNING\u001b[1;0m - Request 14940 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9f368346dce44961f8e94b8b5ccfe44a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:28:16,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 14354 failed with Exception \n",
      "2023-07-12 19:28:20,165 - \u001b[1;32mINFO\u001b[1;0m - Parallel processing complete. Results saved to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174412.jsonl\n",
      "2023-07-12 19:28:20,169 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174412.jsonl\n",
      "2023-07-12 19:28:20,552 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"right PIC.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,552 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...r request?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"right PIC\"}] for sentence \"right PIC\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"right PIC.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,552 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need a complete sentence describing a fact from a chest X-ray report in order to provide the paraphrases. Could you please provide a full sentence?\n",
      "2023-07-12 19:28:20,552 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...tence?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 35, \"total_tokens\": 317}}, {\"observation\": \"slightly more\"}] for sentence \"slightly more\": Could not parse output: I'm sorry, but I need a complete sentence describing a fact from a chest X-ray report in order to provide the paraphrases. Could you please provide a full sentence?\n",
      "2023-07-12 19:28:20,552 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"CT 2\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 19:28:20,553 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...y your request?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 29, \"total_tokens\": 311}}, {\"observation\": \"CT 2\"}] for sentence \"CT 2\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"CT 2\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 19:28:20,556 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"line is removed\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 19:28:20,556 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...est?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 29, \"total_tokens\": 311}}, {\"observation\": \"line is removed\"}] for sentence \"line is removed\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"line is removed\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 19:28:20,556 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to generate the paraphrases. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 19:28:20,556 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...ase?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 37, \"total_tokens\": 318}}, {\"observation\": \"patient history\"}] for sentence \"patient history\": Could not parse output: I'm sorry, but I need more information to generate the paraphrases. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 19:28:20,558 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I can only provide paraphrases for the given sentence describing a fact from a chest X-ray report. If you have a specific sentence or fact you would like me to paraphrase, please provide it and I'll be happy to assist you.\n",
      "2023-07-12 19:28:20,558 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...ou.\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 53, \"total_tokens\": 334}}, {\"observation\": \"outside material\"}] for sentence \"outside material\": Could not parse output: I'm sorry, but I can only provide paraphrases for the given sentence describing a fact from a chest X-ray report. If you have a specific sentence or fact you would like me to paraphrase, please provide it and I'll be happy to assist you.\n",
      "2023-07-12 19:28:20,559 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need a specific sentence describing a fact from a chest X-ray report in order to provide the paraphrases. Could you please provide a sentence?\n",
      "2023-07-12 19:28:20,559 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...e?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 34, \"total_tokens\": 315}}, {\"observation\": \"different patient\"}] for sentence \"different patient\": Could not parse output: I'm sorry, but I need a specific sentence describing a fact from a chest X-ray report in order to provide the paraphrases. Could you please provide a sentence?\n",
      "2023-07-12 19:28:20,560 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...]\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 220, \"total_tokens\": 502}}, {\"observation\": \"similar to before\"}] for sentence \"similar to before\": Could not parse output: benign calcification\n",
      "[\n",
      "\"non-malignant calcification\",\n",
      "\"non-cancerous calcification\",\n",
      "\"harmless calcification\",\n",
      "\"innocuous calcification\",\n",
      "\"benign calcified lesion\",\n",
      "\"non-threatening calcification\",\n",
      "\"not indicative of cancer calcification\",\n",
      "\"safe calcification\",\n",
      "\"non-dangerous calcification\",\n",
      "\"non-metastatic calcification\"\n",
      "]\n",
      "\n",
      "osteoporosis\n",
      "[\n",
      "\"decreased bone density\",\n",
      "\"brittle bones\",\n",
      "\"low bone mass\",\n",
      "\"thinning of the bones\",\n",
      "\"weakening of the bones\",\n",
      "\"porous bones\",\n",
      "\"fragile bones\",\n",
      "\"reduced bone strength\",\n",
      "\"loss of bone density\",\n",
      "\"degenerative bone disease\"\n",
      "]\n",
      "\n",
      "no osteoporosis\n",
      "[\n",
      "\"normal bone density\",\n",
      "\"healthy bones\",\n",
      "\"adequate bone mass\",\n",
      "\"strong bones\",\n",
      "\"normal bone strength\",\n",
      "\"no signs of osteoporosis\",\n",
      "\"absence of osteoporosis\",\n",
      "\"no evidence of bone thinning\",\n",
      "\"no indication of bone weakening\",\n",
      "\"no osteoporotic changes\",\n",
      "\"no degenerative bone disease\"\n",
      "]\n",
      "2023-07-12 19:28:20,561 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: Sure! Here is a more expanded list of paraphrases for the given examples:\n",
      "\n",
      "1. Benign Calcification:\n",
      "- Non-cancerous calcification\n",
      "- Harmless calcification\n",
      "- Innocuous calcification\n",
      "- Benign calcified lesion\n",
      "- Non-malignant calcification\n",
      "- Non-threatening calcification\n",
      "- Not indicative of cancer calcification\n",
      "- Safe calcification\n",
      "- Non-dangerous calcification\n",
      "- Non-metastatic calcification\n",
      "- Non-neoplastic calcification\n",
      "\n",
      "2. Osteoporosis:\n",
      "- Decreased bone density\n",
      "- Brittle bones\n",
      "- Low bone mass\n",
      "- Thinning of the bones\n",
      "- Weakening of the bones\n",
      "- Porous bones\n",
      "- Fragile bones\n",
      "- Reduced bone strength\n",
      "- Loss of bone density\n",
      "- Degenerative bone disease\n",
      "- Bone demineralization\n",
      "\n",
      "3. No Osteoporosis:\n",
      "- Normal bone density\n",
      "- Healthy bones\n",
      "- Adequate bone mass\n",
      "- Strong bones\n",
      "- Normal bone strength\n",
      "- No signs of osteoporosis\n",
      "- Absence of osteoporosis\n",
      "- No evidence of bone thinning\n",
      "- No indication of bone weakening\n",
      "- No osteoporotic changes\n",
      "- No degenerative bone disease\n",
      "\n",
      "Please note that these are just a few examples, and there can be many more variations and synonyms for each term.\n",
      "2023-07-12 19:28:20,562 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 272, \"total_tokens\": 553}}, {\"observation\": \"complete expansion\"}] for sentence \"complete expansion\": Could not parse output: Sure! Here is a more expanded list of paraphrases for the given examples:\n",
      "\n",
      "1. Benign Calcification:\n",
      "- Non-cancerous calcification\n",
      "- Harmless calcification\n",
      "- Innocuous calcification\n",
      "- Benign calcified lesion\n",
      "- Non-malignant calcification\n",
      "- Non-threatening calcification\n",
      "- Not indicative of cancer calcification\n",
      "- Safe calcification\n",
      "- Non-dangerous calcification\n",
      "- Non-metastatic calcification\n",
      "- Non-neoplastic calcification\n",
      "\n",
      "2. Osteoporosis:\n",
      "- Decreased bone density\n",
      "- Brittle bones\n",
      "- Low bone mass\n",
      "- Thinning of the bones\n",
      "- Weakening of the bones\n",
      "- Porous bones\n",
      "- Fragile bones\n",
      "- Reduced bone strength\n",
      "- Loss of bone density\n",
      "- Degenerative bone disease\n",
      "- Bone demineralization\n",
      "\n",
      "3. No Osteoporosis:\n",
      "- Normal bone density\n",
      "- Healthy bones\n",
      "- Adequate bone mass\n",
      "- Strong bones\n",
      "- Normal bone strength\n",
      "- No signs of osteoporosis\n",
      "- Absence of osteoporosis\n",
      "- No evidence of bone thinning\n",
      "- No indication of bone weakening\n",
      "- No osteoporotic changes\n",
      "- No degenerative bone disease\n",
      "\n",
      "Please note that these are just a few examples, and there can be many more variations and synonyms for each term.\n",
      "2023-07-12 19:28:20,563 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 00:46\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,563 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 00:46\"}] for sentence \"comparison to 00:46\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 00:46\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,564 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 11:51\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,564 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 11:51\"}] for sentence \"comparison to 11:51\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 11:51\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,564 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison with :44\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,564 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 30, \"total_tokens\": 313}}, {\"observation\": \"comparison with :44\"}] for sentence \"comparison with :44\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison with :44\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,564 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"in similar position.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,564 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 29, \"total_tokens\": 311}}, {\"observation\": \"in similar position\"}] for sentence \"in similar position\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"in similar position.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:28:20,570 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS..., \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 66, \"total_tokens\": 349}}, {\"observation\": \"document re-expansion\"}] for sentence \"document re-expansion\": Could not parse output: The task is to generate a diverse set of paraphrases for a given sentence describing a fact from a chest X-ray report. The paraphrases should cover a wide range of terminology, synonyms, and abbreviations commonly used by radiologists to express the same idea. The output should be in the form of a JSON array of strings.\n",
      "2023-07-12 19:28:20,571 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm unable to assist with your request as the input sentence is missing. Could you please provide the sentence describing the fact from a chest X-ray report?\n",
      "2023-07-12 19:28:20,571 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS..., \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 36, \"total_tokens\": 319}}, {\"observation\": \"line has been removed\"}] for sentence \"line has been removed\": Could not parse output: I'm sorry, but I'm unable to assist with your request as the input sentence is missing. Could you please provide the sentence describing the fact from a chest X-ray report?\n",
      "2023-07-12 19:28:20,573 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 280, \"total_tokens\": 562}}, {\"observation\": \"changes more extensive\"}] for sentence \"changes more extensive\": Could not parse output: I apologize for the confusion. Here are some additional paraphrases for the given examples:\n",
      "\n",
      "For \"benign calcification\":\n",
      "- \"non-malignant calcified area\"\n",
      "- \"harmless calcified spot\"\n",
      "- \"innocuous calcified region\"\n",
      "- \"benign calcified deposit\"\n",
      "- \"non-cancerous calcified mass\"\n",
      "- \"not indicative of malignancy calcification\"\n",
      "- \"safe calcified lesion\"\n",
      "- \"non-threatening calcified area\"\n",
      "- \"non-metastatic calcified spot\"\n",
      "\n",
      "For \"osteoporosis\":\n",
      "- \"decreased bone density condition\"\n",
      "- \"brittle bones disease\"\n",
      "- \"low bone mass syndrome\"\n",
      "- \"thinning of the bones disorder\"\n",
      "- \"weakening of the bones condition\"\n",
      "- \"porous bones condition\"\n",
      "- \"fragile bones syndrome\"\n",
      "- \"reduced bone strength disorder\"\n",
      "- \"loss of bone density condition\"\n",
      "- \"degenerative bone disease characterized by decreased bone density\"\n",
      "\n",
      "For \"no osteoporosis\":\n",
      "- \"normal bone density condition\"\n",
      "- \"healthy bones without osteoporosis\"\n",
      "- \"adequate bone mass without signs of osteoporosis\"\n",
      "- \"strong bones with no indication of osteoporosis\"\n",
      "- \"normal bone strength without osteoporotic changes\"\n",
      "- \"no evidence of bone thinning or weakening\"\n",
      "- \"absence of degenerative bone disease like osteoporosis\"\n",
      "2023-07-12 19:28:20,584 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to provide a response. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 19:28:20,585 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 36, \"total_tokens\": 320}}, {\"observation\": \"radiographs under clip #\"}] for sentence \"radiographs under clip #\": Could not parse output: I'm sorry, but I need more information to provide a response. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 19:28:20,598 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...sh_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 245, \"total_tokens\": 527}}, {\"observation\": \"better inspirational effort\"}] for sentence \"better inspirational effort\": Could not parse output: I apologize for the previous response. Here are some alternative paraphrases for the given examples:\n",
      "\n",
      "1. Benign calcification:\n",
      "- Non-cancerous calcification\n",
      "- Harmless calcification\n",
      "- Innocuous calcification\n",
      "- Benign calcified lesion\n",
      "- Non-malignant calcification\n",
      "- Non-threatening calcification\n",
      "- Not indicative of cancer calcification\n",
      "- Safe calcification\n",
      "- Non-dangerous calcification\n",
      "- Non-metastatic calcification\n",
      "\n",
      "2. Osteoporosis:\n",
      "- Decreased bone density\n",
      "- Brittle bones\n",
      "- Low bone mass\n",
      "- Thinning of the bones\n",
      "- Weakening of the bones\n",
      "- Porous bones\n",
      "- Fragile bones\n",
      "- Reduced bone strength\n",
      "- Loss of bone density\n",
      "- Degenerative bone disease\n",
      "\n",
      "3. No osteoporosis:\n",
      "- Normal bone density\n",
      "- Healthy bones\n",
      "- Adequate bone mass\n",
      "- Strong bones\n",
      "- Normal bone strength\n",
      "- No signs of osteoporosis\n",
      "- Absence of osteoporosis\n",
      "- No evidence of bone thinning\n",
      "- No indication of bone weakening\n",
      "- No osteoporotic changes\n",
      "- No degenerative bone disease\n",
      "\n",
      "I hope these paraphrases better meet your expectations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:28:20,609 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I can't assist with that request.\n",
      "2023-07-12 19:28:20,610 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...sh_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 13, \"total_tokens\": 298}}, {\"observation\": \"selected images of a neck CT\"}] for sentence \"selected images of a neck CT\": Could not parse output: I'm sorry, but I can't assist with that request.\n",
      "2023-07-12 19:28:20,620 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information in order to provide a complete assessment of the findings. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 19:28:20,620 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS..._reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 42, \"total_tokens\": 325}}, {\"observation\": \"complete assessment of finding\"}] for sentence \"complete assessment of finding\": Could not parse output: I'm sorry, but I need more information in order to provide a complete assessment of the findings. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 19:28:20,640 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to provide a response. Could you please provide the specific finding from the chest X-ray report?\n",
      "2023-07-12 19:28:20,640 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...eason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 28, \"total_tokens\": 313}}, {\"observation\": \"finding #2 reported by telephone\"}] for sentence \"finding #2 reported by telephone\": Could not parse output: I'm sorry, but I need more information to provide a response. Could you please provide the specific finding from the chest X-ray report?\n",
      "2023-07-12 19:28:20,658 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I can only generate paraphrases for chest X-ray reports. If you have a chest X-ray report, please provide the sentence describing a fact from it and I'll be happy to help you paraphrase it.\n",
      "2023-07-12 19:28:20,658 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...son\": \"stop\"}], \"usage\": {\"prompt_tokens\": 287, \"completion_tokens\": 47, \"total_tokens\": 334}}, {\"observation\": \"chest CT report from 1 day earlier\"}] for sentence \"chest CT report from 1 day earlier\": Could not parse output: I'm sorry, but I can only generate paraphrases for chest X-ray reports. If you have a chest X-ray report, please provide the sentence describing a fact from it and I'll be happy to help you paraphrase it.\n",
      "2023-07-12 19:28:20,675 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...on\": \"stop\"}], \"usage\": {\"prompt_tokens\": 287, \"completion_tokens\": 86, \"total_tokens\": 373}}, {\"observation\": \"pulmonary edema developed between [\"}] for sentence \"pulmonary edema developed between [\": Could not parse output: \"fluid accumulation in the lungs\",\n",
      "\"fluid in the lungs\",\n",
      "\"excess fluid in the pulmonary system\",\n",
      "\"pulmonary congestion\",\n",
      "\"fluid overload in the lungs\",\n",
      "\"pulmonary fluid buildup\",\n",
      "\"accumulation of fluid in the pulmonary parenchyma\",\n",
      "\"pulmonary interstitial edema\",\n",
      "\"fluid infiltration in the lungs\",\n",
      "\"pulmonary edematous changes\",\n",
      "\"fluid-filled lungs\"\n",
      "]\n",
      "2023-07-12 19:28:20,703 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS... \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 240, \"total_tokens\": 525}}, {\"observation\": \"emphasis on inspiration may be helpful\"}] for sentence \"emphasis on inspiration may be helpful\": Could not parse output: benign calcification\n",
      "[\n",
      "\"non-cancerous calcification\",\n",
      "\"harmless calcification\",\n",
      "\"innocuous calcification\",\n",
      "\"benign calcified lesion\",\n",
      "\"non-malignant calcification\",\n",
      "\"non-threatening calcification\",\n",
      "\"not indicative of cancer calcification\",\n",
      "\"safe calcification\",\n",
      "\"non-dangerous calcification\",\n",
      "\"non-metastatic calcification\",\n",
      "\"calcification without malignancy\"\n",
      "]\n",
      "\n",
      "osteoporosis\n",
      "[\n",
      "\"decreased bone density\",\n",
      "\"brittle bones\",\n",
      "\"low bone mass\",\n",
      "\"thinning of the bones\",\n",
      "\"weakening of the bones\",\n",
      "\"porous bones\",\n",
      "\"fragile bones\",\n",
      "\"reduced bone strength\",\n",
      "\"loss of bone density\",\n",
      "\"degenerative bone disease\",\n",
      "\"demineralization of bones\"\n",
      "]\n",
      "\n",
      "no osteoporosis\n",
      "[\n",
      "\"normal bone density\",\n",
      "\"healthy bones\",\n",
      "\"adequate bone mass\",\n",
      "\"strong bones\",\n",
      "\"normal bone strength\",\n",
      "\"no signs of osteoporosis\",\n",
      "\"absence of osteoporosis\",\n",
      "\"no evidence of bone thinning\",\n",
      "\"no indication of bone weakening\",\n",
      "\"no osteoporotic changes\",\n",
      "\"no degenerative bone disease\",\n",
      "\"intact bone density\"\n",
      "]\n",
      "2023-07-12 19:28:20,751 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I cannot access or compare specific chest radiographs as I am a language model AI and do not have access to external data. However, I can help you with any questions or provide information related to chest X-ray reports.\n",
      "2023-07-12 19:28:20,751 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...top\"}], \"usage\": {\"prompt_tokens\": 289, \"completion_tokens\": 49, \"total_tokens\": 338}}, {\"observation\": \"comparison with chest radiographs at 16:37\"}] for sentence \"comparison with chest radiographs at 16:37\": Could not parse output: I'm sorry, but I cannot access or compare specific chest radiographs as I am a language model AI and do not have access to external data. However, I can help you with any questions or provide information related to chest X-ray reports.\n",
      "2023-07-12 19:28:20,752 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...top\"}], \"usage\": {\"prompt_tokens\": 290, \"completion_tokens\": 18, \"total_tokens\": 308}}, {\"observation\": \"findings reported by telephone at 11:57 AM\"}] for sentence \"findings reported by telephone at 11:57 AM\": Could not parse output: {\n",
      "  \"findings\": \"reported by telephone at 11:57 AM\"\n",
      "}\n",
      "2023-07-12 19:28:20,779 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to generate the paraphrases. Could you please provide the sentence describing the fact from the chest X-ray report?\n",
      "2023-07-12 19:28:20,779 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...p\"}], \"usage\": {\"prompt_tokens\": 287, \"completion_tokens\": 31, \"total_tokens\": 318}}, {\"observation\": \"dictated report assigned to this clip number\"}] for sentence \"dictated report assigned to this clip number\": Could not parse output: I'm sorry, but I need more information to generate the paraphrases. Could you please provide the sentence describing the fact from the chest X-ray report?\n",
      "2023-07-12 19:28:20,812 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 300, \"total_tokens\": 585}}, {\"observation\": \"recommend repeat with more optimized technique\"}] for sentence \"recommend repeat with more optimized technique\": Could not parse output: Sure! Here's an optimized technique to generate a diverse range of paraphrases for the given fact from a chest X-ray report:\n",
      "\n",
      "1. Create a dictionary of synonyms, abbreviations, and alternative terms for the key medical terms used in the fact. For example, for \"benign calcification,\" you can include synonyms like \"non-cancerous calcification,\" \"harmless calcification,\" and abbreviations like \"benign calcified lesion.\"\n",
      "\n",
      "2. Iterate through each term in the dictionary and generate paraphrases by combining it with other terms in the dictionary. This will help create variations in the phrasing and terminology used. For example, combining \"non-cancerous\" with \"calcification\" can give you \"non-cancerous calcified lesion.\"\n",
      "\n",
      "3. Add additional variations by using adjectives or phrases that convey similar meanings. For example, for \"osteoporosis,\" you can include phrases like \"decreased bone density,\" \"brittle bones,\" and \"thinning of the bones.\"\n",
      "\n",
      "4. Include negations or opposite terms to cover both positive and negative statements. For example, for \"no osteoporosis,\" you can include phrases like \"normal bone density,\" \"healthy bones,\" and \"no signs of osteoporosis.\"\n",
      "\n",
      "5. Combine all the generated paraphrases into a JSON array and return it as the output.\n",
      "\n",
      "Here's an example implementation in Python:\n",
      "\n",
      "```python\n",
      "import itertools\n",
      "import json\n",
      "\n",
      "def generate_paraphrases(fact):\n",
      "    paraphrases\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:28:21,438 - \u001b[1;32mINFO\u001b[1;0m - Deleting API requests and responses\r\n",
      "2023-07-12 19:28:21,445 - \u001b[1;32mINFO\u001b[1;0m - Succesfully processed 14972 of 15000 API responses.\r\n",
      "                    28 of 15000 API responses could not be processed.\r\n",
      "                    Saving paraphrased observations to /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "    --integrated_fact_metadata_filepath \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "    --min_num_words_per_sentence 2 \\\n",
    "    --offset 0 \\\n",
    "    --num_sentences 15000 \\\n",
    "    --sample_equally_spaced_sentences \\\n",
    "    --process_kth_of_every_n_sentences 2 4 \\\n",
    "    --max_requests_per_minute 3000 \\\n",
    "    --max_tokens_per_minute 85000 \\\n",
    "    --max_tokens_per_request 300 \\\n",
    "    --logging_level \"INFO\" \\\n",
    "    --api_key_name \"OPENAI_API_KEY_2\" \\\n",
    "    --openai_model_name \"gpt-3.5-turbo-0613\" \\\n",
    "    --alias \"__two-or-more-words__part3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90076bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.files import load_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df823e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = load_jsonl('/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14da5913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14972"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5d8b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'observation': 'new opacity in the right medial lower lobe from the prior two radiographs'},\n",
       " 'parsed_response': ['new density in the right medial lower lobe from the previous two radiographs',\n",
       "  'recent opacity in the right medial lower lobe compared to the prior two radiographs',\n",
       "  'fresh infiltrate in the right medial lower lobe from the previous two radiographs',\n",
       "  'recent shadow in the right medial lower lobe compared to the prior two radiographs',\n",
       "  'new opacification in the right medial lower lobe from the prior two radiographs',\n",
       "  'recent haziness in the right medial lower lobe compared to the prior two radiographs',\n",
       "  'recent consolidation in the right medial lower lobe from the previous two radiographs',\n",
       "  'recent increased density in the right medial lower lobe compared to the prior two radiographs',\n",
       "  'recent ground-glass opacity in the right medial lower lobe from the prior two radiographs',\n",
       "  'recent alveolar infiltrate in the right medial lower lobe compared to the prior two radiographs']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
