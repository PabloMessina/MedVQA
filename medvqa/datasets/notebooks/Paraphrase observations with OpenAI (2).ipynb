{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab94131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:40:33,328 - \u001b[1;32mINFO\u001b[1;0m - Loading facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "100%|███████████████████████████████| 578733/578733 [00:03<00:00, 182316.53it/s]\n",
      "2023-07-12 17:40:41,737 - \u001b[1;32mINFO\u001b[1;0m - Found 771124 unique observations\n",
      "2023-07-12 17:40:44,098 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to those with at least 2 words\n",
      "2023-07-12 17:40:44,830 - \u001b[1;32mINFO\u001b[1;0m - Found 769122 observations with at least 2 words\n",
      "2023-07-12 17:40:44,830 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to the 1-th of every 4 sentences\n",
      "2023-07-12 17:40:45,057 - \u001b[1;32mINFO\u001b[1;0m - Found 192281 observations that are the 1-th of every 4 sentences\n",
      "2023-07-12 17:40:45,057 - \u001b[1;32mINFO\u001b[1;0m - Sampling 15000 equally spaced sentences\n",
      "2023-07-12 17:40:45,066 - \u001b[1;32mINFO\u001b[1;0m - Total number of sentences to paraphrase: 15000\n",
      "2023-07-12 17:40:45,066 - \u001b[1;32mINFO\u001b[1;0m - Example sentences to paraphrase:\n",
      "2023-07-12 17:40:45,066 - \u001b[1;32mINFO\u001b[1;0m - 1. 8 VR\n",
      "2023-07-12 17:40:45,066 - \u001b[1;32mINFO\u001b[1;0m - 1667. likely a medullary infarct\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 3334. chronic and healed rib fractures\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 5000. patient received an nasogastric tube\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 6667. resolved bibasilar pulmonary abnormality\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 8333. small lucency in the median sternotomy wires\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 10000. increased opacity projecting over the upper lungs\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 11666. similar appearance of hilar contours compared to prior\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 13333. contrast agent not imaged on a study of the abdomen at midnight\n",
      "2023-07-12 17:40:45,067 - \u001b[1;32mINFO\u001b[1;0m - 15000. worsening of combined alveolar opacities in the bilateral asymmetrically distributed combined interstitial opacities in the bilateral asymmetrically distributed combined alveolar opacities in the bilateral asymmetrically distributed combined interstitial opacities\n",
      "2023-07-12 17:40:45,118 - \u001b[1;32mINFO\u001b[1;0m - Saving API requests to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_requests_20230712_174045.jsonl\n",
      "2023-07-12 17:40:45,118 - \u001b[1;32mINFO\u001b[1;0m - Saving API responses to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174045.jsonl\n",
      "2023-07-12 17:40:46,122 - \u001b[1;32mINFO\u001b[1;0m - Starting request #0\n",
      "2023-07-12 17:40:46,264 - \u001b[1;32mINFO\u001b[1;0m - Starting request #50\n",
      "2023-07-12 17:40:46,440 - \u001b[1;32mINFO\u001b[1;0m - Starting request #100\n",
      "2023-07-12 17:40:46,609 - \u001b[1;32mINFO\u001b[1;0m - Starting request #150\n",
      "2023-07-12 17:40:46,735 - \u001b[1;32mINFO\u001b[1;0m - Starting request #200\n",
      "2023-07-12 17:40:46,891 - \u001b[1;32mINFO\u001b[1;0m - Starting request #250\n",
      "2023-07-12 17:40:48,095 - \u001b[1;32mINFO\u001b[1;0m - Starting request #300\n",
      "2023-07-12 17:40:58,401 - \u001b[1;32mINFO\u001b[1;0m - Starting request #350\n",
      "2023-07-12 17:41:08,703 - \u001b[1;32mINFO\u001b[1;0m - Starting request #400\n",
      "2023-07-12 17:41:19,009 - \u001b[1;32mINFO\u001b[1;0m - Starting request #450\n",
      "2023-07-12 17:41:29,321 - \u001b[1;32mINFO\u001b[1;0m - Starting request #500\n",
      "2023-07-12 17:41:39,627 - \u001b[1;32mINFO\u001b[1;0m - Starting request #550\n",
      "2023-07-12 17:41:49,937 - \u001b[1;32mINFO\u001b[1;0m - Starting request #600\n",
      "2023-07-12 17:42:00,248 - \u001b[1;32mINFO\u001b[1;0m - Starting request #650\n",
      "2023-07-12 17:42:10,565 - \u001b[1;32mINFO\u001b[1;0m - Starting request #700\n",
      "2023-07-12 17:42:20,885 - \u001b[1;32mINFO\u001b[1;0m - Starting request #750\n",
      "2023-07-12 17:42:31,198 - \u001b[1;32mINFO\u001b[1;0m - Starting request #800\n",
      "2023-07-12 17:42:41,516 - \u001b[1;32mINFO\u001b[1;0m - Starting request #850\n",
      "2023-07-12 17:42:51,835 - \u001b[1;32mINFO\u001b[1;0m - Starting request #900\n",
      "2023-07-12 17:43:02,150 - \u001b[1;32mINFO\u001b[1;0m - Starting request #950\n",
      "2023-07-12 17:43:12,477 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1000\n",
      "2023-07-12 17:43:22,801 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1050\n",
      "2023-07-12 17:43:33,121 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1100\n",
      "2023-07-12 17:43:43,446 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1150\n",
      "2023-07-12 17:43:53,778 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1200\n",
      "2023-07-12 17:44:04,107 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1250\n",
      "2023-07-12 17:44:14,428 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1300\n",
      "2023-07-12 17:44:24,751 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1350\n",
      "2023-07-12 17:44:35,083 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1400\n",
      "2023-07-12 17:44:45,415 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1450\n",
      "2023-07-12 17:44:55,745 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1500\n",
      "2023-07-12 17:45:06,069 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1550\n",
      "2023-07-12 17:45:16,409 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1600\n",
      "2023-07-12 17:45:26,737 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1650\n",
      "2023-07-12 17:45:37,069 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1700\n",
      "2023-07-12 17:45:47,395 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1750\n",
      "2023-07-12 17:45:57,726 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1800\n",
      "2023-07-12 17:46:08,066 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1850\n",
      "2023-07-12 17:46:18,396 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1900\n",
      "2023-07-12 17:46:28,732 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1950\n",
      "2023-07-12 17:46:39,066 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2000\n",
      "2023-07-12 17:46:49,404 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2050\n",
      "2023-07-12 17:46:59,749 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2100\n",
      "2023-07-12 17:47:10,090 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2150\n",
      "2023-07-12 17:47:20,427 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2200\n",
      "2023-07-12 17:47:30,765 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2250\n",
      "2023-07-12 17:47:41,099 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2300\n",
      "2023-07-12 17:47:51,450 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2350\n",
      "2023-07-12 17:48:01,796 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2400\n",
      "2023-07-12 17:48:12,141 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2450\n",
      "2023-07-12 17:48:22,487 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2500\n",
      "2023-07-12 17:48:32,827 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2550\n",
      "2023-07-12 17:48:43,170 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2600\n",
      "2023-07-12 17:48:53,528 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2650\n",
      "2023-07-12 17:49:03,872 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2700\n",
      "2023-07-12 17:49:14,227 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2750\n",
      "2023-07-12 17:49:24,576 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2800\n",
      "2023-07-12 17:49:34,925 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2850\n",
      "2023-07-12 17:49:45,275 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2900\n",
      "2023-07-12 17:49:55,624 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2950\n",
      "2023-07-12 17:50:05,986 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3000\n",
      "2023-07-12 17:50:16,338 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3050\n",
      "2023-07-12 17:50:26,692 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3100\n",
      "2023-07-12 17:50:37,046 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3150\n",
      "2023-07-12 17:50:47,394 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3200\n",
      "2023-07-12 17:50:57,747 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3250\n",
      "2023-07-12 17:51:08,108 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3300\n",
      "2023-07-12 17:51:18,464 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3350\n",
      "2023-07-12 17:51:28,817 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3400\n",
      "2023-07-12 17:51:39,174 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3450\n",
      "2023-07-12 17:51:49,529 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3500\n",
      "2023-07-12 17:51:59,879 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3550\n",
      "2023-07-12 17:52:10,234 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3600\n",
      "2023-07-12 17:52:20,597 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3650\n",
      "2023-07-12 17:52:30,951 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3700\n",
      "2023-07-12 17:52:41,306 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3750\n",
      "2023-07-12 17:52:51,661 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3800\n",
      "2023-07-12 17:53:02,018 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3850\n",
      "2023-07-12 17:53:12,366 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3900\n",
      "2023-07-12 17:53:22,721 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3950\n",
      "2023-07-12 17:53:33,092 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4000\n",
      "2023-07-12 17:53:43,454 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4050\n",
      "2023-07-12 17:53:53,818 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4100\n",
      "2023-07-12 17:54:04,178 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:54:14,539 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4200\n",
      "2023-07-12 17:54:24,898 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4250\n",
      "2023-07-12 17:54:35,259 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4300\n",
      "2023-07-12 17:54:45,621 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4350\n",
      "2023-07-12 17:54:55,993 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4400\n",
      "2023-07-12 17:55:06,349 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4450\n",
      "2023-07-12 17:55:16,711 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4500\n",
      "2023-07-12 17:55:27,076 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4550\n",
      "2023-07-12 17:55:37,432 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4600\n",
      "2023-07-12 17:55:47,791 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4650\n",
      "2023-07-12 17:55:58,153 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4700\n",
      "2023-07-12 17:56:08,526 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4750\n",
      "2023-07-12 17:56:18,895 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4800\n",
      "2023-07-12 17:56:29,264 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4850\n",
      "2023-07-12 17:56:39,634 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4900\n",
      "2023-07-12 17:56:50,005 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4950\n",
      "2023-07-12 17:57:00,366 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5000\n",
      "2023-07-12 17:57:10,730 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5050\n",
      "2023-07-12 17:57:21,096 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5100\n",
      "2023-07-12 17:57:31,469 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5150\n",
      "2023-07-12 17:57:41,841 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5200\n",
      "2023-07-12 17:57:52,214 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5250\n",
      "2023-07-12 17:58:02,581 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5300\n",
      "2023-07-12 17:58:12,956 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5350\n",
      "2023-07-12 17:58:23,315 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5400\n",
      "2023-07-12 17:58:33,682 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5450\n",
      "2023-07-12 17:58:44,050 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5500\n",
      "2023-07-12 17:58:54,437 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5550\n",
      "2023-07-12 17:59:04,812 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5600\n",
      "2023-07-12 17:59:15,194 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5650\n",
      "2023-07-12 17:59:25,572 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5700\n",
      "2023-07-12 17:59:35,945 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5750\n",
      "2023-07-12 17:59:46,311 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5800\n",
      "2023-07-12 17:59:56,682 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5850\n",
      "2023-07-12 18:00:07,054 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5900\n",
      "2023-07-12 18:00:17,436 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5950\n",
      "2023-07-12 18:00:27,813 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6000\n",
      "2023-07-12 18:00:38,189 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6050\n",
      "2023-07-12 18:00:48,568 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6100\n",
      "2023-07-12 18:00:58,955 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6150\n",
      "2023-07-12 18:01:09,326 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6200\n",
      "2023-07-12 18:01:19,701 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6250\n",
      "2023-07-12 18:01:30,072 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6300\n",
      "2023-07-12 18:01:40,450 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6350\n",
      "2023-07-12 18:01:50,833 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6400\n",
      "2023-07-12 18:02:01,219 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6450\n",
      "2023-07-12 18:02:11,598 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6500\n",
      "2023-07-12 18:02:21,979 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6550\n",
      "2023-07-12 18:02:32,359 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6600\n",
      "2023-07-12 18:02:42,730 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6650\n",
      "2023-07-12 18:02:53,109 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6700\n",
      "2023-07-12 18:03:03,494 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6750\n",
      "2023-07-12 18:03:13,885 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6800\n",
      "2023-07-12 18:03:24,269 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6850\n",
      "2023-07-12 18:03:34,665 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6900\n",
      "2023-07-12 18:03:45,043 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6950\n",
      "2023-07-12 18:03:55,425 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7000\n",
      "2023-07-12 18:04:05,808 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7050\n",
      "2023-07-12 18:04:16,189 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7100\n",
      "2023-07-12 18:04:26,583 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7150\n",
      "2023-07-12 18:04:36,973 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7200\n",
      "2023-07-12 18:04:47,362 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7250\n",
      "2023-07-12 18:04:57,754 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7300\n",
      "2023-07-12 18:05:08,139 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7350\n",
      "2023-07-12 18:05:18,522 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7400\n",
      "2023-07-12 18:05:28,902 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7450\n",
      "2023-07-12 18:05:39,285 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7500\n",
      "2023-07-12 18:05:49,674 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7550\n",
      "2023-07-12 18:06:00,077 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7600\n",
      "2023-07-12 18:06:10,474 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7650\n",
      "2023-07-12 18:06:20,862 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7700\n",
      "2023-07-12 18:06:31,256 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7750\n",
      "2023-07-12 18:06:41,656 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7800\n",
      "2023-07-12 18:06:52,039 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7850\n",
      "2023-07-12 18:07:02,428 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7900\n",
      "2023-07-12 18:07:12,826 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7950\n",
      "2023-07-12 18:07:23,229 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8000\n",
      "2023-07-12 18:07:33,623 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8050\n",
      "2023-07-12 18:07:44,019 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8100\n",
      "2023-07-12 18:07:54,412 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8150\n",
      "2023-07-12 18:08:04,809 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8200\n",
      "2023-07-12 18:08:15,199 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8250\n",
      "2023-07-12 18:08:25,579 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8300\n",
      "2023-07-12 18:08:35,970 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8350\n",
      "2023-07-12 18:08:46,372 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8400\n",
      "2023-07-12 18:08:56,769 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8450\n",
      "2023-07-12 18:09:07,175 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8500\n",
      "2023-07-12 18:09:17,572 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8550\n",
      "2023-07-12 18:09:27,975 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8600\n",
      "2023-07-12 18:09:38,365 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8650\n",
      "2023-07-12 18:09:48,761 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8700\n",
      "2023-07-12 18:09:56,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7290 failed with Exception \n",
      "2023-07-12 18:09:59,155 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8749\n",
      "2023-07-12 18:10:04,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7328 failed with Exception \n",
      "2023-07-12 18:10:09,563 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8798\n",
      "2023-07-12 18:10:18,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7396 failed with Exception \n",
      "2023-07-12 18:10:19,962 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8847\n",
      "2023-07-12 18:10:30,370 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8897\n",
      "2023-07-12 18:10:39,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 7500 failed with Exception \n",
      "2023-07-12 18:10:40,768 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8946\n",
      "2023-07-12 18:10:51,169 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8996\n",
      "2023-07-12 18:11:01,563 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9046\n",
      "2023-07-12 18:11:11,963 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9096\n",
      "2023-07-12 18:11:22,363 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9146\n",
      "2023-07-12 18:11:30,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 7743 failed with Exception \n",
      "2023-07-12 18:11:32,776 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9195\n",
      "2023-07-12 18:11:43,184 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9245\n",
      "2023-07-12 18:11:44,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7810 failed with Exception \n",
      "2023-07-12 18:11:53,586 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9294\n",
      "2023-07-12 18:12:03,990 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9344\n",
      "2023-07-12 18:12:14,392 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9394\n",
      "2023-07-12 18:12:22,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 7992 failed with Exception \n",
      "2023-07-12 18:12:24,792 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9443\n",
      "2023-07-12 18:12:35,205 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9493\n",
      "2023-07-12 18:12:45,622 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9543\n",
      "2023-07-12 18:12:49,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8126 failed with Exception \n",
      "2023-07-12 18:12:56,032 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:13:01,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8183 failed with Exception \n",
      "2023-07-12 18:13:06,441 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9641\n",
      "2023-07-12 18:13:16,849 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9691\n",
      "2023-07-12 18:13:17,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8259 failed with Exception \n",
      "2023-07-12 18:13:27,252 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9740\n",
      "2023-07-12 18:13:37,651 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9790\n",
      "2023-07-12 18:13:48,061 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9840\n",
      "2023-07-12 18:13:58,473 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9890\n",
      "2023-07-12 18:14:08,886 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9940\n",
      "2023-07-12 18:14:19,300 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9990\n",
      "2023-07-12 18:14:21,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8565 failed with Exception \n",
      "2023-07-12 18:14:27,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8594 failed with Exception \n",
      "2023-07-12 18:14:29,706 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10038\n",
      "2023-07-12 18:14:32,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8617 failed with Exception \n",
      "2023-07-12 18:14:37,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8644 failed with Exception \n",
      "2023-07-12 18:14:40,123 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10086\n",
      "2023-07-12 18:14:50,530 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10136\n",
      "2023-07-12 18:15:00,939 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10186\n",
      "2023-07-12 18:15:11,364 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10236\n",
      "2023-07-12 18:15:21,774 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10286\n",
      "2023-07-12 18:15:32,195 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10336\n",
      "2023-07-12 18:15:41,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8946 failed with Exception \n",
      "2023-07-12 18:15:42,614 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10385\n",
      "2023-07-12 18:15:50,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8990 failed with Exception \n",
      "2023-07-12 18:15:53,030 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10434\n",
      "2023-07-12 18:16:03,443 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10484\n",
      "2023-07-12 18:16:13,861 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10534\n",
      "2023-07-12 18:16:24,288 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10584\n",
      "2023-07-12 18:16:24,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9152 failed with Exception \n",
      "2023-07-12 18:16:34,708 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10633\n",
      "2023-07-12 18:16:45,122 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10683\n",
      "2023-07-12 18:16:55,540 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10733\n",
      "2023-07-12 18:17:05,955 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10783\n",
      "2023-07-12 18:17:16,367 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10833\n",
      "2023-07-12 18:17:26,799 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10883\n",
      "2023-07-12 18:17:36,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9497 failed with Exception \n",
      "2023-07-12 18:17:37,223 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10932\n",
      "2023-07-12 18:17:42,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9525 failed with Exception \n",
      "2023-07-12 18:17:46,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9544 failed with Exception \n",
      "2023-07-12 18:17:47,647 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10980\n",
      "2023-07-12 18:17:53,495 - \u001b[1;33mWARNING\u001b[1;0m - Request 9576 failed with Exception \n",
      "2023-07-12 18:17:58,066 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11029\n",
      "2023-07-12 18:18:08,469 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11079\n",
      "2023-07-12 18:18:18,887 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11129\n",
      "2023-07-12 18:18:27,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9741 failed with Exception \n",
      "2023-07-12 18:18:28,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9743 failed with Exception \n",
      "2023-07-12 18:18:29,317 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11177\n",
      "2023-07-12 18:18:39,741 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11227\n",
      "2023-07-12 18:18:50,163 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11277\n",
      "2023-07-12 18:19:00,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9899 failed with Exception \n",
      "2023-07-12 18:19:00,582 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11327\n",
      "2023-07-12 18:19:11,001 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11376\n",
      "2023-07-12 18:19:21,437 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11426\n",
      "2023-07-12 18:19:31,875 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11476\n",
      "2023-07-12 18:19:40,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10083 failed with Exception \n",
      "2023-07-12 18:19:42,305 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11525\n",
      "2023-07-12 18:19:45,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10111 failed with Exception \n",
      "2023-07-12 18:19:52,730 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11574\n",
      "2023-07-12 18:19:56,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10163 failed with Exception \n",
      "2023-07-12 18:20:03,152 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11623\n",
      "2023-07-12 18:20:13,574 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11673\n",
      "2023-07-12 18:20:24,005 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11723\n",
      "2023-07-12 18:20:31,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10331 failed with Exception \n",
      "2023-07-12 18:20:34,437 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11772\n",
      "2023-07-12 18:20:44,864 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11822\n",
      "2023-07-12 18:20:54,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10438 failed with Exception \n",
      "2023-07-12 18:20:55,298 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11871\n",
      "2023-07-12 18:21:01,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10471 failed with Exception \n",
      "2023-07-12 18:21:05,729 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11920\n",
      "2023-07-12 18:21:11,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10519 failed with Exception \n",
      "2023-07-12 18:21:16,170 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11969\n",
      "2023-07-12 18:21:17,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10550 failed with Exception \n",
      "2023-07-12 18:21:26,606 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12018\n",
      "2023-07-12 18:21:28,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10601 failed with Exception \n",
      "2023-07-12 18:21:37,034 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12067\n",
      "2023-07-12 18:21:47,469 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12117\n",
      "2023-07-12 18:21:47,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10693 failed with Exception \n",
      "2023-07-12 18:21:52,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10714 failed with Exception \n",
      "2023-07-12 18:21:57,901 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12165\n",
      "2023-07-12 18:22:08,347 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12215\n",
      "2023-07-12 18:22:18,790 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12265\n",
      "2023-07-12 18:22:23,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10865 failed with Exception \n",
      "2023-07-12 18:22:29,234 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12314\n",
      "2023-07-12 18:22:39,660 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12364\n",
      "2023-07-12 18:22:48,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10982 failed with Exception \n",
      "2023-07-12 18:22:48,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10983 failed with Exception \n",
      "2023-07-12 18:22:50,108 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12412\n",
      "2023-07-12 18:22:52,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11002 failed with Exception \n",
      "2023-07-12 18:23:00,545 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12461\n",
      "2023-07-12 18:23:10,977 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12511\n",
      "2023-07-12 18:23:21,413 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12561\n",
      "2023-07-12 18:23:31,855 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12611\n",
      "2023-07-12 18:23:42,303 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12661\n",
      "2023-07-12 18:23:52,753 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12711\n",
      "2023-07-12 18:24:03,193 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12761\n",
      "2023-07-12 18:24:07,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11355 failed with Exception \n",
      "2023-07-12 18:24:13,646 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12810\n",
      "2023-07-12 18:24:24,100 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12860\n",
      "2023-07-12 18:24:34,550 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12910\n",
      "2023-07-12 18:24:37,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 11499 failed with Exception \n",
      "2023-07-12 18:24:44,990 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12959\n",
      "2023-07-12 18:24:50,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11562 failed with Exception \n",
      "2023-07-12 18:24:51,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11568 failed with Exception \n",
      "2023-07-12 18:24:54,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11579 failed with Exception \n",
      "2023-07-12 18:24:55,445 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13006\n",
      "2023-07-12 18:25:03,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11624 failed with Exception \n",
      "2023-07-12 18:25:05,892 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13055\n",
      "2023-07-12 18:25:10,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11656 failed with Exception \n",
      "2023-07-12 18:25:16,347 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:25:26,803 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13154\n",
      "2023-07-12 18:25:37,261 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13204\n",
      "2023-07-12 18:25:47,710 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13254\n",
      "2023-07-12 18:25:58,173 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13304\n",
      "2023-07-12 18:25:58,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 11882 failed with Exception \n",
      "2023-07-12 18:26:08,631 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13353\n",
      "2023-07-12 18:26:19,089 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13403\n",
      "2023-07-12 18:26:29,559 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13453\n",
      "2023-07-12 18:26:30,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 12035 failed with Exception \n",
      "2023-07-12 18:26:37,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12069 failed with Exception \n",
      "2023-07-12 18:26:40,014 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13501\n",
      "2023-07-12 18:26:47,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12114 failed with Exception \n",
      "2023-07-12 18:26:50,473 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13550\n",
      "2023-07-12 18:26:58,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12165 failed with Exception \n",
      "2023-07-12 18:27:00,939 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13599\n",
      "2023-07-12 18:27:11,403 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13649\n",
      "2023-07-12 18:27:14,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12242 failed with Exception \n",
      "2023-07-12 18:27:21,864 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13698\n",
      "2023-07-12 18:27:32,340 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13748\n",
      "2023-07-12 18:27:42,802 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13798\n",
      "2023-07-12 18:27:53,277 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13848\n",
      "2023-07-12 18:28:03,741 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13898\n",
      "2023-07-12 18:28:14,222 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13948\n",
      "2023-07-12 18:28:24,699 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13998\n",
      "2023-07-12 18:28:35,187 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14048\n",
      "2023-07-12 18:28:45,661 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14098\n",
      "2023-07-12 18:28:56,136 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14148\n",
      "2023-07-12 18:29:06,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12775 failed with Exception \n",
      "2023-07-12 18:29:06,617 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14198\n",
      "2023-07-12 18:29:12,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12802 failed with Exception \n",
      "2023-07-12 18:29:17,098 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14246\n",
      "2023-07-12 18:29:17,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 12827 failed with Exception \n",
      "2023-07-12 18:29:18,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12833 failed with Exception \n",
      "2023-07-12 18:29:27,584 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14294\n",
      "2023-07-12 18:29:38,073 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14344\n",
      "2023-07-12 18:29:48,571 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14394\n",
      "2023-07-12 18:29:59,067 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14444\n",
      "2023-07-12 18:30:00,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13026 failed with Exception \n",
      "2023-07-12 18:30:09,562 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14493\n",
      "2023-07-12 18:30:20,081 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14543\n",
      "2023-07-12 18:30:30,590 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14593\n",
      "2023-07-12 18:30:41,094 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14643\n",
      "2023-07-12 18:30:51,609 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14693\n",
      "2023-07-12 18:31:02,136 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14743\n",
      "2023-07-12 18:31:04,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 13329 failed with Exception \n",
      "2023-07-12 18:31:12,664 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14792\n",
      "2023-07-12 18:31:14,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13377 failed with Exception \n",
      "2023-07-12 18:31:16,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 13388 failed with Exception \n",
      "2023-07-12 18:31:23,193 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14840\n",
      "2023-07-12 18:31:27,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13439 failed with Exception \n",
      "2023-07-12 18:31:33,735 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14889\n",
      "2023-07-12 18:31:44,311 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14939\n",
      "2023-07-12 18:31:52,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13555 failed with Exception \n",
      "2023-07-12 18:31:54,909 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14988\n",
      "2023-07-12 18:31:56,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 13578 failed with Exception \n",
      "2023-07-12 18:32:04,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13613 failed with Exception \n",
      "2023-07-12 18:33:14,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13946 failed with Exception \n",
      "2023-07-12 18:33:18,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 13964 failed with Exception \n",
      "2023-07-12 18:33:36,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 14053 failed with Exception \n",
      "2023-07-12 18:33:53,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 14135 failed with Exception \n",
      "2023-07-12 18:34:22,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 14267 failed with Exception \n",
      "2023-07-12 18:34:53,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 14415 failed with Exception \n",
      "2023-07-12 18:34:53,495 - \u001b[1;33mWARNING\u001b[1;0m - Request 14413 failed with Exception \n",
      "2023-07-12 18:34:55,847 - \u001b[1;32mINFO\u001b[1;0m - Parallel processing complete. Results saved to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174045.jsonl\n",
      "2023-07-12 18:34:55,850 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174045.jsonl\n",
      "2023-07-12 18:34:56,655 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I apologize, but I'm not sure what \"PK\" refers to in this context. Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,656 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...equest?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 29, \"total_tokens\": 310}}, {\"observation\": \"PK discussed\"}] for sentence \"PK discussed\": Could not parse output: I apologize, but I'm not sure what \"PK\" refers to in this context. Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,656 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"\\n}\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 18, \"total_tokens\": 300}}, {\"observation\": \"line introducer\"}] for sentence \"line introducer\": Could not parse output: {\n",
      "  \"line_introducer\": \"The radiologist's report states that\"\n",
      "}\n",
      "2023-07-12 18:34:56,657 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I apologize for the confusion. Could you please provide the new sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 18:34:56,657 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...rt?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 23, \"total_tokens\": 305}}, {\"observation\": \"change in report\"}] for sentence \"change in report\": Could not parse output: I apologize for the confusion. Could you please provide the new sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 18:34:56,657 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"ICD pack projects.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,657 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...t?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 30, \"total_tokens\": 313}}, {\"observation\": \"ICD pack projects\"}] for sentence \"ICD pack projects\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"ICD pack projects.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,663 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...n]\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 256, \"total_tokens\": 538}}, {\"observation\": \"up several lines\"}] for sentence \"up several lines\": Could not parse output: I apologize for the confusion. Here are a few more examples:\n",
      "\n",
      "pulmonary edema\n",
      "[\n",
      "\"fluid in the lungs\",\n",
      "\"congestion in the lungs\",\n",
      "\"accumulation of fluid in the pulmonary system\",\n",
      "\"swelling in the lungs\",\n",
      "\"excess fluid in the pulmonary region\",\n",
      "\"pulmonary fluid overload\",\n",
      "\"fluid-filled lungs\",\n",
      "\"pulmonary congestion\",\n",
      "\"fluid retention in the lungs\",\n",
      "\"pulmonary effusion\"\n",
      "]\n",
      "\n",
      "pleural effusion\n",
      "[\n",
      "\"fluid around the lungs\",\n",
      "\"accumulation of fluid in the pleural space\",\n",
      "\"excess fluid in the pleural cavity\",\n",
      "\"fluid collection in the pleural region\",\n",
      "\"pleural fluid buildup\",\n",
      "\"fluid-filled pleural space\",\n",
      "\"pleural fluid accumulation\",\n",
      "\"fluid retention around the lungs\",\n",
      "\"pleural fluid effusion\",\n",
      "\"pleural cavity effusion\"\n",
      "]\n",
      "\n",
      "pneumothorax\n",
      "[\n",
      "\"collapsed lung\",\n",
      "\"air in the pleural space\",\n",
      "\"lung deflation\",\n",
      "\"pulmonary air leakage\",\n",
      "\"collapsed pulmonary parenchyma\",\n",
      "\"pleural air accumulation\",\n",
      "\"pulmonary air pocket\",\n",
      "\"air-filled pleural cavity\",\n",
      "\"pulmonary collapse due to air leakage\",\n",
      "\"pneumothoracic condition\"\n",
      "]\n",
      "2023-07-12 18:34:56,664 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to generate the paraphrases. Could you please provide a specific fact from a chest X-ray report?\n",
      "2023-07-12 18:34:56,664 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 29, \"total_tokens\": 310}}, {\"observation\": \"clinical situation\"}] for sentence \"clinical situation\": Could not parse output: I'm sorry, but I need more information to generate the paraphrases. Could you please provide a specific fact from a chest X-ray report?\n",
      "2023-07-12 18:34:56,666 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 00:37\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,667 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 00:37\"}] for sentence \"comparison to 00:37\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 00:37\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,667 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 06:27\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,667 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 06:27\"}] for sentence \"comparison to 06:27\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 06:27\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,667 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 11:35\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,667 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 11:35\"}] for sentence \"comparison to 11:35\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 11:35\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,667 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to provide the desired output. Could you please provide the specific findings from the chest X-ray report?\n",
      "2023-07-12 18:34:56,667 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 29, \"total_tokens\": 311}}, {\"observation\": \"findings discovered\"}] for sentence \"findings discovered\": Could not parse output: I'm sorry, but I need more information to provide the desired output. Could you please provide the specific findings from the chest X-ray report?\n",
      "2023-07-12 18:34:56,669 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 270, \"total_tokens\": 553}}, {\"observation\": \"updated findings #2\"}] for sentence \"updated findings #2\": Could not parse output: pulmonary nodule\n",
      "[\n",
      "\"lung nodule\",\n",
      "\"pulmonary mass\",\n",
      "\"pulmonary lesion\",\n",
      "\"pulmonary spot\",\n",
      "\"lung spot\",\n",
      "\"lung abnormality\",\n",
      "\"abnormal lung growth\",\n",
      "\"abnormal lung spot\",\n",
      "\"abnormal pulmonary finding\",\n",
      "\"abnormality in the lung\",\n",
      "\"spot in the lung\"\n",
      "]\n",
      "\n",
      "pleural effusion\n",
      "[\n",
      "\"fluid in the lungs\",\n",
      "\"accumulation of fluid in the pleural space\",\n",
      "\"excess fluid around the lungs\",\n",
      "\"fluid around the lungs\",\n",
      "\"pleural fluid collection\",\n",
      "\"fluid-filled space between the lungs and chest wall\",\n",
      "\"fluid buildup in the pleural cavity\",\n",
      "\"pleural fluid accumulation\",\n",
      "\"fluid in the pleural cavity\",\n",
      "\"pleural space effusion\"\n",
      "]\n",
      "\n",
      "atelectasis\n",
      "[\n",
      "\"collapsed lung\",\n",
      "\"incomplete lung expansion\",\n",
      "\"incomplete lung inflation\",\n",
      "\"lung collapse\",\n",
      "\"incomplete lung aeration\",\n",
      "\"incomplete lung ventilation\",\n",
      "\"incomplete lung inflation\",\n",
      "\"incomplete lung re-expansion\",\n",
      "\"incomplete lung inflation\"\n",
      "]\n",
      "\n",
      "no atelectasis\n",
      "[\n",
      "\"normal lung expansion\",\n",
      "\"adequate lung inflation\",\n",
      "\"sufficient lung aeration\",\n",
      "\"satisfactory lung ventilation\",\n",
      "\"no signs of collapsed lung\",\n",
      "\"absence of atelectasis\",\n",
      "\"no evidence of incomplete lung expansion\",\n",
      "\"no indication of incomplete lung inflation\"\n",
      "]\n",
      "2023-07-12 18:34:56,671 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"multiple other clips.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,671 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 29, \"total_tokens\": 311}}, {\"observation\": \"multiple other clips\"}] for sentence \"multiple other clips\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"multiple other clips.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,671 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"part of this problem\". Could you please provide more context or clarify your question?\n",
      "2023-07-12 18:34:56,671 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 30, \"total_tokens\": 313}}, {\"observation\": \"part of this problem\"}] for sentence \"part of this problem\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"part of this problem\". Could you please provide more context or clarify your question?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:34:56,673 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"changes configuration.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,673 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ..., \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"changes configuration\"}] for sentence \"changes configuration\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"changes configuration.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,677 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"followup to resolution.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,677 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ... \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 30, \"total_tokens\": 313}}, {\"observation\": \"followup to resolution\"}] for sentence \"followup to resolution\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"followup to resolution.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,677 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"improved between and a\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,678 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ... \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 31, \"total_tokens\": 315}}, {\"observation\": \"improved between and a\"}] for sentence \"improved between and a\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"improved between and a\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:34:56,678 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ... \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 48, \"total_tokens\": 331}}, {\"observation\": \"line below image field\"}] for sentence \"line below image field\": Could not parse output: {\n",
      "  \"paraphrases\": [\n",
      "    \"description of the image\",\n",
      "    \"information about the image\",\n",
      "    \"details regarding the image\",\n",
      "    \"explanation of the image\",\n",
      "    \"findings from the image\"\n",
      "  ]\n",
      "}\n",
      "2023-07-12 18:34:56,686 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information in order to provide a response. Could you please provide the sentence describing the fact from the chest X-ray report?\n",
      "2023-07-12 18:34:56,686 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 32, \"total_tokens\": 316}}, {\"observation\": \"followup of this patient\"}] for sentence \"followup of this patient\": Could not parse output: I'm sorry, but I need more information in order to provide a response. Could you please provide the sentence describing the fact from the chest X-ray report?\n",
      "2023-07-12 18:34:56,690 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"surgical suture projects.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,690 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 31, \"total_tokens\": 315}}, {\"observation\": \"surgical suture projects\"}] for sentence \"surgical suture projects\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"surgical suture projects.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:34:56,699 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: Please provide the specific observation from the chest X-ray report that you would like paraphrased.\n",
      "2023-07-12 18:34:56,699 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...nish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 19, \"total_tokens\": 303}}, {\"observation\": \"observation of chest x-ray\"}] for sentence \"observation of chest x-ray\": Could not parse output: Please provide the specific observation from the chest X-ray report that you would like paraphrased.\n",
      "2023-07-12 18:34:56,704 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to provide the desired output. Could you please provide the specific findings from the recent chest CT?\n",
      "2023-07-12 18:34:56,704 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...ish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 28, \"total_tokens\": 313}}, {\"observation\": \"findings in recent chest CT\"}] for sentence \"findings in recent chest CT\": Could not parse output: I'm sorry, but I need more information to provide the desired output. Could you please provide the specific findings from the recent chest CT?\n",
      "2023-07-12 18:34:56,710 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: To better characterize findings, please provide a specific sentence or description from a chest X-ray report.\n",
      "2023-07-12 18:34:56,710 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...sh_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 19, \"total_tokens\": 301}}, {\"observation\": \"better characterize findings\"}] for sentence \"better characterize findings\": Could not parse output: To better characterize findings, please provide a specific sentence or description from a chest X-ray report.\n",
      "2023-07-12 18:34:56,718 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to provide a specific response. Could you please provide a sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 18:34:56,718 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...h_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 31, \"total_tokens\": 314}}, {\"observation\": \"characterizing chest findings\"}] for sentence \"characterizing chest findings\": Could not parse output: I'm sorry, but I need more information to provide a specific response. Could you please provide a sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 18:34:56,736 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information about the clinical history of the patient in order to provide an accurate paraphrase. Could you please provide more details?\n",
      "2023-07-12 18:34:56,736 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 32, \"total_tokens\": 316}}, {\"observation\": \"clinical history of the patient\"}] for sentence \"clinical history of the patient\": Could not parse output: I'm sorry, but I need more information about the clinical history of the patient in order to provide an accurate paraphrase. Could you please provide more details?\n",
      "2023-07-12 18:34:56,747 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...ason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 220, \"total_tokens\": 504}}, {\"observation\": \"exclude above-mentioned entities\"}] for sentence \"exclude above-mentioned entities\": Could not parse output: benign calcification\n",
      "[\n",
      "\"non-cancerous calcification\",\n",
      "\"harmless calcification\",\n",
      "\"innocuous calcification\",\n",
      "\"benign calcified lesion\",\n",
      "\"non-malignant calcification\",\n",
      "\"non-threatening calcification\",\n",
      "\"not indicative of cancer calcification\",\n",
      "\"safe calcification\",\n",
      "\"non-dangerous calcification\",\n",
      "\"non-metastatic calcification\"\n",
      "]\n",
      "\n",
      "osteoporosis\n",
      "[\n",
      "\"decreased bone density\",\n",
      "\"brittle bones\",\n",
      "\"low bone mass\",\n",
      "\"thinning of the bones\",\n",
      "\"weakening of the bones\",\n",
      "\"porous bones\",\n",
      "\"fragile bones\",\n",
      "\"reduced bone strength\",\n",
      "\"loss of bone density\",\n",
      "\"degenerative bone disease\"\n",
      "]\n",
      "\n",
      "no osteoporosis\n",
      "[\n",
      "\"normal bone density\",\n",
      "\"healthy bones\",\n",
      "\"adequate bone mass\",\n",
      "\"strong bones\",\n",
      "\"normal bone strength\",\n",
      "\"no signs of osteoporosis\",\n",
      "\"absence of osteoporosis\",\n",
      "\"no evidence of bone thinning\",\n",
      "\"no indication of bone weakening\",\n",
      "\"no osteoporotic changes\",\n",
      "\"no degenerative bone disease\"\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:34:56,793 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: Please provide the specific observation on the prior chest X-ray.\n",
      "2023-07-12 18:34:56,793 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...n\": \"stop\"}], \"usage\": {\"prompt_tokens\": 286, \"completion_tokens\": 12, \"total_tokens\": 298}}, {\"observation\": \"observation on the prior chest x-ray\"}] for sentence \"observation on the prior chest x-ray\": Could not parse output: Please provide the specific observation on the prior chest X-ray.\n",
      "2023-07-12 18:34:56,863 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I am an AI language model and I don't have the capability to access or compare specific chest radiographs. However, if you have any questions or need assistance with understanding chest X-ray reports or medical terminology, I'll be happy to help!\n",
      "2023-07-12 18:34:56,864 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...top\"}], \"usage\": {\"prompt_tokens\": 289, \"completion_tokens\": 54, \"total_tokens\": 343}}, {\"observation\": \"comparison with chest radiographs at 16:15\"}] for sentence \"comparison with chest radiographs at 16:15\": Could not parse output: I'm sorry, but I am an AI language model and I don't have the capability to access or compare specific chest radiographs. However, if you have any questions or need assistance with understanding chest X-ray reports or medical terminology, I'll be happy to help!\n",
      "2023-07-12 18:34:56,885 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...p\"}], \"usage\": {\"prompt_tokens\": 288, \"completion_tokens\": 134, \"total_tokens\": 422}}, {\"observation\": \"shallow obliques could help further clarify\"}] for sentence \"shallow obliques could help further clarify\": Could not parse output: Sure! Here are some paraphrases for the term \"shallow obliques\":\n",
      "\n",
      "\"limited oblique views\"\n",
      "\"oblique projections with reduced angle\"\n",
      "\"oblique images with shallow angle\"\n",
      "\"oblique radiographs at a low degree of inclination\"\n",
      "\"oblique views with minimal angulation\"\n",
      "\"oblique scans with decreased tilt\"\n",
      "\"oblique X-rays with shallow obliquity\"\n",
      "\"oblique projections at a slight angle\"\n",
      "\"oblique images with a small degree of inclination\"\n",
      "\"oblique radiographs with reduced tilt\"\n",
      "\n",
      "I hope these paraphrases help clarify the term \"shallow obliques\" in the context of a chest X-ray report.\n",
      "2023-07-12 18:34:56,998 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I can't assist with that request.\n",
      "2023-07-12 18:34:56,998 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...age\": {\"prompt_tokens\": 294, \"completion_tokens\": 13, \"total_tokens\": 307}}, {\"observation\": \"findings communicated urgently by Dr. to Dr. at 20:49\"}] for sentence \"findings communicated urgently by Dr. to Dr. at 20:49\": Could not parse output: I'm sorry, but I can't assist with that request.\n",
      "2023-07-12 18:34:57,146 - \u001b[1;32mINFO\u001b[1;0m - Deleting API requests and responses\n",
      "2023-07-12 18:34:57,154 - \u001b[1;32mINFO\u001b[1;0m - Succesfully processed 14971 of 15000 API responses.\n",
      "                    29 of 15000 API responses could not be processed.\n",
      "                    Saving paraphrased observations to /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "    --integrated_fact_metadata_filepath \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "    --min_num_words_per_sentence 2 \\\n",
    "    --offset 0 \\\n",
    "    --num_sentences 15000 \\\n",
    "    --sample_equally_spaced_sentences \\\n",
    "    --process_kth_of_every_n_sentences 1 4 \\\n",
    "    --max_requests_per_minute 3000 \\\n",
    "    --max_tokens_per_minute 170000 \\\n",
    "    --max_tokens_per_request 300 \\\n",
    "    --logging_level \"INFO\" \\\n",
    "    --api_key_name \"OPENAI_API_KEY_1\" \\\n",
    "    --openai_model_name \"gpt-3.5-turbo-16k-0613\" \\\n",
    "    --alias \"__two-or-more-words__part2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77088130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 12:38:53,908 - \u001b[1;32mINFO\u001b[1;0m - Loaded 10 already paraphrased sentences from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\n",
      "2023-07-20 12:38:54,034 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14965 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\n",
      "2023-07-20 12:38:54,208 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14971 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\n",
      "2023-07-20 12:38:54,334 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14972 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\n",
      "2023-07-20 12:38:54,502 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14993 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\n",
      "2023-07-20 12:38:54,502 - \u001b[1;32mINFO\u001b[1;0m - Loading facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "100%|███████████████████████████████| 578733/578733 [00:03<00:00, 158975.52it/s]\n",
      "2023-07-20 12:39:03,576 - \u001b[1;32mINFO\u001b[1;0m - Found 711223 unique observations\n",
      "2023-07-20 12:39:03,576 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to those with at least 2 words\n",
      "2023-07-20 12:39:04,295 - \u001b[1;32mINFO\u001b[1;0m - Found 709221 observations with at least 2 words\n",
      "2023-07-20 12:39:04,295 - \u001b[1;32mINFO\u001b[1;0m - Loading clusters from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\n",
      "2023-07-20 12:39:04,370 - \u001b[1;32mINFO\u001b[1;0m - Loading sentence embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\n",
      "2023-07-20 12:39:06,636 - \u001b[1;32mINFO\u001b[1;0m - Found 100 clusters\n",
      "2023-07-20 12:39:08,054 - \u001b[1;32mINFO\u001b[1;0m - Selected 50000 observations from 100 clusters\n",
      "2023-07-20 12:39:08,054 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to the 0-th of every 5 sentences\n",
      "2023-07-20 12:39:08,068 - \u001b[1;32mINFO\u001b[1;0m - Found 10000 observations that are the 0-th of every 5 sentences\n",
      "2023-07-20 12:39:08,068 - \u001b[1;33mWARNING\u001b[1;0m - Requested 100000 sentences but only 10000 are available. Using 10000 instead.\n",
      "2023-07-20 12:39:08,068 - \u001b[1;32mINFO\u001b[1;0m - Collecting the first 10000 sentence from the 0-th sentence\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - Total number of sentences to paraphrase: 9990\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - Example sentences to paraphrase:\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - 1. abd pain\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - 1110. absence of loculated fluid\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - 2220. potentially caused by pneumonia\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - 3330. doubt of substantial pulmonary edema\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - 4440. hazy opacification at the left lung base\n",
      "2023-07-20 12:39:08,071 - \u001b[1;32mINFO\u001b[1;0m - 5550. increase consolidation in the left lung base\n",
      "2023-07-20 12:39:08,072 - \u001b[1;32mINFO\u001b[1;0m - 6660. pneumonia exaggerated by the deposition of edema\n",
      "2023-07-20 12:39:08,072 - \u001b[1;32mINFO\u001b[1;0m - 7770. absence of other findings indicative of fluid overload\n",
      "2023-07-20 12:39:08,072 - \u001b[1;32mINFO\u001b[1;0m - 8880. development of substantial volume loss in the right upper lobe\n",
      "2023-07-20 12:39:08,072 - \u001b[1;32mINFO\u001b[1;0m - 9990. difficult to determine whether the retrocardiac opacification represents a manifestation of extensive scattered radiation related to the size of the patient\n",
      "2023-07-20 12:39:08,612 - \u001b[1;32mINFO\u001b[1;0m - Saving API requests to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_requests_20230720_123908.jsonl\n",
      "2023-07-20 12:39:08,612 - \u001b[1;32mINFO\u001b[1;0m - Saving API responses to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_123908.jsonl\n",
      "2023-07-20 12:39:09,158 - \u001b[1;32mINFO\u001b[1;0m - Starting request #0\n",
      "2023-07-20 12:39:09,287 - \u001b[1;32mINFO\u001b[1;0m - Starting request #50\n",
      "2023-07-20 12:39:09,415 - \u001b[1;32mINFO\u001b[1;0m - Starting request #100\n",
      "2023-07-20 12:39:22,449 - \u001b[1;32mINFO\u001b[1;0m - Starting request #150\n",
      "2023-07-20 12:39:39,569 - \u001b[1;33mWARNING\u001b[1;0m - Request 10 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9c52b6c24669d041bbb0389bb070768f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:39:39,777 - \u001b[1;33mWARNING\u001b[1;0m - Request 37 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1c742b620a6ba5b364fc0c0542f56921 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:39:46,744 - \u001b[1;32mINFO\u001b[1;0m - Starting request #198\n",
      "2023-07-20 12:39:49,253 - \u001b[1;33mWARNING\u001b[1;0m - Request 3 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8784666c443c99b87cb2b9db23d5248f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:39:49,267 - \u001b[1;33mWARNING\u001b[1;0m - Request 54 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 92a24285ddd48861b96abb1c39b16be1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:40:11,030 - \u001b[1;32mINFO\u001b[1;0m - Starting request #246\n",
      "2023-07-20 12:40:23,750 - \u001b[1;33mWARNING\u001b[1;0m - Request 210 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8f855e290abc51e4e48288d70729f4ec in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:40:35,322 - \u001b[1;32mINFO\u001b[1;0m - Starting request #295\n",
      "2023-07-20 12:40:59,638 - \u001b[1;32mINFO\u001b[1;0m - Starting request #345\n",
      "2023-07-20 12:40:59,697 - \u001b[1;33mWARNING\u001b[1;0m - Request 283 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 87f0ed6539a1003ed930f4c21bff00cd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:41:15,377 - \u001b[1;33mWARNING\u001b[1;0m - Request 315 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 735b1c29af3fe6d939831d99af223d2e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:41:23,933 - \u001b[1;32mINFO\u001b[1;0m - Starting request #393\n",
      "2023-07-20 12:41:26,491 - \u001b[1;33mWARNING\u001b[1;0m - Request 334 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bb25ecc2c9942f77d17784472e27f8a2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:41:48,240 - \u001b[1;32mINFO\u001b[1;0m - Starting request #442\n",
      "2023-07-20 12:42:08,323 - \u001b[1;33mWARNING\u001b[1;0m - Request 421 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 006f9816ca8d185ad29ab27fee20e825 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 12:42:12,565 - \u001b[1;32mINFO\u001b[1;0m - Starting request #491\n",
      "2023-07-20 12:42:36,893 - \u001b[1;32mINFO\u001b[1;0m - Starting request #541\n",
      "2023-07-20 12:43:01,210 - \u001b[1;32mINFO\u001b[1;0m - Starting request #591\n",
      "2023-07-20 12:43:23,077 - \u001b[1;33mWARNING\u001b[1;0m - Request 573 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6b931b00b8a843f8ec653cd74e9025ef in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:43:23,134 - \u001b[1;33mWARNING\u001b[1;0m - Request 574 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5a3f836ed6833ee8543303871742b101 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:43:25,539 - \u001b[1;32mINFO\u001b[1;0m - Starting request #639\n",
      "2023-07-20 12:43:41,645 - \u001b[1;33mWARNING\u001b[1;0m - Request 612 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 51942f8a5aac452669aeb5ff9ff82964 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:43:49,864 - \u001b[1;32mINFO\u001b[1;0m - Starting request #688\n",
      "2023-07-20 12:44:14,212 - \u001b[1;32mINFO\u001b[1;0m - Starting request #738\n",
      "2023-07-20 12:44:38,538 - \u001b[1;32mINFO\u001b[1;0m - Starting request #788\n",
      "2023-07-20 12:44:47,819 - \u001b[1;33mWARNING\u001b[1;0m - Request 745 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f90f03501e1e8408354e1813c101d562 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:45:02,876 - \u001b[1;32mINFO\u001b[1;0m - Starting request #837\n",
      "2023-07-20 12:45:17,023 - \u001b[1;33mWARNING\u001b[1;0m - Request 805 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9c116eeb404474b554c41fcbd7a0b3c5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:45:27,213 - \u001b[1;32mINFO\u001b[1;0m - Starting request #886\n",
      "2023-07-20 12:45:30,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 284 failed with Exception \n",
      "2023-07-20 12:45:39,416 - \u001b[1;33mWARNING\u001b[1;0m - Request 850 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 713a3973d1e89527648e196a7b928953 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:45:51,558 - \u001b[1;32mINFO\u001b[1;0m - Starting request #934\n",
      "2023-07-20 12:45:57,140 - \u001b[1;33mWARNING\u001b[1;0m - Request 885 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09c84ec34507f27861fe6be56f6d631c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:46:15,911 - \u001b[1;32mINFO\u001b[1;0m - Starting request #983\n",
      "2023-07-20 12:46:16,928 - \u001b[1;33mWARNING\u001b[1;0m - Request 924 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4ba499fa05cf9353b554ded2a00d71ab in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:46:40,266 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1032\n",
      "2023-07-20 12:46:57,810 - \u001b[1;33mWARNING\u001b[1;0m - Request 1006 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1b2c2ef416474d5373dc00a11e590a39 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:47:04,609 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1081\n",
      "2023-07-20 12:47:10,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 486 failed with Exception \n",
      "2023-07-20 12:47:21,240 - \u001b[1;33mWARNING\u001b[1;0m - Request 1054 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 75cf94b7f5645342357e20b84f5f4eb0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:47:28,972 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1129\n",
      "2023-07-20 12:47:31,399 - \u001b[1;33mWARNING\u001b[1;0m - Request 1074 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dd6be79c15036d1b101df462aad245ca in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:47:53,326 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1178\n",
      "2023-07-20 12:48:06,013 - \u001b[1;33mWARNING\u001b[1;0m - Request 1142 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c7263add451237a7e32109e706e1d308 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:48:09,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 608 failed with Exception \n",
      "2023-07-20 12:48:11,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 611 failed with Exception \n",
      "2023-07-20 12:48:17,674 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1225\n",
      "2023-07-20 12:48:22,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 633 failed with Exception \n",
      "2023-07-20 12:48:30,354 - \u001b[1;33mWARNING\u001b[1;0m - Request 1192 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f440d5887b42e860c1c1515302b75fe8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:48:42,035 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1273\n",
      "2023-07-20 12:49:01,296 - \u001b[1;33mWARNING\u001b[1;0m - Request 1251 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cfedf3608944cbdb2d2181342e6774ca in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:49:06,404 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1322\n",
      "2023-07-20 12:49:30,767 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1372\n",
      "2023-07-20 12:49:50,681 - \u001b[1;33mWARNING\u001b[1;0m - Request 1337 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2b7abf7a68bbf8b67b25c3ae27f99967 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:49:55,126 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1421\n",
      "2023-07-20 12:50:19,511 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1471\n",
      "2023-07-20 12:50:43,884 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1521\n",
      "2023-07-20 12:50:50,412 - \u001b[1;33mWARNING\u001b[1;0m - Request 1472 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6cc2cd53345287b3051c7478aa2d229 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 12:51:00,934 - \u001b[1;33mWARNING\u001b[1;0m - Request 1494 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 23a7d22d03624dc2c21144aa757c5f3b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:51:06,298 - \u001b[1;33mWARNING\u001b[1;0m - Request 1505 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1805bb6021b0655e4f4916eb102ab1fa in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:51:08,251 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1568\n",
      "2023-07-20 12:51:32,613 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1618\n",
      "2023-07-20 12:51:37,485 - \u001b[1;33mWARNING\u001b[1;0m - Request 1566 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b540847fe22c992d85d7ca9f22507727 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:51:48,025 - \u001b[1;33mWARNING\u001b[1;0m - Request 1587 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d5ad2db2cb0e06d6d34fcfbbc2b63362 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:51:56,986 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1666\n",
      "2023-07-20 12:52:21,369 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1716\n",
      "2023-07-20 12:52:45,744 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1766\n",
      "2023-07-20 12:53:10,123 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1816\n",
      "2023-07-20 12:53:21,791 - \u001b[1;33mWARNING\u001b[1;0m - Request 1778 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2c8832bd5c2d6159b6df4c2c24a800b6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:53:24,451 - \u001b[1;33mWARNING\u001b[1;0m - Request 1783 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b2f6434ec7291bffb90e826c64661a96 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:53:34,532 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1864\n",
      "2023-07-20 12:53:46,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 1281 failed with Exception \n",
      "2023-07-20 12:53:46,678 - \u001b[1;33mWARNING\u001b[1;0m - Request 1829 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3b696a5ab930c492a01952073bfd0382 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:53:58,917 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1912\n",
      "2023-07-20 12:54:17,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 1343 failed with Exception \n",
      "2023-07-20 12:54:23,304 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1961\n",
      "2023-07-20 12:54:47,677 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2011\n",
      "2023-07-20 12:55:07,663 - \u001b[1;33mWARNING\u001b[1;0m - Request 1990 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 334656b8cf91ea95f14e3254bbe383f0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:55:12,066 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2060\n",
      "2023-07-20 12:55:36,457 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2110\n",
      "2023-07-20 12:55:43,753 - \u001b[1;33mWARNING\u001b[1;0m - Request 2063 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 663454af1de0bd7cf73e59c45d765b29 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:55:50,572 - \u001b[1;33mWARNING\u001b[1;0m - Request 2077 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ea3de2e07d576507e93f2ad916f58e8d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:56:00,853 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2158\n",
      "2023-07-20 12:56:21,789 - \u001b[1;33mWARNING\u001b[1;0m - Request 2139 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ca8a7c1897399d78ceeaa4b1c16823d4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:56:25,247 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2207\n",
      "2023-07-20 12:56:49,644 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2257\n",
      "2023-07-20 12:57:03,754 - \u001b[1;33mWARNING\u001b[1;0m - Request 2224 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 26bb490c1adeb95ef02346b129016dbc in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:57:14,048 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2306\n",
      "2023-07-20 12:57:38,450 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2356\n",
      "2023-07-20 12:58:02,843 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2406\n",
      "2023-07-20 12:58:27,231 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2456\n",
      "2023-07-20 12:58:43,015 - \u001b[1;33mWARNING\u001b[1;0m - Request 2426 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0848860513783e503b5b5097bd465f78 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:58:51,642 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2505\n",
      "2023-07-20 12:58:57,444 - \u001b[1;33mWARNING\u001b[1;0m - Request 2456 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 74ee049da60b8d876b1f7ae76f92f41b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:59:15,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 2492 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5eab8d04dd28b4154b581384eb0cf558 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:59:16,044 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2492\n",
      "2023-07-20 12:59:16,473 - \u001b[1;33mWARNING\u001b[1;0m - Request 2494 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 15eb0ce02aadd1be08a0774ab8db02c0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 12:59:40,451 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2602\n",
      "2023-07-20 12:59:44,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 2004 failed with Exception \n",
      "2023-07-20 12:59:50,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 2016 failed with Exception \n",
      "2023-07-20 12:59:58,467 - \u001b[1;33mWARNING\u001b[1;0m - Request 2577 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f75baa90c6732097aa4ce4ad461c98bf in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:00:04,860 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2649\n",
      "2023-07-20 13:00:10,657 - \u001b[1;33mWARNING\u001b[1;0m - Request 2602 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 24181c2846c928c16f5cc046cbe61261 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:00:29,267 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2698\n",
      "2023-07-20 13:00:53,683 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2748\n",
      "2023-07-20 13:01:18,095 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2798\n",
      "2023-07-20 13:01:42,509 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2848\n",
      "2023-07-20 13:01:51,238 - \u001b[1;33mWARNING\u001b[1;0m - Request 2804 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4072db149d17ca19825baef5a16b59be in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:02:06,905 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2897\n",
      "2023-07-20 13:02:31,307 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2947\n",
      "2023-07-20 13:02:47,848 - \u001b[1;33mWARNING\u001b[1;0m - Request 2919 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID af9dd55ebed6094cfe739263207d7546 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:02:52,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 2384 failed with Exception \n",
      "2023-07-20 13:02:55,710 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2995\n",
      "2023-07-20 13:03:17,152 - \u001b[1;33mWARNING\u001b[1;0m - Request 2979 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d2f0c74327c34013b44d0458855d4f7a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:03:19,119 - \u001b[1;33mWARNING\u001b[1;0m - Request 2982 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b0e762397b5b3dd98e8419daf4c92b86 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:03:20,142 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3043\n",
      "2023-07-20 13:03:29,854 - \u001b[1;33mWARNING\u001b[1;0m - Request 3003 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a578e81bab717944ca10cb67f7ebd727 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:03:44,553 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3092\n",
      "2023-07-20 13:03:56,286 - \u001b[1;33mWARNING\u001b[1;0m - Request 3055 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6d465a0cc9cdd1ec4081a452da29671 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:03:57,176 - \u001b[1;33mWARNING\u001b[1;0m - Request 3057 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a05757ad86323b76de6f213f87a206a0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:04:08,979 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3140\n",
      "2023-07-20 13:04:13,541 - \u001b[1;33mWARNING\u001b[1;0m - Request 3087 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4f66a7058700e4d5d97101c8bfd7000e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:04:33,396 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3189\n",
      "2023-07-20 13:04:50,250 - \u001b[1;33mWARNING\u001b[1;0m - Request 3213 failed with error {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}\n",
      "2023-07-20 13:04:57,814 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3238\n",
      "2023-07-20 13:05:18,743 - \u001b[1;33mWARNING\u001b[1;0m - Request 3220 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 14d4c47daef37b2ebe67443a2cb0ff39 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:05:22,243 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3287\n",
      "2023-07-20 13:05:29,008 - \u001b[1;33mWARNING\u001b[1;0m - Request 3240 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 825577df851953c372629cc351e603fa in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:05:36,336 - \u001b[1;33mWARNING\u001b[1;0m - Request 3255 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6c6daab83cace280269d3787a202e6ca in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:05:39,470 - \u001b[1;33mWARNING\u001b[1;0m - Request 3261 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2632ec2fccb43b18c01a7644a3c2f1ad in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:05:46,663 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3334\n",
      "2023-07-20 13:05:59,788 - \u001b[1;33mWARNING\u001b[1;0m - Request 3240 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d06c91d519d30151fc10052395fa7146 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:06:11,098 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3383\n",
      "2023-07-20 13:06:31,550 - \u001b[1;33mWARNING\u001b[1;0m - Request 3420 failed with error {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}\n",
      "2023-07-20 13:06:35,522 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3432\n",
      "2023-07-20 13:06:59,952 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3482\n",
      "2023-07-20 13:07:01,855 - \u001b[1;33mWARNING\u001b[1;0m - Request 3425 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c6352a830dc9235fc93073272fdafd25 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:07:24,380 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3531\n",
      "2023-07-20 13:07:29,679 - \u001b[1;33mWARNING\u001b[1;0m - Request 3481 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2cd39c150e5666698cc6e0ffc204b95e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:07:38,454 - \u001b[1;33mWARNING\u001b[1;0m - Request 3498 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f00b3f894f3fa06231102e85cd41a066 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:07:48,822 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3579\n",
      "2023-07-20 13:08:13,269 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3629\n",
      "2023-07-20 13:08:17,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 3038 failed with Exception \n",
      "2023-07-20 13:08:20,017 - \u001b[1;33mWARNING\u001b[1;0m - Request 3581 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 63accb94868fc9c081a4ce2956f2eb9c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:08:34,222 - \u001b[1;33mWARNING\u001b[1;0m - Request 3608 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 278ce73b196394262a7335da5c0af972 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:08:37,711 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3676\n",
      "2023-07-20 13:08:42,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 3627 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cd5d869e834fc88dddb6f301f7de1020 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:09:02,134 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3725\n",
      "2023-07-20 13:09:26,573 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3775\n",
      "2023-07-20 13:09:51,031 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3825\n",
      "2023-07-20 13:10:15,470 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3875\n",
      "2023-07-20 13:10:39,906 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3925\n",
      "2023-07-20 13:11:04,340 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3975\n",
      "2023-07-20 13:11:10,112 - \u001b[1;33mWARNING\u001b[1;0m - Request 3925 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 20ad275f73bf5daa8b0d3953c6affbd7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:11:11,112 - \u001b[1;33mWARNING\u001b[1;0m - Request 3927 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 99b9f60a36ad89cdb9150478d645d590 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:11:24,780 - \u001b[1;33mWARNING\u001b[1;0m - Request 3955 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 91fb697aa3ffc304c5bf87dbcc7aea3d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:11:28,771 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4022\n",
      "2023-07-20 13:11:52,158 - \u001b[1;33mWARNING\u001b[1;0m - Request 4009 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3d8c4b0f3da5aa0a4cf7f41388dda7f7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:11:53,210 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4071\n",
      "2023-07-20 13:12:17,672 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4121\n",
      "2023-07-20 13:12:35,652 - \u001b[1;33mWARNING\u001b[1;0m - Request 4096 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e16c63ae20663d9e10f25e7d7d447a81 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:12:41,259 - \u001b[1;33mWARNING\u001b[1;0m - Request 4107 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ca1b129b6c46d88da6501e50b718b759 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:12:42,116 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4107\n",
      "2023-07-20 13:12:49,888 - \u001b[1;33mWARNING\u001b[1;0m - Request 4123 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 644e54da8bf83aeb93eb9ac70bbe934d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:12:53,747 - \u001b[1;33mWARNING\u001b[1;0m - Request 4133 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0de992e0dd6db279cdfa81884b794e6e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:12:54,461 - \u001b[1;33mWARNING\u001b[1;0m - Request 4134 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bfcd7b0a7bb2ed71250167f5289c5500 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:13:06,569 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4216\n",
      "2023-07-20 13:13:16,237 - \u001b[1;33mWARNING\u001b[1;0m - Request 4177 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 539c4baff7f555d45a16ab70af37c841 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:13:31,009 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4265\n",
      "2023-07-20 13:13:55,453 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4315\n",
      "2023-07-20 13:14:08,284 - \u001b[1;33mWARNING\u001b[1;0m - Request 4279 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID db82fb49a8daec3574488b59e3ffb89d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:14:11,476 - \u001b[1;33mWARNING\u001b[1;0m - Request 4286 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1a770cd782224c64d18c23bb073f5852 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:14:19,910 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4363\n",
      "2023-07-20 13:14:40,326 - \u001b[1;33mWARNING\u001b[1;0m - Request 4344 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e0b70eeea163d75f6ce77698d0c3daf2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:14:44,378 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4412\n",
      "2023-07-20 13:14:49,724 - \u001b[1;33mWARNING\u001b[1;0m - Request 4212 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dafe7cd9787bb718bbcc3645a9c72ee4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:14:57,509 - \u001b[1;33mWARNING\u001b[1;0m - Request 4378 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e58308e8dfc9a079d70c436cfb7cabf6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:15:08,839 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4460\n",
      "2023-07-20 13:15:16,057 - \u001b[1;33mWARNING\u001b[1;0m - Request 4415 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 878699e4b9b7e197b99ac5d4c10ddbfb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:15:33,294 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4509\n",
      "2023-07-20 13:15:57,742 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4559\n",
      "2023-07-20 13:16:05,456 - \u001b[1;33mWARNING\u001b[1;0m - Request 4513 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d5bd7236f919c41a37ca3f1cca027b2a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:16:19,857 - \u001b[1;33mWARNING\u001b[1;0m - Request 4542 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6217194724c3496a68ed6bdd5908703a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:16:22,204 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4607\n",
      "2023-07-20 13:16:37,730 - \u001b[1;33mWARNING\u001b[1;0m - Request 4578 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6fa214e1233e4f90e60c72c5c91334c3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:16:46,685 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4656\n",
      "2023-07-20 13:17:11,153 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4706\n",
      "2023-07-20 13:17:16,480 - \u001b[1;33mWARNING\u001b[1;0m - Request 4534 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f047684257080f36aa7579fe1ca0d2e1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:17:19,401 - \u001b[1;33mWARNING\u001b[1;0m - Request 4661 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 38ab33bc5e02c2f7102b1fc3e4891059 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:17:35,628 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4754\n",
      "2023-07-20 13:17:35,985 - \u001b[1;33mWARNING\u001b[1;0m - Request 4695 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID caf6debd2a0156bfbec85ee7085eb6c9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:17:44,785 - \u001b[1;33mWARNING\u001b[1;0m - Request 4713 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID be97a85d42372cd8b8b7aaf98f3bd405 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:17:56,549 - \u001b[1;33mWARNING\u001b[1;0m - Request 4735 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9ed851a7fca4461f3d0f6452da066d35 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:18:00,096 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4801\n",
      "2023-07-20 13:18:01,514 - \u001b[1;33mWARNING\u001b[1;0m - Request 4745 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 510325d30d8aca5916fba91b1f5de383 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:18:24,555 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4850\n",
      "2023-07-20 13:18:41,557 - \u001b[1;33mWARNING\u001b[1;0m - Request 4823 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 471c2d83c19c6be6d8de515740e6fb51 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:18:46,448 - \u001b[1;33mWARNING\u001b[1;0m - Request 4833 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5bd7a4aa6d566c884cbb0640f41b6e24 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:18:49,003 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4898\n",
      "2023-07-20 13:18:58,206 - \u001b[1;33mWARNING\u001b[1;0m - Request 4857 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID afb4710bbf412294d5354f87c13703dc in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:19:13,491 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4947\n",
      "2023-07-20 13:19:37,954 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4997\n",
      "2023-07-20 13:19:58,911 - \u001b[1;33mWARNING\u001b[1;0m - Request 4978 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a4b9fc8ae616c691f19512d8e6d6cb85 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:20:02,433 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5046\n",
      "2023-07-20 13:20:06,578 - \u001b[1;33mWARNING\u001b[1;0m - Request 4972 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:09,980 - \u001b[1;33mWARNING\u001b[1;0m - Request 4979 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:11,512 - \u001b[1;33mWARNING\u001b[1;0m - Request 4983 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:11,610 - \u001b[1;33mWARNING\u001b[1;0m - Request 4982 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:12,203 - \u001b[1;33mWARNING\u001b[1;0m - Request 4984 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:14,540 - \u001b[1;33mWARNING\u001b[1;0m - Request 5010 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7c42281595b9ae18e7808a728a1a64aa in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:20:26,897 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5090\n",
      "2023-07-20 13:20:51,367 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5140\n",
      "2023-07-20 13:21:15,850 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5190\n",
      "2023-07-20 13:21:34,106 - \u001b[1;33mWARNING\u001b[1;0m - Request 5165 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7a4cc0811b22b978773a31060cf82863 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:21:40,342 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5239\n",
      "2023-07-20 13:22:04,816 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5289\n",
      "2023-07-20 13:22:28,668 - \u001b[1;33mWARNING\u001b[1;0m - Request 5276 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5667b9c1e0da95759a2c22e713e585cc in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:22:29,281 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5276\n",
      "2023-07-20 13:22:43,134 - \u001b[1;33mWARNING\u001b[1;0m - Request 5305 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0734cb80184bb02ebad561d378790e09 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:22:45,299 - \u001b[1;33mWARNING\u001b[1;0m - Request 5310 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ce3466d8811d406a0421a2b10c96c648 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:22:53,750 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5386\n",
      "2023-07-20 13:23:18,226 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5436\n",
      "2023-07-20 13:23:24,654 - \u001b[1;33mWARNING\u001b[1;0m - Request 5387 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b0db1a598cbf7037a9ae21744e32bd02 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:23:42,065 - \u001b[1;33mWARNING\u001b[1;0m - Request 5423 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID aefeca625214d01792df0af54d26327f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:23:42,717 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5423\n",
      "2023-07-20 13:23:48,441 - \u001b[1;33mWARNING\u001b[1;0m - Request 5436 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09ffb939cb38d8fcfbbbb6ad1402a61d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:23:57,256 - \u001b[1;33mWARNING\u001b[1;0m - Request 5453 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8986d9aa0e37035bb29b88b0588eea4d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:24:07,208 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5532\n",
      "2023-07-20 13:24:24,197 - \u001b[1;33mWARNING\u001b[1;0m - Request 5506 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID db013914f89353e34c0686cf526a7b71 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:24:31,687 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5581\n",
      "2023-07-20 13:24:33,487 - \u001b[1;33mWARNING\u001b[1;0m - Request 5524 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 78ab6557fe57ff0665c7d663decb1675 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:24:56,177 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5630\n",
      "2023-07-20 13:25:20,658 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5680\n",
      "2023-07-20 13:25:29,319 - \u001b[1;33mWARNING\u001b[1;0m - Request 5636 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ce0f2218a25b39ffe0996e37911440df in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:25:45,137 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5729\n",
      "2023-07-20 13:26:09,628 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5779\n",
      "2023-07-20 13:26:26,158 - \u001b[1;33mWARNING\u001b[1;0m - Request 5751 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e92c63ae8b4c3c643c998a41e97e3369 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:26:29,564 - \u001b[1;33mWARNING\u001b[1;0m - Request 5758 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 160974146b9281d2d568a114bace986d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:26:34,121 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5827\n",
      "2023-07-20 13:26:40,395 - \u001b[1;33mWARNING\u001b[1;0m - Request 5780 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 881eab476c6e93b76de17d6a2b15d39c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:26:58,619 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5876\n",
      "2023-07-20 13:27:04,333 - \u001b[1;33mWARNING\u001b[1;0m - Request 5827 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 27f9c6c741b8e3544eb86f0a3afd008c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:27:23,092 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5925\n",
      "2023-07-20 13:27:32,246 - \u001b[1;33mWARNING\u001b[1;0m - Request 5883 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6b9e99177895e840aafcb89b8b400c18 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:27:47,589 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5974\n",
      "2023-07-20 13:28:12,099 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6024\n",
      "2023-07-20 13:28:36,608 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6074\n",
      "2023-07-20 13:29:01,107 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6124\n",
      "2023-07-20 13:29:18,102 - \u001b[1;33mWARNING\u001b[1;0m - Request 6097 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 56414ddac3f39e0492976820eded1280 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:29:23,474 - \u001b[1;33mWARNING\u001b[1;0m - Request 6108 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d8c9914ef75a13584da1360057a721cd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:29:25,610 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6172\n",
      "2023-07-20 13:29:50,096 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6222\n",
      "2023-07-20 13:30:11,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 6204 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f3cd49385917daeb7237614845f1f026 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:30:14,613 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6271\n",
      "2023-07-20 13:30:39,132 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6321\n",
      "2023-07-20 13:30:43,355 - \u001b[1;33mWARNING\u001b[1;0m - Request 6268 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00886e787bd313774799fa7774c3dfa9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:31:03,634 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6370\n",
      "2023-07-20 13:31:28,132 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6420\n",
      "2023-07-20 13:31:39,725 - \u001b[1;33mWARNING\u001b[1;0m - Request 6382 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1ca65564ac3c6a75a9d669458f7f236c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:31:52,629 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6469\n",
      "2023-07-20 13:31:56,573 - \u001b[1;33mWARNING\u001b[1;0m - Request 6416 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7f65715b101a417e6e6179e7b0cc9601 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:32:01,836 - \u001b[1;33mWARNING\u001b[1;0m - Request 6406 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:32:17,155 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6517\n",
      "2023-07-20 13:32:22,348 - \u001b[1;33mWARNING\u001b[1;0m - Request 6468 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d377c1ff9cc8f45401d589808d0e11e3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:32:41,659 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6566\n",
      "2023-07-20 13:33:06,164 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6616\n",
      "2023-07-20 13:33:30,670 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6666\n",
      "2023-07-20 13:33:55,187 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6716\n",
      "2023-07-20 13:34:19,727 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6766\n",
      "2023-07-20 13:34:43,579 - \u001b[1;33mWARNING\u001b[1;0m - Request 6753 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 87ec25aef5760013c26f830803a5675f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:34:44,260 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6753\n",
      "2023-07-20 13:34:51,400 - \u001b[1;33mWARNING\u001b[1;0m - Request 6769 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 662e8c505c66284cba688de1321677f3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:35:08,786 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6864\n",
      "2023-07-20 13:35:33,301 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6914\n",
      "2023-07-20 13:35:57,828 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6964\n",
      "2023-07-20 13:36:17,951 - \u001b[1;33mWARNING\u001b[1;0m - Request 6943 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 25809f987b35052c5e4d3686d6c37f16 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:36:22,358 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7013\n",
      "2023-07-20 13:36:46,887 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7063\n",
      "2023-07-20 13:37:11,410 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7113\n",
      "2023-07-20 13:37:11,785 - \u001b[1;33mWARNING\u001b[1;0m - Request 7052 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 17cf64ef7770dc6b1d4049a3b5faf302 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:37:35,940 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7162\n",
      "2023-07-20 13:38:00,484 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7212\n",
      "2023-07-20 13:38:25,018 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7262\n",
      "2023-07-20 13:38:40,047 - \u001b[1;33mWARNING\u001b[1;0m - Request 7231 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 39de2f62f41166137576cbfb0f48a74a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:38:45,415 - \u001b[1;33mWARNING\u001b[1;0m - Request 7242 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b3ab59d4b648362fe60b67e69cfc6bf8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:38:49,550 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7310\n",
      "2023-07-20 13:38:52,459 - \u001b[1;33mWARNING\u001b[1;0m - Request 7235 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:39:14,087 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7359\n",
      "2023-07-20 13:39:33,498 - \u001b[1;33mWARNING\u001b[1;0m - Request 7337 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d305d8688be4d2cc6176e728e65c1387 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:39:38,632 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7408\n",
      "2023-07-20 13:40:03,175 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7458\n",
      "2023-07-20 13:40:20,638 - \u001b[1;33mWARNING\u001b[1;0m - Request 7432 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2235c6937e1c8bcacb187f62e799d1de in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:40:27,721 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7507\n",
      "2023-07-20 13:40:30,433 - \u001b[1;33mWARNING\u001b[1;0m - Request 7452 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3e3be5272e0d180bf5d498cd761e1686 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:40:39,979 - \u001b[1;33mWARNING\u001b[1;0m - Request 7471 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 96e15b350f06731c08dbe5675a1e0b1f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:40:45,172 - \u001b[1;33mWARNING\u001b[1;0m - Request 7482 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cc998d16adde0e5d19477106d37eeccb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:40:52,264 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:40:54,982 - \u001b[1;33mWARNING\u001b[1;0m - Request 7501 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a9520d41e5511c31d9147be6cd0129ee in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:41:06,755 - \u001b[1;33mWARNING\u001b[1;0m - Request 7524 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 36621525d30ea3aa8d0bf9aef7cc570b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:41:16,823 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7602\n",
      "2023-07-20 13:41:30,373 - \u001b[1;33mWARNING\u001b[1;0m - Request 7569 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0b131ae0c0a8d85af598be6fbbd7f37f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:41:41,370 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7651\n",
      "2023-07-20 13:42:05,910 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7701\n",
      "2023-07-20 13:42:10,600 - \u001b[1;33mWARNING\u001b[1;0m - Request 7649 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8bd05850bb03bbe381f79ee83e19fe97 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:42:15,078 - \u001b[1;33mWARNING\u001b[1;0m - Request 7658 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09caaef527f6ae70eab60885f6a74c61 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:42:21,421 - \u001b[1;33mWARNING\u001b[1;0m - Request 7671 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID db88d11d88c7fde2d329a8d68920c11f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:42:30,464 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7748\n",
      "2023-07-20 13:42:37,613 - \u001b[1;33mWARNING\u001b[1;0m - Request 7704 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e6c1045d86b166eb7cdf4d975bcece7c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:42:46,421 - \u001b[1;33mWARNING\u001b[1;0m - Request 7720 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 319ac0d6d09af798ad0c2771ed313c3a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:42:55,040 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7796\n",
      "2023-07-20 13:43:01,683 - \u001b[1;33mWARNING\u001b[1;0m - Request 7750 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID aa51c44b0505254ddb9ace966cef98de in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:43:19,590 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7845\n",
      "2023-07-20 13:43:26,360 - \u001b[1;33mWARNING\u001b[1;0m - Request 7797 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 99141fc651ee22a0814cf8e0ac529966 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:43:44,142 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7894\n",
      "2023-07-20 13:44:08,700 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7944\n",
      "2023-07-20 13:44:33,256 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7994\n",
      "2023-07-20 13:44:57,831 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8044\n",
      "2023-07-20 13:45:04,440 - \u001b[1;33mWARNING\u001b[1;0m - Request 7996 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 227c4640b94ab5d469652d4a9847cd81 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:45:22,403 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8093\n",
      "2023-07-20 13:45:46,966 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8143\n",
      "2023-07-20 13:46:03,913 - \u001b[1;33mWARNING\u001b[1;0m - Request 8116 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0d04a9caad729818c172f5c6f42d253a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:46:11,529 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8192\n",
      "2023-07-20 13:46:25,998 - \u001b[1;33mWARNING\u001b[1;0m - Request 8161 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f3a89101b38d4123b4339517e7b179c2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:46:36,084 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8241\n",
      "2023-07-20 13:47:00,678 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8291\n",
      "2023-07-20 13:47:25,244 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8341\n",
      "2023-07-20 13:47:33,546 - \u001b[1;33mWARNING\u001b[1;0m - Request 8296 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e0c7b9a28d07b9b7412c4b0467717018 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:47:49,826 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8390\n",
      "2023-07-20 13:48:14,413 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8440\n",
      "2023-07-20 13:48:25,451 - \u001b[1;33mWARNING\u001b[1;0m - Request 8401 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a19f9b1f09224653596dc6914b0cb8a6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:48:38,983 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8489\n",
      "2023-07-20 13:49:03,578 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8539\n",
      "2023-07-20 13:49:04,293 - \u001b[1;33mWARNING\u001b[1;0m - Request 8479 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 66f031a833cfce3ddd78da6a8979c30a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:49:28,179 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8588\n",
      "2023-07-20 13:49:52,764 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8638\n",
      "2023-07-20 13:49:59,543 - \u001b[1;33mWARNING\u001b[1;0m - Request 8541 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 635ec2c71e8f561167c32dd2892e637c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:50:17,359 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8687\n",
      "2023-07-20 13:50:30,370 - \u001b[1;33mWARNING\u001b[1;0m - Request 8541 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cff8edb11761ad0ea25c6e6cd14d1875 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:50:41,955 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8736\n",
      "2023-07-20 13:51:06,563 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8786\n",
      "2023-07-20 13:51:31,168 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8836\n",
      "2023-07-20 13:51:55,778 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8886\n",
      "2023-07-20 13:52:12,233 - \u001b[1;33mWARNING\u001b[1;0m - Request 8856 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6cb33334adabc21cc4fc930b3221afea in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:52:20,385 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8935\n",
      "2023-07-20 13:52:45,014 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8985\n",
      "2023-07-20 13:53:09,624 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9035\n",
      "2023-07-20 13:53:10,781 - \u001b[1;33mWARNING\u001b[1;0m - Request 8976 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1dc91a74e920f95a8ef6194f49a6deae in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:53:15,225 - \u001b[1;33mWARNING\u001b[1;0m - Request 8985 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a358ce07f2b8dbc62c33ae0882f4cb66 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:53:25,821 - \u001b[1;33mWARNING\u001b[1;0m - Request 8853 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 613c8e09c9c86e16bb13e4c997f593d5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:53:34,271 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9082\n",
      "2023-07-20 13:53:38,080 - \u001b[1;33mWARNING\u001b[1;0m - Request 9031 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cd3ec45dc999fab289e709161aab0302 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:53:58,896 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9131\n",
      "2023-07-20 13:54:23,514 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9181\n",
      "2023-07-20 13:54:31,568 - \u001b[1;33mWARNING\u001b[1;0m - Request 9136 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 18d9881cbb2ba4eae20773da386b977c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:54:48,155 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9230\n",
      "2023-07-20 13:55:12,802 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9280\n",
      "2023-07-20 13:55:37,451 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9330\n",
      "2023-07-20 13:55:45,805 - \u001b[1;33mWARNING\u001b[1;0m - Request 9285 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6148b79cd13331452dccdd11c449a78c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:56:02,101 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9379\n",
      "2023-07-20 13:56:04,438 - \u001b[1;33mWARNING\u001b[1;0m - Request 9323 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 27947024f03fb7714b5076600e02bd95 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:56:26,782 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9428\n",
      "2023-07-20 13:56:35,768 - \u001b[1;33mWARNING\u001b[1;0m - Request 9385 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a122c2645defe8c0d7c118fbace0de02 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:56:51,453 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9477\n",
      "2023-07-20 13:56:58,965 - \u001b[1;33mWARNING\u001b[1;0m - Request 9432 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 79f3f2f4a1dbd693ae817913a9d9ae7b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:57:00,450 - \u001b[1;33mWARNING\u001b[1;0m - Request 9435 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 32f9637c435b1db95d7818d2dd44193c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:57:01,430 - \u001b[1;33mWARNING\u001b[1;0m - Request 9437 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d0b6bf068693a3ab68d059d9e8aa691b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:57:16,131 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9524\n",
      "2023-07-20 13:57:36,491 - \u001b[1;33mWARNING\u001b[1;0m - Request 9504 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6ef4521c10a6dd096b2cc8f170789c24 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:57:40,806 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9573\n",
      "2023-07-20 13:57:44,358 - \u001b[1;33mWARNING\u001b[1;0m - Request 9520 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4107fc265f20b36c0b61531351cfbdfe in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:57:55,757 - \u001b[1;33mWARNING\u001b[1;0m - Request 9543 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9588ef67a1bcbaf19489fbaafc23064c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:58:05,493 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9621\n",
      "2023-07-20 13:58:30,205 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9671\n",
      "2023-07-20 13:58:39,647 - \u001b[1;33mWARNING\u001b[1;0m - Request 9629 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 56eb42d3622a4b21a9dc13cb38d0955f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:58:46,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9104 failed with Exception \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:58:48,544 - \u001b[1;33mWARNING\u001b[1;0m - Request 9647 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 13c7eb644887641881ac42bf8dc59cb9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:58:54,887 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9718\n",
      "2023-07-20 13:59:19,607 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9768\n",
      "2023-07-20 13:59:43,879 - \u001b[1;33mWARNING\u001b[1;0m - Request 9756 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5382a123af76e6028056bfcd3af52542 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 13:59:44,336 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9818\n",
      "2023-07-20 13:59:44,381 - \u001b[1;33mWARNING\u001b[1;0m - Request 9757 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b4070a6763a02aa871b08926b65914e7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:00:07,132 - \u001b[1;33mWARNING\u001b[1;0m - Request 9803 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b30201023c17da9b0b55b13a620051a9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:00:09,108 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9865\n",
      "2023-07-20 14:00:19,507 - \u001b[1;33mWARNING\u001b[1;0m - Request 9762 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 650f6fc2e1a08dab4dfe698c571fe7c6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:00:20,979 - \u001b[1;33mWARNING\u001b[1;0m - Request 9829 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b24420345aec6060511c6a09ac517785 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:00:33,903 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9913\n",
      "2023-07-20 14:00:58,755 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9963\n",
      "2023-07-20 14:00:59,144 - \u001b[1;33mWARNING\u001b[1;0m - Request 9903 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3c3809c8eb7b9d4c09275af2f878e758 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:00:59,650 - \u001b[1;33mWARNING\u001b[1;0m - Request 9904 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a5388b90c633c8573139442b470378ba in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:01:35,993 - \u001b[1;33mWARNING\u001b[1;0m - Request 9975 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ee9564ad41bae471a347ed63d57a007c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:01:45,274 - \u001b[1;32mINFO\u001b[1;0m - Parallel processing complete. Results saved to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_123908.jsonl\n",
      "2023-07-20 14:01:45,277 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_123908.jsonl\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/utils/openai_api.py\", line 141, in run_common_boilerplate_for_api_requests\n",
      "    parsed_output = parse_openai_output(text)\n",
      "  File \"/home/pamessina/medvqa/medvqa/datasets/notebooks/../../scripts/mimiccxr/paraphrase_observations_with_openai.py\", line 97, in parse_openai_model_output\n",
      "    assert match, f\"Could not parse output: {text}\"\n",
      "AssertionError: Could not parse output: 1. Rapid advancement observed between\n",
      "2. Swift development noted between\n",
      "3. Quick progression seen between\n",
      "4. Speedy growth observed between\n",
      "5. Rapid increase noted between\n",
      "6. Fast advancement seen between\n",
      "7. Swift progression observed between\n",
      "8. Quick development noted between\n",
      "9. Rapid evolution seen between\n",
      "10. Speedy progression observed between\n",
      "11. Rapid change noted between\n",
      "12. Fast growth seen between\n",
      "13. Swift evolution observed between\n",
      "14. Quick increase noted between\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/datasets/notebooks/../../scripts/mimiccxr/paraphrase_observations_with_openai.py\", line 253, in <module>\n",
      "    run_common_boilerplate_for_api_requests(\n",
      "  File \"/home/pamessina/medvqa/medvqa/utils/openai_api.py\", line 150, in run_common_boilerplate_for_api_requests\n",
      "    logger.error(f\"Error parsing response {api_response_string} for sentence \\\"{metadata['sentence']}\\\": {e}\")\n",
      "KeyError: 'sentence'\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "--integrated_fact_metadata_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--preprocessed_sentences_to_skip_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "--precomputed_sentence_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\" \\\n",
    "--precomputed_clusters_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\" \\\n",
    "--num_sentences_per_cluster 500 \\\n",
    "--min_num_words_per_sentence 2 \\\n",
    "--offset 0 \\\n",
    "--num_sentences 100000 \\\n",
    "--process_kth_of_every_n_sentences 0 5 \\\n",
    "--max_requests_per_minute 3000 \\\n",
    "--max_tokens_per_minute 80000 \\\n",
    "--max_tokens_per_request 512 \\\n",
    "--temperature 0.5 \\\n",
    "--logging_level \"INFO\" \\\n",
    "--api_key_name \"OPENAI_API_KEY_1\" \\\n",
    "--openai_model_name \"gpt-3.5-turbo-0613\" \\\n",
    "--alias \"__two-or-more-words_cluster-balanced_1of5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e1281dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 16:21:59,922 - \u001b[1;32mINFO\u001b[1;0m - API responses already exist at /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_123908.jsonl. Skipping API requests.\n",
      "2023-07-20 16:21:59,922 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_123908.jsonl\n",
      "2023-07-20 16:22:00,224 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a fact extracted from a Chest X-ray report, output a JSON array of ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 134, \"completion_tokens\": 99, \"total_tokens\": 233}}, {\"query\": \"rapid progression between\"}] for query \"rapid progression between\": Could not parse output: 1. Rapid advancement observed between\n",
      "2. Swift development noted between\n",
      "3. Quick progression seen between\n",
      "4. Speedy growth observed between\n",
      "5. Rapid increase noted between\n",
      "6. Fast advancement seen between\n",
      "7. Swift progression observed between\n",
      "8. Quick development noted between\n",
      "9. Rapid evolution seen between\n",
      "10. Speedy progression observed between\n",
      "11. Rapid change noted between\n",
      "12. Fast growth seen between\n",
      "13. Swift evolution observed between\n",
      "14. Quick increase noted between\n",
      "2023-07-20 16:22:00,695 - \u001b[1;32mINFO\u001b[1;0m - Succesfully processed 9989 of 9990 API responses.\n",
      "                    1 of 9990 API responses could not be processed.\n",
      "                    Saving processed texts to /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "--api_responses_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_123908.jsonl\" \\\n",
    "--integrated_fact_metadata_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--preprocessed_sentences_to_skip_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "--precomputed_sentence_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\" \\\n",
    "--precomputed_clusters_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\" \\\n",
    "--num_sentences_per_cluster 500 \\\n",
    "--min_num_words_per_sentence 2 \\\n",
    "--offset 0 \\\n",
    "--num_sentences 100000 \\\n",
    "--process_kth_of_every_n_sentences 0 5 \\\n",
    "--max_requests_per_minute 3000 \\\n",
    "--max_tokens_per_minute 80000 \\\n",
    "--max_tokens_per_request 512 \\\n",
    "--temperature 0.5 \\\n",
    "--logging_level \"INFO\" \\\n",
    "--api_key_name \"OPENAI_API_KEY_1\" \\\n",
    "--openai_model_name \"gpt-3.5-turbo-0613\" \\\n",
    "--alias \"__two-or-more-words_cluster-balanced_1of5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923f9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:01:49,187 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14965 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\n",
      "2023-07-20 14:01:49,361 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14971 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\n",
      "2023-07-20 14:01:49,485 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14972 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\n",
      "2023-07-20 14:01:49,653 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14993 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\n",
      "2023-07-20 14:01:49,653 - \u001b[1;32mINFO\u001b[1;0m - Loading facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "100%|███████████████████████████████| 578733/578733 [00:03<00:00, 159519.22it/s]\n",
      "2023-07-20 14:01:58,652 - \u001b[1;32mINFO\u001b[1;0m - Found 711223 unique observations\n",
      "2023-07-20 14:01:58,652 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to those with at least 2 words\n",
      "2023-07-20 14:01:59,376 - \u001b[1;32mINFO\u001b[1;0m - Found 709221 observations with at least 2 words\n",
      "2023-07-20 14:01:59,376 - \u001b[1;32mINFO\u001b[1;0m - Loading clusters from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\n",
      "2023-07-20 14:01:59,451 - \u001b[1;32mINFO\u001b[1;0m - Loading sentence embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\n",
      "2023-07-20 14:02:01,718 - \u001b[1;32mINFO\u001b[1;0m - Found 100 clusters\n",
      "2023-07-20 14:02:03,107 - \u001b[1;32mINFO\u001b[1;0m - Selected 50000 observations from 100 clusters\n",
      "2023-07-20 14:02:03,107 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to the 1-th of every 5 sentences\n",
      "2023-07-20 14:02:03,121 - \u001b[1;32mINFO\u001b[1;0m - Found 10000 observations that are the 1-th of every 5 sentences\n",
      "2023-07-20 14:02:03,121 - \u001b[1;33mWARNING\u001b[1;0m - Requested 100000 sentences but only 10000 are available. Using 10000 instead.\n",
      "2023-07-20 14:02:03,121 - \u001b[1;32mINFO\u001b[1;0m - Collecting the first 10000 sentence from the 0-th sentence\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - Total number of sentences to paraphrase: 10000\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - Example sentences to paraphrase:\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 1. SI P\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 1112. worsening of vein opacity\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 2223. pneumonia could have appearance\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 3334. development of necrotizing pneumonia\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 4445. focus of pneumonia in the left lung base\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 5556. improved pulmonary consolidation on the left\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 6667. pleural effusion in the right costophrenic angle\n",
      "2023-07-20 14:02:03,124 - \u001b[1;32mINFO\u001b[1;0m - 7778. Right Port-A-Cath tip in the SVC right atrial junction\n",
      "2023-07-20 14:02:03,125 - \u001b[1;32mINFO\u001b[1;0m - 8889. demineralization of the vertebral bodies of the thoracic spine\n",
      "2023-07-20 14:02:03,125 - \u001b[1;32mINFO\u001b[1;0m - 10000. slightly worse severe vertebral body compression fractures of the T4 vertebral body compression fractures of the T11 vertebral body compared to the radiograph\n",
      "2023-07-20 14:02:03,662 - \u001b[1;32mINFO\u001b[1;0m - Saving API requests to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_requests_20230720_140203.jsonl\n",
      "2023-07-20 14:02:03,662 - \u001b[1;32mINFO\u001b[1;0m - Saving API responses to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_140203.jsonl\n",
      "2023-07-20 14:02:04,226 - \u001b[1;32mINFO\u001b[1;0m - Starting request #0\n",
      "2023-07-20 14:02:04,391 - \u001b[1;32mINFO\u001b[1;0m - Starting request #50\n",
      "2023-07-20 14:02:04,575 - \u001b[1;32mINFO\u001b[1;0m - Starting request #100\n",
      "2023-07-20 14:02:17,514 - \u001b[1;32mINFO\u001b[1;0m - Starting request #150\n",
      "2023-07-20 14:02:34,745 - \u001b[1;33mWARNING\u001b[1;0m - Request 56 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b3084450304ba69a8da8315180a34cb2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:02:41,804 - \u001b[1;32mINFO\u001b[1;0m - Starting request #199\n",
      "2023-07-20 14:03:06,097 - \u001b[1;32mINFO\u001b[1;0m - Starting request #249\n",
      "2023-07-20 14:03:08,131 - \u001b[1;33mWARNING\u001b[1;0m - Request 191 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6511658e464ff569b7865dcadbb55810 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:03:15,885 - \u001b[1;33mWARNING\u001b[1;0m - Request 207 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 84b565434d733de8071bb835affea11e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:03:30,397 - \u001b[1;32mINFO\u001b[1;0m - Starting request #297\n",
      "2023-07-20 14:03:45,152 - \u001b[1;33mWARNING\u001b[1;0m - Request 264 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b90cf5f9bab63a686f5ea92db35600cf in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:03:54,705 - \u001b[1;32mINFO\u001b[1;0m - Starting request #346\n",
      "2023-07-20 14:04:01,588 - \u001b[1;33mWARNING\u001b[1;0m - Request 299 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 83fd0759de8f3de1a82b91a6de2b8247 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:04:19,005 - \u001b[1;32mINFO\u001b[1;0m - Starting request #395\n",
      "2023-07-20 14:04:24,915 - \u001b[1;33mWARNING\u001b[1;0m - Request 346 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fbf633de1e6b9a3ca369b19af3273727 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:04:31,439 - \u001b[1;33mWARNING\u001b[1;0m - Request 359 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 03cd68258d41a3d2e94b8432b4099514 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:04:43,322 - \u001b[1;32mINFO\u001b[1;0m - Starting request #443\n",
      "2023-07-20 14:05:01,866 - \u001b[1;33mWARNING\u001b[1;0m - Request 420 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4b4dbb5a298fca7b15e78195946146da in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:05:07,638 - \u001b[1;32mINFO\u001b[1;0m - Starting request #492\n",
      "2023-07-20 14:05:31,963 - \u001b[1;32mINFO\u001b[1;0m - Starting request #542\n",
      "2023-07-20 14:05:56,288 - \u001b[1;32mINFO\u001b[1;0m - Starting request #592\n",
      "2023-07-20 14:06:08,504 - \u001b[1;33mWARNING\u001b[1;0m - Request 555 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 63e6e985699efadf6ec95d06bf3b33a8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:06:08,987 - \u001b[1;33mWARNING\u001b[1;0m - Request 556 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8b7df9fc7c68f219851829c7c00c7e76 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:06:20,621 - \u001b[1;32mINFO\u001b[1;0m - Starting request #640\n",
      "2023-07-20 14:06:40,800 - \u001b[1;33mWARNING\u001b[1;0m - Request 551 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d67277835d7573e56c017b439e3d8b13 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:06:44,950 - \u001b[1;32mINFO\u001b[1;0m - Starting request #689\n",
      "2023-07-20 14:07:03,962 - \u001b[1;33mWARNING\u001b[1;0m - Request 667 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 50439febf1f5eb9813be704031896931 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:07:09,289 - \u001b[1;32mINFO\u001b[1;0m - Starting request #738\n",
      "2023-07-20 14:07:12,249 - \u001b[1;33mWARNING\u001b[1;0m - Request 669 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 22f4b07170df02abaa345973491f6939 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:07:18,779 - \u001b[1;33mWARNING\u001b[1;0m - Request 645 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 08ce05a16558ef1d5a93c4ee6407bad3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:07:33,616 - \u001b[1;32mINFO\u001b[1;0m - Starting request #786\n",
      "2023-07-20 14:07:35,113 - \u001b[1;33mWARNING\u001b[1;0m - Request 667 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0fc12346685f0e32e8bfe745f70776c3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:07:56,045 - \u001b[1;33mWARNING\u001b[1;0m - Request 770 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 73f7c1954cce563801246239b247f1f3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:07:57,952 - \u001b[1;32mINFO\u001b[1;0m - Starting request #834\n",
      "2023-07-20 14:08:02,896 - \u001b[1;33mWARNING\u001b[1;0m - Request 784 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9e404ba4745b921433e23c57201617a8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:08:22,284 - \u001b[1;32mINFO\u001b[1;0m - Starting request #883\n",
      "2023-07-20 14:08:27,671 - \u001b[1;33mWARNING\u001b[1;0m - Request 833 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 42ddb6d14f5486c11278f774ef7433fa in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:08:46,624 - \u001b[1;32mINFO\u001b[1;0m - Starting request #932\n",
      "2023-07-20 14:09:10,952 - \u001b[1;32mINFO\u001b[1;0m - Starting request #982\n",
      "2023-07-20 14:09:35,304 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1032\n",
      "2023-07-20 14:09:59,643 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1082\n",
      "2023-07-20 14:10:06,975 - \u001b[1;33mWARNING\u001b[1;0m - Request 1035 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2c9ff3aa889ea429b9b4b07e5f5f9870 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:10:11,921 - \u001b[1;33mWARNING\u001b[1;0m - Request 1045 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e08320a18a8829db3dcffbba117a0e45 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:10:24,013 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1130\n",
      "2023-07-20 14:10:48,378 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1180\n",
      "2023-07-20 14:10:52,788 - \u001b[1;33mWARNING\u001b[1;0m - Request 1127 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 813a056b606b16438458bd78b14f8a46 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:11:12,727 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1229\n",
      "2023-07-20 14:11:35,628 - \u001b[1;33mWARNING\u001b[1;0m - Request 1214 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7abccfe5b69635256e191e01001710f3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:11:36,116 - \u001b[1;33mWARNING\u001b[1;0m - Request 1215 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 449da18239b5c79ee1d4a86a0a995b97 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:11:37,085 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1215\n",
      "2023-07-20 14:11:38,083 - \u001b[1;33mWARNING\u001b[1;0m - Request 1219 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID eafe0a8154d4a0b75985a61ce8e264a7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:11:50,740 - \u001b[1;33mWARNING\u001b[1;0m - Request 1245 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 85772bdde1a295a732ba010a2212bc98 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:12:01,459 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1325\n",
      "2023-07-20 14:12:06,808 - \u001b[1;33mWARNING\u001b[1;0m - Request 1214 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1fa1a05ef2ca67bd17746b251194b9c6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:12:25,824 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1374\n",
      "2023-07-20 14:12:44,343 - \u001b[1;33mWARNING\u001b[1;0m - Request 1350 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a1a14e3b27bf567c524422a7e7e3e43a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:12:50,177 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:12:59,924 - \u001b[1;33mWARNING\u001b[1;0m - Request 1382 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 90102920fa5c267e7719c957ce7707e8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:13:10,276 - \u001b[1;33mWARNING\u001b[1;0m - Request 1403 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 62fcecaae0605228fc7a650432802873 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:13:14,545 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1471\n",
      "2023-07-20 14:13:24,978 - \u001b[1;33mWARNING\u001b[1;0m - Request 1432 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID abb5089c1a7cfef07fae780a5b7650a2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:13:31,088 - \u001b[1;33mWARNING\u001b[1;0m - Request 1382 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b0e2a799545d20ec2e61ad59d89fb2ff in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:13:38,916 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1519\n",
      "2023-07-20 14:13:41,369 - \u001b[1;33mWARNING\u001b[1;0m - Request 1403 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8c0f33fc528c3cc583fbef8f9a046e5a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:13:52,071 - \u001b[1;33mWARNING\u001b[1;0m - Request 1486 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 218c7406844305809ec028187d892da6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:14:03,277 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1567\n",
      "2023-07-20 14:14:03,310 - \u001b[1;33mWARNING\u001b[1;0m - Request 1507 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID accb8771ed68dbe39aefe0d9b787f201 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:14:18,881 - \u001b[1;33mWARNING\u001b[1;0m - Request 1538 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b4de5a44fc2166c1c4bd8cb31561e222 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:14:27,640 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1615\n",
      "2023-07-20 14:14:52,019 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1665\n",
      "2023-07-20 14:15:03,216 - \u001b[1;33mWARNING\u001b[1;0m - Request 1626 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 36892c9e0f0e35f0bb8b6e591fc512c7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:15:16,397 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1714\n",
      "2023-07-20 14:15:34,243 - \u001b[1;33mWARNING\u001b[1;0m - Request 1626 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 88422810108e3416ef1427588d5de0ef in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:15:36,370 - \u001b[1;33mWARNING\u001b[1;0m - Request 1693 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 507acb209689dd543245040cca9b772a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:15:40,773 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1762\n",
      "2023-07-20 14:15:42,921 - \u001b[1;33mWARNING\u001b[1;0m - Request 1706 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 60eb9eb05781eba25e7c3ba7859450fa in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:16:05,140 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1811\n",
      "2023-07-20 14:16:11,113 - \u001b[1;33mWARNING\u001b[1;0m - Request 1762 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7378d3ae9b183380b0a3b9dffeefb04f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:16:27,653 - \u001b[1;33mWARNING\u001b[1;0m - Request 1733 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3973ab51a677c2117daa6fbbbf645af1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:16:29,534 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1859\n",
      "2023-07-20 14:16:47,051 - \u001b[1;33mWARNING\u001b[1;0m - Request 1834 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0ef0ed931cf9f82763a0dde37bc153ae in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:16:53,913 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1908\n",
      "2023-07-20 14:17:18,297 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1958\n",
      "2023-07-20 14:17:18,770 - \u001b[1;33mWARNING\u001b[1;0m - Request 1897 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2b7293d0385f5637b23a9bd2569028fd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:17:30,451 - \u001b[1;33mWARNING\u001b[1;0m - Request 1921 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e9983c74420e559280f593453f9bf59a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:17:42,679 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2006\n",
      "2023-07-20 14:18:07,053 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2056\n",
      "2023-07-20 14:18:31,461 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2106\n",
      "2023-07-20 14:18:55,847 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2156\n",
      "2023-07-20 14:19:20,234 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2206\n",
      "2023-07-20 14:19:29,473 - \u001b[1;33mWARNING\u001b[1;0m - Request 2163 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f76e5ce81f355f129244394832a83504 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:19:44,621 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2255\n",
      "2023-07-20 14:20:05,553 - \u001b[1;33mWARNING\u001b[1;0m - Request 2236 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d9654d1c1f01333731c4e420b2b09765 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:20:09,012 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2304\n",
      "2023-07-20 14:20:13,849 - \u001b[1;33mWARNING\u001b[1;0m - Request 2253 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID da0cf6e830e499ec1c5b65582676c852 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:20:23,957 - \u001b[1;33mWARNING\u001b[1;0m - Request 2272 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b2d021b6c0eb7fc2ba25b1dc6748ca42 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:20:33,417 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2352\n",
      "2023-07-20 14:20:57,818 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2402\n",
      "2023-07-20 14:21:01,188 - \u001b[1;33mWARNING\u001b[1;0m - Request 2347 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1a4ef2149c4494a7bc7de9a44f26b3f2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:21:02,163 - \u001b[1;33mWARNING\u001b[1;0m - Request 2349 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 58ebad50ac97e437ef12b4ace81be7d3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:21:17,289 - \u001b[1;33mWARNING\u001b[1;0m - Request 2380 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e7e24c86389524c163acf730c0fe2bf5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:21:22,223 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2449\n",
      "2023-07-20 14:21:23,626 - \u001b[1;33mWARNING\u001b[1;0m - Request 2393 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 885a02ec971e1a80a212ff58d93b7169 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:21:28,534 - \u001b[1;33mWARNING\u001b[1;0m - Request 2403 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 711aaca67fd9c49faf80315b091e449a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:21:37,337 - \u001b[1;33mWARNING\u001b[1;0m - Request 2419 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5880a141cd45d2b3d66893c509c8e829 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:21:46,614 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2496\n",
      "2023-07-20 14:22:03,173 - \u001b[1;33mWARNING\u001b[1;0m - Request 2469 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 073899b2e59a9a13041e1b35efa7b0e0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:22:11,001 - \u001b[1;33mWARNING\u001b[1;0m - Request 2484 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4c5cc0948062a8c2df78ad6187520e77 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:22:11,033 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2545\n",
      "2023-07-20 14:22:31,478 - \u001b[1;33mWARNING\u001b[1;0m - Request 2526 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 52bf2cf5a7e197bdbb3e96754b268f1b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:22:35,444 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2593\n",
      "2023-07-20 14:22:41,235 - \u001b[1;33mWARNING\u001b[1;0m - Request 2545 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fbb66561d43ea052116b030b007cc409 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:22:52,479 - \u001b[1;33mWARNING\u001b[1;0m - Request 2567 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6a85c47c3b7e9b7bc74f4f1bbf3d5e3f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:22:59,847 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2641\n",
      "2023-07-20 14:23:03,219 - \u001b[1;33mWARNING\u001b[1;0m - Request 2588 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 965182a8290550398ed394e38b47bb79 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:23:03,695 - \u001b[1;33mWARNING\u001b[1;0m - Request 2589 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c37c418ce48d8c76861195e3d53ba28b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:23:16,870 - \u001b[1;33mWARNING\u001b[1;0m - Request 2615 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4af750ed351f6d833a8d2fa18c14516f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:23:20,843 - \u001b[1;33mWARNING\u001b[1;0m - Request 2623 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1f732e140cc29146c240bdb8301d0e4b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:23:24,253 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2687\n",
      "2023-07-20 14:23:40,394 - \u001b[1;33mWARNING\u001b[1;0m - Request 2660 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fcad904a988655554d321cc11d500852 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:23:48,668 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:24:13,082 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2786\n",
      "2023-07-20 14:24:27,165 - \u001b[1;33mWARNING\u001b[1;0m - Request 2753 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f9487e69277cef81055e83a5df93e7cd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:24:35,958 - \u001b[1;33mWARNING\u001b[1;0m - Request 2771 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1781c67521284ccaf9763f606476c7e7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:24:37,496 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2834\n",
      "2023-07-20 14:24:52,593 - \u001b[1;33mWARNING\u001b[1;0m - Request 2805 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 570cfc98447352b40ece4dc000bda5ae in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:24:53,380 - \u001b[1;33mWARNING\u001b[1;0m - Request 2603 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9acc481368d2a2858f49808af237a702 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:24:53,549 - \u001b[1;33mWARNING\u001b[1;0m - Request 2807 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0c0526b7575497ae162202a1478cb6b1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:25:01,902 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2881\n",
      "2023-07-20 14:25:17,258 - \u001b[1;33mWARNING\u001b[1;0m - Request 2853 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4609da168b45bbed524e3554a298c685 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:25:26,318 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2930\n",
      "2023-07-20 14:25:50,722 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2980\n",
      "2023-07-20 14:26:03,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 2411 failed with Exception \n",
      "2023-07-20 14:26:06,854 - \u001b[1;33mWARNING\u001b[1;0m - Request 2951 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8d0c8c56ff8dbfb70a33fb01dd323534 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:26:07,257 - \u001b[1;33mWARNING\u001b[1;0m - Request 2952 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 35a1d046d886714fa86a7c836eb82739 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:26:15,142 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3027\n",
      "2023-07-20 14:26:39,563 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3077\n",
      "2023-07-20 14:26:53,351 - \u001b[1;33mWARNING\u001b[1;0m - Request 2854 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c29bd67f8c24328dd53e4ae6779df736 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:27:01,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 2525 failed with Exception \n",
      "2023-07-20 14:27:03,979 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3125\n",
      "2023-07-20 14:27:09,297 - \u001b[1;33mWARNING\u001b[1;0m - Request 3076 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 435935d5a280ae6623e88647dda53d53 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:27:28,391 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3174\n",
      "2023-07-20 14:27:31,882 - \u001b[1;33mWARNING\u001b[1;0m - Request 3120 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 719767a0555ddd81b424a77d7c92b5e4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:27:35,181 - \u001b[1;33mWARNING\u001b[1;0m - Request 3127 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 35c432da487f8fa1d4926e2cdc33e326 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:27:46,391 - \u001b[1;33mWARNING\u001b[1;0m - Request 3149 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 628db8bd89a5b2259499b1b8d7b855e4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:27:52,791 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3221\n",
      "2023-07-20 14:28:02,026 - \u001b[1;33mWARNING\u001b[1;0m - Request 3181 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5f11ca9ae4e1e7f503dec3c83e6922e0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:28:15,200 - \u001b[1;33mWARNING\u001b[1;0m - Request 3206 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f37c69e3d1e60844de43654518180779 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:28:17,222 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3269\n",
      "2023-07-20 14:28:27,828 - \u001b[1;33mWARNING\u001b[1;0m - Request 3086 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 25173a3aa5a19e0f8018bb9e280a9930 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:28:29,355 - \u001b[1;33mWARNING\u001b[1;0m - Request 3198 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0db1c57590b3bd81943f3804a7f9715e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:28:41,659 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3317\n",
      "2023-07-20 14:28:57,209 - \u001b[1;33mWARNING\u001b[1;0m - Request 3289 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d0187e6a1257f3dd276ae646cc08033b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:29:01,710 - \u001b[1;33mWARNING\u001b[1;0m - Request 3296 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9d6e8f17a45ce5afec9089dca2d67684 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:29:06,079 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3365\n",
      "2023-07-20 14:29:18,708 - \u001b[1;33mWARNING\u001b[1;0m - Request 3331 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c037fa28d137f5ae421bb2eb37ba0969 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:29:22,131 - \u001b[1;33mWARNING\u001b[1;0m - Request 3338 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8d7f83f64b09d3db0978296760e78c7d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:29:30,507 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3413\n",
      "2023-07-20 14:29:37,304 - \u001b[1;33mWARNING\u001b[1;0m - Request 3367 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID edb146610c7cca037e0deba7d0ec5fbe in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:29:50,947 - \u001b[1;33mWARNING\u001b[1;0m - Request 3394 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a38ea1981747ae252fe178f337bcad8a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:29:54,924 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3461\n",
      "2023-07-20 14:30:19,344 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3511\n",
      "2023-07-20 14:30:43,789 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3561\n",
      "2023-07-20 14:30:51,059 - \u001b[1;33mWARNING\u001b[1;0m - Request 3514 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 647712fdfd0f654f0f72849630250b92 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:31:08,222 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3610\n",
      "2023-07-20 14:31:13,538 - \u001b[1;33mWARNING\u001b[1;0m - Request 3560 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ee21583a9477526cc910e1272374a6c3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:31:26,559 - \u001b[1;33mWARNING\u001b[1;0m - Request 3581 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5b3b23ebf98c968ddfbed80fc8e31021 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:31:28,658 - \u001b[1;33mWARNING\u001b[1;0m - Request 3590 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d4db36bedd1c097d0e765f703f3093f0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:31:32,651 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3657\n",
      "2023-07-20 14:31:57,089 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3707\n",
      "2023-07-20 14:32:16,064 - \u001b[1;33mWARNING\u001b[1;0m - Request 3684 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 015df983bcdd1ad44997ec30ef4aaa8f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:32:21,514 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3756\n",
      "2023-07-20 14:32:21,956 - \u001b[1;33mWARNING\u001b[1;0m - Request 3696 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3a3aec3f84106f432477a2f2900e2f4d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:32:45,955 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3805\n",
      "2023-07-20 14:32:48,059 - \u001b[1;33mWARNING\u001b[1;0m - Request 3748 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8b6515ea065178a432dd2f9b96a3e790 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:00,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 3236 failed with Exception \n",
      "2023-07-20 14:33:07,859 - \u001b[1;33mWARNING\u001b[1;0m - Request 3788 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f5ca60c04a26d4dc9ad639bed262c3fe in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:09,563 - \u001b[1;33mWARNING\u001b[1;0m - Request 3790 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6c90c0ce0d57af7b9dac1645904f9cec in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:10,406 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3790\n",
      "2023-07-20 14:33:10,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 3256 failed with Exception \n",
      "2023-07-20 14:33:21,064 - \u001b[1;33mWARNING\u001b[1;0m - Request 3814 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 75ed38ab13286a9ebbfe580a687a7729 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:30,841 - \u001b[1;33mWARNING\u001b[1;0m - Request 3834 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 320190ae4618dcfe73080ea0f858cc52 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:34,851 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3898\n",
      "2023-07-20 14:33:45,020 - \u001b[1;33mWARNING\u001b[1;0m - Request 3859 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 867d1d5d04f7e514786d5743fd669419 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:48,918 - \u001b[1;33mWARNING\u001b[1;0m - Request 3867 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 66ce27ce717d23ddbfea7498b5d8e4ce in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:57,717 - \u001b[1;33mWARNING\u001b[1;0m - Request 3884 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f39062d821a2e4184de7cc1081aeca2e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:33:59,295 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3945\n",
      "2023-07-20 14:34:23,741 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:34:24,659 - \u001b[1;33mWARNING\u001b[1;0m - Request 3936 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8d04d9c7c1def67cafd98f3e12a94e2a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:34:48,174 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4044\n",
      "2023-07-20 14:34:52,975 - \u001b[1;33mWARNING\u001b[1;0m - Request 3993 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2690ee17f1f280b1641fc85376234147 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:35:12,621 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4093\n",
      "2023-07-20 14:35:37,079 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4143\n",
      "2023-07-20 14:35:40,403 - \u001b[1;33mWARNING\u001b[1;0m - Request 4088 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 48ac5f9b704c5ed77a8cc04cb2fced8f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:36:01,537 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4192\n",
      "2023-07-20 14:36:15,616 - \u001b[1;33mWARNING\u001b[1;0m - Request 4159 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c869afd6b5ce1f5bbc634f79e652618a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:36:22,945 - \u001b[1;33mWARNING\u001b[1;0m - Request 4174 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7be1fdf9ab995b7eec33a08f77565958 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:36:25,982 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4240\n",
      "2023-07-20 14:36:50,427 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4290\n",
      "2023-07-20 14:37:14,871 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4340\n",
      "2023-07-20 14:37:21,130 - \u001b[1;33mWARNING\u001b[1;0m - Request 4291 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cc217ef060f41b869f5695b52acb78e4 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:37:39,333 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4389\n",
      "2023-07-20 14:38:03,789 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4439\n",
      "2023-07-20 14:38:28,241 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4489\n",
      "2023-07-20 14:38:52,705 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4539\n",
      "2023-07-20 14:39:17,163 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4589\n",
      "2023-07-20 14:39:41,637 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4639\n",
      "2023-07-20 14:39:56,200 - \u001b[1;33mWARNING\u001b[1;0m - Request 4607 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f941671df466ddd0b6623982126a24b7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:40:06,103 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4688\n",
      "2023-07-20 14:40:20,393 - \u001b[1;33mWARNING\u001b[1;0m - Request 4519 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 01f0228a6a18926527465f809367414f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:40:24,576 - \u001b[1;33mWARNING\u001b[1;0m - Request 4665 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 14ac49918d795f1197235c4ad472870b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:40:30,564 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4736\n",
      "2023-07-20 14:40:55,021 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4786\n",
      "2023-07-20 14:41:17,164 - \u001b[1;33mWARNING\u001b[1;0m - Request 4769 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 071249bf04644afcdb620b77121fa7df in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:41:19,474 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4835\n",
      "2023-07-20 14:41:43,925 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4885\n",
      "2023-07-20 14:42:08,397 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4935\n",
      "2023-07-20 14:42:32,871 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4985\n",
      "2023-07-20 14:42:44,988 - \u001b[1;33mWARNING\u001b[1;0m - Request 4948 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5ec5232cea739b1bfd9afd5cbca9a188 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:42:50,349 - \u001b[1;33mWARNING\u001b[1;0m - Request 4959 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a65a64fece524d584d6bcc15e048662f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:42:57,356 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5033\n",
      "2023-07-20 14:43:02,117 - \u001b[1;33mWARNING\u001b[1;0m - Request 4983 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c1f935511b3e7895d110133aeb172c3c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:43:11,456 - \u001b[1;33mWARNING\u001b[1;0m - Request 5000 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b0e35014801ef4305eebd2a6b4673a82 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:43:21,820 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5081\n",
      "2023-07-20 14:43:29,030 - \u001b[1;33mWARNING\u001b[1;0m - Request 5036 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 51be1238f8690862248c42dcdbcc5807 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:43:46,271 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5130\n",
      "2023-07-20 14:44:10,743 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5180\n",
      "2023-07-20 14:44:35,239 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5230\n",
      "2023-07-20 14:44:59,731 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5280\n",
      "2023-07-20 14:45:24,198 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5330\n",
      "2023-07-20 14:45:39,243 - \u001b[1;33mWARNING\u001b[1;0m - Request 5299 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 19107742ca4c97edeafd1d053ebb08eb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:45:48,655 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:46:13,135 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5429\n",
      "2023-07-20 14:46:37,613 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5479\n",
      "2023-07-20 14:47:02,103 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5529\n",
      "2023-07-20 14:47:26,595 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5579\n",
      "2023-07-20 14:47:51,077 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5629\n",
      "2023-07-20 14:48:15,559 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5679\n",
      "2023-07-20 14:48:32,593 - \u001b[1;33mWARNING\u001b[1;0m - Request 5652 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7257010d0d3f372681c366b1014cff99 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:48:40,039 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5728\n",
      "2023-07-20 14:49:04,549 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5778\n",
      "2023-07-20 14:49:29,047 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5828\n",
      "2023-07-20 14:49:53,545 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5878\n",
      "2023-07-20 14:50:18,031 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5928\n",
      "2023-07-20 14:50:35,515 - \u001b[1;33mWARNING\u001b[1;0m - Request 5902 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 76c5fbbad3955622883f93e11b2bfc76 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:50:42,512 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5977\n",
      "2023-07-20 14:50:44,391 - \u001b[1;33mWARNING\u001b[1;0m - Request 5859 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6e988390d7fcec0065bfaa6cd65ecae0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:51:07,027 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6026\n",
      "2023-07-20 14:51:08,827 - \u001b[1;33mWARNING\u001b[1;0m - Request 5969 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7acc8628e4fcc6a87309fae51929a5e1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:51:15,165 - \u001b[1;33mWARNING\u001b[1;0m - Request 5859 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dcbc671f14ad6f3a66704f6a8d04862c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:51:31,521 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6074\n",
      "2023-07-20 14:51:32,829 - \u001b[1;33mWARNING\u001b[1;0m - Request 6017 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0edf13d79e43ea6b035ac24801ae0283 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:51:37,225 - \u001b[1;33mWARNING\u001b[1;0m - Request 6026 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b67f5a720bedb3b4e23e9d6b4909f53f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:51:40,194 - \u001b[1;33mWARNING\u001b[1;0m - Request 6031 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c3d295161ca3222546676740c61f4645 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:51:56,013 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6121\n",
      "2023-07-20 14:52:20,509 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6171\n",
      "2023-07-20 14:52:44,999 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6221\n",
      "2023-07-20 14:53:07,954 - \u001b[1;33mWARNING\u001b[1;0m - Request 6206 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dde3c5dabe07b3e96641bc8d07e05ae3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:53:09,512 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6270\n",
      "2023-07-20 14:53:27,496 - \u001b[1;33mWARNING\u001b[1;0m - Request 6246 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6380edc1cd36d737f6a4ce341cf17a5a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:53:32,373 - \u001b[1;33mWARNING\u001b[1;0m - Request 6256 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6404b09719cff3e1e86642e934b2f07 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:53:34,027 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6318\n",
      "2023-07-20 14:53:44,150 - \u001b[1;33mWARNING\u001b[1;0m - Request 6279 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ae027fa1f976fe430ae6976aef203521 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:53:44,621 - \u001b[1;33mWARNING\u001b[1;0m - Request 6280 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7c4bbd02f4a2c0ff2f6f2f38834cbd75 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:53:55,399 - \u001b[1;33mWARNING\u001b[1;0m - Request 6302 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ca08d2358c544528d7a219d4b7e80199 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:53:58,521 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6365\n",
      "2023-07-20 14:53:59,219 - \u001b[1;33mWARNING\u001b[1;0m - Request 6354 failed with error {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}\n",
      "2023-07-20 14:54:07,670 - \u001b[1;33mWARNING\u001b[1;0m - Request 6325 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 16a56b89197570d684d3f4b64282c829 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:54:09,620 - \u001b[1;33mWARNING\u001b[1;0m - Request 6329 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1ac91e43fb12a667b68b7081f9993db8 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:54:23,020 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6412\n",
      "2023-07-20 14:54:24,816 - \u001b[1;33mWARNING\u001b[1;0m - Request 6358 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e65d3812039e5beccf0ed14bc4297e1f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:54:47,516 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6461\n",
      "2023-07-20 14:54:59,663 - \u001b[1;33mWARNING\u001b[1;0m - Request 6424 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 884ba057cb852bf6f75c621edb0be081 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:55:05,489 - \u001b[1;33mWARNING\u001b[1;0m - Request 6436 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 60c848f0a888fb050e67814885437260 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:55:12,026 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6509\n",
      "2023-07-20 14:55:20,176 - \u001b[1;33mWARNING\u001b[1;0m - Request 6466 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9e1275383414d6fc1d1f094cbe1abd01 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:55:36,540 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6558\n",
      "2023-07-20 14:56:01,050 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6608\n",
      "2023-07-20 14:56:25,553 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6658\n",
      "2023-07-20 14:56:43,509 - \u001b[1;33mWARNING\u001b[1;0m - Request 6633 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 771b6586e00b84ea751e5b63c537a68b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:56:50,075 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6707\n",
      "2023-07-20 14:57:14,591 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6757\n",
      "2023-07-20 14:57:32,066 - \u001b[1;33mWARNING\u001b[1;0m - Request 6731 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 622fcb7be15baf99aaf55bc6361aeb78 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:57:39,104 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6806\n",
      "2023-07-20 14:58:00,986 - \u001b[1;33mWARNING\u001b[1;0m - Request 6790 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ec767e061b6cb21ac7e2db527d6ca73d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:58:01,791 - \u001b[1;33mWARNING\u001b[1;0m - Request 6753 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c79fd6062e8889917a2fdab137e8ba80 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:58:03,620 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6854\n",
      "2023-07-20 14:58:28,121 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6904\n",
      "2023-07-20 14:58:52,652 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6954\n",
      "2023-07-20 14:58:58,360 - \u001b[1;33mWARNING\u001b[1;0m - Request 6904 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4159731dcbfef9e11fdfbf0e74fae53e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:59:04,212 - \u001b[1;33mWARNING\u001b[1;0m - Request 6916 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 562b2a0e091f87d7015a2f2f2cf168d7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 14:59:17,209 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7002\n",
      "2023-07-20 14:59:41,731 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7052\n",
      "2023-07-20 15:00:06,238 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7102\n",
      "2023-07-20 15:00:21,269 - \u001b[1;33mWARNING\u001b[1;0m - Request 7071 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f611b90b93fe5db6bb46369b58bbbe3a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:00:30,746 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7151\n",
      "2023-07-20 15:00:46,272 - \u001b[1;33mWARNING\u001b[1;0m - Request 7122 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ba0be05e3e18f63e67bc81c88f75a70f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:00:55,288 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7200\n",
      "2023-07-20 15:01:01,450 - \u001b[1;33mWARNING\u001b[1;0m - Request 7152 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0dd1399b8e24252209e38855e84d629a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:01:19,817 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7249\n",
      "2023-07-20 15:01:20,603 - \u001b[1;33mWARNING\u001b[1;0m - Request 7190 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f1313cd49695b858c6a7fd3422b26e57 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:01:44,345 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7298\n",
      "2023-07-20 15:02:08,862 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7348\n",
      "2023-07-20 15:02:28,294 - \u001b[1;33mWARNING\u001b[1;0m - Request 7326 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 264b8bc14e6a1071a83dbbcdcd101da7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:02:33,390 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7397\n",
      "2023-07-20 15:02:45,469 - \u001b[1;33mWARNING\u001b[1;0m - Request 7361 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d3738b2388ddb4b7811377eefe8051fd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:02:57,928 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7446\n",
      "2023-07-20 15:03:06,545 - \u001b[1;33mWARNING\u001b[1;0m - Request 7403 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 241ceeb6dbf6cc234a2ea11210377c10 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:03:16,877 - \u001b[1;33mWARNING\u001b[1;0m - Request 7423 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8d071b507410ef42afa92dc46d1997f3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:03:22,459 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 15:03:28,640 - \u001b[1;33mWARNING\u001b[1;0m - Request 7447 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ea0e5ad206556b62bb5a8d248fe158b3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:03:40,901 - \u001b[1;33mWARNING\u001b[1;0m - Request 7471 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a6f68b6d932e916959f0170953e43f09 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:03:46,976 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7542\n",
      "2023-07-20 15:04:11,531 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7592\n",
      "2023-07-20 15:04:26,613 - \u001b[1;33mWARNING\u001b[1;0m - Request 7561 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2ccc3c73a68624cf14542e6c464dc79b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:04:35,863 - \u001b[1;33mWARNING\u001b[1;0m - Request 7580 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f0652bbbcaa3d91321b5cb1b7d6fd5f2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:04:36,070 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7641\n",
      "2023-07-20 15:04:50,100 - \u001b[1;33mWARNING\u001b[1;0m - Request 7609 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 32b9275ae8996c1f277ed4a0b266d0fd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:04:54,997 - \u001b[1;33mWARNING\u001b[1;0m - Request 7619 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bc01d409b213421c2755ecc476510a7f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:05:00,616 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7688\n",
      "2023-07-20 15:05:08,772 - \u001b[1;33mWARNING\u001b[1;0m - Request 7645 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3badf0db73489ed6e668c67b5abc3ebb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:05:25,155 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7737\n",
      "2023-07-20 15:05:26,509 - \u001b[1;33mWARNING\u001b[1;0m - Request 7679 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1e693a94d2e861f54c17c18e106ee355 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:05:27,394 - \u001b[1;33mWARNING\u001b[1;0m - Request 7681 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f29643ff9b15308f5997a395d926c004 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:05:49,720 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7785\n",
      "2023-07-20 15:06:01,767 - \u001b[1;33mWARNING\u001b[1;0m - Request 7748 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c349412956e103ffb3ab7b26c12d5f6c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:06:14,259 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7834\n",
      "2023-07-20 15:06:38,813 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7884\n",
      "2023-07-20 15:07:03,148 - \u001b[1;33mWARNING\u001b[1;0m - Request 7872 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 459b5ea06cbd8a4be45b0f9131b7769b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:07:03,358 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7934\n",
      "2023-07-20 15:07:23,372 - \u001b[1;33mWARNING\u001b[1;0m - Request 7913 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 87fcc759f18f305275588ff1b14dad3e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:07:25,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7380 failed with Exception \n",
      "2023-07-20 15:07:27,946 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7981\n",
      "2023-07-20 15:07:42,417 - \u001b[1;33mWARNING\u001b[1;0m - Request 7951 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f8a90d757e0d3d7e44689deea5a715a9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:07:49,884 - \u001b[1;33mWARNING\u001b[1;0m - Request 7966 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 26b42fb93795aafb7699fd8fc8364243 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:07:52,513 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8029\n",
      "2023-07-20 15:08:17,062 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8079\n",
      "2023-07-20 15:08:41,643 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8129\n",
      "2023-07-20 15:09:04,488 - \u001b[1;33mWARNING\u001b[1;0m - Request 8114 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID eb3c5260cd7374469e24862341736639 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:09:06,218 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8178\n",
      "2023-07-20 15:09:13,334 - \u001b[1;33mWARNING\u001b[1;0m - Request 8132 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID adde9653c2115bc00dd5fcb9801eebc3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:09:24,853 - \u001b[1;33mWARNING\u001b[1;0m - Request 8155 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2f6c56ce16d26ed7d5b8e0e2235d6b83 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:09:25,130 - \u001b[1;33mWARNING\u001b[1;0m - Request 8156 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bb152dc8085c535b1ff93f5cf15bb5fb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:09:30,781 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 15:09:55,359 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8275\n",
      "2023-07-20 15:10:02,956 - \u001b[1;33mWARNING\u001b[1;0m - Request 8229 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 41c9d973a2b7c0532a3a7a44ee90ff2e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:10:19,938 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8324\n",
      "2023-07-20 15:10:42,521 - \u001b[1;33mWARNING\u001b[1;0m - Request 8308 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 933dde4937b8f087b433d1394b513a3f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:10:44,508 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8373\n",
      "2023-07-20 15:10:56,535 - \u001b[1;33mWARNING\u001b[1;0m - Request 8337 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 353805f3282da81cf5bb3348ed2d04db in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:11:03,925 - \u001b[1;33mWARNING\u001b[1;0m - Request 8352 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 827c42aaefc90c82c75486bbee92133a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:11:09,093 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8421\n",
      "2023-07-20 15:11:33,678 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8471\n",
      "2023-07-20 15:11:40,769 - \u001b[1;33mWARNING\u001b[1;0m - Request 8424 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b50a1a0192d88fea9eddf087e56dc86d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:11:50,557 - \u001b[1;33mWARNING\u001b[1;0m - Request 8289 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 162b9edffbb65c2f273e63c3c8cfed73 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:11:58,273 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8519\n",
      "2023-07-20 15:12:03,046 - \u001b[1;33mWARNING\u001b[1;0m - Request 8350 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 68a0114b29c8c8fda4ddf54b77093d36 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:12:22,868 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8568\n",
      "2023-07-20 15:12:27,509 - \u001b[1;33mWARNING\u001b[1;0m - Request 8517 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 024252b11ae42dd85b82753469f737dd in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:12:35,873 - \u001b[1;33mWARNING\u001b[1;0m - Request 8533 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 25544c661d78365ddd3c996352063728 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:12:47,450 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8616\n",
      "2023-07-20 15:13:12,041 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8666\n",
      "2023-07-20 15:13:34,375 - \u001b[1;33mWARNING\u001b[1;0m - Request 8650 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ac463867b8470cee7a6f0eee1f06e22f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:13:36,650 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8715\n",
      "2023-07-20 15:13:43,229 - \u001b[1;33mWARNING\u001b[1;0m - Request 8668 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6eb53ecf8f82d5642f7ce272081846e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:14:01,238 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8764\n",
      "2023-07-20 15:14:25,837 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8814\n",
      "2023-07-20 15:14:29,961 - \u001b[1;33mWARNING\u001b[1;0m - Request 8761 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 38875fd8f5bab26895b6b64a160d3799 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:14:44,238 - \u001b[1;33mWARNING\u001b[1;0m - Request 8790 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5a4905d71ff81fe41696fc5d51c9b0e3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:14:46,722 - \u001b[1;33mWARNING\u001b[1;0m - Request 8795 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7a61265990af49b90ed54caef7a20c40 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:14:50,429 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8861\n",
      "2023-07-20 15:14:57,020 - \u001b[1;33mWARNING\u001b[1;0m - Request 8816 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3fe77ff3aa933ffc16a671741a6c893e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:15:15,046 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8910\n",
      "2023-07-20 15:15:37,140 - \u001b[1;33mWARNING\u001b[1;0m - Request 8893 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 939fc3be99672f1fb75b5f3451babd58 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:15:39,661 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8959\n",
      "2023-07-20 15:16:04,301 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9009\n",
      "2023-07-20 15:16:28,927 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9059\n",
      "2023-07-20 15:16:53,537 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9109\n",
      "2023-07-20 15:17:18,185 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9159\n",
      "2023-07-20 15:17:34,099 - \u001b[1;33mWARNING\u001b[1;0m - Request 9130 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7ddc1c09028264d4e24370e1fe12d70e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:17:42,822 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9208\n",
      "2023-07-20 15:18:07,451 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9258\n",
      "2023-07-20 15:18:32,082 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 15:18:50,512 - \u001b[1;33mWARNING\u001b[1;0m - Request 9284 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e1ccc54f2a5406e4300a0b9d3457b0bb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:18:56,728 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9357\n",
      "2023-07-20 15:19:21,371 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9407\n",
      "2023-07-20 15:19:35,358 - \u001b[1;33mWARNING\u001b[1;0m - Request 9374 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fb7f438bc9137c9f8f68962dcd01d94a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:19:38,281 - \u001b[1;33mWARNING\u001b[1;0m - Request 9380 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 80505533d275b53259dddb33d43298fc in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:19:46,021 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9455\n",
      "2023-07-20 15:19:56,524 - \u001b[1;33mWARNING\u001b[1;0m - Request 9417 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7c9e921a72638050cdb4d5cc3eebb125 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:20:10,695 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9504\n",
      "2023-07-20 15:20:16,236 - \u001b[1;33mWARNING\u001b[1;0m - Request 9455 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8add814094e356236cf2c95253843732 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:20:31,052 - \u001b[1;33mWARNING\u001b[1;0m - Request 9484 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6825439b32e53e461a409ad537490f05 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:20:34,001 - \u001b[1;33mWARNING\u001b[1;0m - Request 9490 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9509393479d96de710aeb6839f69d341 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:20:35,367 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9551\n",
      "2023-07-20 15:21:00,042 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9601\n",
      "2023-07-20 15:21:24,737 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9651\n",
      "2023-07-20 15:21:30,760 - \u001b[1;33mWARNING\u001b[1;0m - Request 9602 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 565355f82b920e24feb0f1cf1f42f2de in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:21:49,445 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9700\n",
      "2023-07-20 15:21:54,774 - \u001b[1;33mWARNING\u001b[1;0m - Request 9650 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c654a393181fdafdaa32a0541151e2e6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:22:14,153 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9749\n",
      "2023-07-20 15:22:38,903 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9799\n",
      "2023-07-20 15:23:03,646 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9849\n",
      "2023-07-20 15:23:04,165 - \u001b[1;33mWARNING\u001b[1;0m - Request 9789 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 407a46b3c7a8f0bd666e7f8bd05a6c36 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:23:28,410 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9898\n",
      "2023-07-20 15:23:47,823 - \u001b[1;33mWARNING\u001b[1;0m - Request 9604 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6739421460750703d1387728fb8aecc in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:23:53,200 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9947\n",
      "2023-07-20 15:24:04,615 - \u001b[1;33mWARNING\u001b[1;0m - Request 9910 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7c36882499f286392f7a4848754fa790 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:24:16,641 - \u001b[1;33mWARNING\u001b[1;0m - Request 9756 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 35e3ead031781f99b05647137d41fcab in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:24:18,132 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9995\n",
      "2023-07-20 15:24:44,837 - \u001b[1;33mWARNING\u001b[1;0m - Request 9989 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a24ecb002d57bebb7f05575cf20cf169 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-20 15:24:57,371 - \u001b[1;32mINFO\u001b[1;0m - Parallel processing complete. Results saved to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_140203.jsonl\n",
      "2023-07-20 15:24:57,375 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_140203.jsonl\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/utils/openai_api.py\", line 141, in run_common_boilerplate_for_api_requests\n",
      "    jobs = None\n",
      "  File \"/home/pamessina/medvqa/medvqa/datasets/notebooks/../../scripts/mimiccxr/paraphrase_observations_with_openai.py\", line 97, in parse_openai_model_output\n",
      "    assert match, f\"Could not parse output: {text}\"\n",
      "AssertionError: Could not parse output: The finding is more clearly visible.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/datasets/notebooks/../../scripts/mimiccxr/paraphrase_observations_with_openai.py\", line 253, in <module>\n",
      "    logger.info(f\"{i+1}. {sentences_to_paraphrase[i]}\")\n",
      "  File \"/home/pamessina/medvqa/medvqa/utils/openai_api.py\", line 150, in run_common_boilerplate_for_api_requests\n",
      "    for i in range(len(api_responses)):\n",
      "KeyError: 'sentence'\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "--integrated_fact_metadata_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--preprocessed_sentences_to_skip_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "--precomputed_sentence_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\" \\\n",
    "--precomputed_clusters_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\" \\\n",
    "--num_sentences_per_cluster 500 \\\n",
    "--min_num_words_per_sentence 2 \\\n",
    "--offset 0 \\\n",
    "--num_sentences 100000 \\\n",
    "--process_kth_of_every_n_sentences 1 5 \\\n",
    "--max_requests_per_minute 3000 \\\n",
    "--max_tokens_per_minute 80000 \\\n",
    "--max_tokens_per_request 512 \\\n",
    "--temperature 0.5 \\\n",
    "--logging_level \"INFO\" \\\n",
    "--api_key_name \"OPENAI_API_KEY_1\" \\\n",
    "--openai_model_name \"gpt-3.5-turbo-0613\" \\\n",
    "--alias \"__two-or-more-words_cluster-balanced_2of5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399b45f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 16:24:11,595 - \u001b[1;32mINFO\u001b[1;0m - API responses already exist at /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_140203.jsonl. Skipping API requests.\n",
      "2023-07-20 16:24:11,595 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_140203.jsonl\n",
      "2023-07-20 16:24:11,855 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a fact extracted from a Chest X-ray report, output a JSON array of ...learly visible.\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 133, \"completion_tokens\": 7, \"total_tokens\": 140}}, {\"query\": \"better seen\"}] for query \"better seen\": Could not parse output: The finding is more clearly visible.\n",
      "2023-07-20 16:24:11,865 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm here to help! What is your doubt in diagnosis? Please provide me with the necessary information so that I can assist you better.\n",
      "2023-07-20 16:24:11,865 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a fact extracted from a Chest X-ray report, output a JSON array of ...better.\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 136, \"completion_tokens\": 28, \"total_tokens\": 164}}, {\"query\": \"doubt in diagnosis\"}] for query \"doubt in diagnosis\": Could not parse output: I'm here to help! What is your doubt in diagnosis? Please provide me with the necessary information so that I can assist you better.\n",
      "2023-07-20 16:24:12,052 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a fact extracted from a Chest X-ray report, output a JSON array of ...ason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 137, \"completion_tokens\": 7, \"total_tokens\": 144}}, {\"query\": \"chronic interstitial process potentially\"}] for query \"chronic interstitial process potentially\": Could not parse output: leading to fibrosis in the lungs\n",
      "2023-07-20 16:24:12,366 - \u001b[1;32mINFO\u001b[1;0m - Succesfully processed 9997 of 10000 API responses.\n",
      "                    3 of 10000 API responses could not be processed.\n",
      "                    Saving processed texts to /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "--api_responses_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_140203.jsonl\" \\\n",
    "--integrated_fact_metadata_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--preprocessed_sentences_to_skip_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "--precomputed_sentence_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\" \\\n",
    "--precomputed_clusters_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\" \\\n",
    "--num_sentences_per_cluster 500 \\\n",
    "--min_num_words_per_sentence 2 \\\n",
    "--offset 0 \\\n",
    "--num_sentences 100000 \\\n",
    "--process_kth_of_every_n_sentences 1 5 \\\n",
    "--max_requests_per_minute 3000 \\\n",
    "--max_tokens_per_minute 80000 \\\n",
    "--max_tokens_per_request 512 \\\n",
    "--temperature 0.5 \\\n",
    "--logging_level \"INFO\" \\\n",
    "--api_key_name \"OPENAI_API_KEY_1\" \\\n",
    "--openai_model_name \"gpt-3.5-turbo-0613\" \\\n",
    "--alias \"__two-or-more-words_cluster-balanced_2of5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa1ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.files import load_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e16fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = load_jsonl('/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc5770f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "300069fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'query': 'fusion plate'},\n",
       " 'parsed_response': ['The fusion plate was observed in the X-ray image',\n",
       "  'A plate used for fusion was detected in the chest X-ray',\n",
       "  'A fusion plate was visible on the radiograph',\n",
       "  'The X-ray showed the presence of a fusion plate',\n",
       "  'A plate for fusion was noted in the chest X-ray report',\n",
       "  'A fusion plate was identified in the radiographic examination',\n",
       "  'The X-ray image revealed the presence of a fusion plate',\n",
       "  'A plate used for fusion was seen in the chest X-ray',\n",
       "  'A fusion plate was detected in the radiograph',\n",
       "  'The X-ray report indicated the presence of a fusion plate',\n",
       "  'A plate for fusion was observed in the chest X-ray image',\n",
       "  'A fusion plate was noted in the radiographic findings',\n",
       "  'The X-ray showed the presence of a plate used for fusion',\n",
       "  'A fusion plate was identified in the chest X-ray report']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[102]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
