{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab94131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:45:14,113 - \u001b[1;32mINFO\u001b[1;0m - Loading facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "100%|███████████████████████████████| 578733/578733 [00:03<00:00, 177549.63it/s]\n",
      "2023-07-12 17:45:22,763 - \u001b[1;32mINFO\u001b[1;0m - Found 771124 unique observations\n",
      "2023-07-12 17:45:25,183 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to those with at least 2 words\n",
      "2023-07-12 17:45:25,924 - \u001b[1;32mINFO\u001b[1;0m - Found 769122 observations with at least 2 words\n",
      "2023-07-12 17:45:25,925 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to the 3-th of every 4 sentences\n",
      "2023-07-12 17:45:26,157 - \u001b[1;32mINFO\u001b[1;0m - Found 192280 observations that are the 3-th of every 4 sentences\n",
      "2023-07-12 17:45:26,157 - \u001b[1;32mINFO\u001b[1;0m - Sampling 15000 equally spaced sentences\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - Total number of sentences to paraphrase: 15000\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - Example sentences to paraphrase:\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 1. SI P\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 1667. likely an enlarged thyroid\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 3334. chronic ascending thoracic aorta\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 5000. patient receiving a nasogastric tube\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 6667. resolved consolidation at the lung bases\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 8333. small loculated right-sided pleural effusion\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 10000. increased opacity projecting over the middle lobe\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 11666. similar appearance of bilateral interstitial opacities\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 13333. contour prominence of the left ventricle in posterior direction\n",
      "2023-07-12 17:45:26,166 - \u001b[1;32mINFO\u001b[1;0m - 15000. possible pulmonary infiltrate on the lower descending thoracic aorta on the left base in retrocardiac position on the lower descending thoracic aorta left base in retrocardiac position\n",
      "2023-07-12 17:45:26,218 - \u001b[1;32mINFO\u001b[1;0m - Saving API requests to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_requests_20230712_174526.jsonl\n",
      "2023-07-12 17:45:26,218 - \u001b[1;32mINFO\u001b[1;0m - Saving API responses to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174526.jsonl\n",
      "2023-07-12 17:45:27,235 - \u001b[1;32mINFO\u001b[1;0m - Starting request #0\n",
      "2023-07-12 17:45:27,373 - \u001b[1;32mINFO\u001b[1;0m - Starting request #50\n",
      "2023-07-12 17:45:27,560 - \u001b[1;32mINFO\u001b[1;0m - Starting request #100\n",
      "2023-07-12 17:45:27,698 - \u001b[1;32mINFO\u001b[1;0m - Starting request #150\n",
      "2023-07-12 17:45:27,826 - \u001b[1;32mINFO\u001b[1;0m - Starting request #200\n",
      "2023-07-12 17:45:27,980 - \u001b[1;32mINFO\u001b[1;0m - Starting request #250\n",
      "2023-07-12 17:45:29,216 - \u001b[1;32mINFO\u001b[1;0m - Starting request #300\n",
      "2023-07-12 17:45:39,522 - \u001b[1;32mINFO\u001b[1;0m - Starting request #350\n",
      "2023-07-12 17:45:49,825 - \u001b[1;32mINFO\u001b[1;0m - Starting request #400\n",
      "2023-07-12 17:46:00,134 - \u001b[1;32mINFO\u001b[1;0m - Starting request #450\n",
      "2023-07-12 17:46:10,446 - \u001b[1;32mINFO\u001b[1;0m - Starting request #500\n",
      "2023-07-12 17:46:20,754 - \u001b[1;32mINFO\u001b[1;0m - Starting request #550\n",
      "2023-07-12 17:46:31,066 - \u001b[1;32mINFO\u001b[1;0m - Starting request #600\n",
      "2023-07-12 17:46:41,375 - \u001b[1;32mINFO\u001b[1;0m - Starting request #650\n",
      "2023-07-12 17:46:51,690 - \u001b[1;32mINFO\u001b[1;0m - Starting request #700\n",
      "2023-07-12 17:47:02,013 - \u001b[1;32mINFO\u001b[1;0m - Starting request #750\n",
      "2023-07-12 17:47:12,328 - \u001b[1;32mINFO\u001b[1;0m - Starting request #800\n",
      "2023-07-12 17:47:22,645 - \u001b[1;32mINFO\u001b[1;0m - Starting request #850\n",
      "2023-07-12 17:47:32,967 - \u001b[1;32mINFO\u001b[1;0m - Starting request #900\n",
      "2023-07-12 17:47:43,283 - \u001b[1;32mINFO\u001b[1;0m - Starting request #950\n",
      "2023-07-12 17:47:53,608 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1000\n",
      "2023-07-12 17:48:03,934 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1050\n",
      "2023-07-12 17:48:14,252 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1100\n",
      "2023-07-12 17:48:24,578 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1150\n",
      "2023-07-12 17:48:34,910 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1200\n",
      "2023-07-12 17:48:45,233 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1250\n",
      "2023-07-12 17:48:55,556 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1300\n",
      "2023-07-12 17:49:05,886 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1350\n",
      "2023-07-12 17:49:16,221 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1400\n",
      "2023-07-12 17:49:26,550 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1450\n",
      "2023-07-12 17:49:36,874 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1500\n",
      "2023-07-12 17:49:47,199 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1550\n",
      "2023-07-12 17:49:57,537 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1600\n",
      "2023-07-12 17:50:07,872 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1650\n",
      "2023-07-12 17:50:18,198 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1700\n",
      "2023-07-12 17:50:28,528 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1750\n",
      "2023-07-12 17:50:38,861 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1800\n",
      "2023-07-12 17:50:49,203 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1850\n",
      "2023-07-12 17:50:59,541 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1900\n",
      "2023-07-12 17:51:09,874 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1950\n",
      "2023-07-12 17:51:20,204 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2000\n",
      "2023-07-12 17:51:30,543 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2050\n",
      "2023-07-12 17:51:40,888 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2100\n",
      "2023-07-12 17:51:51,226 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2150\n",
      "2023-07-12 17:52:01,566 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2200\n",
      "2023-07-12 17:52:11,909 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2250\n",
      "2023-07-12 17:52:22,243 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2300\n",
      "2023-07-12 17:52:32,586 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2350\n",
      "2023-07-12 17:52:42,934 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2400\n",
      "2023-07-12 17:52:53,281 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2450\n",
      "2023-07-12 17:53:03,625 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2500\n",
      "2023-07-12 17:53:13,963 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2550\n",
      "2023-07-12 17:53:24,306 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2600\n",
      "2023-07-12 17:53:34,661 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2650\n",
      "2023-07-12 17:53:45,013 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2700\n",
      "2023-07-12 17:53:55,362 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2750\n",
      "2023-07-12 17:54:05,710 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2800\n",
      "2023-07-12 17:54:16,058 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2850\n",
      "2023-07-12 17:54:26,407 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2900\n",
      "2023-07-12 17:54:36,757 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2950\n",
      "2023-07-12 17:54:47,120 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3000\n",
      "2023-07-12 17:54:57,470 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3050\n",
      "2023-07-12 17:55:07,825 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3100\n",
      "2023-07-12 17:55:18,177 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3150\n",
      "2023-07-12 17:55:28,526 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3200\n",
      "2023-07-12 17:55:38,874 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3250\n",
      "2023-07-12 17:55:49,230 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3300\n",
      "2023-07-12 17:55:59,576 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3350\n",
      "2023-07-12 17:56:09,929 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3400\n",
      "2023-07-12 17:56:20,277 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3450\n",
      "2023-07-12 17:56:30,628 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3500\n",
      "2023-07-12 17:56:40,975 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3550\n",
      "2023-07-12 17:56:51,328 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3600\n",
      "2023-07-12 17:57:01,691 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3650\n",
      "2023-07-12 17:57:12,047 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3700\n",
      "2023-07-12 17:57:22,407 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3750\n",
      "2023-07-12 17:57:32,764 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3800\n",
      "2023-07-12 17:57:43,121 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3850\n",
      "2023-07-12 17:57:53,478 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3900\n",
      "2023-07-12 17:58:03,834 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3950\n",
      "2023-07-12 17:58:14,197 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4000\n",
      "2023-07-12 17:58:24,561 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4050\n",
      "2023-07-12 17:58:34,927 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4100\n",
      "2023-07-12 17:58:45,282 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4150\n",
      "2023-07-12 17:58:55,641 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:59:05,999 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4250\n",
      "2023-07-12 17:59:16,358 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4300\n",
      "2023-07-12 17:59:26,723 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4350\n",
      "2023-07-12 17:59:37,099 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4400\n",
      "2023-07-12 17:59:47,461 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4450\n",
      "2023-07-12 17:59:57,827 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4500\n",
      "2023-07-12 18:00:08,190 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4550\n",
      "2023-07-12 18:00:18,552 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4600\n",
      "2023-07-12 18:00:28,917 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4650\n",
      "2023-07-12 18:00:39,282 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4700\n",
      "2023-07-12 18:00:49,664 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4750\n",
      "2023-07-12 18:01:00,034 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4800\n",
      "2023-07-12 18:01:10,402 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4850\n",
      "2023-07-12 18:01:20,766 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4900\n",
      "2023-07-12 18:01:31,141 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4950\n",
      "2023-07-12 18:01:41,495 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5000\n",
      "2023-07-12 18:01:51,851 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5050\n",
      "2023-07-12 18:02:02,218 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5100\n",
      "2023-07-12 18:02:12,596 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5150\n",
      "2023-07-12 18:02:22,970 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5200\n",
      "2023-07-12 18:02:33,347 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5250\n",
      "2023-07-12 18:02:43,718 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5300\n",
      "2023-07-12 18:02:54,096 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5350\n",
      "2023-07-12 18:03:04,461 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5400\n",
      "2023-07-12 18:03:14,828 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5450\n",
      "2023-07-12 18:03:25,205 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5500\n",
      "2023-07-12 18:03:35,587 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5550\n",
      "2023-07-12 18:03:45,960 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5600\n",
      "2023-07-12 18:03:56,343 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5650\n",
      "2023-07-12 18:04:06,715 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5700\n",
      "2023-07-12 18:04:17,093 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5750\n",
      "2023-07-12 18:04:27,459 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5800\n",
      "2023-07-12 18:04:37,829 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5850\n",
      "2023-07-12 18:04:48,211 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5900\n",
      "2023-07-12 18:04:58,601 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5950\n",
      "2023-07-12 18:05:08,980 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6000\n",
      "2023-07-12 18:05:19,364 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6050\n",
      "2023-07-12 18:05:29,739 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6100\n",
      "2023-07-12 18:05:40,122 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6150\n",
      "2023-07-12 18:05:50,495 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6200\n",
      "2023-07-12 18:06:00,867 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6250\n",
      "2023-07-12 18:06:11,245 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6300\n",
      "2023-07-12 18:06:21,630 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6350\n",
      "2023-07-12 18:06:32,017 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6400\n",
      "2023-07-12 18:06:42,399 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6450\n",
      "2023-07-12 18:06:52,783 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6500\n",
      "2023-07-12 18:07:03,165 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6550\n",
      "2023-07-12 18:07:13,551 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6600\n",
      "2023-07-12 18:07:23,921 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6650\n",
      "2023-07-12 18:07:34,297 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6700\n",
      "2023-07-12 18:07:44,682 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6750\n",
      "2023-07-12 18:07:55,076 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6800\n",
      "2023-07-12 18:08:05,460 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6850\n",
      "2023-07-12 18:08:15,851 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6900\n",
      "2023-07-12 18:08:26,233 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6950\n",
      "2023-07-12 18:08:36,617 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7000\n",
      "2023-07-12 18:08:46,996 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7050\n",
      "2023-07-12 18:08:57,380 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7100\n",
      "2023-07-12 18:09:07,770 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7150\n",
      "2023-07-12 18:09:18,164 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7200\n",
      "2023-07-12 18:09:28,553 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7250\n",
      "2023-07-12 18:09:38,942 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7300\n",
      "2023-07-12 18:09:47,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 5892 failed with Exception \n",
      "2023-07-12 18:09:49,333 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7349\n",
      "2023-07-12 18:09:59,722 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7399\n",
      "2023-07-12 18:10:10,099 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7449\n",
      "2023-07-12 18:10:15,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 6030 failed with Exception \n",
      "2023-07-12 18:10:20,480 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7498\n",
      "2023-07-12 18:10:30,865 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7548\n",
      "2023-07-12 18:10:41,266 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7598\n",
      "2023-07-12 18:10:51,656 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7648\n",
      "2023-07-12 18:10:54,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 6215 failed with Exception \n",
      "2023-07-12 18:10:58,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 6236 failed with Exception \n",
      "2023-07-12 18:11:02,051 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7696\n",
      "2023-07-12 18:11:12,434 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7746\n",
      "2023-07-12 18:11:21,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 6346 failed with Exception \n",
      "2023-07-12 18:11:22,824 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7795\n",
      "2023-07-12 18:11:28,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 6380 failed with Exception \n",
      "2023-07-12 18:11:31,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 6396 failed with Exception \n",
      "2023-07-12 18:11:33,207 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7843\n",
      "2023-07-12 18:11:43,590 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7893\n",
      "2023-07-12 18:11:53,984 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7943\n",
      "2023-07-12 18:12:04,387 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7993\n",
      "2023-07-12 18:12:05,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 6558 failed with Exception \n",
      "2023-07-12 18:12:14,786 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8042\n",
      "2023-07-12 18:12:15,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 6606 failed with Exception \n",
      "2023-07-12 18:12:25,182 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8091\n",
      "2023-07-12 18:12:26,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 6659 failed with Exception \n",
      "2023-07-12 18:12:35,575 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8140\n",
      "2023-07-12 18:12:45,973 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8190\n",
      "2023-07-12 18:12:56,370 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8240\n",
      "2023-07-12 18:13:06,761 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8290\n",
      "2023-07-12 18:13:17,154 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8340\n",
      "2023-07-12 18:13:27,556 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8390\n",
      "2023-07-12 18:13:28,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 6959 failed with Exception \n",
      "2023-07-12 18:13:37,963 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8439\n",
      "2023-07-12 18:13:48,371 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8489\n",
      "2023-07-12 18:13:58,773 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8539\n",
      "2023-07-12 18:14:09,164 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8589\n",
      "2023-07-12 18:14:19,555 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8639\n",
      "2023-07-12 18:14:29,951 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8689\n",
      "2023-07-12 18:14:40,344 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8739\n",
      "2023-07-12 18:14:42,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7313 failed with Exception \n",
      "2023-07-12 18:14:49,496 - \u001b[1;33mWARNING\u001b[1;0m - Request 7347 failed with Exception \n",
      "2023-07-12 18:14:50,743 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8787\n",
      "2023-07-12 18:15:01,140 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8837\n",
      "2023-07-12 18:15:11,550 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8887\n",
      "2023-07-12 18:15:13,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7461 failed with Exception \n",
      "2023-07-12 18:15:21,950 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8936\n",
      "2023-07-12 18:15:32,347 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8986\n",
      "2023-07-12 18:15:42,743 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9036\n",
      "2023-07-12 18:15:53,143 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9086\n",
      "2023-07-12 18:16:03,539 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9136\n",
      "2023-07-12 18:16:13,949 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9186\n",
      "2023-07-12 18:16:24,349 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9236\n",
      "2023-07-12 18:16:25,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7806 failed with Exception \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:16:27,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 7814 failed with Exception \n",
      "2023-07-12 18:16:34,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 7847 failed with Exception \n",
      "2023-07-12 18:16:34,762 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7847\n",
      "2023-07-12 18:16:45,164 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9333\n",
      "2023-07-12 18:16:55,570 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9383\n",
      "2023-07-12 18:16:59,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 7965 failed with Exception \n",
      "2023-07-12 18:17:05,966 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9432\n",
      "2023-07-12 18:17:16,378 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9482\n",
      "2023-07-12 18:17:18,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8057 failed with Exception \n",
      "2023-07-12 18:17:26,785 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9531\n",
      "2023-07-12 18:17:37,195 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9581\n",
      "2023-07-12 18:17:38,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8151 failed with Exception \n",
      "2023-07-12 18:17:47,610 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9630\n",
      "2023-07-12 18:17:58,016 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9680\n",
      "2023-07-12 18:17:59,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8252 failed with Exception \n",
      "2023-07-12 18:18:04,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8276 failed with Exception \n",
      "2023-07-12 18:18:08,419 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9728\n",
      "2023-07-12 18:18:18,817 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9778\n",
      "2023-07-12 18:18:29,223 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9828\n",
      "2023-07-12 18:18:35,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8423 failed with Exception \n",
      "2023-07-12 18:18:39,642 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9877\n",
      "2023-07-12 18:18:46,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8476 failed with Exception \n",
      "2023-07-12 18:18:47,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8481 failed with Exception \n",
      "2023-07-12 18:18:50,066 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9925\n",
      "2023-07-12 18:19:00,478 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9975\n",
      "2023-07-12 18:19:10,886 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10025\n",
      "2023-07-12 18:19:17,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8627 failed with Exception \n",
      "2023-07-12 18:19:21,304 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10074\n",
      "2023-07-12 18:19:27,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8675 failed with Exception \n",
      "2023-07-12 18:19:31,702 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10123\n",
      "2023-07-12 18:19:32,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8700 failed with Exception \n",
      "2023-07-12 18:19:38,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8727 failed with Exception \n",
      "2023-07-12 18:19:42,112 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10171\n",
      "2023-07-12 18:19:52,520 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10221\n",
      "2023-07-12 18:20:02,936 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10271\n",
      "2023-07-12 18:20:04,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 8853 failed with Exception \n",
      "2023-07-12 18:20:13,355 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10320\n",
      "2023-07-12 18:20:19,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 8923 failed with Exception \n",
      "2023-07-12 18:20:23,768 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10369\n",
      "2023-07-12 18:20:34,178 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10419\n",
      "2023-07-12 18:20:44,586 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10469\n",
      "2023-07-12 18:20:55,002 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10519\n",
      "2023-07-12 18:20:57,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9103 failed with Exception \n",
      "2023-07-12 18:21:05,430 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10568\n",
      "2023-07-12 18:21:15,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9192 failed with Exception \n",
      "2023-07-12 18:21:15,851 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9192\n",
      "2023-07-12 18:21:26,267 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10667\n",
      "2023-07-12 18:21:36,685 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10717\n",
      "2023-07-12 18:21:47,092 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10767\n",
      "2023-07-12 18:21:57,505 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10817\n",
      "2023-07-12 18:21:58,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9397 failed with Exception \n",
      "2023-07-12 18:22:07,927 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10866\n",
      "2023-07-12 18:22:10,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9452 failed with Exception \n",
      "2023-07-12 18:22:18,350 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10915\n",
      "2023-07-12 18:22:18,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9491 failed with Exception \n",
      "2023-07-12 18:22:25,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 9524 failed with Exception \n",
      "2023-07-12 18:22:28,779 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10963\n",
      "2023-07-12 18:22:39,197 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11013\n",
      "2023-07-12 18:22:40,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9593 failed with Exception \n",
      "2023-07-12 18:22:49,611 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11062\n",
      "2023-07-12 18:23:00,027 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11112\n",
      "2023-07-12 18:23:10,453 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11162\n",
      "2023-07-12 18:23:20,881 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11212\n",
      "2023-07-12 18:23:31,307 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11262\n",
      "2023-07-12 18:23:32,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 9840 failed with Exception \n",
      "2023-07-12 18:23:41,731 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11311\n",
      "2023-07-12 18:23:52,153 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11361\n",
      "2023-07-12 18:24:02,575 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11411\n",
      "2023-07-12 18:24:08,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10013 failed with Exception \n",
      "2023-07-12 18:24:09,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10017 failed with Exception \n",
      "2023-07-12 18:24:13,017 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11459\n",
      "2023-07-12 18:24:20,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10069 failed with Exception \n",
      "2023-07-12 18:24:22,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10076 failed with Exception \n",
      "2023-07-12 18:24:23,450 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11507\n",
      "2023-07-12 18:24:33,882 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11557\n",
      "2023-07-12 18:24:40,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10161 failed with Exception \n",
      "2023-07-12 18:24:44,306 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11606\n",
      "2023-07-12 18:24:54,727 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11656\n",
      "2023-07-12 18:25:05,152 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11706\n",
      "2023-07-12 18:25:15,589 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11756\n",
      "2023-07-12 18:25:26,017 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11806\n",
      "2023-07-12 18:25:28,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10388 failed with Exception \n",
      "2023-07-12 18:25:36,448 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11855\n",
      "2023-07-12 18:25:36,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10429 failed with Exception \n",
      "2023-07-12 18:25:46,875 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11904\n",
      "2023-07-12 18:25:57,308 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11954\n",
      "2023-07-12 18:26:07,748 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12004\n",
      "2023-07-12 18:26:18,183 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12054\n",
      "2023-07-12 18:26:28,614 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12104\n",
      "2023-07-12 18:26:39,042 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12154\n",
      "2023-07-12 18:26:49,492 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12204\n",
      "2023-07-12 18:26:56,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10809 failed with Exception \n",
      "2023-07-12 18:26:59,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 10825 failed with Exception \n",
      "2023-07-12 18:26:59,939 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12252\n",
      "2023-07-12 18:27:10,381 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12302\n",
      "2023-07-12 18:27:20,820 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12352\n",
      "2023-07-12 18:27:31,251 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12402\n",
      "2023-07-12 18:27:41,694 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12452\n",
      "2023-07-12 18:27:52,143 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12502\n",
      "2023-07-12 18:28:02,577 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12552\n",
      "2023-07-12 18:28:13,010 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12602\n",
      "2023-07-12 18:28:23,462 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12652\n",
      "2023-07-12 18:28:33,912 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12702\n",
      "2023-07-12 18:28:44,353 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12752\n",
      "2023-07-12 18:28:54,799 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12802\n",
      "2023-07-12 18:29:05,243 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12852\n",
      "2023-07-12 18:29:15,702 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12902\n",
      "2023-07-12 18:29:21,495 - \u001b[1;33mWARNING\u001b[1;0m - Request 11497 failed with Exception \n",
      "2023-07-12 18:29:26,139 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12951\n",
      "2023-07-12 18:29:36,600 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:29:47,046 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13051\n",
      "2023-07-12 18:29:57,495 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13101\n",
      "2023-07-12 18:30:07,946 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13151\n",
      "2023-07-12 18:30:18,401 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13201\n",
      "2023-07-12 18:30:28,856 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13251\n",
      "2023-07-12 18:30:32,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11835 failed with Exception \n",
      "2023-07-12 18:30:39,317 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13300\n",
      "2023-07-12 18:30:49,777 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13350\n",
      "2023-07-12 18:30:53,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11933 failed with Exception \n",
      "2023-07-12 18:31:00,235 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13399\n",
      "2023-07-12 18:31:08,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 12005 failed with Exception \n",
      "2023-07-12 18:31:10,690 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13448\n",
      "2023-07-12 18:31:21,154 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13498\n",
      "2023-07-12 18:31:25,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12088 failed with Exception \n",
      "2023-07-12 18:31:31,613 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13547\n",
      "2023-07-12 18:31:36,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12138 failed with Exception \n",
      "2023-07-12 18:31:42,074 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13596\n",
      "2023-07-12 18:31:48,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12198 failed with Exception \n",
      "2023-07-12 18:31:52,549 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13645\n",
      "2023-07-12 18:32:03,009 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13695\n",
      "2023-07-12 18:32:13,494 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13745\n",
      "2023-07-12 18:32:23,957 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13795\n",
      "2023-07-12 18:32:29,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12390 failed with Exception \n",
      "2023-07-12 18:32:34,438 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13844\n",
      "2023-07-12 18:32:44,902 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13894\n",
      "2023-07-12 18:32:53,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12507 failed with Exception \n",
      "2023-07-12 18:32:55,383 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13943\n",
      "2023-07-12 18:33:00,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 12538 failed with Exception \n",
      "2023-07-12 18:33:03,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12556 failed with Exception \n",
      "2023-07-12 18:33:05,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12563 failed with Exception \n",
      "2023-07-12 18:33:05,845 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12563\n",
      "2023-07-12 18:33:11,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12592 failed with Exception \n",
      "2023-07-12 18:33:16,326 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14039\n",
      "2023-07-12 18:33:25,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12657 failed with Exception \n",
      "2023-07-12 18:33:26,801 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14088\n",
      "2023-07-12 18:33:37,281 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14138\n",
      "2023-07-12 18:33:47,768 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14188\n",
      "2023-07-12 18:33:58,252 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14238\n",
      "2023-07-12 18:34:08,743 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14288\n",
      "2023-07-12 18:34:19,234 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14338\n",
      "2023-07-12 18:34:27,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12957 failed with Exception \n",
      "2023-07-12 18:34:29,725 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14387\n",
      "2023-07-12 18:34:40,218 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14437\n",
      "2023-07-12 18:34:50,715 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14487\n",
      "2023-07-12 18:35:01,225 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14537\n",
      "2023-07-12 18:35:11,730 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14587\n",
      "2023-07-12 18:35:22,238 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14637\n",
      "2023-07-12 18:35:32,758 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14687\n",
      "2023-07-12 18:35:43,281 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14737\n",
      "2023-07-12 18:35:53,807 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14787\n",
      "2023-07-12 18:36:04,343 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14837\n",
      "2023-07-12 18:36:14,893 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14887\n",
      "2023-07-12 18:36:25,461 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14937\n",
      "2023-07-12 18:36:36,061 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14987\n",
      "2023-07-12 18:36:41,585 - \u001b[1;32mINFO\u001b[1;0m - Parallel processing complete. Results saved to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174526.jsonl\n",
      "2023-07-12 18:36:41,588 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_174526.jsonl\n",
      "2023-07-12 18:36:42,382 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I apologize for the confusion. Could you please provide a specific fact from a chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...phrase?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"not specific\"}] for sentence \"not specific\": Could not parse output: I apologize for the confusion. Could you please provide a specific fact from a chest X-ray report that you would like me to paraphrase?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"last port\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...r request?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"last port\"}] for sentence \"last port\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"last port\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm unable to process your request. Could you please provide a sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...ray report?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 29, \"total_tokens\": 310}}, {\"observation\": \"new line\"}] for sentence \"new line\": Could not parse output: I'm sorry, but I'm unable to process your request. Could you please provide a sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"new component\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...quest?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"new component\"}] for sentence \"new component\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"new component\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what \"SI P\" refers to. Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...y your request?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"SI P\"}] for sentence \"SI P\": Could not parse output: I'm sorry, but I'm not sure what \"SI P\" refers to. Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"OGT tip\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...our request?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 29, \"total_tokens\": 311}}, {\"observation\": \"OGT tip\"}] for sentence \"OGT tip\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"OGT tip\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"skin project.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...equest?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"skin project\"}] for sentence \"skin project\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"skin project.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,384 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"both due to both\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,384 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...st?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 30, \"total_tokens\": 313}}, {\"observation\": \"both due to both\"}] for sentence \"both due to both\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"both due to both\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,393 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 01:03\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,394 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 01:03\"}] for sentence \"comparison to 01:03\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 01:03\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,394 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 07:21\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,394 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 07:21\"}] for sentence \"comparison to 07:21\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 07:21\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,394 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 11:54\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,394 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 11:54\"}] for sentence \"comparison to 11:54\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 11:54\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,394 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison with :55\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,394 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 30, \"total_tokens\": 313}}, {\"observation\": \"comparison with :55\"}] for sentence \"comparison with :55\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison with :55\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,395 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 17, \"total_tokens\": 299}}, {\"observation\": \"need for assistance\"}] for sentence \"need for assistance\": Could not parse output: Of course! I'd be happy to help. What do you need assistance with?\n",
      "2023-07-12 18:36:42,397 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison with :30:\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,397 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 30, \"total_tokens\": 314}}, {\"observation\": \"comparison with :30:\"}] for sentence \"comparison with :30:\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison with :30:\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,397 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not able to generate paraphrases for \"contemplation of MRI\" as it is not a specific fact from a chest X-ray report. Could you please provide a specific fact or finding from a chest X-ray report?\n",
      "2023-07-12 18:36:42,397 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 51, \"total_tokens\": 335}}, {\"observation\": \"contemplation of MRI\"}] for sentence \"contemplation of MRI\": Could not parse output: I'm sorry, but I'm not able to generate paraphrases for \"contemplation of MRI\" as it is not a specific fact from a chest X-ray report. Could you please provide a specific fact or finding from a chest X-ray report?\n",
      "2023-07-12 18:36:42,400 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but as an AI text-based model, I am unable to process or analyze images. I can only work with text inputs. If you have any specific information or descriptions from the chest X-ray report that you would like me to paraphrase, please provide them in text format and I'll be happy to assist you.\n",
      "2023-07-12 18:36:42,400 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ..., \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 67, \"total_tokens\": 350}}, {\"observation\": \"2 images now provided\"}] for sentence \"2 images now provided\": Could not parse output: I'm sorry, but as an AI text-based model, I am unable to process or analyze images. I can only work with text inputs. If you have any specific information or descriptions from the chest X-ray report that you would like me to paraphrase, please provide them in text format and I'll be happy to assist you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:36:42,411 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 253, \"total_tokens\": 535}}, {\"observation\": \"not completely included\"}] for sentence \"not completely included\": Could not parse output: I apologize for not including all the possible variations. Here are some additional paraphrases for the given examples:\r\n",
      "\r\n",
      "benign calcification:\r\n",
      "[\r\n",
      "\"non-cancerous calcified area\",\r\n",
      "\"safe calcified spot\",\r\n",
      "\"non-malignant calcified region\",\r\n",
      "\"not indicative of malignancy calcification\",\r\n",
      "\"non-threatening calcified mass\",\r\n",
      "\"not suggestive of cancer calcification\",\r\n",
      "\"innocuous calcified spot\",\r\n",
      "\"benign calcified deposit\",\r\n",
      "\"non-dangerous calcified lesion\",\r\n",
      "\"non-metastatic calcified area\"\r\n",
      "]\r\n",
      "\r\n",
      "osteoporosis:\r\n",
      "[\r\n",
      "\"decreased bone strength and density\",\r\n",
      "\"brittle and porous bones\",\r\n",
      "\"low bone mineral density\",\r\n",
      "\"thinning and weakening of the skeletal system\",\r\n",
      "\"reduced bone mass and density\",\r\n",
      "\"fragile and brittle skeletal structure\",\r\n",
      "\"loss of bone density and strength\",\r\n",
      "\"degenerative disease affecting bone health\"\r\n",
      "]\r\n",
      "\r\n",
      "no osteoporosis:\r\n",
      "[\r\n",
      "\"normal bone strength and density\",\r\n",
      "\"healthy and strong bones\",\r\n",
      "\"sufficient bone mineral density\",\r\n",
      "\"intact and robust skeletal system\",\r\n",
      "\"adequate bone mass and density\",\r\n",
      "\"no signs of bone thinning or weakening\",\r\n",
      "\"absence of osteoporotic changes in the bones\",\r\n",
      "\"no indication of degenerative bone disease\"\r\n",
      "]\r\n",
      "2023-07-12 18:36:42,421 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to generate the paraphrases. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\r\n",
      "2023-07-12 18:36:42,421 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...inish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 37, \"total_tokens\": 321}}, {\"observation\": \"patient's site of concern\"}] for sentence \"patient's site of concern\": Could not parse output: I'm sorry, but I need more information to generate the paraphrases. Could you please provide the specific fact from the chest X-ray report that you would like me to paraphrase?\r\n",
      "2023-07-12 18:36:42,421 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information in order to provide a response. Could you please provide a specific fact from a chest X-ray report that you would like me to paraphrase?\r\n",
      "2023-07-12 18:36:42,421 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...inish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 38, \"total_tokens\": 320}}, {\"observation\": \"possible clinical concern\"}] for sentence \"possible clinical concern\": Could not parse output: I'm sorry, but I need more information in order to provide a response. Could you please provide a specific fact from a chest X-ray report that you would like me to paraphrase?\r\n",
      "2023-07-12 18:36:42,432 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...ish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 77, \"total_tokens\": 360}}, {\"observation\": \"lines unchanged from before\"}] for sentence \"lines unchanged from before\": Could not parse output: \"no osteoporosis\"\r\n",
      "[\r\n",
      "\"normal bone density\",\r\n",
      "\"healthy bones\",\r\n",
      "\"adequate bone mass\",\r\n",
      "\"strong bones\",\r\n",
      "\"normal bone strength\",\r\n",
      "\"no signs of osteoporosis\",\r\n",
      "\"absence of osteoporosis\",\r\n",
      "\"no evidence of bone thinning\",\r\n",
      "\"no indication of bone weakening\",\r\n",
      "\"no osteoporotic changes\",\r\n",
      "\"no degenerative bone disease\"\r\n",
      "]\r\n",
      "2023-07-12 18:36:42,432 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...sh_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 254, \"total_tokens\": 536}}, {\"observation\": \"limitations mentioned above\"}] for sentence \"limitations mentioned above\": Could not parse output: I apologize for the limitations mentioned above. Here are the revised examples:\r\n",
      "\r\n",
      "benign calcification\r\n",
      "[\r\n",
      "\"non-cancerous calcification\",\r\n",
      "\"harmless calcification\",\r\n",
      "\"innocuous calcification\",\r\n",
      "\"benign calcified lesion\",\r\n",
      "\"non-malignant calcification\",\r\n",
      "\"non-threatening calcification\",\r\n",
      "\"not indicative of cancer calcification\",\r\n",
      "\"safe calcification\",\r\n",
      "\"non-dangerous calcification\",\r\n",
      "\"non-metastatic calcification\",\r\n",
      "\"calcification without malignancy\"\r\n",
      "]\r\n",
      "\r\n",
      "osteoporosis\r\n",
      "[\r\n",
      "\"decreased bone density\",\r\n",
      "\"brittle bones\",\r\n",
      "\"low bone mass\",\r\n",
      "\"thinning of bones\",\r\n",
      "\"weakening of bones\",\r\n",
      "\"porous bones\",\r\n",
      "\"fragile bones\",\r\n",
      "\"reduced bone strength\",\r\n",
      "\"loss of bone density\",\r\n",
      "\"degenerative bone disease\",\r\n",
      "\"deterioration of bone quality\"\r\n",
      "]\r\n",
      "\r\n",
      "no osteoporosis\r\n",
      "[\r\n",
      "\"normal bone density\",\r\n",
      "\"healthy bones\",\r\n",
      "\"adequate bone mass\",\r\n",
      "\"strong bones\",\r\n",
      "\"normal bone strength\",\r\n",
      "\"no signs of osteoporosis\",\r\n",
      "\"absence of osteoporosis\",\r\n",
      "\"no evidence of bone thinning\",\r\n",
      "\"no indication of bone weakening\",\r\n",
      "\"no osteoporotic changes\",\r\n",
      "\"no degenerative bone disease\",\r\n",
      "\"bone density within normal range\"\r\n",
      "]\r\n",
      "2023-07-12 18:36:42,433 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ..._reason\": \"length\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 300, \"total_tokens\": 583}}, {\"observation\": \"may offer additional detail\"}] for sentence \"may offer additional detail\": Could not parse output: Of course! Here are some additional examples:\r\n",
      "\r\n",
      "benign tumor\r\n",
      "[\r\n",
      "\"non-cancerous tumor\",\r\n",
      "\"harmless growth\",\r\n",
      "\"innocuous neoplasm\",\r\n",
      "\"benign mass\",\r\n",
      "\"non-malignant lesion\",\r\n",
      "\"non-threatening growth\",\r\n",
      "\"not indicative of cancerous growth\",\r\n",
      "\"safe tumor\",\r\n",
      "\"non-dangerous neoplasm\",\r\n",
      "\"non-metastatic mass\"\r\n",
      "]\r\n",
      "\r\n",
      "malignant tumor\r\n",
      "[\r\n",
      "\"cancerous tumor\",\r\n",
      "\"harmful growth\",\r\n",
      "\"dangerous neoplasm\",\r\n",
      "\"malignant mass\",\r\n",
      "\"cancerous lesion\",\r\n",
      "\"threatening growth\",\r\n",
      "\"indicative of cancerous growth\",\r\n",
      "\"dangerous tumor\",\r\n",
      "\"metastatic neoplasm\"\r\n",
      "]\r\n",
      "\r\n",
      "pulmonary edema\r\n",
      "[\r\n",
      "\"fluid in the lungs\",\r\n",
      "\"lung congestion\",\r\n",
      "\"accumulation of fluid in the pulmonary system\",\r\n",
      "\"pulmonary fluid overload\",\r\n",
      "\"excessive fluid in the lungs\",\r\n",
      "\"fluid-filled lungs\",\r\n",
      "\"lung water retention\"\r\n",
      "]\r\n",
      "\r\n",
      "pleural effusion\r\n",
      "[\r\n",
      "\"fluid in the pleural space\",\r\n",
      "\"accumulation of fluid around the lungs\",\r\n",
      "\"excess fluid in the pleural cavity\",\r\n",
      "\"pleural fluid buildup\",\r\n",
      "\"fluid-filled pleural cavity\"\r\n",
      "]\r\n",
      "\r\n",
      "atelectasis\r\n",
      "[\r\n",
      "\"collapsed lung\",\r\n",
      "\"incomplete lung expansion\",\r\n",
      "\"inflated lung collapse\",\r\n",
      "\"incomplete lung inflation\",\r\n",
      "\"lung collapse\"\r\n",
      "]\r\n",
      "\r\n",
      "pneumothorax\r\n",
      "[\r\n",
      "\"collapsed lung due to air in the pleural space\",\r\n",
      "\"air in the pleural cavity causing lung\r\n",
      "2023-07-12 18:36:42,442 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"question of exchange of line.\" Could you please provide more information or clarify your request?\r\n",
      "2023-07-12 18:36:42,442 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...sh_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 31, \"total_tokens\": 315}}, {\"observation\": \"question of exchange of line\"}] for sentence \"question of exchange of line\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"question of exchange of line.\" Could you please provide more information or clarify your request?\r\n",
      "2023-07-12 18:36:42,442 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I can only assist with generating paraphrases for sentences describing facts from chest X-ray reports. I am not able to provide images or assist with pelvic CT scans.\r\n",
      "2023-07-12 18:36:42,442 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...sh_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 37, \"total_tokens\": 321}}, {\"observation\": \"selected images of pelvic CT\"}] for sentence \"selected images of pelvic CT\": Could not parse output: I'm sorry, but I can only assist with generating paraphrases for sentences describing facts from chest X-ray reports. I am not able to provide images or assist with pelvic CT scans.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:36:42,458 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"presenting care of the patient.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,458 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ..._reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"presenting care of the patient\"}] for sentence \"presenting care of the patient\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"presenting care of the patient.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,472 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not able to generate a response based on the input you provided. Could you please provide more specific information or rephrase your request?\n",
      "2023-07-12 18:36:42,472 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...eason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 33, \"total_tokens\": 318}}, {\"observation\": \"condition of the patient per min\"}] for sentence \"condition of the patient per min\": Could not parse output: I'm sorry, but I'm not able to generate a response based on the input you provided. Could you please provide more specific information or rephrase your request?\n",
      "2023-07-12 18:36:42,504 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"follow-up that worsened between and.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,504 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...on\": \"stop\"}], \"usage\": {\"prompt_tokens\": 286, \"completion_tokens\": 33, \"total_tokens\": 319}}, {\"observation\": \"follow-up that worsened between and\"}] for sentence \"follow-up that worsened between and\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"follow-up that worsened between and.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,535 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...: \"stop\"}], \"usage\": {\"prompt_tokens\": 286, \"completion_tokens\": 284, \"total_tokens\": 570}}, {\"observation\": \"repeat with lateral view of the chest\"}] for sentence \"repeat with lateral view of the chest\": Could not parse output: pulmonary edema\n",
      "[\n",
      "\"fluid in the lungs\",\n",
      "\"lung congestion\",\n",
      "\"accumulation of fluid in the lungs\",\n",
      "\"pulmonary fluid overload\",\n",
      "\"excess fluid in the lungs\",\n",
      "\"fluid-filled lungs\",\n",
      "\"lung swelling\",\n",
      "\"fluid retention in the lungs\",\n",
      "\"pulmonary fluid accumulation\",\n",
      "\"fluid buildup in the lungs\"\n",
      "]\n",
      "\n",
      "pleural effusion\n",
      "[\n",
      "\"fluid around the lungs\",\n",
      "\"accumulation of fluid in the pleural space\",\n",
      "\"excess fluid in the pleural cavity\",\n",
      "\"pleural fluid collection\",\n",
      "\"fluid-filled pleural space\",\n",
      "\"pleural fluid buildup\",\n",
      "\"fluid accumulation between the lung and chest wall\",\n",
      "\"pleural fluid retention\",\n",
      "\"pleural fluid accumulation\",\n",
      "\"fluid in the space between the lung and chest wall\"\n",
      "]\n",
      "\n",
      "atelectasis\n",
      "[\n",
      "\"collapsed lung\",\n",
      "\"incomplete lung expansion\",\n",
      "\"partial lung collapse\",\n",
      "\"lung collapse\",\n",
      "\"incomplete lung inflation\",\n",
      "\"lung deflation\",\n",
      "\"incomplete lung aeration\",\n",
      "\"lung collapse syndrome\",\n",
      "\"incomplete lung ventilation\",\n",
      "\"incomplete lung re-expansion\"\n",
      "]\n",
      "\n",
      "no focal consolidation\n",
      "[\n",
      "\"absence of localized consolidation\",\n",
      "\"no area of consolidation present\",\n",
      "\"no focal infiltrate seen\",\n",
      "\"no evidence of localized consolidation\",\n",
      "\"no focal opacity on imaging\",\n",
      "\"no focal consolidation identified\",\n",
      "\"no localized consolidation on radiograph\",\n",
      "\"no area of consolidation detected\",\n",
      "\"no focal consolidation observed\"\n",
      "]\n",
      "2023-07-12 18:36:42,548 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm unable to generate the desired output given the severe technical limitations. Is there anything else I can help you with?\n",
      "2023-07-12 18:36:42,548 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...: \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 29, \"total_tokens\": 313}}, {\"observation\": \"soft from severe technical limitations\"}] for sentence \"soft from severe technical limitations\": Could not parse output: I'm sorry, but I'm unable to generate the desired output given the severe technical limitations. Is there anything else I can help you with?\n",
      "2023-07-12 18:36:42,590 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I cannot access or compare specific chest radiographs as I am a text-based AI language model. However, I can help you with any questions or provide information related to chest X-ray reports.\n",
      "2023-07-12 18:36:42,590 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...top\"}], \"usage\": {\"prompt_tokens\": 289, \"completion_tokens\": 43, \"total_tokens\": 332}}, {\"observation\": \"comparison with chest radiographs at 16:41\"}] for sentence \"comparison with chest radiographs at 16:41\": Could not parse output: I'm sorry, but I cannot access or compare specific chest radiographs as I am a text-based AI language model. However, I can help you with any questions or provide information related to chest X-ray reports.\n",
      "2023-07-12 18:36:42,629 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"comparison with chest radiographs through :53\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,629 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\"}], \"usage\": {\"prompt_tokens\": 287, \"completion_tokens\": 34, \"total_tokens\": 321}}, {\"observation\": \"comparison with chest radiographs through :53\"}] for sentence \"comparison with chest radiographs through :53\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"comparison with chest radiographs through :53\". Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,720 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure I understand what you mean by \"recommendation for correlation with clinical setting.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,720 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...sage\": {\"prompt_tokens\": 286, \"completion_tokens\": 35, \"total_tokens\": 321}}, {\"observation\": \"recommendation for correlation with clinical setting\"}] for sentence \"recommendation for correlation with clinical setting\": Could not parse output: I'm sorry, but I'm not sure I understand what you mean by \"recommendation for correlation with clinical setting.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 18:36:42,724 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure I understand what you mean by \"changed appearance allowing for technical differences.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,724 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...age\": {\"prompt_tokens\": 285, \"completion_tokens\": 34, \"total_tokens\": 319}}, {\"observation\": \"changed appearance allowing for technical differences\"}] for sentence \"changed appearance allowing for technical differences\": Could not parse output: I'm sorry, but I'm not sure I understand what you mean by \"changed appearance allowing for technical differences.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 18:36:42,745 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...\": {\"prompt_tokens\": 287, \"completion_tokens\": 300, \"total_tokens\": 587}}, {\"observation\": \"detail views of locations of physical findings required\"}] for sentence \"detail views of locations of physical findings required\": Could not parse output: I apologize for the confusion. Here are the paraphrases for the given examples, including the details of the locations of physical findings:\n",
      "\n",
      "1. Benign calcification:\n",
      "   - Non-cancerous calcification in the specified location.\n",
      "   - Harmless calcification observed in the mentioned area.\n",
      "   - Innocuous calcification found at the indicated site.\n",
      "   - Benign calcified lesion detected in the specified region.\n",
      "   - Non-malignant calcification identified in the mentioned location.\n",
      "   - Non-threatening calcification observed at the indicated site.\n",
      "   - Calcification not indicative of cancer found in the specified area.\n",
      "   - Safe calcification seen in the mentioned region.\n",
      "   - Non-dangerous calcification detected at the indicated location.\n",
      "   - Non-metastatic calcification observed in the specified area.\n",
      "\n",
      "2. Osteoporosis:\n",
      "   - Decreased bone density noted in the specified location.\n",
      "   - Brittle bones observed at the mentioned site.\n",
      "   - Low bone mass detected in the indicated area.\n",
      "   - Thinning of the bones seen in the specified region.\n",
      "   - Weakening of the bones observed at the indicated location.\n",
      "   - Porous bones found in the mentioned area.\n",
      "   - Fragile bones identified in the specified region.\n",
      "   - Reduced bone strength noted at the indicated site.\n",
      "   - Loss of bone density detected in the specified location.\n",
      "   - Degenerative bone disease characterized by decreased bone density.\n",
      "\n",
      "3. No osteoporosis:\n",
      "   - Normal bone density observed in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:36:42,842 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-16k-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output ...ns\": 294, \"completion_tokens\": 249, \"total_tokens\": 543}}, {\"observation\": \"stable blunting of the posterior left costophrenic angle since at least\"}] for sentence \"stable blunting of the posterior left costophrenic angle since at least\": Could not parse output: \"unchanged blunting of the posterior left costophrenic angle since at least\",\n",
      "\"persistent blunting of the posterior left costophrenic angle since at least\",\n",
      "\"consistent blunting of the posterior left costophrenic angle since at least\",\n",
      "\"unchanging blunting of the posterior left costophrenic angle since at least\",\n",
      "\"stable obliteration of the posterior left costophrenic angle since at least\",\n",
      "\"unchanged obliteration of the posterior left costophrenic angle since at least\",\n",
      "\"persistent obliteration of the posterior left costophrenic angle since at least\",\n",
      "\"consistent obliteration of the posterior left costophrenic angle since at least\",\n",
      "\"unchanging obliteration of the posterior left costophrenic angle since at least\",\n",
      "\"stable reduction in visibility of the posterior left costophrenic angle since at least\",\n",
      "\"unchanged reduction in visibility of the posterior left costophrenic angle since at least\",\n",
      "\"persistent reduction in visibility of the posterior left costophrenic angle since at least\",\n",
      "\"consistent reduction in visibility of the posterior left costophrenic angle since at least\",\n",
      "\"unchanging reduction in visibility of the posterior left costophrenic angle since at least\"\n",
      "2023-07-12 18:36:42,876 - \u001b[1;32mINFO\u001b[1;0m - Deleting API requests and responses\n",
      "2023-07-12 18:36:42,882 - \u001b[1;32mINFO\u001b[1;0m - Succesfully processed 14965 of 15000 API responses.\n",
      "                    35 of 15000 API responses could not be processed.\n",
      "                    Saving paraphrased observations to /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "    --integrated_fact_metadata_filepath \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "    --min_num_words_per_sentence 2 \\\n",
    "    --offset 0 \\\n",
    "    --num_sentences 15000 \\\n",
    "    --sample_equally_spaced_sentences \\\n",
    "    --process_kth_of_every_n_sentences 3 4 \\\n",
    "    --max_requests_per_minute 3000 \\\n",
    "    --max_tokens_per_minute 170000 \\\n",
    "    --max_tokens_per_request 300 \\\n",
    "    --logging_level \"INFO\" \\\n",
    "    --api_key_name \"OPENAI_API_KEY_2\" \\\n",
    "    --openai_model_name \"gpt-3.5-turbo-16k-0613\" \\\n",
    "    --alias \"__two-or-more-words__part4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e282e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gpt-3.5-turbo-0301_comparisons__part1.jsonl\r\n",
      "'gpt-3.5-turbo-0301_comparisons__part1(obsolete).jsonl'\r\n",
      " gpt-3.5-turbo-0301_comparisons__part2.jsonl\r\n",
      " gpt-3.5-turbo-0301_parsed_sentences.jsonl\r\n",
      " gpt-3.5-turbo-0613_comparisons__part1.jsonl\r\n",
      " gpt-3.5-turbo-0613_comparisons__part2.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\r\n",
      " gpt-3.5-turbo-0613_parsed_facts__hard.jsonl\r\n",
      " gpt-3.5-turbo-0613_parsed_facts__uniform.jsonl\r\n",
      " gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\r\n",
      "'gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl'\r\n",
      "'gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl'\r\n",
      " gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\r\n",
      "'gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl'\r\n",
      " gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\r\n",
      " gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\r\n",
      " gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\r\n",
      " gpt-3.5-turbo_parsed_backgrounds.jsonl\r\n",
      " gpt-3.5-turbo_parsed_reports__backup.jsonl\r\n",
      " gpt-3.5-turbo_parsed_reports.jsonl\r\n",
      "'gpt-3.5-turbo_parsed_reports(old).jsonl'\r\n",
      " gpt-3.5-turbo_parsed_sentences.jsonl\r\n",
      " gpt-4-0613_parsed_reports.jsonl\r\n",
      " gpt-4-0613_parsed_sentences.jsonl\r\n",
      " gpt-4-0613_parsed_sentences__v2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "719c82b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 12:21:17,397 - \u001b[1;32mINFO\u001b[1;0m - Loaded 10 already paraphrased sentences from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\n",
      "2023-07-20 12:21:17,518 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14965 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\n",
      "2023-07-20 12:21:17,687 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14971 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\n",
      "2023-07-20 12:21:17,810 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14972 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\n",
      "2023-07-20 12:21:17,975 - \u001b[1;32mINFO\u001b[1;0m - Loaded 14993 paraphrased sentences to skip from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\n",
      "2023-07-20 12:21:17,976 - \u001b[1;32mINFO\u001b[1;0m - Loading facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "100%|███████████████████████████████| 578733/578733 [00:03<00:00, 162401.95it/s]\n",
      "2023-07-20 12:21:26,873 - \u001b[1;32mINFO\u001b[1;0m - Found 711223 unique observations\n",
      "2023-07-20 12:21:26,874 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to those with at least 2 words\n",
      "2023-07-20 12:21:27,591 - \u001b[1;32mINFO\u001b[1;0m - Found 709221 observations with at least 2 words\n",
      "2023-07-20 12:21:27,591 - \u001b[1;32mINFO\u001b[1;0m - Loading clusters from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\n",
      "2023-07-20 12:21:27,663 - \u001b[1;32mINFO\u001b[1;0m - Loding sentence embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\n",
      "2023-07-20 12:21:29,894 - \u001b[1;32mINFO\u001b[1;0m - Found 100 clusters\n",
      "2023-07-20 12:21:31,272 - \u001b[1;32mINFO\u001b[1;0m - Selected 50000 observations from 100 clusters\n",
      "2023-07-20 12:21:31,272 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to the 4-th of every 5 sentences\n",
      "2023-07-20 12:21:31,284 - \u001b[1;32mINFO\u001b[1;0m - Found 10000 observations that are the 4-th of every 5 sentences\n",
      "2023-07-20 12:21:31,285 - \u001b[1;33mWARNING\u001b[1;0m - Requested 100000 sentences but only 10000 are available. Using 10000 instead.\n",
      "2023-07-20 12:21:31,285 - \u001b[1;32mINFO\u001b[1;0m - Collecting the first 10000 sentence from the 0-th sentence\n",
      "2023-07-20 12:21:31,287 - \u001b[1;32mINFO\u001b[1;0m - Total number of sentences to paraphrase: 9990\n",
      "2023-07-20 12:21:31,287 - \u001b[1;32mINFO\u001b[1;0m - Example sentences to paraphrase:\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 1. low hila\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 1110. adenopathy more pronounced\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 2220. presence of a left pneumothorax\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 3330. early infiltrate in both lower lobes\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 4440. heart projecting over the cervical spine\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 5550. increase in size of ascending thoracic aorta\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 6660. portable AP of the chest film at 14:58 submitted\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 7770. age-indeterminate right posterior seventh rib fracture\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 8880. diffuse coarsening of the interstitium in the right lower lobe\n",
      "2023-07-20 12:21:31,288 - \u001b[1;32mINFO\u001b[1;0m - 9990. possible lack of recent trauma at left lateral rib fractures involving left posterior 5th rib of indeterminate age possibly due to lack of possibly 4th rib fractures\n",
      "2023-07-20 12:21:31,832 - \u001b[1;32mINFO\u001b[1;0m - Saving API requests to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_requests_20230720_122131.jsonl\n",
      "2023-07-20 12:21:31,832 - \u001b[1;32mINFO\u001b[1;0m - Saving API responses to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_122131.jsonl\n",
      "2023-07-20 12:21:32,365 - \u001b[1;32mINFO\u001b[1;0m - Starting request #0\n",
      "2023-07-20 12:21:32,498 - \u001b[1;32mINFO\u001b[1;0m - Starting request #50\n",
      "2023-07-20 12:21:33,680 - \u001b[1;33mWARNING\u001b[1;0m - Request 62 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:21:48,680 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:21:48 2023\n",
      "2023-07-20 12:22:10,421 - \u001b[1;32mINFO\u001b[1;0m - Starting request #99\n",
      "2023-07-20 12:22:29,995 - \u001b[1;33mWARNING\u001b[1;0m - Request 119 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:22:45,001 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:22:44 2023\n",
      "2023-07-20 12:22:58,984 - \u001b[1;32mINFO\u001b[1;0m - Starting request #148\n",
      "2023-07-20 12:23:47,553 - \u001b[1;32mINFO\u001b[1;0m - Starting request #198\n",
      "2023-07-20 12:24:30,458 - \u001b[1;33mWARNING\u001b[1;0m - Request 242 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:24:45,463 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:24:45 2023\n",
      "2023-07-20 12:24:45,477 - \u001b[1;32mINFO\u001b[1;0m - Starting request #247\n",
      "2023-07-20 12:24:56,314 - \u001b[1;33mWARNING\u001b[1;0m - Request 227 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 12:25:24,756 - \u001b[1;32mINFO\u001b[1;0m - Starting request #296\n",
      "2023-07-20 12:26:13,390 - \u001b[1;32mINFO\u001b[1;0m - Starting request #346\n",
      "2023-07-20 12:26:44,638 - \u001b[1;33mWARNING\u001b[1;0m - Request 378 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:26:59,645 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:26:59 2023\n",
      "2023-07-20 12:27:02,012 - \u001b[1;32mINFO\u001b[1;0m - Starting request #395\n",
      "2023-07-20 12:27:50,653 - \u001b[1;32mINFO\u001b[1;0m - Starting request #445\n",
      "2023-07-20 12:28:39,271 - \u001b[1;32mINFO\u001b[1;0m - Starting request #495\n",
      "2023-07-20 12:28:40,399 - \u001b[1;33mWARNING\u001b[1;0m - Request 496 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:28:55,403 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:28:55 2023\n",
      "2023-07-20 12:29:27,905 - \u001b[1;32mINFO\u001b[1;0m - Starting request #544\n",
      "2023-07-20 12:30:16,570 - \u001b[1;32mINFO\u001b[1;0m - Starting request #594\n",
      "2023-07-20 12:30:40,073 - \u001b[1;33mWARNING\u001b[1;0m - Request 618 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:30:55,078 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:30:55 2023\n",
      "2023-07-20 12:31:05,202 - \u001b[1;32mINFO\u001b[1;0m - Starting request #643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 12:31:53,873 - \u001b[1;32mINFO\u001b[1;0m - Starting request #693\n",
      "2023-07-20 12:32:42,548 - \u001b[1;32mINFO\u001b[1;0m - Starting request #743\n",
      "2023-07-20 12:32:42,702 - \u001b[1;33mWARNING\u001b[1;0m - Request 743 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:32:57,709 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:32:57 2023\n",
      "2023-07-20 12:33:31,202 - \u001b[1;32mINFO\u001b[1;0m - Starting request #792\n",
      "2023-07-20 12:34:19,878 - \u001b[1;32mINFO\u001b[1;0m - Starting request #842\n",
      "2023-07-20 12:34:39,504 - \u001b[1;33mWARNING\u001b[1;0m - Request 862 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:34:54,507 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:34:54 2023\n",
      "2023-07-20 12:35:08,554 - \u001b[1;32mINFO\u001b[1;0m - Starting request #891\n",
      "2023-07-20 12:35:57,227 - \u001b[1;32mINFO\u001b[1;0m - Starting request #941\n",
      "2023-07-20 12:36:26,616 - \u001b[1;33mWARNING\u001b[1;0m - Request 971 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:36:41,617 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:36:41 2023\n",
      "2023-07-20 12:36:45,935 - \u001b[1;32mINFO\u001b[1;0m - Starting request #990\n",
      "2023-07-20 12:37:34,615 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1040\n",
      "2023-07-20 12:38:14,690 - \u001b[1;33mWARNING\u001b[1;0m - Request 1081 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:38:29,692 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:38:29 2023\n",
      "2023-07-20 12:38:29,721 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1089\n",
      "2023-07-20 12:39:12,030 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1139\n",
      "2023-07-20 12:40:00,736 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1189\n",
      "2023-07-20 12:40:12,581 - \u001b[1;33mWARNING\u001b[1;0m - Request 1201 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:40:27,585 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:40:27 2023\n",
      "2023-07-20 12:40:49,433 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1238\n",
      "2023-07-20 12:41:38,160 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1288\n",
      "2023-07-20 12:42:10,489 - \u001b[1;33mWARNING\u001b[1;0m - Request 1321 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:42:25,490 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:42:25 2023\n",
      "2023-07-20 12:42:26,891 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1337\n",
      "2023-07-20 12:43:15,599 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1387\n",
      "2023-07-20 12:44:04,318 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1437\n",
      "2023-07-20 12:44:07,393 - \u001b[1;33mWARNING\u001b[1;0m - Request 1440 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:44:22,395 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:44:22 2023\n",
      "2023-07-20 12:44:53,064 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1486\n",
      "2023-07-20 12:45:41,813 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1536\n",
      "2023-07-20 12:45:51,710 - \u001b[1;33mWARNING\u001b[1;0m - Request 1546 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:46:06,717 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:46:06 2023\n",
      "2023-07-20 12:46:30,540 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1585\n",
      "2023-07-20 12:47:19,292 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1635\n",
      "2023-07-20 12:47:40,935 - \u001b[1;33mWARNING\u001b[1;0m - Request 1657 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:47:55,937 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:47:55 2023\n",
      "2023-07-20 12:48:08,067 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1684\n",
      "2023-07-20 12:48:56,831 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1734\n",
      "2023-07-20 12:49:28,180 - \u001b[1;33mWARNING\u001b[1;0m - Request 1766 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:49:43,182 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:49:43 2023\n",
      "2023-07-20 12:49:45,572 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1783\n",
      "2023-07-20 12:50:34,339 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1833\n",
      "2023-07-20 12:51:22,325 - \u001b[1;33mWARNING\u001b[1;0m - Request 1882 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:51:37,327 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:51:37 2023\n",
      "2023-07-20 12:51:37,327 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1883\n",
      "2023-07-20 12:52:11,915 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1932\n",
      "2023-07-20 12:53:00,679 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1982\n",
      "2023-07-20 12:53:12,547 - \u001b[1;33mWARNING\u001b[1;0m - Request 1994 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:53:27,549 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:53:27 2023\n",
      "2023-07-20 12:53:49,457 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2031\n",
      "2023-07-20 12:54:38,254 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2081\n",
      "2023-07-20 12:55:10,616 - \u001b[1;33mWARNING\u001b[1;0m - Request 2114 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 12:55:25,618 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:55:25 2023\n",
      "2023-07-20 12:55:27,059 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2130\n",
      "2023-07-20 12:56:15,847 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2180\n",
      "2023-07-20 12:57:04,614 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2230\n",
      "2023-07-20 12:57:06,723 - \u001b[1;33mWARNING\u001b[1;0m - Request 2232 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:57:21,729 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:57:21 2023\n",
      "2023-07-20 12:57:53,390 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2279\n",
      "2023-07-20 12:58:42,224 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2329\n",
      "2023-07-20 12:58:57,026 - \u001b[1;33mWARNING\u001b[1;0m - Request 2344 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 12:59:12,028 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 12:59:12 2023\n",
      "2023-07-20 12:59:31,042 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2378\n",
      "2023-07-20 13:00:19,831 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2428\n",
      "2023-07-20 13:00:39,499 - \u001b[1;33mWARNING\u001b[1;0m - Request 2448 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:00:54,500 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:00:54 2023\n",
      "2023-07-20 13:01:08,614 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2477\n",
      "2023-07-20 13:01:57,430 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2527\n",
      "2023-07-20 13:02:20,044 - \u001b[1;33mWARNING\u001b[1;0m - Request 2550 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:02:35,046 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:02:35 2023\n",
      "2023-07-20 13:02:46,255 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2576\n",
      "2023-07-20 13:03:35,053 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2626\n",
      "2023-07-20 13:04:02,545 - \u001b[1;33mWARNING\u001b[1;0m - Request 2654 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:04:17,546 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:04:17 2023\n",
      "2023-07-20 13:04:23,843 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2675\n",
      "2023-07-20 13:05:12,680 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2725\n",
      "2023-07-20 13:05:49,954 - \u001b[1;33mWARNING\u001b[1;0m - Request 2763 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:06:04,959 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:06:04 2023\n",
      "2023-07-20 13:06:04,992 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2774\n",
      "2023-07-20 13:06:50,364 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2824\n",
      "2023-07-20 13:07:38,349 - \u001b[1;33mWARNING\u001b[1;0m - Request 2873 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:07:53,351 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:07:53 2023\n",
      "2023-07-20 13:07:53,351 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2874\n",
      "2023-07-20 13:08:28,009 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2923\n",
      "2023-07-20 13:09:16,828 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2973\n",
      "2023-07-20 13:09:25,772 - \u001b[1;33mWARNING\u001b[1;0m - Request 2982 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:09:40,777 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:09:40 2023\n",
      "2023-07-20 13:10:05,670 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3022\n",
      "2023-07-20 13:10:54,508 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3072\n",
      "2023-07-20 13:11:18,127 - \u001b[1;33mWARNING\u001b[1;0m - Request 3096 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:11:33,129 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:11:33 2023\n",
      "2023-07-20 13:11:43,365 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3121\n",
      "2023-07-20 13:12:32,215 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3171\n",
      "2023-07-20 13:13:10,470 - \u001b[1;33mWARNING\u001b[1;0m - Request 3210 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:13:25,472 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:13:25 2023\n",
      "2023-07-20 13:13:25,504 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3220\n",
      "2023-07-20 13:14:09,903 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3270\n",
      "2023-07-20 13:14:55,004 - \u001b[1;33mWARNING\u001b[1;0m - Request 3316 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:15:10,005 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:15:10 2023\n",
      "2023-07-20 13:15:10,012 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3319\n",
      "2023-07-20 13:15:47,621 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3369\n",
      "2023-07-20 13:16:35,681 - \u001b[1;33mWARNING\u001b[1;0m - Request 3418 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:16:50,686 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:16:50 2023\n",
      "2023-07-20 13:16:50,686 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3419\n",
      "2023-07-20 13:17:25,335 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3468\n",
      "2023-07-20 13:18:11,417 - \u001b[1;33mWARNING\u001b[1;0m - Request 3515 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:18:26,421 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:18:26 2023\n",
      "2023-07-20 13:18:26,425 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3517\n",
      "2023-07-20 13:19:03,079 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3567\n",
      "2023-07-20 13:19:51,975 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3617\n",
      "2023-07-20 13:19:58,982 - \u001b[1;33mWARNING\u001b[1;0m - Request 3624 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:20:02,806 - \u001b[1;33mWARNING\u001b[1;0m - Request 3586 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:07,096 - \u001b[1;33mWARNING\u001b[1;0m - Request 3591 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:12,118 - \u001b[1;33mWARNING\u001b[1;0m - Request 3596 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:20:13,985 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:20:13 2023\n",
      "2023-07-20 13:20:40,874 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3663\n",
      "2023-07-20 13:21:29,752 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3713\n",
      "2023-07-20 13:21:40,663 - \u001b[1;33mWARNING\u001b[1;0m - Request 3724 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:21:55,669 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:21:55 2023\n",
      "2023-07-20 13:22:18,603 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3762\n",
      "2023-07-20 13:23:07,487 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3812\n",
      "2023-07-20 13:23:23,339 - \u001b[1;33mWARNING\u001b[1;0m - Request 3828 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:23:38,341 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:23:38 2023\n",
      "2023-07-20 13:23:56,373 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3861\n",
      "2023-07-20 13:24:45,248 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3911\n",
      "2023-07-20 13:25:03,965 - \u001b[1;33mWARNING\u001b[1;0m - Request 3930 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:25:18,969 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:25:18 2023\n",
      "2023-07-20 13:25:34,134 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3960\n",
      "2023-07-20 13:26:23,009 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4010\n",
      "2023-07-20 13:26:45,639 - \u001b[1;33mWARNING\u001b[1;0m - Request 4033 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:27:00,641 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:27:00 2023\n",
      "2023-07-20 13:27:11,891 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4059\n",
      "2023-07-20 13:28:00,810 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4109\n",
      "2023-07-20 13:28:36,175 - \u001b[1;33mWARNING\u001b[1;0m - Request 4145 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:28:51,179 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:28:51 2023\n",
      "2023-07-20 13:28:51,219 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4158\n",
      "2023-07-20 13:29:38,594 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4208\n",
      "2023-07-20 13:30:21,792 - \u001b[1;33mWARNING\u001b[1;0m - Request 4252 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:30:36,794 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:30:36 2023\n",
      "2023-07-20 13:30:36,808 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4257\n",
      "2023-07-20 13:31:16,408 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4307\n",
      "2023-07-20 13:31:52,114 - \u001b[1;33mWARNING\u001b[1;0m - Request 4302 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:31:55,011 - \u001b[1;33mWARNING\u001b[1;0m - Request 4305 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:31:56,120 - \u001b[1;33mWARNING\u001b[1;0m - Request 4306 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:31:58,253 - \u001b[1;33mWARNING\u001b[1;0m - Request 4308 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:32:05,293 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4353\n",
      "2023-07-20 13:32:08,383 - \u001b[1;33mWARNING\u001b[1;0m - Request 4356 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:32:23,386 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:32:23 2023\n",
      "2023-07-20 13:32:54,239 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4402\n",
      "2023-07-20 13:33:43,172 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4452\n",
      "2023-07-20 13:33:47,241 - \u001b[1;33mWARNING\u001b[1;0m - Request 4456 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:34:02,243 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:34:02 2023\n",
      "2023-07-20 13:34:32,070 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4501\n",
      "2023-07-20 13:35:20,991 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4551\n",
      "2023-07-20 13:35:22,126 - \u001b[1;33mWARNING\u001b[1;0m - Request 4552 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:35:37,127 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:35:37 2023\n",
      "2023-07-20 13:36:09,891 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4600\n",
      "2023-07-20 13:36:58,015 - \u001b[1;33mWARNING\u001b[1;0m - Request 4649 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:37:13,017 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:37:13 2023\n",
      "2023-07-20 13:37:13,018 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4650\n",
      "2023-07-20 13:37:47,796 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4699\n",
      "2023-07-20 13:38:36,738 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4749\n",
      "2023-07-20 13:38:41,788 - \u001b[1;33mWARNING\u001b[1;0m - Request 4754 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:38:49,834 - \u001b[1;33mWARNING\u001b[1;0m - Request 4721 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-20 13:38:56,792 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:38:56 2023\n",
      "2023-07-20 13:39:25,696 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4797\n",
      "2023-07-20 13:40:14,582 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4847\n",
      "2023-07-20 13:40:15,714 - \u001b[1;33mWARNING\u001b[1;0m - Request 4848 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:40:30,718 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:40:30 2023\n",
      "2023-07-20 13:41:03,511 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4896\n",
      "2023-07-20 13:41:52,479 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4946\n",
      "2023-07-20 13:41:57,529 - \u001b[1;33mWARNING\u001b[1;0m - Request 4951 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:42:12,530 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:42:12 2023\n",
      "2023-07-20 13:42:41,441 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4995\n",
      "2023-07-20 13:43:30,371 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5045\n",
      "2023-07-20 13:43:42,286 - \u001b[1;33mWARNING\u001b[1;0m - Request 5057 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:43:57,288 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:43:57 2023\n",
      "2023-07-20 13:44:19,307 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5094\n",
      "2023-07-20 13:45:08,240 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5144\n",
      "2023-07-20 13:45:17,191 - \u001b[1;33mWARNING\u001b[1;0m - Request 5153 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:45:32,193 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:45:32 2023\n",
      "2023-07-20 13:45:57,169 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5193\n",
      "2023-07-20 13:46:46,167 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5243\n",
      "2023-07-20 13:47:03,957 - \u001b[1;33mWARNING\u001b[1;0m - Request 5261 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:47:18,959 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:47:18 2023\n",
      "2023-07-20 13:47:35,117 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5292\n",
      "2023-07-20 13:48:24,079 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5342\n",
      "2023-07-20 13:48:43,806 - \u001b[1;33mWARNING\u001b[1;0m - Request 5362 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:48:58,807 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:48:58 2023\n",
      "2023-07-20 13:49:13,013 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5391\n",
      "2023-07-20 13:50:01,979 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5441\n",
      "2023-07-20 13:50:28,584 - \u001b[1;33mWARNING\u001b[1;0m - Request 5468 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:50:43,587 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:50:43 2023\n",
      "2023-07-20 13:50:50,978 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5490\n",
      "2023-07-20 13:51:39,955 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5540\n",
      "2023-07-20 13:52:02,637 - \u001b[1;33mWARNING\u001b[1;0m - Request 5563 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:52:17,639 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:52:17 2023\n",
      "2023-07-20 13:52:28,938 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5589\n",
      "2023-07-20 13:53:17,894 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5639\n",
      "2023-07-20 13:53:33,721 - \u001b[1;33mWARNING\u001b[1;0m - Request 5655 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:53:48,723 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:53:48 2023\n",
      "2023-07-20 13:54:06,860 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5688\n",
      "2023-07-20 13:54:55,862 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5738\n",
      "2023-07-20 13:55:10,727 - \u001b[1;33mWARNING\u001b[1;0m - Request 5753 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:55:25,729 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:55:25 2023\n",
      "2023-07-20 13:55:44,850 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5787\n",
      "2023-07-20 13:56:30,377 - \u001b[1;33mWARNING\u001b[1;0m - Request 5828 failed with error {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}\n",
      "2023-07-20 13:56:33,842 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5836\n",
      "2023-07-20 13:56:43,779 - \u001b[1;33mWARNING\u001b[1;0m - Request 5846 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:56:58,782 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:56:58 2023\n",
      "2023-07-20 13:57:22,808 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5885\n",
      "2023-07-20 13:58:11,783 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 13:58:15,850 - \u001b[1;33mWARNING\u001b[1;0m - Request 5939 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 13:58:30,852 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 13:58:30 2023\n",
      "2023-07-20 13:59:00,766 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5984\n",
      "2023-07-20 13:59:49,750 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6034\n",
      "2023-07-20 13:59:49,908 - \u001b[1;33mWARNING\u001b[1;0m - Request 6034 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:00:04,910 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:00:04 2023\n",
      "2023-07-20 14:00:38,756 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6083\n",
      "2023-07-20 14:01:26,932 - \u001b[1;33mWARNING\u001b[1;0m - Request 6132 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:01:41,934 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:01:41 2023\n",
      "2023-07-20 14:01:41,935 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6133\n",
      "2023-07-20 14:02:16,736 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6182\n",
      "2023-07-20 14:03:03,952 - \u001b[1;33mWARNING\u001b[1;0m - Request 6230 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:03:18,954 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:03:18 2023\n",
      "2023-07-20 14:03:18,956 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6230\n",
      "2023-07-20 14:03:54,770 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6281\n",
      "2023-07-20 14:04:42,954 - \u001b[1;33mWARNING\u001b[1;0m - Request 6330 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:04:57,955 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:04:57 2023\n",
      "2023-07-20 14:04:57,956 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6331\n",
      "2023-07-20 14:05:32,829 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6380\n",
      "2023-07-20 14:06:21,810 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6430\n",
      "2023-07-20 14:06:23,943 - \u001b[1;33mWARNING\u001b[1;0m - Request 6432 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:06:38,946 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:06:38 2023\n",
      "2023-07-20 14:07:10,812 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6479\n",
      "2023-07-20 14:07:59,868 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6529\n",
      "2023-07-20 14:08:00,992 - \u001b[1;33mWARNING\u001b[1;0m - Request 6530 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:08:15,994 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:08:15 2023\n",
      "2023-07-20 14:08:48,932 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6578\n",
      "2023-07-20 14:09:35,186 - \u001b[1;33mWARNING\u001b[1;0m - Request 6625 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:09:50,187 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:09:50 2023\n",
      "2023-07-20 14:09:50,193 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6627\n",
      "2023-07-20 14:10:26,961 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6677\n",
      "2023-07-20 14:11:00,465 - \u001b[1;33mWARNING\u001b[1;0m - Request 6711 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:11:15,467 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:11:15 2023\n",
      "2023-07-20 14:11:16,001 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6726\n",
      "2023-07-20 14:12:05,063 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6776\n",
      "2023-07-20 14:12:35,633 - \u001b[1;33mWARNING\u001b[1;0m - Request 6807 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:12:50,635 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:12:50 2023\n",
      "2023-07-20 14:12:54,112 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6825\n",
      "2023-07-20 14:13:43,127 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6875\n",
      "2023-07-20 14:14:03,904 - \u001b[1;33mWARNING\u001b[1;0m - Request 6896 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:14:18,910 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:14:18 2023\n",
      "2023-07-20 14:14:32,178 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6924\n",
      "2023-07-20 14:15:21,248 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6974\n",
      "2023-07-20 14:15:38,072 - \u001b[1;33mWARNING\u001b[1;0m - Request 6991 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:15:53,073 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:15:53 2023\n",
      "2023-07-20 14:16:10,313 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7023\n",
      "2023-07-20 14:16:59,350 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7073\n",
      "2023-07-20 14:17:09,302 - \u001b[1;33mWARNING\u001b[1;0m - Request 7083 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:17:24,303 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:17:24 2023\n",
      "2023-07-20 14:17:48,387 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7122\n",
      "2023-07-20 14:18:37,444 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7172\n",
      "2023-07-20 14:18:44,483 - \u001b[1;33mWARNING\u001b[1;0m - Request 7179 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:18:59,485 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:18:59 2023\n",
      "2023-07-20 14:19:26,533 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7221\n",
      "2023-07-20 14:20:15,573 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7271\n",
      "2023-07-20 14:20:19,660 - \u001b[1;33mWARNING\u001b[1;0m - Request 7275 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:20:34,661 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:20:34 2023\n",
      "2023-07-20 14:21:04,632 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7320\n",
      "2023-07-20 14:21:53,723 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7370\n",
      "2023-07-20 14:21:56,818 - \u001b[1;33mWARNING\u001b[1;0m - Request 7373 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:22:11,819 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:22:11 2023\n",
      "2023-07-20 14:22:42,817 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7419\n",
      "2023-07-20 14:23:18,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 7152 failed with Exception \n",
      "2023-07-20 14:23:24,204 - \u001b[1;33mWARNING\u001b[1;0m - Request 7460 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:23:39,206 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:23:39 2023\n",
      "2023-07-20 14:23:39,233 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7467\n",
      "2023-07-20 14:24:21,016 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7517\n",
      "2023-07-20 14:24:53,585 - \u001b[1;33mWARNING\u001b[1;0m - Request 7550 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:25:08,588 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:25:08 2023\n",
      "2023-07-20 14:25:10,110 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7566\n",
      "2023-07-20 14:25:59,234 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7616\n",
      "2023-07-20 14:26:25,913 - \u001b[1;33mWARNING\u001b[1;0m - Request 7643 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:26:40,917 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:26:40 2023\n",
      "2023-07-20 14:26:48,332 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7665\n",
      "2023-07-20 14:27:37,406 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7715\n",
      "2023-07-20 14:27:51,322 - \u001b[1;33mWARNING\u001b[1;0m - Request 7729 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:28:06,324 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:28:06 2023\n",
      "2023-07-20 14:28:26,505 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7764\n",
      "2023-07-20 14:29:15,595 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7814\n",
      "2023-07-20 14:30:04,723 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7864\n",
      "2023-07-20 14:30:32,465 - \u001b[1;33mWARNING\u001b[1;0m - Request 7759 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:30:47,466 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:30:47 2023\n",
      "2023-07-20 14:30:49,053 - \u001b[1;33mWARNING\u001b[1;0m - Request 7908 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:31:04,055 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:31:04 2023\n",
      "2023-07-20 14:31:04,065 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7912\n",
      "2023-07-20 14:31:42,919 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7962\n",
      "2023-07-20 14:32:18,453 - \u001b[1;33mWARNING\u001b[1;0m - Request 7998 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:32:33,455 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:32:33 2023\n",
      "2023-07-20 14:32:33,495 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8011\n",
      "2023-07-20 14:33:21,142 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8061\n",
      "2023-07-20 14:33:50,785 - \u001b[1;33mWARNING\u001b[1;0m - Request 8091 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:34:05,788 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:34:05 2023\n",
      "2023-07-20 14:34:10,281 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8110\n",
      "2023-07-20 14:34:59,399 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8160\n",
      "2023-07-20 14:35:13,321 - \u001b[1;33mWARNING\u001b[1;0m - Request 8174 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:35:28,323 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:35:28 2023\n",
      "2023-07-20 14:35:48,506 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8209\n",
      "2023-07-20 14:36:32,877 - \u001b[1;33mWARNING\u001b[1;0m - Request 8254 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:36:47,878 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:36:47 2023\n",
      "2023-07-20 14:36:47,889 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8258\n",
      "2023-07-20 14:37:26,805 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8308\n",
      "2023-07-20 14:38:01,387 - \u001b[1;33mWARNING\u001b[1;0m - Request 8343 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:38:16,389 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:38:16 2023\n",
      "2023-07-20 14:38:16,430 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:39:05,156 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8407\n",
      "2023-07-20 14:39:30,887 - \u001b[1;33mWARNING\u001b[1;0m - Request 8433 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:39:45,890 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:39:45 2023\n",
      "2023-07-20 14:39:54,330 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8456\n",
      "2023-07-20 14:40:43,477 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8506\n",
      "2023-07-20 14:40:58,394 - \u001b[1;33mWARNING\u001b[1;0m - Request 8521 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:41:13,395 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:41:13 2023\n",
      "2023-07-20 14:41:32,678 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8555\n",
      "2023-07-20 14:42:21,871 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8605\n",
      "2023-07-20 14:42:28,906 - \u001b[1;33mWARNING\u001b[1;0m - Request 8612 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:42:43,908 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:42:43 2023\n",
      "2023-07-20 14:43:11,042 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8654\n",
      "2023-07-20 14:43:51,513 - \u001b[1;33mWARNING\u001b[1;0m - Request 8695 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:44:06,516 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:44:06 2023\n",
      "2023-07-20 14:44:06,542 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8703\n",
      "2023-07-20 14:44:49,391 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8753\n",
      "2023-07-20 14:45:12,203 - \u001b[1;33mWARNING\u001b[1;0m - Request 8776 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:45:27,206 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:45:27 2023\n",
      "2023-07-20 14:45:38,615 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8802\n",
      "2023-07-20 14:46:27,812 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8852\n",
      "2023-07-20 14:46:34,878 - \u001b[1;33mWARNING\u001b[1;0m - Request 8859 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:46:49,880 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:46:49 2023\n",
      "2023-07-20 14:47:17,081 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8901\n",
      "2023-07-20 14:48:01,529 - \u001b[1;33mWARNING\u001b[1;0m - Request 8946 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:48:16,531 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:48:16 2023\n",
      "2023-07-20 14:48:16,542 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8950\n",
      "2023-07-20 14:48:55,564 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9000\n",
      "2023-07-20 14:49:31,163 - \u001b[1;33mWARNING\u001b[1;0m - Request 9036 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:49:46,164 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:49:46 2023\n",
      "2023-07-20 14:49:46,205 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9049\n",
      "2023-07-20 14:50:34,049 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9099\n",
      "2023-07-20 14:50:50,954 - \u001b[1;33mWARNING\u001b[1;0m - Request 9116 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:51:05,956 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:51:05 2023\n",
      "2023-07-20 14:51:23,319 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9148\n",
      "2023-07-20 14:52:12,575 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9198\n",
      "2023-07-20 14:52:13,714 - \u001b[1;33mWARNING\u001b[1;0m - Request 9199 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:52:28,716 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:52:28 2023\n",
      "2023-07-20 14:53:01,832 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9247\n",
      "2023-07-20 14:53:32,550 - \u001b[1;33mWARNING\u001b[1;0m - Request 9278 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:53:47,552 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:53:47 2023\n",
      "2023-07-20 14:53:51,137 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9296\n",
      "2023-07-20 14:54:40,396 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9346\n",
      "2023-07-20 14:54:52,365 - \u001b[1;33mWARNING\u001b[1;0m - Request 9358 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:55:07,368 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:55:07 2023\n",
      "2023-07-20 14:55:29,711 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9395\n",
      "2023-07-20 14:56:11,300 - \u001b[1;33mWARNING\u001b[1;0m - Request 9437 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:56:26,301 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:56:26 2023\n",
      "2023-07-20 14:56:26,326 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9444\n",
      "2023-07-20 14:57:08,333 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9494\n",
      "2023-07-20 14:57:31,196 - \u001b[1;33mWARNING\u001b[1;0m - Request 9517 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 14:57:46,198 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:57:46 2023\n",
      "2023-07-20 14:57:57,671 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9543\n",
      "2023-07-20 14:58:47,084 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9593\n",
      "2023-07-20 14:58:48,217 - \u001b[1;33mWARNING\u001b[1;0m - Request 9594 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 14:59:03,218 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 14:59:03 2023\n",
      "2023-07-20 14:59:36,467 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9642\n",
      "2023-07-20 15:00:06,263 - \u001b[1;33mWARNING\u001b[1;0m - Request 9672 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 15:00:21,266 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 15:00:21 2023\n",
      "2023-07-20 15:00:25,863 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9691\n",
      "2023-07-20 15:01:15,303 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9741\n",
      "2023-07-20 15:01:21,392 - \u001b[1;33mWARNING\u001b[1;0m - Request 9747 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 15:01:36,393 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 15:01:36 2023\n",
      "2023-07-20 15:02:04,772 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9790\n",
      "2023-07-20 15:02:35,591 - \u001b[1;33mWARNING\u001b[1;0m - Request 9821 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 15:02:50,592 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 15:02:50 2023\n",
      "2023-07-20 15:02:54,248 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9839\n",
      "2023-07-20 15:03:43,780 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9889\n",
      "2023-07-20 15:03:46,902 - \u001b[1;33mWARNING\u001b[1;0m - Request 9892 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 15:04:01,904 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 15:04:01 2023\n",
      "2023-07-20 15:04:33,437 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9938\n",
      "2023-07-20 15:04:58,466 - \u001b[1;33mWARNING\u001b[1;0m - Request 9963 failed with error {'message': 'Rate limit reached for default-gpt-4 in organization org-6MF1Te7RXD4IZ1S7ywx5pC7H on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "2023-07-20 15:05:13,467 - \u001b[1;33mWARNING\u001b[1;0m - Pausing to cool down until Thu Jul 20 15:05:13 2023\n",
      "2023-07-20 15:05:23,350 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9987\n",
      "2023-07-20 15:06:07,406 - \u001b[1;32mINFO\u001b[1;0m - Parallel processing complete. Results saved to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_122131.jsonl\n",
      "2023-07-20 15:06:07,406 - \u001b[1;33mWARNING\u001b[1;0m - 102 rate limit errors received. Consider running at a lower rate.\n",
      "2023-07-20 15:06:07,410 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230720_122131.jsonl\n",
      "2023-07-20 15:06:08,135 - \u001b[1;32mINFO\u001b[1;0m - Deleting API requests and responses\n",
      "2023-07-20 15:06:08,141 - \u001b[1;32mINFO\u001b[1;0m - Succesfully processed 9990 of 9990 API responses.\n",
      "                    0 of 9990 API responses could not be processed.\n",
      "                    Saving processed texts to /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "--integrated_fact_metadata_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--preprocessed_sentences_to_skip_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "--precomputed_sentence_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/cxr_bert_sentence_embeddings(1605733,71051120).pkl\" \\\n",
    "--precomputed_clusters_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/clusters(100,100297669,1315702).pkl\" \\\n",
    "--num_sentences_per_cluster 500 \\\n",
    "--min_num_words_per_sentence 2 \\\n",
    "--offset 0 \\\n",
    "--num_sentences 100000 \\\n",
    "--process_kth_of_every_n_sentences 4 5 \\\n",
    "--max_requests_per_minute 200 \\\n",
    "--max_tokens_per_minute 40000 \\\n",
    "--max_tokens_per_request 512 \\\n",
    "--temperature 0.5 \\\n",
    "--logging_level \"INFO\" \\\n",
    "--api_key_name \"OPENAI_API_KEY_3\" \\\n",
    "--openai_model_name \"gpt-4-0613\" \\\n",
    "--alias \"__two-or-more-words_cluster-balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7779ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.files import load_jsonl, load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6f9962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = load_jsonl('/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f0adaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0cb3f5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'query': 'heavily calcified mitral annulus does not seem to be hemodynamically significant'},\n",
       " 'parsed_response': [\"The mitral annulus is heavily calcified but doesn't appear to have significant hemodynamic impact\",\n",
       "  \"There's a substantial calcification of the mitral annulus, however, it's not causing any noticeable hemodynamic effects\",\n",
       "  \"Heavy calcification is observed in the mitral annulus, but it isn't causing any significant hemodynamic issues\",\n",
       "  \"The mitral annulus shows heavy calcification, but it doesn't seem to be of hemodynamic significance\",\n",
       "  \"Significant calcification is noted on the mitral annulus, yet it doesn't seem to affect hemodynamics\",\n",
       "  \"There's a heavy calcification on the mitral annulus, but it doesn't seem to be hemodynamically significant\",\n",
       "  'The mitral annulus is heavily calcified, without any significant impact on hemodynamics',\n",
       "  \"Mitral annulus is heavily calcified but doesn't exhibit any hemodynamic significance\",\n",
       "  'Heavy calcification of the mitral annulus is observed, but it appears to have no significant hemodynamic effect',\n",
       "  \"There is a heavy calcification in the mitral annulus, but it doesn't seem to be causing any significant hemodynamic problems\",\n",
       "  \"The mitral annulus is heavily calcified, but it doesn't seem to have a significant hemodynamic effect\",\n",
       "  \"Despite the heavy calcification of the mitral annulus, it doesn't appear to be hemodynamically significant\",\n",
       "  \"The mitral annulus is heavily calcified, yet it doesn't seem to hold any significant hemodynamic implications\",\n",
       "  \"Heavy calcification is noted in the mitral annulus, but it doesn't seem to have a significant effect on hemodynamics\"]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[-200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
