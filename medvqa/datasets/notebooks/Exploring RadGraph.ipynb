{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812b42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tmp = json.loads(\"\"\"\n",
    "{\"/mnt/data/radgraph/data/fake_reports/report2.txt\": {\"text\": \"Lungs are clear . No focal acute infiltrate . No pleural effusion .\", \"entities\": {\"1\": {\"tokens\": \"Lungs\", \"label\": \"ANAT-DP\", \"start_ix\": 0, \"end_ix\": 0, \"relations\": []}, \"2\": {\"tokens\": \"clear\", \"label\": \"OBS-DP\", \"start_ix\": 2, \"end_ix\": 2, \"relations\": [[\"located_at\", \"1\"]]}, \"3\": {\"tokens\": \"focal\", \"label\": \"OBS-DA\", \"start_ix\": 5, \"end_ix\": 5, \"relations\": [[\"modify\", \"5\"]]}, \"4\": {\"tokens\": \"acute\", \"label\": \"OBS-DA\", \"start_ix\": 6, \"end_ix\": 6, \"relations\": [[\"modify\", \"5\"]]}, \"5\": {\"tokens\": \"infiltrate\", \"label\": \"OBS-DA\", \"start_ix\": 7, \"end_ix\": 7, \"relations\": []}, \"6\": {\"tokens\": \"pleural\", \"label\": \"ANAT-DP\", \"start_ix\": 10, \"end_ix\": 10, \"relations\": []}, \"7\": {\"tokens\": \"effusion\", \"label\": \"OBS-DA\", \"start_ix\": 11, \"end_ix\": 11, \"relations\": [[\"located_at\", \"6\"]]}}, \"data_source\": null, \"data_split\": \"inference\"}, \"/mnt/data/radgraph/data/fake_reports/report.txt\": {\"text\": \"The lungs are clear , without evidence of focal acute infiltrate or effusion .\", \"entities\": {\"1\": {\"tokens\": \"lungs\", \"label\": \"ANAT-DP\", \"start_ix\": 1, \"end_ix\": 1, \"relations\": []}, \"2\": {\"tokens\": \"clear\", \"label\": \"OBS-DP\", \"start_ix\": 3, \"end_ix\": 3, \"relations\": [[\"located_at\", \"1\"]]}, \"3\": {\"tokens\": \"focal\", \"label\": \"OBS-DA\", \"start_ix\": 8, \"end_ix\": 8, \"relations\": [[\"modify\", \"5\"]]}, \"4\": {\"tokens\": \"acute\", \"label\": \"OBS-DA\", \"start_ix\": 9, \"end_ix\": 9, \"relations\": [[\"modify\", \"5\"]]}, \"5\": {\"tokens\": \"infiltrate\", \"label\": \"OBS-DA\", \"start_ix\": 10, \"end_ix\": 10, \"relations\": []}, \"6\": {\"tokens\": \"effusion\", \"label\": \"OBS-DA\", \"start_ix\": 12, \"end_ix\": 12, \"relations\": []}}, \"data_source\": null, \"data_split\": \"inference\"}}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4259ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/mnt/data/radgraph/data/fake_reports/report2.txt': {'text': 'Lungs are clear . No focal acute infiltrate . No pleural effusion .',\n",
       "  'entities': {'1': {'tokens': 'Lungs',\n",
       "    'label': 'ANAT-DP',\n",
       "    'start_ix': 0,\n",
       "    'end_ix': 0,\n",
       "    'relations': []},\n",
       "   '2': {'tokens': 'clear',\n",
       "    'label': 'OBS-DP',\n",
       "    'start_ix': 2,\n",
       "    'end_ix': 2,\n",
       "    'relations': [['located_at', '1']]},\n",
       "   '3': {'tokens': 'focal',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 5,\n",
       "    'end_ix': 5,\n",
       "    'relations': [['modify', '5']]},\n",
       "   '4': {'tokens': 'acute',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 6,\n",
       "    'end_ix': 6,\n",
       "    'relations': [['modify', '5']]},\n",
       "   '5': {'tokens': 'infiltrate',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 7,\n",
       "    'end_ix': 7,\n",
       "    'relations': []},\n",
       "   '6': {'tokens': 'pleural',\n",
       "    'label': 'ANAT-DP',\n",
       "    'start_ix': 10,\n",
       "    'end_ix': 10,\n",
       "    'relations': []},\n",
       "   '7': {'tokens': 'effusion',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 11,\n",
       "    'end_ix': 11,\n",
       "    'relations': [['located_at', '6']]}},\n",
       "  'data_source': None,\n",
       "  'data_split': 'inference'},\n",
       " '/mnt/data/radgraph/data/fake_reports/report.txt': {'text': 'The lungs are clear , without evidence of focal acute infiltrate or effusion .',\n",
       "  'entities': {'1': {'tokens': 'lungs',\n",
       "    'label': 'ANAT-DP',\n",
       "    'start_ix': 1,\n",
       "    'end_ix': 1,\n",
       "    'relations': []},\n",
       "   '2': {'tokens': 'clear',\n",
       "    'label': 'OBS-DP',\n",
       "    'start_ix': 3,\n",
       "    'end_ix': 3,\n",
       "    'relations': [['located_at', '1']]},\n",
       "   '3': {'tokens': 'focal',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 8,\n",
       "    'end_ix': 8,\n",
       "    'relations': [['modify', '5']]},\n",
       "   '4': {'tokens': 'acute',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 9,\n",
       "    'end_ix': 9,\n",
       "    'relations': [['modify', '5']]},\n",
       "   '5': {'tokens': 'infiltrate',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 10,\n",
       "    'end_ix': 10,\n",
       "    'relations': []},\n",
       "   '6': {'tokens': 'effusion',\n",
       "    'label': 'OBS-DA',\n",
       "    'start_ix': 12,\n",
       "    'end_ix': 12,\n",
       "    'relations': []}},\n",
       "  'data_source': None,\n",
       "  'data_split': 'inference'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71f3ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.hashing import hash_string\n",
    "\n",
    "def compute_label_set(data):\n",
    "    entities = data['entities']\n",
    "    n = len(entities)\n",
    "    e_strings = [None] * n\n",
    "    strings = []\n",
    "    hashes = set()\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        e_strings[i] = f\"{e['tokens']}|{e['label']}\" # tokens|label\n",
    "        hashes.add(hash_string(e_strings[i]))\n",
    "        strings.append(e_strings[i])\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        for r in e['relations']:\n",
    "            j = int(r[1])-1\n",
    "            rel_s1 = f\"{e_strings[i]}|{r[0]}|{e_strings[j]}\" # e1|rel|e2\n",
    "            rel_s2 = f\"{e_strings[i]}|{e_strings[j]}\" # e1|e2\n",
    "            hashes.add(hash_string(rel_s1))\n",
    "            hashes.add(hash_string(rel_s2))\n",
    "            strings.append(rel_s1)\n",
    "            strings.append(rel_s2)\n",
    "    return strings, hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ab65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_entities_and_relations(data):\n",
    "    print('Original text:')\n",
    "    print(data['text'])\n",
    "    print()\n",
    "    entities = data['entities']\n",
    "    n = len(entities)\n",
    "    e_pairs = [None] * n\n",
    "    for k, e in entities.items():\n",
    "        e_pair = (e['tokens'], e['label'])\n",
    "        e_pairs[int(k)-1] = e_pair\n",
    "    relations = []\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        for r in e['relations']:\n",
    "            j = int(r[1])-1\n",
    "            relations.append((e_pairs[i], r[0], e_pairs[j]))\n",
    "    print('Entities:')\n",
    "    for x in e_pairs:\n",
    "        print(x)\n",
    "    print()\n",
    "    print('Relations:')\n",
    "    for x in relations:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38070ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(tmp.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a158a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Lungs are clear . No focal acute infiltrate . No pleural effusion .\n",
      "\n",
      "Entities:\n",
      "('Lungs', 'ANAT-DP')\n",
      "('clear', 'OBS-DP')\n",
      "('focal', 'OBS-DA')\n",
      "('acute', 'OBS-DA')\n",
      "('infiltrate', 'OBS-DA')\n",
      "('pleural', 'ANAT-DP')\n",
      "('effusion', 'OBS-DA')\n",
      "\n",
      "Relations:\n",
      "(('clear', 'OBS-DP'), 'located_at', ('Lungs', 'ANAT-DP'))\n",
      "(('focal', 'OBS-DA'), 'modify', ('infiltrate', 'OBS-DA'))\n",
      "(('acute', 'OBS-DA'), 'modify', ('infiltrate', 'OBS-DA'))\n",
      "(('effusion', 'OBS-DA'), 'located_at', ('pleural', 'ANAT-DP'))\n"
     ]
    }
   ],
   "source": [
    "print_entities_and_relations(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20b2137f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Lungs|ANAT-DP',\n",
       "  'clear|OBS-DP',\n",
       "  'focal|OBS-DA',\n",
       "  'acute|OBS-DA',\n",
       "  'infiltrate|OBS-DA',\n",
       "  'pleural|ANAT-DP',\n",
       "  'effusion|OBS-DA',\n",
       "  'clear|OBS-DP|located_at|Lungs|ANAT-DP',\n",
       "  'clear|OBS-DP|Lungs|ANAT-DP',\n",
       "  'focal|OBS-DA|modify|infiltrate|OBS-DA',\n",
       "  'focal|OBS-DA|infiltrate|OBS-DA',\n",
       "  'acute|OBS-DA|modify|infiltrate|OBS-DA',\n",
       "  'acute|OBS-DA|infiltrate|OBS-DA',\n",
       "  'effusion|OBS-DA|located_at|pleural|ANAT-DP',\n",
       "  'effusion|OBS-DA|pleural|ANAT-DP'],\n",
       " {(12, 309747879334862366),\n",
       "  (12, 855426046901773212),\n",
       "  (12, 2215373754232534800),\n",
       "  (13, 1614911951124434649),\n",
       "  (15, 19119047849380968),\n",
       "  (15, 3677406714112377304),\n",
       "  (17, 2836997461612593144),\n",
       "  (26, 2987285217759247654),\n",
       "  (30, 194462586615084465),\n",
       "  (30, 3399811303483428276),\n",
       "  (31, 2673881681973443813),\n",
       "  (37, 408053797035404307),\n",
       "  (37, 2527974376104740564),\n",
       "  (37, 3180474612184124109),\n",
       "  (42, 3185582518253405045)})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_label_set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aab4eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'lungs',\n",
       " 'are',\n",
       " 'clear',\n",
       " ',',\n",
       " 'without',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'focal',\n",
       " 'acute',\n",
       " 'infiltrate',\n",
       " 'or',\n",
       " 'effusion',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea70121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting paths to all the reports...\n",
      "Got all the paths.\n",
      "Preprocessing all the reports...\n",
      "1 reports done\n",
      "2 reports done\n",
      "Done with preprocessing.\n",
      "Running the inference now... This can take a bit of time\n",
      "dygie_package_parent_folder: /home/pamessina/dygiepp/\n",
      "Imported dygie successfully\n",
      "/home/pamessina/dygiepp/dygie/__init__.py\n",
      "Running command:  PYTHONPATH=/home/pamessina/dygiepp/:$PYTHONPATH conda run -n dygiepp allennlp predict /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz /home/pamessina/medvqa-workspace/tmp/radgraph/temp_dygie_input.json             --predictor dygie             --include-package dygie             --use-dataset-reader             --output-file /home/pamessina/medvqa-workspace/tmp/radgraph/temp_dygie_output.json             --cuda-device -1             --silent\n",
      "Done with inference\n",
      "Inference completed.\n",
      "Postprocessing output file...\n",
      "Done postprocessing.\n",
      "Saving results and performing final cleanup...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 13:46:54,785 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2023-08-05 13:46:54,804 - INFO - allennlp.models.archival - loading archive file /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz\n",
      "2023-08-05 13:46:54,805 - INFO - allennlp.models.archival - extracting archive file /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz to temp dir /tmp/tmp6syt6e4e\n",
      "2023-08-05 13:46:57,513 - INFO - allennlp.common.params - type = from_instances\n",
      "2023-08-05 13:46:57,514 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmp6syt6e4e/vocabulary.\n",
      "2023-08-05 13:46:57,514 - INFO - allennlp.common.params - model.type = dygie\n",
      "2023-08-05 13:46:57,515 - INFO - allennlp.common.params - model.embedder.type = basic\n",
      "2023-08-05 13:46:57,515 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.type = pretrained_transformer_mismatched\n",
      "2023-08-05 13:46:57,515 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "2023-08-05 13:46:57,515 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.max_length = 512\n",
      "2023-08-05 13:46:57,515 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.train_parameters = True\n",
      "2023-08-05 13:46:57,515 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.last_layer_only = True\n",
      "2023-08-05 13:46:57,515 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.gradient_checkpointing = None\n",
      "2023-08-05 13:46:58,190 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2023-08-05 13:46:58,191 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2023-08-05 13:46:59,538 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/pytorch_model.bin from cache at /home/pamessina/.cache/torch/transformers/b37e65e3b849ec2eaf7af4c77b653dcdb3fa19df5fccc86ff0ffa16f126f5b75.a6a084e84faf3eb5eba47a0a81a786cbf5635e8b5fb67072ad3c918042935a30\n",
      "2023-08-05 13:47:03,320 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2023-08-05 13:47:03,321 - INFO - transformers.modeling_utils - All the weights of BertModel were initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2023-08-05 13:47:04,058 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2023-08-05 13:47:04,060 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2023-08-05 13:47:04,060 - INFO - transformers.tokenization_utils_base - Model name 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2023-08-05 13:47:07,185 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/vocab.txt from cache at /home/pamessina/.cache/torch/transformers/b5cd04b1472559c44d90fe1a1f3e8813538c0774130f99287bb3ef2f71574c34.3acf5667a44623ef633872cc290168316f7124c3bc23562e163aee8dcae4bb30\n",
      "2023-08-05 13:47:07,185 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/added_tokens.json from cache at None\n",
      "2023-08-05 13:47:07,186 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/special_tokens_map.json from cache at None\n",
      "2023-08-05 13:47:07,186 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer_config.json from cache at None\n",
      "2023-08-05 13:47:07,186 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer.json from cache at None\n",
      "2023-08-05 13:47:07,830 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2023-08-05 13:47:07,831 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2023-08-05 13:47:07,831 - INFO - transformers.tokenization_utils_base - Model name 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2023-08-05 13:47:11,027 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/vocab.txt from cache at /home/pamessina/.cache/torch/transformers/b5cd04b1472559c44d90fe1a1f3e8813538c0774130f99287bb3ef2f71574c34.3acf5667a44623ef633872cc290168316f7124c3bc23562e163aee8dcae4bb30\n",
      "2023-08-05 13:47:11,027 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/added_tokens.json from cache at None\n",
      "2023-08-05 13:47:11,027 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/special_tokens_map.json from cache at None\n",
      "2023-08-05 13:47:11,027 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer_config.json from cache at None\n",
      "2023-08-05 13:47:11,028 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer.json from cache at None\n",
      "2023-08-05 13:47:11,061 - INFO - allennlp.common.params - model.feature_size = 20\n",
      "2023-08-05 13:47:11,061 - INFO - allennlp.common.params - model.max_span_width = 8\n",
      "2023-08-05 13:47:11,061 - INFO - allennlp.common.params - model.target_task = relation\n",
      "2023-08-05 13:47:11,062 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = xavier_normal\n",
      "2023-08-05 13:47:11,062 - INFO - allennlp.common.params - model.initializer.regexes.0.1.gain = 1.0\n",
      "2023-08-05 13:47:11,062 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None\n",
      "2023-08-05 13:47:11,063 - INFO - allennlp.common.params - model.module_initializer.regexes.0.1.type = xavier_normal\n",
      "2023-08-05 13:47:11,063 - INFO - allennlp.common.params - model.module_initializer.regexes.0.1.gain = 1.0\n",
      "2023-08-05 13:47:11,063 - INFO - allennlp.common.params - model.module_initializer.regexes.1.1.type = xavier_normal\n",
      "2023-08-05 13:47:11,063 - INFO - allennlp.common.params - model.module_initializer.regexes.1.1.gain = 1.0\n",
      "2023-08-05 13:47:11,063 - INFO - allennlp.common.params - model.module_initializer.prevent_regexes = None\n",
      "2023-08-05 13:47:11,063 - INFO - allennlp.common.params - model.regularizer = None\n",
      "2023-08-05 13:47:11,063 - INFO - allennlp.common.params - model.display_metrics = None\n",
      "2023-08-05 13:47:11,064 - INFO - allennlp.common.params - ner.regularizer = None\n",
      "2023-08-05 13:47:11,068 - INFO - allennlp.common.params - coref.spans_per_word = 0.3\n",
      "2023-08-05 13:47:11,068 - INFO - allennlp.common.params - coref.max_antecedents = 100\n",
      "2023-08-05 13:47:11,068 - INFO - allennlp.common.params - coref.coref_prop = 0\n",
      "2023-08-05 13:47:11,068 - INFO - allennlp.common.params - coref.coref_prop_dropout_f = 0.0\n",
      "2023-08-05 13:47:11,068 - INFO - allennlp.common.params - coref.regularizer = None\n",
      "2023-08-05 13:47:11,150 - INFO - allennlp.common.params - relation.spans_per_word = 0.5\n",
      "2023-08-05 13:47:11,150 - INFO - allennlp.common.params - relation.positive_label_weight = 1.0\n",
      "2023-08-05 13:47:11,150 - INFO - allennlp.common.params - relation.regularizer = None\n",
      "2023-08-05 13:47:11,165 - INFO - allennlp.common.params - events.trigger_spans_per_word = 0.3\n",
      "2023-08-05 13:47:11,165 - INFO - allennlp.common.params - events.argument_spans_per_word = 0.8\n",
      "2023-08-05 13:47:11,165 - INFO - allennlp.common.params - events.regularizer = None\n",
      "2023-08-05 13:47:11,165 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2023-08-05 13:47:11,166 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.1._module.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,170 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.0._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.0._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.1._module.bias\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2023-08-05 13:47:11,170 - INFO - allennlp.nn.initializers - Initializing _distance_embedding.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,171 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,182 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,183 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,187 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,188 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.1._module.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,188 - INFO - allennlp.nn.initializers - Initializing _antecedent_scorer._module.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,188 - INFO - allennlp.nn.initializers - Initializing _f_network._linear_layers.0.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,263 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2023-08-05 13:47:11,263 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers -    _f_network._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.1._module.bias\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2023-08-05 13:47:11,264 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,268 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,268 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.1._module.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,268 - INFO - allennlp.nn.initializers - Initializing _relation_feedforwards.None__relation_labels._linear_layers.0.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,280 - INFO - allennlp.nn.initializers - Initializing _relation_feedforwards.None__relation_labels._linear_layers.1.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers - Initializing _relation_scorers.None__relation_labels.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,281 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.1._module.bias\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers -    _relation_feedforwards.None__relation_labels._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers -    _relation_feedforwards.None__relation_labels._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers -    _relation_scorers.None__relation_labels.bias\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers - Initializing _distance_embedding.weight using .*weight initializer\n",
      "2023-08-05 13:47:11,281 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2023-08-05 13:47:11,281 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2023-08-05 13:47:11,282 - INFO - allennlp.nn.initializers - Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.0.weight\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.1.weight\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._antecedent_scorer._module.bias\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._antecedent_scorer._module.weight\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._distance_embedding.weight\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._f_network._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,283 - INFO - allennlp.nn.initializers -    _coref._f_network._linear_layers.0.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.0.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.1.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.1._module.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.1._module.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,284 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,285 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,286 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "2023-08-05 13:47:11,287 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "2023-08-05 13:47:11,288 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.pooler.dense.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.pooler.dense.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _events._distance_embedding.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.0.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.1.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.1._module.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.1._module.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.1._module.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.1._module.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.0.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.0.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.1.bias\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.1.weight\n",
      "2023-08-05 13:47:11,289 - INFO - allennlp.nn.initializers -    _relation._relation_scorers.None__relation_labels.bias\n",
      "2023-08-05 13:47:11,290 - INFO - allennlp.nn.initializers -    _relation._relation_scorers.None__relation_labels.weight\n",
      "2023-08-05 13:47:11,292 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2023-08-05 13:47:11,294 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2023-08-05 13:47:11,294 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.type = dygie\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.max_span_width = 8\n",
      "2023-08-05 13:47:11,532 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = pretrained_transformer_mismatched\n",
      "2023-08-05 13:47:11,533 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.token_min_padding_length = 0\n",
      "2023-08-05 13:47:11,533 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "2023-08-05 13:47:11,533 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.namespace = tags\n",
      "2023-08-05 13:47:11,533 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_length = 512\n",
      "\r",
      "reading instances: 0it [00:00, ?it/s]\r",
      "reading instances: 2it [00:00, 1325.63it/s]\n",
      "2023-08-05 13:47:11,711 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp6syt6e4e\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='conda run -n dygiepp python3 /home/pamessina/medvqa/medvqa/scripts/radgraph/radgraph_allennlp_inference.py --model_path /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz --dygie_package_parent_folder /home/pamessina/dygiepp/ --data_path /mnt/data/radgraph/data/fake_reports/ --out_path /home/pamessina/medvqa-workspace/tmp/radgraph/results.jsonl --temp_folder /home/pamessina/medvqa-workspace/tmp/radgraph/', returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "\n",
    "# --allennlp_bin_path /home/pamessina/miniconda3/envs/dygiepp/bin/allennlp \\\n",
    "# --digie_package_folder /home/pamessina/dygiepp/dygie/ \\\n",
    "\n",
    "subprocess.run(\"\"\"conda run -n dygiepp python3 /home/pamessina/medvqa/medvqa/scripts/radgraph/radgraph_allennlp_inference.py \\\n",
    "--model_path /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz \\\n",
    "--dygie_package_parent_folder /home/pamessina/dygiepp/ \\\n",
    "--data_path /mnt/data/radgraph/data/fake_reports/ \\\n",
    "--out_path /home/pamessina/medvqa-workspace/tmp/radgraph/results.jsonl \\\n",
    "--temp_folder /home/pamessina/medvqa-workspace/tmp/radgraph/\"\"\", shell=True)\n",
    "# subprocess.run('bash -c \"conda activate dygiepp; python3 -V\"', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3ebb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.files import load_jsonl\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a52fd22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 pamessina pamessina 1.7K Aug  5 11:26 /home/pamessina/medvqa-workspace/tmp/radgraph/results.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/pamessina/medvqa-workspace/tmp/radgraph/results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf05109",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = load_jsonl('/home/pamessina/medvqa-workspace/tmp/radgraph/results.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26603e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'/mnt/data/radgraph/data/fake_reports/report2.txt': {'text': 'Lungs are clear . No focal acute infiltrate . No pleural effusion .',\n",
       "   'entities': {'1': {'tokens': 'Lungs',\n",
       "     'label': 'ANAT-DP',\n",
       "     'start_ix': 0,\n",
       "     'end_ix': 0,\n",
       "     'relations': []},\n",
       "    '2': {'tokens': 'clear',\n",
       "     'label': 'OBS-DP',\n",
       "     'start_ix': 2,\n",
       "     'end_ix': 2,\n",
       "     'relations': [['located_at', '1']]},\n",
       "    '3': {'tokens': 'focal',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 5,\n",
       "     'end_ix': 5,\n",
       "     'relations': [['modify', '5']]},\n",
       "    '4': {'tokens': 'acute',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 6,\n",
       "     'end_ix': 6,\n",
       "     'relations': [['modify', '5']]},\n",
       "    '5': {'tokens': 'infiltrate',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 7,\n",
       "     'end_ix': 7,\n",
       "     'relations': []},\n",
       "    '6': {'tokens': 'pleural',\n",
       "     'label': 'ANAT-DP',\n",
       "     'start_ix': 10,\n",
       "     'end_ix': 10,\n",
       "     'relations': []},\n",
       "    '7': {'tokens': 'effusion',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 11,\n",
       "     'end_ix': 11,\n",
       "     'relations': [['located_at', '6']]}},\n",
       "   'data_source': None,\n",
       "   'data_split': 'inference'},\n",
       "  '/mnt/data/radgraph/data/fake_reports/report.txt': {'text': 'The lungs are clear , without evidence of focal acute infiltrate or effusion .',\n",
       "   'entities': {'1': {'tokens': 'lungs',\n",
       "     'label': 'ANAT-DP',\n",
       "     'start_ix': 1,\n",
       "     'end_ix': 1,\n",
       "     'relations': []},\n",
       "    '2': {'tokens': 'clear',\n",
       "     'label': 'OBS-DP',\n",
       "     'start_ix': 3,\n",
       "     'end_ix': 3,\n",
       "     'relations': [['located_at', '1']]},\n",
       "    '3': {'tokens': 'focal',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 8,\n",
       "     'end_ix': 8,\n",
       "     'relations': [['modify', '5']]},\n",
       "    '4': {'tokens': 'acute',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 9,\n",
       "     'end_ix': 9,\n",
       "     'relations': [['modify', '5']]},\n",
       "    '5': {'tokens': 'infiltrate',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 10,\n",
       "     'end_ix': 10,\n",
       "     'relations': []},\n",
       "    '6': {'tokens': 'effusion',\n",
       "     'label': 'OBS-DA',\n",
       "     'start_ix': 12,\n",
       "     'end_ix': 12,\n",
       "     'relations': []}},\n",
       "   'data_source': None,\n",
       "   'data_split': 'inference'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d5fd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/mnt/data/radgraph/data/fake_reports/report2.txt': {'text': 'Lungs are clear . No focal acute infiltrate . No pleural effusion .', 'entities': {'1': {'tokens': 'Lungs', 'label': 'ANAT-DP', 'start_ix': 0, 'end_ix': 0, 'relations': []}, '2': {'tokens': 'clear', 'label': 'OBS-DP', 'start_ix': 2, 'end_ix': 2, 'relations': [['located_at', '1']]}, '3': {'tokens': 'focal', 'label': 'OBS-DA', 'start_ix': 5, 'end_ix': 5, 'relations': [['modify', '5']]}, '4': {'tokens': 'acute', 'label': 'OBS-DA', 'start_ix': 6, 'end_ix': 6, 'relations': [['modify', '5']]}, '5': {'tokens': 'infiltrate', 'label': 'OBS-DA', 'start_ix': 7, 'end_ix': 7, 'relations': []}, '6': {'tokens': 'pleural', 'label': 'ANAT-DP', 'start_ix': 10, 'end_ix': 10, 'relations': []}, '7': {'tokens': 'effusion', 'label': 'OBS-DA', 'start_ix': 11, 'end_ix': 11, 'relations': [['located_at', '6']]}}, 'data_source': None, 'data_split': 'inference'}, '/mnt/data/radgraph/data/fake_reports/report.txt': {'text': 'The lungs are clear , without evidence of focal acute infiltrate or effusion .', 'entities': {'1': {'tokens': 'lungs', 'label': 'ANAT-DP', 'start_ix': 1, 'end_ix': 1, 'relations': []}, '2': {'tokens': 'clear', 'label': 'OBS-DP', 'start_ix': 3, 'end_ix': 3, 'relations': [['located_at', '1']]}, '3': {'tokens': 'focal', 'label': 'OBS-DA', 'start_ix': 8, 'end_ix': 8, 'relations': [['modify', '5']]}, '4': {'tokens': 'acute', 'label': 'OBS-DA', 'start_ix': 9, 'end_ix': 9, 'relations': [['modify', '5']]}, '5': {'tokens': 'infiltrate', 'label': 'OBS-DA', 'start_ix': 10, 'end_ix': 10, 'relations': []}, '6': {'tokens': 'effusion', 'label': 'OBS-DA', 'start_ix': 12, 'end_ix': 12, 'relations': []}}, 'data_source': None, 'data_split': 'inference'}}\n"
     ]
    }
   ],
   "source": [
    "with open('/home/pamessina/medvqa-workspace/tmp/radgraph/results.json') as f:\n",
    "    x = f.readline()\n",
    "    print(json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d11b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a4ffac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dygie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdygie\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dygie'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.import_module('dygie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fb5865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t     models\t __pycache__  spacy_interface  training\r\n",
      "__init__.py  predictors  pytest.ini   tests\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/pamessina/dygiepp/dygie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e636f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Add the path to the folder where 'digie' module is defined\n",
    "module_path = '/home/pamessina/dygiepp'\n",
    "sys.path.append(module_path)\n",
    "\n",
    "# Now import the module\n",
    "try:\n",
    "    importlib.import_module('dygie')\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Module 'digie' not found. Please check if the path is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf6cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dygie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4494f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dygie' from '/home/pamessina/dygiepp/dygie/__init__.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dygie import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
