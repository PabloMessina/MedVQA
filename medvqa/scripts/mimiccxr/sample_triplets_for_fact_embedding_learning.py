import argparse
from collections import defaultdict
import multiprocessing
from typing import Any, Dict, List, Optional, Set, Tuple
import numpy as np
import os
import random
import math
import logging
from sklearn.cluster import KMeans
from tqdm import tqdm
from Levenshtein import distance as levenshtein_distance
import nltk
from nltk.translate.bleu_score import sentence_bleu

from medvqa.datasets.mimiccxr import MIMICCXR_LARGE_FAST_CACHE_DIR
from medvqa.datasets.text_data_utils import wordpunct_tokenize_texts_in_parallel
from medvqa.scripts.chest_imagenome.generate_phrase_groundings_from_scene_graphs import collect_phrase_groundings_from_padchest_gr_and_mscxr
from medvqa.utils.logging_utils import ANSI_BLUE_BOLD, ANSI_BOLD, ANSI_RESET, setup_logging
from medvqa.utils.files_utils import get_file_path_with_hashing_if_too_long, load_jsonl, load_pickle, save_pickle
from medvqa.models.huggingface_utils import (
    CachedTextEmbeddingExtractor,
    SupportedHuggingfaceMedicalBERTModels,
)
from medvqa.utils.metrics_utils import jaccard_between_sets
from medvqa.utils.hashing_utils import hash_string, hash_string_list

# Set up logging
setup_logging()
logger = logging.getLogger(__name__)

MAX_TRIES = 10

def _levenshtein_similarity(sent_a: str, sent_b: str):
    return 1 - levenshtein_distance(sent_a, sent_b) / max(len(sent_a), len(sent_b))

def _bleu1_similarity(tok_sent_a: List[str], tok_sent_b: List[str]) -> float:
    return sentence_bleu([tok_sent_a], tok_sent_b, weights=(1.0,))
    
def randomly_select_nonduplicate_from_list(L, x):
    while True:
        y = random.choice(L)
        if y != x:
            return y

def _find_negatives_and_add_triplest_for_AP(A, P, n_negs, sentence_embeddings, sentence_list, forbidden_idxs_set,
                                            n_clusters, cluster2indices, used_triplets, triplets_list,
                                            same_cluster_as_anchor=False, a_cid=None):
    if same_cluster_as_anchor:
        assert a_cid is not None
        
    AP_sim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
    AP_levd = levenshtein_distance(sentence_list[A], sentence_list[P])
    for _ in range(n_negs):
        for _ in range(MAX_TRIES):
            if same_cluster_as_anchor:
                N = random.choice(cluster2indices[a_cid])
            else:
                cid = random.randint(0, n_clusters-1) # randomly select a cluster
                N = random.choice(cluster2indices[cid]) # randomly select an index from the cluster
            if N == A or N in forbidden_idxs_set: # if same as anchor or forbidden -> skip
                continue
            triplet = (A, P, N)
            if triplet in used_triplets: # if already used -> skip
                continue
            AN_sim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
            AN_levd = levenshtein_distance(sentence_list[A], sentence_list[N])
            if AN_sim > AP_sim and AN_levd < AP_levd: # if both CXR-BERT and Leveinshtein disagree -> skip
                continue
            # if we reach here, then we have found a valid triplet
            used_triplets.add(triplet)
            triplets_list.append(triplet)
            break # break out of for loop


# -----------------------------------------------
# Definitions:
# -----------------------------------------------
# dot(E(X), E(Y)) = dot product of embeddings of X and Y, where E(.) is the embedding function that we want to learn
# cos(X, Y) = cosine similarity between X and Y, where X and Y are represented as vectors using a pre-trained
#            sentence embedding model, e.g. CXR BERT from Microsoft Research
# leveinshtein(X, Y) = Leveinshtein distance between X and Y

def _rule1_common(
    n_triplets_per_rule,
    sentence2paraphrases,
    n_clusters,
    cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    field_name,
):
    # Rule 1: "Rank paraphrases very highly"
    # dot(E(A), E(P)) > dot(E(A), E(N)) unless cos(A, P) < cos(A, N) AND leveinshtein(A, P) > leveinshtein(A, N)
    # Where:
    # A = anchor (anatomical location or observation)
    # P = positive = a paraphrased version of A
    # N = negative = randomly sampled anatomical location or observation
    # In words: include the triplet (A, P, N) (i.e. trust the paraphrase generated by GPT-3.5/4)
    #           except if both CXR-BERT and Leveinshtein disagree at the same time.

    logger.info(f'{ANSI_BLUE_BOLD}Rule 1: "Rank paraphrases very highly"{ANSI_RESET}')

    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(sentence2paraphrases))
    rule_triplets = []
    
    for sentence, paraphrases in tqdm(sentence2paraphrases.items(), mininterval=2):
        idx = sentence2index[sentence]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{sentence}". Skipping...')
            continue

        n_triplets_per_para = math.ceil(n_triplets_per_iter * 0.5 / len(para_idxs))

        # triplets where analoc_idx is anchor
        A = idx
        for P in para_idxs:
            _find_negatives_and_add_triplest_for_AP(A, P, n_triplets_per_para, sentence_embeddings, sentence_list,
                                                    para_idxs_set, n_clusters, cluster2indices,
                                                    used_triplets, rule_triplets)

        # triplets where paraphrases are anchors
        para_idxs_ = para_idxs.copy()
        para_idxs_.append(A) # add A to para_idxs_
        para_idxs_set.add(A) # add A to para_idxs_set
        for A in para_idxs:
            for _ in range(n_triplets_per_para):
                P = randomly_select_nonduplicate_from_list(para_idxs_, A)
                _find_negatives_and_add_triplest_for_AP(A, P, 1, sentence_embeddings, sentence_list,
                                                        para_idxs_set, n_clusters, cluster2indices,
                                                        used_triplets, rule_triplets)
    
    logger.info(f'Actual number of triplets sampled for Rule 1: "Rank paraphrases very highly": {len(rule_triplets)}')
    triplets_dict['train'][field_name].append({
        'rule': 'Rank paraphrases very highly',
        'triplets': rule_triplets,
    })

def _rule2_common(
    n_triplets_per_rule,
    sentence2paraphrases,
    n_clusters,
    cluster2indices,
    idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    field_name,
):
    # Rule 2: "Rank paraphrases very highly - Hard negative"
    # We will sample negatives from the same cluster as the anchor

    logger.info(f'{ANSI_BLUE_BOLD}Rule 2: "Rank paraphrases very highly - Hard negative"{ANSI_RESET}')
    
    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(sentence2paraphrases))
    rule_triplets = []
    
    for sentence, paraphrases in tqdm(sentence2paraphrases.items(), mininterval=2):
        idx = sentence2index[sentence]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{sentence}". Skipping...')
            continue

        n_triplets_per_para = math.ceil(n_triplets_per_iter * 0.5 / len(para_idxs))

        # triplets where analoc_idx is anchor
        A = idx
        A_cid = idx2cluster[A]
        for P in para_idxs:
            _find_negatives_and_add_triplest_for_AP(A, P, n_triplets_per_para, sentence_embeddings, sentence_list,
                                                    para_idxs_set, n_clusters, cluster2indices,
                                                    used_triplets, rule_triplets, same_cluster_as_anchor=True, a_cid=A_cid)

        # triplets where paraphrases are anchors
        para_idxs_ = para_idxs.copy()
        para_idxs_.append(A) # add A to para_idxs_
        para_idxs_set.add(A) # add A to para_idxs_set
        for A in para_idxs:
            for _ in range(n_triplets_per_para):
                P = randomly_select_nonduplicate_from_list(para_idxs_, A)
                _find_negatives_and_add_triplest_for_AP(A, P, 1, sentence_embeddings, sentence_list,
                                                        para_idxs_set, n_clusters, cluster2indices,
                                                        used_triplets, rule_triplets, same_cluster_as_anchor=True, a_cid=A_cid)
    
    logger.info(f'Actual number of triplets sampled for Rule 2: "Rank paraphrases very highly - Hard negative": {len(rule_triplets)}')
    triplets_dict['train'][field_name].append({
        'rule': 'Rank paraphrases very highly - Hard negative',
        'triplets': rule_triplets,
    })

def sample_anatomical_location_triplets__rule1(
    n_triplets_per_rule,
    anatloc2paraphrases,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):  
    _rule1_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=anatloc2paraphrases,
        n_clusters=n_anatloc_clusters,
        cluster2indices=anatloc_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='anatomical_locations',
    )

def sample_anatomical_location_triplets__rule2(
    n_triplets_per_rule,
    anatloc2paraphrases,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    anatloc_idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule2_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=anatloc2paraphrases,
        n_clusters=n_anatloc_clusters,
        cluster2indices=anatloc_cluster2indices,
        idx2cluster=anatloc_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='anatomical_locations',
    )



# --- Global dictionary for multiprocessing shared data ---
_worker_shared_data_anatloc_rule3 = {}

def _init_worker_anatloc_rule3(
    n_anatloc_clusters_sh: int,
    anatloc_cluster2indices_sh: List[List[int]],
    anatloc_idx2cluster_sh: Dict[int, int],
    sentence_embeddings_sh: np.ndarray,
    sentences_lower_list_sh: List[str],
    sentences_tokens_list_sh: List[List[str]],
    initial_used_triplets_sh: Set[Tuple[int, int, int]],
    n_triplets_per_anchor_sh: int,
    hybrid_alpha_sh: float,
    hybrid_beta_sh: float,
    hybrid_gamma_sh: float,
    # hybrid_score_fn, # If passing the function itself
):
    """Initializes shared data for worker processes for Anatomical Location Rule 3."""
    global _worker_shared_data_anatloc_rule3
    _worker_shared_data_anatloc_rule3["n_anatloc_clusters"] = n_anatloc_clusters_sh
    _worker_shared_data_anatloc_rule3["anatloc_cluster2indices"] = anatloc_cluster2indices_sh
    _worker_shared_data_anatloc_rule3["anatloc_idx2cluster"] = anatloc_idx2cluster_sh
    _worker_shared_data_anatloc_rule3["sentence_embeddings"] = sentence_embeddings_sh
    _worker_shared_data_anatloc_rule3["sentences_lower_list"] = sentences_lower_list_sh
    _worker_shared_data_anatloc_rule3["sentences_tokens_list"] = sentences_tokens_list_sh
    _worker_shared_data_anatloc_rule3["initial_used_triplets"] = initial_used_triplets_sh
    _worker_shared_data_anatloc_rule3["n_triplets_per_anchor"] = n_triplets_per_anchor_sh
    _worker_shared_data_anatloc_rule3["hybrid_alpha"] = hybrid_alpha_sh
    _worker_shared_data_anatloc_rule3["hybrid_beta"] = hybrid_beta_sh
    _worker_shared_data_anatloc_rule3["hybrid_gamma"] = hybrid_gamma_sh
    # _worker_shared_data_anatloc_rule3["_hybrid_score_similarity"] = hybrid_score_fn
    # Assuming _hybrid_score_similarity is globally available

def _process_anchor_chunk_anatloc_rule3(
    anchor_indices_chunk: List[int]
) -> List[Tuple[int, int, int]]:
    """
    Worker function to sample triplets for a chunk of anatomical location anchors.
    """
    # Access shared data
    n_anatloc_clusters = _worker_shared_data_anatloc_rule3["n_anatloc_clusters"]
    anatloc_cluster2indices = _worker_shared_data_anatloc_rule3["anatloc_cluster2indices"]
    anatloc_idx2cluster = _worker_shared_data_anatloc_rule3["anatloc_idx2cluster"]
    sentence_embeddings = _worker_shared_data_anatloc_rule3["sentence_embeddings"]
    sentences_lower_list = _worker_shared_data_anatloc_rule3["sentences_lower_list"]
    sentences_tokens_list = _worker_shared_data_anatloc_rule3["sentences_tokens_list"]
    initial_used_triplets = _worker_shared_data_anatloc_rule3["initial_used_triplets"]
    n_triplets_per_anchor = _worker_shared_data_anatloc_rule3["n_triplets_per_anchor"]
    hybrid_alpha = _worker_shared_data_anatloc_rule3["hybrid_alpha"]
    hybrid_beta = _worker_shared_data_anatloc_rule3["hybrid_beta"]
    hybrid_gamma = _worker_shared_data_anatloc_rule3["hybrid_gamma"]

    worker_generated_triplets: List[Tuple[int, int, int]] = []
    local_seen_triplets_in_worker = set()

    if n_triplets_per_anchor == 0:
        return []

    for A in anchor_indices_chunk:
        cid_A = anatloc_idx2cluster[A]
        cluster_idxs_A = anatloc_cluster2indices[cid_A]
        
        for _ in range(n_triplets_per_anchor):

            # Max attempts to find a unique triplet for this slot for anchor A
            for _ in range(20): # Added max attempts for the while True loop
                sampled_candidate_idxs = set()
            
                # Sample indices from the cluster of A
                if len(cluster_idxs_A) > 10:
                    sampled_candidate_idxs.update(random.sample(cluster_idxs_A, 10))
                else:
                    sampled_candidate_idxs.update(cluster_idxs_A)
                
                if A in sampled_candidate_idxs:
                    sampled_candidate_idxs.remove(A) # P and N cannot be A

                # Sample indices from other clusters uniformly
                # Ensure we complete at least 15 samples
                while len(sampled_candidate_idxs) < 15:
                    c_other = random.randint(0, n_anatloc_clusters - 1)
                    if c_other == cid_A: continue
                    idx_other = random.choice(anatloc_cluster2indices[c_other])
                    if idx_other == A: continue
                    sampled_candidate_idxs.add(idx_other)

                sampled_candidate_idxs_list = list(sampled_candidate_idxs)

                best_P_idx, best_P_score = -1, -float('inf') # P: Positive (most similar)
                worst_N_idx, worst_N_score = -1, float('inf') # N: Negative (least similar)

                A_embedding = sentence_embeddings[A]
                A_sentence_lower = sentences_lower_list[A]
                A_tokens = sentences_tokens_list[A]

                for cand_idx in sampled_candidate_idxs_list:

                    sample_embedding = sentence_embeddings[cand_idx]
                    sample_sentence_lower = sentences_lower_list[cand_idx]
                    sample_tokens = sentences_tokens_list[cand_idx]

                    score = _hybrid_score_similarity(
                        A_sentence_lower, A_tokens, A_embedding,
                        sample_sentence_lower, sample_tokens, sample_embedding,
                        hybrid_alpha, hybrid_beta, hybrid_gamma
                    )
                    
                    if score > best_P_score:
                        best_P_score = score
                        best_P_idx = cand_idx
                    if score < worst_N_score:
                        worst_N_score = score
                        worst_N_idx = cand_idx
                
                # Check if valid P and N were found and they are distinct
                assert best_P_idx != -1 and worst_N_idx != -1 and best_P_idx != worst_N_idx
                
                triplet = (A, best_P_idx, worst_N_idx)

                if triplet not in initial_used_triplets and triplet not in local_seen_triplets_in_worker:
                    local_seen_triplets_in_worker.add(triplet)
                    worker_generated_triplets.append(triplet)
                    break # Successfully found a unique triplet for this slot
        # End of n_triplets_per_anchor loop
    return worker_generated_triplets


def sample_anatomical_location_triplets__rule3(
    n_triplets_per_rule: int,
    anatloc_indices: List[int], # These are the potential anchors
    n_anatloc_clusters: int,
    anatloc_cluster2indices: List[List[int]],
    anatloc_idx2cluster: Dict[int, int],
    sentence_embeddings: np.ndarray,
    sentences_lower_list: List[str],
    sentences_tokens_list: List[List[str]],
    used_triplets: Set[Tuple[int, int, int]], # Master set
    triplets_dict: Dict[str, Any],
    logger: logging.Logger,
    num_processes: Optional[int] = None,
    hybrid_alpha: float = 0.5,
    hybrid_beta: float = 0.3,
    hybrid_gamma: float = 0.2,
):
    rule_name = "Rule 3 (AnatLoc): Rank triplets by Embedding, BLEU-1, & Leveinshtein consensus"
    logger.info(f'{ANSI_BLUE_BOLD}{rule_name}{ANSI_RESET}')

    # Determine the actual anchors to process and n_triplets_per_anchor
    if len(anatloc_indices) > n_triplets_per_rule:
        anchors_to_process = random.sample(anatloc_indices, n_triplets_per_rule)
        n_triplets_per_anchor_for_worker = 1
    else: # Use all anatloc_indices as anchors
        anchors_to_process = anatloc_indices
        n_triplets_per_anchor_for_worker = math.ceil(n_triplets_per_rule / len(anchors_to_process))
    
    actual_num_processes = min(num_processes, len(anchors_to_process))
    assert actual_num_processes > 0, "Number of processes must be greater than 0."

    initargs_payload = (
        n_anatloc_clusters,
        anatloc_cluster2indices,
        anatloc_idx2cluster,
        sentence_embeddings,
        sentences_lower_list,
        sentences_tokens_list,
        used_triplets,
        n_triplets_per_anchor_for_worker,
        hybrid_alpha,
        hybrid_beta,
        hybrid_gamma,
    )

    worker_results: List[List[Tuple[int, int, int]]] = []

    if actual_num_processes == 1 or len(anchors_to_process) < actual_num_processes * 2:
        logger.info(f"{rule_name}: Using sequential processing for {len(anchors_to_process)} anchors.")
        _init_worker_anatloc_rule3(*initargs_payload)
        worker_results.append(_process_anchor_chunk_anatloc_rule3(anchors_to_process))
    else:
        logger.info(f"{rule_name}: Using {actual_num_processes} processes for {len(anchors_to_process)} anchors.")
        chunk_size = math.ceil(len(anchors_to_process) / actual_num_processes)
        anchor_chunks = [
            anchors_to_process[i : i + chunk_size]
            for i in range(0, len(anchors_to_process), chunk_size)
        ]
        
        with multiprocessing.Pool(processes=actual_num_processes, initializer=_init_worker_anatloc_rule3,
                                  initargs=initargs_payload) as pool:
            with tqdm(total=len(anchor_chunks), desc=f"{rule_name} Processing Chunks") as pbar:
                for result in pool.imap_unordered(_process_anchor_chunk_anatloc_rule3, anchor_chunks):
                    worker_results.append(result)
                    pbar.update(1)

    final_rule_triplets: List[Tuple[int, int, int]] = []
    for result_list in worker_results:
        for triplet in result_list:
            if triplet not in used_triplets: # Final check against master set
                used_triplets.add(triplet)
                final_rule_triplets.append(triplet)

    logger.info(f'{rule_name}: Actual number of triplets sampled: {len(final_rule_triplets)}')
    triplets_dict['train']['anatomical_locations'].append({
        'rule': rule_name, # Use consistent rule name
        'triplets': final_rule_triplets,
    })


# -----------------------------------------------

def sample_observation_triplets__rule1(
    n_triplets_per_rule,
    obs2paraphrases,
    n_obs_clusters,
    obs_cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule1_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=obs2paraphrases,
        n_clusters=n_obs_clusters,
        cluster2indices=obs_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='observations',
    )

def sample_observation_triplets__rule2(
    n_triplets_per_rule,
    obs2paraphrases,
    n_obs_clusters,
    obs_cluster2indices,
    obs_idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule2_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=obs2paraphrases,
        n_clusters=n_obs_clusters,
        cluster2indices=obs_cluster2indices,
        idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='observations',
    )



# --- Global dictionary for multiprocessing shared data ---
_worker_shared_data_rule3 = {}

def _init_worker_rule3(
    obs_idx_2_hsid_cid_sh: Dict[int, Tuple[Any, Any]],
    hsid_cid_2_obs_idxs_sh: Dict[Tuple[Any, Any], List[int]],
    n_obs_clusters_sh: int,
    obs_cluster2indices_sh: List[List[int]], # Assuming List of Lists for cluster indices
    sentence_embeddings_sh: np.ndarray,
    sentences_lower_list_sh: List[str],
    sentences_tokens_list_sh: List[List[str]],
    initial_used_triplets_sh: Set[Tuple[int, int, int]],
    n_triplets_per_anchor_sh: int,
    # Additional parameters for hybrid scoring
    hybrid_alpha: float = 0.5,
    hybrid_beta: float = 0.3,
    hybrid_gamma: float = 0.2,
):
    """Initializes shared data for worker processes for Rule 3."""
    global _worker_shared_data_rule3
    _worker_shared_data_rule3["obs_idx_2_hsid_cid"] = obs_idx_2_hsid_cid_sh
    _worker_shared_data_rule3["hsid_cid_2_obs_idxs"] = hsid_cid_2_obs_idxs_sh
    _worker_shared_data_rule3["n_obs_clusters"] = n_obs_clusters_sh
    _worker_shared_data_rule3["obs_cluster2indices"] = obs_cluster2indices_sh
    _worker_shared_data_rule3["sentence_embeddings"] = sentence_embeddings_sh
    _worker_shared_data_rule3["sentences_lower_list"] = sentences_lower_list_sh
    _worker_shared_data_rule3["sentences_tokens_list"] = sentences_tokens_list_sh
    _worker_shared_data_rule3["initial_used_triplets"] = initial_used_triplets_sh
    _worker_shared_data_rule3["n_triplets_per_anchor"] = n_triplets_per_anchor_sh
    _worker_shared_data_rule3["hybrid_alpha"] = hybrid_alpha
    _worker_shared_data_rule3["hybrid_beta"] = hybrid_beta
    _worker_shared_data_rule3["hybrid_gamma"] = hybrid_gamma
    # Store functions if passed
    # _worker_shared_data_rule3["_levenshtein_similarity"] = levenshtein_fn
    # _worker_shared_data_rule3["_is_at_least_x_percent_better"] = is_better_fn
    # Ensure helper functions are accessible, e.g. by direct call if global
    # For this example, assuming _levenshtein_similarity and _is_at_least_x_percent_better are global

def _process_anchor_chunk_rule3(
    anchor_item_chunk: List[Tuple[int, Tuple[Any, Any]]]
) -> List[Tuple[int, int, int]]:
    """
    Worker function to sample triplets for a chunk of anchors for Rule 3.
    Accesses shared data via the global `_worker_shared_data_rule3`.
    """
    # Access shared data
    hsid_cid_2_obs_idxs = _worker_shared_data_rule3["hsid_cid_2_obs_idxs"]
    n_obs_clusters = _worker_shared_data_rule3["n_obs_clusters"]
    obs_cluster2indices = _worker_shared_data_rule3["obs_cluster2indices"]
    sentence_embeddings = _worker_shared_data_rule3["sentence_embeddings"]
    sentences_lower_list = _worker_shared_data_rule3["sentences_lower_list"]
    sentences_tokens_list = _worker_shared_data_rule3["sentences_tokens_list"]
    initial_used_triplets = _worker_shared_data_rule3["initial_used_triplets"]
    n_triplets_per_anchor = _worker_shared_data_rule3["n_triplets_per_anchor"]
    hybrid_alpha = _worker_shared_data_rule3["hybrid_alpha"]
    hybrid_beta = _worker_shared_data_rule3["hybrid_beta"]
    hybrid_gamma = _worker_shared_data_rule3["hybrid_gamma"]

    worker_generated_triplets: List[Tuple[int, int, int]] = []
    # Local set to avoid duplicates generated *by this worker* for its chunk
    local_seen_triplets_in_worker = set()

    if n_triplets_per_anchor == 0: # No triplets to generate per anchor
        return []

    for A, hsid_cid_A in anchor_item_chunk:

        obs_idxs = hsid_cid_2_obs_idxs[hsid_cid_A]
        assert len(obs_idxs) > 0, f"No observations found for health status {hsid_cid_A}."

        for _ in range(n_triplets_per_anchor):

            while True:
                sampled_idxs = set()
                
                # Sample indices from the cluster and health status pair of A
                if len(obs_idxs) > 10:
                    sampled_idxs.update(random.sample(obs_idxs, 10))
                else:
                    sampled_idxs.update(obs_idxs)
                if A in sampled_idxs:
                    sampled_idxs.remove(A)
                
                # Sample indices from other clusters uniformly
                while len(sampled_idxs) < 15: # Ensure we have at least 15 indices to sample from
                    c = random.randint(0, n_obs_clusters - 1) # Randomly select a cluster
                    idx = random.choice(obs_cluster2indices[c]) # Randomly select an index from the cluster
                    if idx == A: continue
                    sampled_idxs.add(idx)
                sampled_idxs = list(sampled_idxs)

                best_idx, best_score = -1, -1
                worst_idx, worst_score = -1, float('inf')

                A_embedding = sentence_embeddings[A]
                A_sentence = sentences_lower_list[A]
                A_tokens = sentences_tokens_list[A]

                for idx in sampled_idxs:

                    sample_embedding = sentence_embeddings[idx]
                    sample_sentence = sentences_lower_list[idx]
                    sample_tokens = sentences_tokens_list[idx]

                    score = _hybrid_score_similarity(
                            A_sentence,  A_tokens, A_embedding,
                            sample_sentence, sample_tokens, sample_embedding,
                            hybrid_alpha, hybrid_beta, hybrid_gamma)
                    
                    if score > best_score:
                        best_score = score
                        best_idx = idx
                    if score < worst_score:
                        worst_score = score
                        worst_idx = idx

                assert best_idx != -1 and worst_idx != -1, "Failed to find valid indices for best and worst scores."
                
                triplet = (A, best_idx, worst_idx)

                if triplet in initial_used_triplets or triplet in local_seen_triplets_in_worker:
                    continue
                
                local_seen_triplets_in_worker.add(triplet) # Mark this triplet as seen in this worker
                worker_generated_triplets.append(triplet)
                break # Break out of the while loop to generate a new triplet

    return worker_generated_triplets


def sample_observation_triplets__rule3(
    n_triplets_per_rule: int,
    obs_idx_2_hsid_cid: Dict[int, Tuple[Any, Any]],
    hsid_cid_2_obs_idxs: Dict[Tuple[Any, Any], List[int]],
    n_obs_clusters: int,
    obs_cluster2indices: List[List[int]], # Assuming List of Lists
    sentence_embeddings: np.ndarray,
    sentences_lower_list: List[str],
    sentences_tokens_list: List[List[str]], # List of tokenized sentences
    used_triplets: Set[Tuple[int, int, int]], # Master set, updated by this function
    triplets_dict: Dict[str, Any],
    logger: logging.Logger,
    num_processes: Optional[int] = None,
):
    logger.info(f'{ANSI_BLUE_BOLD}Rule 3: "Rank triplets by Embedding, BLEU-1, Leveinshtein consensus, A&P same health status"{ANSI_RESET}')

    # Determine the set of anchors (items) to process
    all_possible_anchors = list(obs_idx_2_hsid_cid.items()) # List of (A_idx, (hsid, cid))
    
    if len(all_possible_anchors) > n_triplets_per_rule:
        # If we have more potential anchors than total triplets needed,
        # sample anchors and aim for 1 triplet per sampled anchor.
        anchors_to_process = random.sample(all_possible_anchors, n_triplets_per_rule)
        n_triplets_per_anchor_for_worker = 1
    else:
        anchors_to_process = all_possible_anchors
        n_triplets_per_anchor_for_worker = math.ceil(n_triplets_per_rule / len(anchors_to_process))

    if num_processes is None:
        num_processes = os.cpu_count() or 1
    
    # Ensure num_processes does not exceed the number of anchors if it's very small
    actual_num_processes = min(num_processes, len(anchors_to_process))

    initargs_payload = (
        obs_idx_2_hsid_cid, hsid_cid_2_obs_idxs, n_obs_clusters,
        obs_cluster2indices, sentence_embeddings, sentences_lower_list,
        sentences_tokens_list, used_triplets, n_triplets_per_anchor_for_worker,
    )

    # Split anchors_to_process into chunks for workers
    if actual_num_processes == 1 or len(anchors_to_process) < actual_num_processes * 2 : # Fallback to sequential for small data or 1 process
        logger.info(f"Rule 3: Using sequential processing for {len(anchors_to_process)} anchors.")
        # Initialize data for the "single worker" (main process)
        _init_worker_rule3(*initargs_payload)
        # Process all anchors in the main thread
        worker_results = [_process_anchor_chunk_rule3(anchors_to_process)]
    else:
        logger.info(f"Rule 3: Using {actual_num_processes} processes for {len(anchors_to_process)} anchors.")
        chunk_size = math.ceil(len(anchors_to_process) / actual_num_processes)
        anchor_chunks = [
            anchors_to_process[i : i + chunk_size]
            for i in range(0, len(anchors_to_process), chunk_size)
        ]

        with multiprocessing.Pool(processes=actual_num_processes, initializer=_init_worker_rule3,
                                  initargs=initargs_payload) as pool:
            worker_results = []
            with tqdm(total=len(anchor_chunks), desc="Rule 3 Processing Chunks") as pbar:
                for result in pool.imap_unordered(_process_anchor_chunk_rule3, anchor_chunks):
                    worker_results.append(result)
                    pbar.update(1)
    
    final_rule_triplets: List[Tuple[int, int, int]] = []
    for result_list in worker_results:
        for triplet in result_list:
            # The worker already checked against initial_used_triplets and its own local set.
            # This final check in main process handles extremely rare cases or ensures master set is source of truth.
            if triplet not in used_triplets:
                used_triplets.add(triplet) # Update the master set
                final_rule_triplets.append(triplet)

    logger.info(f'Rule 3: Actual number of triplets sampled: {len(final_rule_triplets)}')

    triplets_dict['train']['observations'].append({
        'rule': 'Rank triplets by Embedding, BLEU-1, Leveinshtein consensus, A&P same health status',
        'triplets': final_rule_triplets,
    })



def sample_observation_triplets__rule4(
    n_triplets_per_rule,
    obs2paraphrases,
    obs_idx_2_hsid,
    obs_cluster2indices,
    obs_idx2cluster,
    sentence2index,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 4: "Hard health-status-vs-others triplets"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if HS(A) == HS(P) AND HS(N) != HS(A)
    # Where:
    # A = anchor = observation
    # P = positive = paraphrase of A
    # N = negative = randomly sampled observation from the same cluster as A but with a different health status
    # In words: include the triplet (A, P, N) if the anchor and positive are paraphrases of each other
    #           and if the negative is from the same cluster as the anchor but with a different health status.
    
    logger.info(f'{ANSI_BLUE_BOLD}Rule 4: "Hard health-status-vs-others triplets"{ANSI_RESET}')
    
    rule_triplets = []

    obs_with_hs_and_p = [ obs for obs in obs2paraphrases.keys() if sentence2index[obs] in obs_idx_2_hsid ]
    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(obs_with_hs_and_p))
    logger.info(f'Number of observations with health status and paraphrases: {len(obs_with_hs_and_p)}')
    logger.info(f'Number of observations with health status: {len(obs_idx_2_hsid)}')
    logger.info(f'Number of observations with paraphrases: {len(obs2paraphrases)}')
    logger.info(f'Number of triplets to sample per iteration: {n_triplets_per_iter}')
    
    for obs in tqdm(obs_with_hs_and_p, mininterval=2):
        paraphrases = obs2paraphrases[obs]
        obs_idx = sentence2index[obs]
        hsid = obs_idx_2_hsid[obs_idx]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.add(obs_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) < 2:
            logger.warning(f'Not enough paraphrases found for "{obs}". Skipping...')
            continue
        cluster_idxs = obs_cluster2indices[obs_idx2cluster[obs_idx]]
        for _ in range(n_triplets_per_iter):
            for _ in range(MAX_TRIES):
                A, P = random.sample(para_idxs, 2)
                assert A != P
                N = random.choice(cluster_idxs)
                if N not in obs_idx_2_hsid:
                    continue
                if obs_idx_2_hsid[N] == hsid:
                    continue
                if N in para_idxs_set:
                    continue
                triplet = (A, P, N)
                if triplet in used_triplets: # already used
                    continue
                used_triplets.add(triplet) # add triplet to used triplets
                rule_triplets.append(triplet)
                break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 4: "Hard health-status-vs-others triplets": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Hard health-status-vs-others triplets',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule5(
    n_triplets_per_rule,
    integrated_fact_metadata,
    obs2paraphrases,
    sentence2index,
    sentence_list,
    sentence_embeddings,
    obs_idx2cluster,
    obs_cluster2indices,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 5: "Short observation, detailed observation, and fact must be close to each other"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = short observation, detailed observation, or fact, or a paraphrase of one of these
    # P = positive = some paraphrase of the same group as A, probably of a different length
    # N = negative = randomly sampled observation from the same cluster as A
    # In words: include the triplet (A, P, N) if A and P are paraphrases of the same fact with different degrees of detail
    #           (e.g. "cardiomegaly" vs "cardiomegaly is present" vs "cardiomegaly is present, which is an enlarged heart")
    #           and if the negative is from the same cluster as the anchor, as long as CXR-BERT and Leveinshtein do not both disagree.

    logger.info(f'{ANSI_BLUE_BOLD}Rule 5: "Short observation, detailed observation, and fact must be close to each other"{ANSI_RESET}')
    
    if len(integrated_fact_metadata) > n_triplets_per_rule:
        integrated_fact_metadata = random.sample(integrated_fact_metadata, n_triplets_per_rule)
        n_triplets_per_iter = 1
    else:
        n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(integrated_fact_metadata))
    
    rule_triplets = []
    n_obs_clusters = len(obs_cluster2indices)

    def _add_triplets(obs_idxs, obs_idxs_set, i_min, i_max, j_min, j_max, n_triplets_per_obs):
        for i in range(i_min, i_max+1): # first half (or second half)
            A = obs_idxs[i]
            A_cid = obs_idx2cluster[A]
            for _ in range(n_triplets_per_obs):
                for _ in range(MAX_TRIES):
                    j = random.randint(j_min, j_max) # second half (or first half)
                    P = obs_idxs[j]
                    assert A != P
                    c = random.randint(0, n_obs_clusters-1) # randomly select a cluster
                    if c == A_cid:
                        continue # skip if cluster is the same as A's cluster
                    N = random.choice(obs_cluster2indices[c]) # randomly select an index from the cluster
                    if N in obs_idxs_set:
                        continue
                    assert A != N and P != N
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AP_sim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
                    AN_sim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AP_levd = levenshtein_distance(sentence_list[A], sentence_list[P])
                    AN_levd = levenshtein_distance(sentence_list[A], sentence_list[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # if both CXR-BERT and Leveinshtein disagree -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop

    fail_count = 0
    
    for row in tqdm(integrated_fact_metadata, mininterval=2):
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        obs_idxs_set = set()
        # Add fact and its paraphrases to obs_idxs_set
        fact_idx = sentence2index[fact]
        obs_idxs_set.add(fact_idx)
        if fact in obs2paraphrases:
            obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[fact])
        # Add detailed observation and its paraphrases to obs_idxs_set
        if det_obs:
            det_obs_idx = sentence2index[det_obs]
            if det_obs_idx != fact_idx:
                obs_idxs_set.add(det_obs_idx)
                if det_obs in obs2paraphrases:
                    obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[det_obs])
        else:
            det_obs_idx = None
        # Add short observation and its paraphrases to obs_idxs_set
        if short_obs:
            short_obs_idx = sentence2index[short_obs]
            if short_obs_idx != fact_idx and short_obs_idx != det_obs_idx:
                obs_idxs_set.add(short_obs_idx)
                if short_obs in obs2paraphrases:
                    obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[short_obs])
        # Skip if obs_idxs_set has less than 2 observations
        if len(obs_idxs_set) < 2:
            fail_count += 1
            continue
        
        # Sort obs_idxs by length
        obs_idxs = list(obs_idxs_set)
        obs_idxs.sort(key=lambda x: len(sentence_list[x]))
        
        # Sample triples where the anchor is in the first half and the positive is in the second half and vice versa
        n_triplets_per_obs = math.ceil(n_triplets_per_iter / len(obs_idxs))
        half = len(obs_idxs) // 2
        _add_triplets(obs_idxs, obs_idxs_set, 0, half-1, half, len(obs_idxs)-1, n_triplets_per_obs)
        _add_triplets(obs_idxs, obs_idxs_set, half, len(obs_idxs)-1, 0, half-1, n_triplets_per_obs)
        
    if fail_count > 0:
        logger.warning(f'Failed to find enough observations for {fail_count}/{len(integrated_fact_metadata)} integrated fact metadata rows.')

    logger.info(f'Actual number of triplets sampled for Rule 5: "Short observation, detailed observation, and fact'
                f' must be close to each other": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Short observation, detailed observation, and fact must be close to each other',
        'triplets': rule_triplets,
    })


# Worker function for multiprocessing
def _process_label_chunk(
    args_chunk: Tuple[
        List[str], np.ndarray, Dict[str, int], int
    ]
) -> List[List[int]]:
    """
    Processes a chunk of sentences and labels to build a local label2idxs.
    """
    sentences_chunk, labels_chunk, sentence2index, m = args_chunk
    local_label2idxs: List[List[int]] = [[] for _ in range(m + 1)]
    for i, s_text in enumerate(sentences_chunk):
        # Ensure sentence is in sentence2index, handle if not (though ideally it always is)
        if s_text not in sentence2index:
            # Or log a warning, or raise an error, depending on desired behavior
            continue
        s_idx = sentence2index[s_text]
        no_label = True
        for j in range(m): # Iterate through each possible label
            if labels_chunk[i, j] == 1:
                local_label2idxs[j].append(s_idx)
                no_label = False
        if no_label:
            local_label2idxs[m].append(s_idx) # Add to "no label" category
    return local_label2idxs

# Helper function to merge results from multiprocessing
def _merge_label2idxs_results(
    list_of_local_label2idxs: List[List[List[int]]], m: int
) -> List[List[int]]:
    """
    Merges multiple local_label2idxs lists into a single global one.
    """
    merged_label2idxs: List[List[int]] = [[] for _ in range(m + 1)]
    for local_label2idxs in list_of_local_label2idxs:
        for j in range(m + 1):
            merged_label2idxs[j].extend(local_label2idxs[j])
    return merged_label2idxs

def sample_observation_triplets__rule6(
    n_triplets_per_rule: int,
    sentence2index: Dict[str, int],
    index2sentence: Dict[int, str],
    sentence_embeddings: np.ndarray,
    sentences: List[str],
    labels: np.ndarray,
    used_triplets: Set[Tuple[int, int, int]],
    triplets_dict: Dict[str, Any], # Consider more specific type for triplets_dict
    logger: Any, # e.g., logging.Logger
    num_processes: int = 4, # Number of processes for parallel processing
):
    # Rule 6: "Rank triplets according to Chest ImaGenome labels"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if acc(A, P) > 0 and acc(P, N) = acc(A, N) = 0, as long as CXR-BERT and Leveinshtein do not both disagree.
    # Where:
    # A = anchor = a sentence annotated with Chest ImaGenome labels
    # P = positive = another sentence annotated with Chest ImaGenome labels, sharing at least one label with A
    # N = negative = randomly sampled sentence annotated with Chest ImaGenome labels, not sharing any label with A and P
    # acc(X, Y) = accuracy between the labels of X and Y
    # In words: include the triplet (A, P, N) if A and P share at least one label and A/P and N share none, as long as CXR-BERT and Leveinshtein do not both disagree.
    
    logger.info(f'{ANSI_BLUE_BOLD}Rule 6: "Rank triplets according to Chest ImaGenome labels"{ANSI_RESET}')
    
    assert len(sentences) == len(labels), f'len(sentences)={len(sentences)} != len(labels)={len(labels)}'
    n, m = labels.shape
    rule_triplets = []
    
    idx2i = {sentence2index[s]: i for i, s in enumerate(sentences)}
    
    logger.info(f"Building label2idxs for {n} sentences and {m} labels...")
    logger.info(
        f"Using {num_processes} processes for parallel label2idxs computation."
    )
    # Split sentences and labels into chunks for multiprocessing
    # Ensure sentences_np is an array of objects if sentences have varying lengths
    sentences_np = np.array(sentences, dtype=object)
    sentence_chunks = np.array_split(sentences_np, num_processes)
    label_chunks = np.array_split(labels, num_processes, axis=0)

    # Prepare arguments for each process
    # Each process gets its chunk of sentences, labels, the full sentence2index, and m
    process_args = [
        (
            list(sentence_chunks[i]), # Convert back to list for the worker
            label_chunks[i],
            sentence2index,
            m,
        )
        for i in range(num_processes)
    ]

    with multiprocessing.Pool(processes=num_processes) as pool:
        # Use tqdm here if you want to track chunk processing progress
        results = list(tqdm(pool.imap(_process_label_chunk, process_args), total=num_processes, desc="Processing label chunks"))

    # Merge results from all processes
    label2idxs = _merge_label2idxs_results(results, m)
    logger.info("Finished building label2idxs.")

    n_triplets_per_label = math.ceil(n_triplets_per_rule / m)
    for i in tqdm(range(m), mininterval=2):
        idxs = label2idxs[i]
        if n_triplets_per_label < len(idxs):
            idxs.sort(key=lambda x: len(index2sentence[x]))
            idxs = [idxs[i] for i in np.linspace(0, len(idxs)-1, n_triplets_per_label, dtype=int)]
            assert len(idxs) == n_triplets_per_label
        n_triplets_per_idx = math.ceil(n_triplets_per_label / len(idxs))
        for A in idxs:
            A_labels = labels[idx2i[A]]
            assert A_labels[i] == 1
            for _ in range(n_triplets_per_idx):
                for _ in range(MAX_TRIES):
                    P = random.choice(label2idxs[i])
                    if A == P:
                        continue
                    L = random.randint(0, m) # randomly select a label (including "no label")
                    if L < m and A_labels[L] == 1: # if A already has label L -> skip
                        continue
                    if len(label2idxs[L]) == 0: # if no sentence has label L -> skip
                        continue
                    N = random.choice(label2idxs[L])
                    N_labels = labels[idx2i[N]]
                    if np.any(A_labels & N_labels): # if A and N share at least one label -> skip
                        continue
                    P_labels = labels[idx2i[P]]
                    if np.any(P_labels & N_labels): # if P and N share at least one label -> skip
                        continue
                    assert P_labels[i] == 1
                    assert np.any(A_labels & P_labels) # A and P must share at least one label
                    assert A != N and P != N
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AP_dotsim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
                    AN_dotsim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    if AN_dotsim > AP_dotsim: # if CXR-BERT disagrees -> skip
                        continue
                    AP_levsim = _levenshtein_similarity(index2sentence[A], index2sentence[P])
                    AN_levsim = _levenshtein_similarity(index2sentence[A], index2sentence[N])
                    if AN_levsim > AP_levsim: # if Leveinshtein disagrees -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop
        
    logger.info(f'Actual number of triplets sampled for Rule 6: "Rank triplets according to Chest ImaGenome labels": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Rank triplets according to Chest ImaGenome labels',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule7(
    n_triplets_per_rule,
    sentence_labels_pairs,
    sentence2index,
    index2cluster,
    used_triplets,
    triplets_dict,
    logger,
    margin=0.5, # margin for Jaccard index
    top_k=30, # top k sentences from the same cluster as A to consider for P
    n_samples_per_cluster=300, # number of random samples per cluster to look for P's
):
    # Rule 7: "RadGraph-based triplets"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if Jaccard(A, P) > Jaccard(A, N) + margin
    # Where:
    # A = anchor = a sentence with RadGraph annotations
    # P = positive = another sentence with RadGraph annotations, from the same cluster as A
    # N = negative = another sentence with RadGraph annotations
    # Jaccard(X, Y) = Jaccard index between sets of RadGraph labels (entities and relations) of X and Y
    # In words: include the triplet (A, P, N) if the Jaccard index between the sets of RadGraph labels of A and P
    #           is higher than the Jaccard index between the sets of RadGraph labels of A and N.
    
    logger.info(f'{ANSI_BLUE_BOLD}Rule 7: "RadGraph-based triplets"{ANSI_RESET}')
    rule_triplets = []

    if len(sentence_labels_pairs) > n_triplets_per_rule:
        sentence_labels_pairs = random.sample(sentence_labels_pairs, n_triplets_per_rule)
        n_triplets_per_anchor = 1
    else:
        n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(sentence_labels_pairs))

    logger.info(f'len(sentence_labels_pairs)={len(sentence_labels_pairs)}')
    logger.info('Computing cluster2idxs...')
    cluster2idxs = dict()
    idx2i = {}
    for i, p in tqdm(enumerate(sentence_labels_pairs), mininterval=2):
        s = p[0]
        s_idx = sentence2index[s]
        idx2i[s_idx] = i
        c_idx = index2cluster[s_idx]
        if c_idx not in cluster2idxs:
            cluster2idxs[c_idx] = []
        cluster2idxs[c_idx].append(s_idx)
    cluster_list = list(cluster2idxs.keys())
    
    logger.info('Sampling triplets...')
    logger.info(f'n_triplets_per_anchor={n_triplets_per_anchor}')
    for sentence, labels in tqdm(sentence_labels_pairs, mininterval=2):
        sentence_idx = sentence2index[sentence]
        sentence_cluster = index2cluster[sentence_idx]
        A = sentence_idx
        if n_samples_per_cluster < len(cluster2idxs[sentence_cluster]):
            random_idxs_for_P = random.sample(cluster2idxs[sentence_cluster], n_samples_per_cluster)
        else:
            random_idxs_for_P = cluster2idxs[sentence_cluster] # all indices in the cluster
        scores = [jaccard_between_sets(labels, sentence_labels_pairs[idx2i[idx]][1]) for idx in random_idxs_for_P]
        sorted_idxs = np.argsort(scores)[::-1] # sort in descending order
        P_list = []
        for j, i in enumerate(sorted_idxs):
            if scores[i] == 0:
                break # if we reach a score of 0, then all subsequent scores will also be 0
            s_idx = random_idxs_for_P[i]
            if s_idx == A:
                continue
            assert index2cluster[s_idx] == sentence_cluster
            P_list.append((s_idx, j)) # (sentence index, rank)
            if len(P_list) == top_k or len(P_list) == n_triplets_per_anchor:
                # We will consider at most the top 'top_k' sentences from the same cluster as A for P
                break
        assert len(P_list) <= n_triplets_per_anchor
        if len(P_list) == 0:
            continue
        n_triplets_per_positive = math.ceil(n_triplets_per_anchor / len(P_list))
        for P, rank in P_list:
            P_score = scores[sorted_idxs[rank]]
            assert P_score > 0
            for _ in range(n_triplets_per_positive):
                for _ in range(MAX_TRIES):
                    c = random.choice(cluster_list)
                    if c == sentence_cluster:
                        continue
                    N = random.choice(cluster2idxs[c])
                    assert N != A and N != P
                    N_score = jaccard_between_sets(labels, sentence_labels_pairs[idx2i[N]][1])
                    if N_score > 0 and N_score + margin > P_score:
                        continue
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop

    logger.info(f'Actual number of triplets sampled for Rule 7: "RadGraph-based triplets": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'RadGraph-based triplets',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule8(
    n_triplets_per_rule,
    sentence2paraphrases,
    anchor2negatives,
    sentence2index,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 8: "Hard negative triplets generated by ChatGPT"
    # dot(E(A), E(P)) > dot(E(A), E(N)) for each (A, P, N) generated by ChatGPT
    # Where:
    # A = anchor
    # P = positive = paraphrase of A (generated by ChatGPT)
    # N = negative = another sentence very similar to A, but with different semantics (generated by ChatGPT)
    
    logger.info(f'{ANSI_BLUE_BOLD}Rule 8: "Hard negative triplets generated by ChatGPT"{ANSI_RESET}')
    rule_triplets = []

    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(anchor2negatives))

    def get_triplets(A, P_list, N_list):
        assert len(P_list) > 0
        assert len(N_list) > 0
        if len(P_list) * len(N_list) <= n_triplets_per_anchor:
            for P in P_list:
                for N in N_list:
                    triplet = (A, P, N)
                    if triplet in used_triplets:
                        continue
                    yield triplet
        else:
            i, j = 0, 0
            for _ in range(n_triplets_per_anchor):
                for _ in range(MAX_TRIES):
                    P = P_list[i]
                    N = N_list[j]
                    triplet = (A, P, N)
                    if triplet in used_triplets:
                        if random.random() < 0.5:
                            i += 1
                            if i == len(P_list):
                                i = 0
                        else:
                            j += 1
                            if j == len(N_list):
                                j = 0
                        continue
                    yield triplet
                    i += 1
                    if i == len(P_list):
                        i = 0
                    j += 1
                    if j == len(N_list):
                        j = 0
                    break

    for anchor, negatives in tqdm(anchor2negatives.items(), mininterval=2):
        A = sentence2index[anchor]
        P_set = set(sentence2index[p] for p in sentence2paraphrases[anchor])
        P_set.discard(A)
        P_list = list(P_set)
        N_set = set()
        N_list_1 = []
        N_list_2 = []
        for n in negatives:
            N = sentence2index[n]
            if N == A or N in P_set or N in N_set:
                continue
            N_set.add(N)
            N_list_1.append(N)
            if n in sentence2paraphrases:
                for p in sentence2paraphrases[n]:
                    N = sentence2index[p]
                    if N == A or N in P_set or N in N_set:
                        continue
                    N_set.add(N)
                    N_list_2.append(N)
        N_list = N_list_1 + N_list_2
        assert A not in N_set
        for n in N_set: assert n not in P_set
        assert len(P_list) > 0
        assert len(N_list) > 0
        for triplet in get_triplets(A, P_list, N_list):
            used_triplets.add(triplet)
            rule_triplets.append(triplet)

    logger.info(f'Actual number of triplets sampled for Rule 8: "Hard negative triplets generated by ChatGPT": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Hard negative triplets generated by ChatGPT',
        'triplets': rule_triplets,
    })



# --- Helper functions for similarity ---

def _hybrid_score_similarity(
    sent_a: str,
    tok_sent_a: List[str],
    sent_embd_a: np.ndarray,
    sent_b: str,
    tok_sent_b: List[str],
    sent_embd_b: np.ndarray,
    alpha: float,
    beta: float,
    gamma: float,
) -> float:
    semantic_sim = np.dot(sent_embd_a, sent_embd_b) # Assumes sent_embd_a and sent_embd_b are normalized
    bleu1_sim_val = _bleu1_similarity(tok_sent_a, tok_sent_b)
    lev_sim_val = _levenshtein_similarity(sent_a, sent_b)
    return alpha * semantic_sim + beta * bleu1_sim_val + gamma * lev_sim_val


# --- Global dictionary for multiprocessing shared data ---
_worker_shared_data_rule9 = {}


def _init_worker_rule9(
    processed_abnormal_findings_data: List[Dict[str, Any]],
    score_weights_data: Tuple[float, float, float],
):
    """Initializes shared data for worker processes."""
    global _worker_shared_data_rule9
    _worker_shared_data_rule9["processed_abnormal_findings"] = processed_abnormal_findings_data
    _worker_shared_data_rule9["score_weights"] = score_weights_data


def _sample_triplets_for_cluster_worker(
    task_args: Tuple[int, int]
) -> List[Tuple[int, int, int]]:
    """
    Worker function to sample triplets for a specific cluster.
    Accesses shared data via the global `_worker_shared_data_rule9`.
    """
    cluster_id_to_process, num_triplets_to_generate_for_cluster = task_args

    # Access shared data (initialized by _init_worker_rule9)
    processed_abnormal_findings = _worker_shared_data_rule9["processed_abnormal_findings"]
    alpha, beta, gamma = _worker_shared_data_rule9["score_weights"]

    current_cluster_items = [
        item
        for item in processed_abnormal_findings
        if item["cluster_id"] == cluster_id_to_process
    ]
    other_clusters_items = [
        item
        for item in processed_abnormal_findings
        if item["cluster_id"] != cluster_id_to_process
    ]

    assert len(current_cluster_items) >= 50 # Sanity check: should have enough items in the current cluster
    assert len(other_clusters_items) >= 1000 # Sanity check: should have enough items in other clusters

    sampled_triplets: List[Tuple[int, int, int]] = []

    seen_triplets = set()  # To avoid duplicate triplets

    # STEP 1: Sample num_triplets_to_generate_for_cluster anchors from current_cluster_items.
    if len(current_cluster_items) < num_triplets_to_generate_for_cluster:
        anchor_items = current_cluster_items
        num_samples_per_item = (
            num_triplets_to_generate_for_cluster // len(anchor_items)
        ) + 1
    else:
        anchor_items = random.sample(
            current_cluster_items, num_triplets_to_generate_for_cluster
        )
        num_samples_per_item = 1
        

    # STEP 2: For each anchor, sample num_samples_per_item triplets.
    for anchor_item in anchor_items:
        for _ in range(num_samples_per_item):
            while True: # Loop until a valid triplet is found
                # Randomly select 10 items from the current cluster
                current_cluster_items_sample = random.sample(current_cluster_items, 10)
                # Remove the anchor item from the sample
                current_cluster_items_sample = [item for item in current_cluster_items_sample
                                                if item["original_idx"] != anchor_item["original_idx"]]
                # Randomly select 5 items from other clusters
                other_clusters_items_sample = random.sample(other_clusters_items, 5)
                # Concatenate the two samples
                candidates_pool = current_cluster_items_sample + other_clusters_items_sample
                # Find the best and worst candidates based on the hybrid score
                best_item = None
                best_score = float("-inf")
                worst_item = None
                worst_score = float("inf")
                for item in candidates_pool:
                    score = _hybrid_score_similarity(
                        anchor_item["text"],
                        anchor_item["tokens"],
                        anchor_item["embedding"],
                        item["text"],
                        item["tokens"],
                        item["embedding"],
                        alpha,
                        beta,
                        gamma,
                    )
                    if score > best_score:
                        best_score = score
                        best_item = item
                    if score < worst_score:
                        worst_score = score
                        worst_item = item
                # Ensure we have valid best and worst items
                assert best_item is not None
                assert worst_item is not None
                # Check if the triplet is unique
                triplet = (
                    anchor_item["original_idx"],
                    best_item["original_idx"],
                    worst_item["original_idx"],
                )
                if triplet not in seen_triplets:
                    seen_triplets.add(triplet)
                    sampled_triplets.append(triplet)
                    break # Exit the while loop if a valid triplet is found
    
    return sampled_triplets


# --- Main function to be completed ---
def sample_observation_triplets__rule9(
    triplets_dict: Dict[str, Any],
    num_triplets_to_sample: int,
    abnormal_visual_findings: List[str],
    abnormal_visual_finding_cluster_ids: List[int],
    sentence2index: Dict[str, int],
    sentence_embeddings: np.ndarray,
    logger: logging.Logger,
    num_processes: int,
    hybrid_alpha: float = 0.5,
    hybrid_beta: float = 0.3,
    hybrid_gamma: float = 0.2,
) -> None:
    """
    Sample triplets of abnormal visual findings.
    """
    assert len(abnormal_visual_findings) == len(abnormal_visual_finding_cluster_ids)
    assert len(abnormal_visual_findings) > 0
    assert num_triplets_to_sample > 0

    logger.info(
        f"{ANSI_BLUE_BOLD}Rule 9: Starting triplet sampling. Target: {num_triplets_to_sample} triplets.{ANSI_RESET}"
    )
    logger.info(
        f"Total abnormal visual findings provided: {len(abnormal_visual_findings)}"
    )

    # Ensure NLTK 'punkt' tokenizer data is available in the main process
    try:
        nltk.word_tokenize("test sentence for punkt check")
    except LookupError:
        logger.info("NLTK 'punkt' tokenizer not found. Downloading...")
        nltk.download("punkt", quiet=False)
        logger.info("'punkt' downloaded successfully.")

    processed_abnormal_findings: List[Dict[str, Any]] = []
    for i, text in enumerate(abnormal_visual_findings):
        original_idx = sentence2index[text]
        text_lower = text.lower()
        tokens = nltk.word_tokenize(text_lower)
        processed_abnormal_findings.append(
            {
                "text": text_lower, # Lowercased text for Levenshtein
                "tokens": tokens,  # Lowercased tokens for BLEU
                "embedding": sentence_embeddings[original_idx],
                "original_idx": original_idx,
                "cluster_id": abnormal_visual_finding_cluster_ids[i],
            }
        )
    
    logger.info(
        f"Successfully processed {len(processed_abnormal_findings)} findings for triplet sampling."
    )

    # Group findings by cluster_id
    clusters = defaultdict(list)
    for item in processed_abnormal_findings:
        clusters[item["cluster_id"]].append(item)

    # Distribute num_triplets_to_sample proportionally among valid clusters
    tasks: List[Tuple[int, int]] = []

    # Sort by cluster size (desc) for deterministic remainder distribution
    sorted_cluster_ids = sorted(
        clusters.keys(),
        key=lambda cid: len(clusters[cid]),
        reverse=True,
    )

    # Log cluster size distribution
    cluster_sizes = {cid: len(clusters[cid]) for cid in sorted_cluster_ids}
    logger.info(
        f"Cluster size distribution: {cluster_sizes}"
    )

    num_samples_per_cluster = math.ceil(num_triplets_to_sample / len(sorted_cluster_ids))
    for cid in sorted_cluster_ids:
        tasks.append((cid, num_samples_per_cluster))

    total_triplets_tasked = sum(task[1] for task in tasks)
    logger.info(
        f"Distributing {total_triplets_tasked} triplets among {len(tasks)} clusters for processing."
    )

    all_rule_triplets: List[Tuple[int, int, int]] = []
    shared_data_payload = (
        processed_abnormal_findings,
        (hybrid_alpha, hybrid_beta, hybrid_gamma),
    )

    # Use multiprocessing if num_processes > 1 and more than one task
    if num_processes > 1 and len(tasks) > 1:
        logger.info(
            f"Using multiprocessing with {num_processes} processes for {len(tasks)} tasks."
        )
        with multiprocessing.Pool(
            processes=num_processes,
            initializer=_init_worker_rule9,
            initargs=shared_data_payload,
        ) as pool:
            results = []
            with tqdm(total=len(tasks), desc="Processing clusters", unit="cluster") as pbar:
                for sampled_triplets in pool.imap_unordered(
                    _sample_triplets_for_cluster_worker,
                    tasks,
                ):
                    results.append(sampled_triplets)
                    pbar.update(1)
        for res_list in results:
            all_rule_triplets.extend(res_list)
        logger.info(
            f"Generated {len(all_rule_triplets)} triplets via multiprocessing."
        )
    else:
        logger.info(
            "Using sequential execution (num_processes <= 1 or single task)."
        )
        _init_worker_rule9(*shared_data_payload)  # Init for main process
        for task_cluster_id, task_num_triplets in tqdm(
            tasks, desc="Processing clusters", unit="cluster"
        ):
            res_list = _sample_triplets_for_cluster_worker((task_cluster_id, task_num_triplets))
            all_rule_triplets.extend(res_list)
        logger.info(f"Generated {len(all_rule_triplets)} triplets sequentially.")

    logger.info(f'Actual number of triplets sampled for Rule 9: "Abnormal visual findings triplets": {len(all_rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Abnormal visual findings triplets',
        'triplets': all_rule_triplets,
    })


def retrieve_or_precompute_embeddings(
    sentences_list: List[str],
    args: Any,
    logger: logging.Logger
) -> np.ndarray:
    """
    Retrieve or compute sentence embeddings for a list of sentences.

    This function attempts to load precomputed embeddings for the given list of
    sentences from disk. If the embeddings are not found, it computes them using
    both a BERT-based model and CheXbert, concatenates the results, reduces
    dimensionality with PCA, saves the result to disk, and returns the final
    embeddings.

    Args:
        sentences_list (List[str]): List of sentences to embed.
        args (Any): Arguments object with the following attributes:
            - bert_model_name (str): Name of the BERT model to use.
            - bert_checkpoint_folder_path (str): Path to BERT model checkpoints.
            - device (str): Device to run the embedding model on.
            - batch_size (int): Batch size for embedding computation.
            - num_workers (int): Number of workers for data loading.
            - pca_components (int): Number of PCA components for dimensionality reduction.
        logger (Logger): Logger object for logging progress and information.

    Returns:
        np.ndarray: Array of embeddings with shape (len(sentences_list), pca_components).
    """
    # Compute a unique hash for the list of sentences to use as a cache key
    sentences_hash = hash_string_list(sentences_list)
    save_path = os.path.join(
        MIMICCXR_LARGE_FAST_CACHE_DIR,
        f"embeddings_cxrbert_chexbert_pca_{args.pca_components}_{sentences_hash[0]}_{sentences_hash[1]}.npy"
    )

    # If embeddings already exist on disk, load and return them
    if os.path.exists(save_path):
        logger.info(f"Loading precomputed embeddings from {save_path}...")
        embeddings = np.load(save_path)
        assert embeddings.shape[0] == len(sentences_list), \
            "Loaded embeddings do not match the number of sentences."
        logger.info(f"Loaded embeddings shape: {embeddings.shape}")
        return embeddings

    # Compute BERT-based embeddings for all sentences
    emb_extractor = CachedTextEmbeddingExtractor(
        model_name=args.bert_model_name,
        model_checkpoint_folder_path=args.bert_checkpoint_folder_path,
        device=args.device,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
    )
    bert_embeddings = emb_extractor.compute_text_embeddings(
        sentences_list, update_cache_on_disk=True
    )
    assert bert_embeddings.shape[0] == len(sentences_list), \
        "BERT embeddings do not match the number of sentences."
    logger.info(f"bert_embeddings.shape: {bert_embeddings.shape}")

    # Compute CheXbert embeddings for all sentences
    from medvqa.metrics.medical.chexbert import CheXbertLabeler
    chexbert_labeler = CheXbertLabeler(
        verbose=True, default_batch_size=args.batch_size
    )
    chexbert_embeddings = chexbert_labeler.get_embeddings(
        sentences_list, update_cache_on_disk=True
    )
    # Normalize CheXbert embeddings to unit norm
    chexbert_embeddings = chexbert_embeddings / np.linalg.norm(
        chexbert_embeddings, axis=1, keepdims=True
    )
    assert chexbert_embeddings.shape[0] == len(sentences_list), \
        "CheXbert embeddings do not match the number of sentences."
    logger.info(f"chexbert_embeddings.shape: {chexbert_embeddings.shape}")

    # Concatenate BERT and CheXbert embeddings along the feature axis
    combined_embeddings = np.concatenate(
        (bert_embeddings, chexbert_embeddings), axis=1
    )
    logger.info(f"combined_embeddings.shape: {combined_embeddings.shape}")

    # Reduce dimensionality of concatenated embeddings using PCA
    from sklearn.decomposition import PCA
    pca = PCA(n_components=args.pca_components, random_state=0)
    embeddings = pca.fit_transform(combined_embeddings)
    logger.info(f"embeddings.shape: {embeddings.shape}")

    # Normalize the embeddings to unit norm
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

    # Save the computed embeddings to disk for future use
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    np.save(save_path, embeddings)
    logger.info(f"Saved embeddings to {save_path}")

    return embeddings

def retrieve_or_precompute_kmeans_clusters(
    embeddings: np.ndarray,
    num_clusters: int,
    num_kmeans_iterations: int,
    logger: logging.Logger,
):
    save_path = get_file_path_with_hashing_if_too_long(
        folder_path=MIMICCXR_LARGE_FAST_CACHE_DIR,
        prefix=f'kmeans_clusters_',
        strings=[
            f'num_clusters={num_clusters}',
            f'num_kmeans_iterations={num_kmeans_iterations}',
            f'embeddings_shape={embeddings.shape}',
            f'embeddings_dtype={embeddings.dtype}',
            f'embeddings[0].sum()={embeddings[0].sum():.2f}',
            f'embeddings[-1].sum()={embeddings[-1].sum():.2f}',
            f'embeddings.sum()={embeddings.sum():.2f}',
        ],
        ext='.npy',
        force_hashing=True,
    )
    if os.path.exists(save_path):
        logger.info(f"Loading precomputed KMeans clusters from {save_path}...")
        kmeans_labels = np.load(save_path)
        assert len(kmeans_labels) == embeddings.shape[0], \
            "Loaded KMeans labels do not match the number of embeddings."
        logger.info(f"Loaded KMeans clusters with shape={kmeans_labels.shape}")
        return kmeans_labels
    logger.info(f"Running KMeans for {num_clusters} clusters on embeddings with shape={embeddings.shape}")
    kmeans = KMeans(
        n_clusters=num_clusters, random_state=0, n_init='auto', verbose=0,
        max_iter=num_kmeans_iterations).fit(embeddings)
    kmeans_labels = kmeans.labels_
    # Save the KMeans labels to disk for future use
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    np.save(save_path, kmeans_labels)
    logger.info(f"Saved KMeans clusters to {save_path}")
    return kmeans_labels

def main():
    parser = argparse.ArgumentParser(description='Sample triplets for fact embedding learning')
    parser.add_argument('--paraphrased_anatomical_locations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--paraphrased_observations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--integrated_fact_metadata_filepath', type=str, required=True)
    parser.add_argument('--integrated_chest_imagenome_observations_filepath', type=str, required=True)
    parser.add_argument('--integrated_chest_imagenome_anatomical_locations_filepath', type=str, required=True)
    parser.add_argument('--radgraph_sentences_and_labels_filepath', type=str, required=True)
    parser.add_argument('--hard_triplets_from_facts_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--revised_groundings_file', type=str, required=True)
    parser.add_argument('--num_anatloc_clusters', type=int, required=True)
    parser.add_argument('--num_obs_clusters', type=int, required=True)
    parser.add_argument('--num_radgraph_clusters', type=int, required=True)
    parser.add_argument('--num_abnormal_visual_finding_clusters', type=int, required=True)
    parser.add_argument('--num_train_triplets_per_rule', type=int, required=True)
    parser.add_argument('--num_val_triplets_per_rule', type=int, required=True)
    parser.add_argument('--num_test_triplets_per_rule', type=int, required=True)

    parser.add_argument('--bert_model_name', type=str, default=SupportedHuggingfaceMedicalBERTModels.BiomedVLP_CXR_BERT_specialized,
                        choices=SupportedHuggingfaceMedicalBERTModels.get_all())
    parser.add_argument('--bert_checkpoint_folder_path', type=str, default=None)
    parser.add_argument('--pca_components', type=int, default=50,
                        help='Number of PCA components to reduce the sentence embeddings to.')
    
    parser.add_argument('--logging_level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'])
    parser.add_argument("--device", type=str, default="GPU", choices=["GPU", "CPU"])
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--num_kmeans_iterations', type=int, default=300)
    parser.add_argument('--num_processes', type=int, default=1)
    parser.add_argument('--hybrid_alpha', type=float, default=0.5,
                        help='Weight for the semantic similarity in the hybrid score.')
    parser.add_argument('--hybrid_beta', type=float, default=0.3,
                        help='Weight for the BLEU-1 similarity in the hybrid score.')
    parser.add_argument('--hybrid_gamma', type=float, default=0.2,
                        help='Weight for the Levenshtein similarity in the hybrid score.')
    args = parser.parse_args()

    # Add validation and test triplets to training triplets (since they will be subtracted from the training triplets later)
    args.num_train_triplets_per_rule += args.num_val_triplets_per_rule
    args.num_train_triplets_per_rule += args.num_test_triplets_per_rule
    
    # Collect all sentences, observations and anatomical locations from input files
    logger.info('Collecting all sentences, observations and anatomical locations from input files...')
    
    sentences_set = set()
    anatomical_locations_set = set()
    observations_set = set()
    radgraph_sentences_set = set()
    
    logger.info(f'Loading integrated fact metadata from {args.integrated_fact_metadata_filepath}...')
    integrated_fact_metadata = load_jsonl(args.integrated_fact_metadata_filepath)
    logger.info(f'Found {len(integrated_fact_metadata)} integrated fact metadata rows.')
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        anat_loc = row['metadata']['anatomical location']
        assert len(fact) > 0
        sentences_set.add(fact)
        observations_set.add(fact)
        if det_obs:
            sentences_set.add(det_obs)
            observations_set.add(det_obs)
        if short_obs:
            sentences_set.add(short_obs)
            observations_set.add(short_obs)
        if anat_loc:
            sentences_set.add(anat_loc)
            anatomical_locations_set.add(anat_loc)

    sentence2paraphrases = {}
    
    for filepath in args.paraphrased_anatomical_locations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_anatomical_locations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_anatomical_locations)} paraphrased anatomical locations.')
        for row in paraphrased_anatomical_locations:
            anatloc = row['metadata']['anatomical location']
            paraphrases = row['parsed_response']
            sentences_set.add(anatloc)
            sentences_set.update(paraphrases)
            anatomical_locations_set.add(anatloc)
            anatomical_locations_set.update(paraphrases)
            if anatloc not in sentence2paraphrases:
                sentence2paraphrases[anatloc] = set()
            sentence2paraphrases[anatloc].update(paraphrases)
    
    for filepath in args.paraphrased_observations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_observations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_observations)} paraphrased observations.')
        for row in paraphrased_observations:
            try:
                observation = row['metadata']['query']
            except KeyError:
                observation = row['metadata']['observation'] # backward compatibility
            paraphrases = row['parsed_response']
            sentences_set.add(observation)
            sentences_set.update(paraphrases)
            observations_set.add(observation)
            observations_set.update(paraphrases)
            if observation not in sentence2paraphrases:
                sentence2paraphrases[observation] = set()
            sentence2paraphrases[observation].update(paraphrases)

    logger.info(f'Loading hard triplets from facts from {args.hard_triplets_from_facts_filepaths}...')
    anchor2negatives = {}
    for filepath in args.hard_triplets_from_facts_filepaths:
        rows = load_jsonl(filepath)
        logger.info(f'Loaded {len(rows)} rows from {filepath}.')
        for row in rows:
            anchor = row['metadata']['query']
            positives = row['parsed_response']['positives']
            negatives = row['parsed_response']['negatives']
            sentences_set.add(anchor)
            sentences_set.update(positives)
            sentences_set.update(negatives)
            if anchor not in sentence2paraphrases:
                sentence2paraphrases[anchor] = set()
            sentence2paraphrases[anchor].update(positives)
            if anchor not in anchor2negatives:
                anchor2negatives[anchor] = set()
            anchor2negatives[anchor].update(negatives)

    logger.info(f'Loading Chest ImaGenome observations from {args.integrated_chest_imagenome_observations_filepath}...')
    chest_imagenome_observations_data = load_pickle(args.integrated_chest_imagenome_observations_filepath)
    for group in chest_imagenome_observations_data['groups']:
        sentences_set.update(group['sentences'])
        logger.info(f'len(group["sentences"])={len(group["sentences"])}')
    
    logger.info(f'Loading Chest ImaGenome anatomical locations from {args.integrated_chest_imagenome_anatomical_locations_filepath}...')
    chest_imagenome_anatomical_locations_data = load_pickle(args.integrated_chest_imagenome_anatomical_locations_filepath)
    for group in chest_imagenome_anatomical_locations_data['groups']:
        sentences_set.update(group['sentences'])
        logger.info(f'len(group["sentences"])={len(group["sentences"])}')

    logger.info(f'Loading RadGraph sentences and labels from {args.radgraph_sentences_and_labels_filepath}...')
    radgraph_data = load_pickle(args.radgraph_sentences_and_labels_filepath)
    for key in ('train', 'dev', 'test', 'chexpert', 'mimiccxr'):
        logger.info(f'len(radgraph_data[{key}])={len(radgraph_data[key])}')
        for s in radgraph_data[key]:
            radgraph_sentences_set.add(s)
            sentences_set.add(s)

    # Load data for rule 9
    revised_data = load_pickle(args.revised_groundings_file)
    logger.info(f"Loaded {len(revised_data)} entries from {args.revised_groundings_file}")

    collected_groundings = collect_phrase_groundings_from_padchest_gr_and_mscxr()
    logger.info(f"Collected {len(collected_groundings)} groundings from PadChest-GR and MS-CXR")

    abnormal_visual_findings = set()
    abnormal_visual_findings.update(phrase for entry in revised_data for phrase in entry["phrase2locations"].keys())
    abnormal_visual_findings.update(item['phrase'] for item in collected_groundings)
    abnormal_visual_findings = list(abnormal_visual_findings)
    abnormal_visual_findings.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    logger.info(f'Found {len(abnormal_visual_findings)} unique abnormal visual findings.')
    
    sentences_set.update(abnormal_visual_findings) # Add to sentences set
    
    logger.info(f'Found {len(sentences_set)} unique sentences.')
    sentences_list = list(sentences_set)
    sentences_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    sentence2index = {s: i for i, s in enumerate(sentences_list)} # Map sentence to index

    sentences_lower_list = [s.lower() for s in sentences_list]
    sentences_tokens_list = wordpunct_tokenize_texts_in_parallel(sentences_lower_list, num_workers=args.num_processes)

    anatloc2paraphrases = { s:ps for s, ps in sentence2paraphrases.items() if s in anatomical_locations_set }
    for ps in anatloc2paraphrases.values():
        anatomical_locations_set.update(ps)
    anatomical_locations_list = list(anatomical_locations_set)
    anatomical_locations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    
    logger.info(f'Found {len(anatomical_locations_set)} unique anatomical locations.')
    logger.info(f'len(anatloc2paraphrases)={len(anatloc2paraphrases)}')

    obs2paraphrases = { s:ps for s, ps in sentence2paraphrases.items() if s in observations_set }
    for ps in obs2paraphrases.values():
        observations_set.update(ps)
    observations_list = list(observations_set)
    observations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically

    logger.info(f'Found {len(observations_set)} unique observations.')
    logger.info(f'len(obs2paraphrases)={len(obs2paraphrases)}')

    # Convert anatomical locations and observations to indices
    anatloc_indices = [sentence2index[s] for s in anatomical_locations_list]
    obs_indices = [sentence2index[s] for s in observations_list]

    # Print some examples
    logger.info('Examples of anatomical locations:')
    for i in np.linspace(0, len(anatomical_locations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {anatomical_locations_list[i]}')

    logger.info('Examples of observations:')
    for i in np.linspace(0, len(observations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {observations_list[i]}')

    # Precompute embeddings for all sentences
    embeddings = retrieve_or_precompute_embeddings(
        sentences_list=sentences_list,
        args=args,
        logger=logger
    )

    # Precompute clusters for anatomical locations and observations
    anatloc_embeddings = embeddings[anatloc_indices]
    logger.info(f"Running KMeans for anatomical locations with anatloc_embeddings.shape={anatloc_embeddings.shape}")
    anatloc_cluster_labels = retrieve_or_precompute_kmeans_clusters(
        embeddings=anatloc_embeddings,
        num_clusters=args.num_anatloc_clusters,
        num_kmeans_iterations=args.num_kmeans_iterations,
        logger=logger,
    )

    obs_embeddings = embeddings[obs_indices]
    logger.info(f"Running KMeans for observations with obs_embeddings.shape={obs_embeddings.shape}")
    obs_cluster_labels = retrieve_or_precompute_kmeans_clusters(
        embeddings=obs_embeddings,
        num_clusters=args.num_obs_clusters,
        num_kmeans_iterations=args.num_kmeans_iterations,
        logger=logger,
    )

    # Convert clusters to a more convenient format    
    obs_idx2cluster = {}
    obs_cluster2indices = [[] for _ in range(args.num_obs_clusters)]
    for i, c in zip(obs_indices, obs_cluster_labels):
        obs_cluster2indices[c].append(i)
        obs_idx2cluster[i] = c
    assert all(len(x) > 0 for x in obs_cluster2indices)
    
    anatloc_idx2cluster = {}
    anatloc_cluster2indices = [[] for _ in range(args.num_anatloc_clusters)]
    for i, c in zip(anatloc_indices, anatloc_cluster_labels):
        anatloc_cluster2indices[c].append(i)
        anatloc_idx2cluster[i] = c
    assert all(len(x) > 0 for x in anatloc_cluster2indices)

    # Precompute clusters for radgraph sentences
    radgraph_indices = [sentence2index[s] for s in radgraph_sentences_set]
    radgraph_indices.sort()
    radgraph_embeddings = embeddings[radgraph_indices]
    logger.info(f"Running KMeans for RadGraph sentences with radgraph_embeddings.shape={radgraph_embeddings.shape}")
    radgraph_cluster_labels = retrieve_or_precompute_kmeans_clusters(
        embeddings=radgraph_embeddings,
        num_clusters=args.num_radgraph_clusters,
        num_kmeans_iterations=args.num_kmeans_iterations,
        logger=logger,
    )

    # Precompute clusters for abnormal visual findings
    abnormal_visual_finding_indices = [sentence2index[s] for s in abnormal_visual_findings]
    abnormal_visual_finding_embeddings = embeddings[abnormal_visual_finding_indices]
    logger.info(f"Running KMeans for abnormal visual findings with abnormal_visual_finding_embeddings.shape={abnormal_visual_finding_embeddings.shape}")
    abnormal_visual_finding_cluster_labels = retrieve_or_precompute_kmeans_clusters(
        embeddings=abnormal_visual_finding_embeddings,
        num_clusters=args.num_abnormal_visual_finding_clusters,
        num_kmeans_iterations=args.num_kmeans_iterations,
        logger=logger,
    )


    # =================
    # Triplet sampling
    # =================

    used_triplets = set()

    triplets = {
        'sentences': sentences_list,
        'train': {
            'anatomical_locations': [],
            'observations': [],
        },
        'val': {
            'anatomical_locations': [],
            'observations': [],
        },
        'test': {
            'anatomical_locations': [],
            'observations': [],
        },
    }

    config_string = ';'.join([
        f'num_anatloc_clusters={args.num_anatloc_clusters}',
        f'num_obs_clusters={args.num_obs_clusters}',
        f'num_radgraph_clusters={args.num_radgraph_clusters}',
        f'num_train_triplets_per_rule={args.num_train_triplets_per_rule}',
        f'num_val_triplets_per_rule={args.num_val_triplets_per_rule}',
        f'num_test_triplets_per_rule={args.num_test_triplets_per_rule}',
        f'num_kmeans_iterations={args.num_kmeans_iterations}',
        f'bert_model_name={args.bert_model_name}',
        f'bert_checkpoint_folder_path={args.bert_checkpoint_folder_path}',
    ])
    config_hash = hash_string(config_string)
    triplets_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                      f'triplets({len(sentences_list)},{len(anatomical_locations_list)},{len(observations_list)},'
                                      f'{len(radgraph_sentences_set)},{len(abnormal_visual_findings)},'
                                      f'{args.num_train_triplets_per_rule},{args.num_val_triplets_per_rule},{args.num_test_triplets_per_rule},'
                                      f'{config_hash[0]},{config_hash[1]}).pkl')

    # --------------------------------------------
    # 1) Sample triplets for anatomical locations
    # --------------------------------------------
    
    logger.info(f'{ANSI_BOLD}============================================================{ANSI_RESET}')
    logger.info(f'{ANSI_BOLD}Sampling triplets for anatomical locations...{ANSI_RESET}')
    
    sample_anatomical_location_triplets__rule1(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc2paraphrases=anatloc2paraphrases,
        n_anatloc_clusters=args.num_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    sample_anatomical_location_triplets__rule2(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc2paraphrases=anatloc2paraphrases,
        n_anatloc_clusters=args.num_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        anatloc_idx2cluster=anatloc_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    sample_anatomical_location_triplets__rule3(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc_indices=anatloc_indices,
        n_anatloc_clusters=args.num_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        anatloc_idx2cluster=anatloc_idx2cluster,
        sentence_embeddings=embeddings,
        sentences_lower_list=sentences_lower_list,
        sentences_tokens_list=sentences_tokens_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
        num_processes=args.num_processes,
    )

    # ------------------------------------
    # 2) Sample triplets for observations
    # ------------------------------------

    logger.info(f'{ANSI_BOLD}============================================================{ANSI_RESET}')
    logger.info(f'{ANSI_BOLD}Sampling triplets for observations...{ANSI_RESET}')

    # Rule 1: "Rank paraphrases very close to each other"
    sample_observation_triplets__rule1(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        n_obs_clusters=args.num_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 2: "Rank paraphrases very close to each other - Hard negative Sampling"
    sample_observation_triplets__rule2(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        n_obs_clusters=args.num_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        obs_idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )
    
    logger.info('---------------------------------------')

    # First, we need to map observations to health statuses
    hs2id = {}
    obs_idx_2_hsid = {}
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        health_status = row['metadata']['health status']
        try:
            hsid = hs2id[health_status]
        except KeyError:
            hsid = len(hs2id)
            hs2id[health_status] = hsid
        assert len(fact) > 0
        obs_idx_2_hsid[sentence2index[fact]] = hsid
        if det_obs:
            obs_idx_2_hsid[sentence2index[det_obs]] = hsid
        if short_obs:
            obs_idx_2_hsid[sentence2index[short_obs]] = hsid
    logger.info(f'len(hs2id): {len(hs2id)}')
    logger.info(f'hs2id: {hs2id}')
    hsid2hs = {v: k for k, v in hs2id.items()}
    count = 0
    for obs, paraphrases in obs2paraphrases.items():
        obs_idx = sentence2index[obs]
        if obs_idx in obs_idx_2_hsid:
            obs_hsid = obs_idx_2_hsid[obs_idx]
            for para in paraphrases:
                para_idx = sentence2index[para]
                obs_idx_2_hsid[para_idx] = obs_hsid
        else:
            count += 1
    # Print number of observations for each health status
    hsid2count = {i: 0 for i in range(len(hs2id))}
    for obs_idx, hsid in obs_idx_2_hsid.items():
        hsid2count[hsid] += 1
    logger.info(f'Number of observations for each health status:')
    for hsid, _ in sorted(list(hsid2count.items()), key=lambda x: x[1], reverse=True):
        logger.info(f'{hsid2count[hsid]} observations with health status "{hsid2hs[hsid]}"')
    # Print warning if some observations do not have a health status
    if count > 0:
        logger.warning(f'Could not find health status for {count} paraphrased observations.')

    hsid_cid_2_obs_idxs = {} # map (hsid, cid) to list of obs_idxs
    obs_idx_2_hsid_cid = {} # map obs_idx to (hsid, cid)
    for obs_idx, hsid in obs_idx_2_hsid.items():
        cid = obs_idx2cluster[obs_idx]
        hsid_cid = (hsid, cid)
        obs_idx_2_hsid_cid[obs_idx] = hsid_cid
        try:
            hsid_cid_2_obs_idxs[hsid_cid].append(obs_idx)
        except KeyError:
            hsid_cid_2_obs_idxs[hsid_cid] = [obs_idx]
    logger.info(f'len(hsid_cid_2_obs_idxs): {len(hsid_cid_2_obs_idxs)}')
    logger.info(f'len(obs_idx_2_hsid_cid): {len(obs_idx_2_hsid_cid)}')
    logger.info('---------------------------------------')

    # Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status"
    sample_observation_triplets__rule3(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs_idx_2_hsid_cid=obs_idx_2_hsid_cid,
        hsid_cid_2_obs_idxs=hsid_cid_2_obs_idxs,
        n_obs_clusters=args.num_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        sentence_embeddings=embeddings,
        sentences_lower_list=sentences_lower_list,
        sentences_tokens_list=sentences_tokens_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        num_processes=args.num_processes,
        logger=logger,
    )

    # Rule 4: "Hard health-status-vs-others triplets"
    sample_observation_triplets__rule4(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        obs_idx_2_hsid=obs_idx_2_hsid,
        obs_cluster2indices=obs_cluster2indices,
        obs_idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 5: "Short observation, detailed observation, and fact must be close to each other"
    sample_observation_triplets__rule5(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        integrated_fact_metadata=integrated_fact_metadata,
        obs2paraphrases=obs2paraphrases,
        sentence2index=sentence2index,
        sentence_list=sentences_list,
        sentence_embeddings=embeddings,
        obs_idx2cluster=obs_idx2cluster,
        obs_cluster2indices=obs_cluster2indices,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )
    
    chest_imagenome_obs_sentences = []
    chest_imagenome_obs_labels = []
    for group in chest_imagenome_observations_data['groups']:
        chest_imagenome_obs_sentences.extend(group['sentences'])
        chest_imagenome_obs_labels.append(group['labels'])
        assert len(group['sentences']) == len(group['labels'])
    chest_imagenome_obs_labels = np.concatenate(chest_imagenome_obs_labels, axis=0)
    logger.info(f'len(chest_imagenome_obs_sentences): {len(chest_imagenome_obs_sentences)}')
    logger.info(f'chest_imagenome_obs_labels.shape: {chest_imagenome_obs_labels.shape}')

    chest_imagenome_anat_sentences = []
    chest_imagenome_anat_labels = []
    for group in chest_imagenome_anatomical_locations_data['groups']:
        chest_imagenome_anat_sentences.extend(group['sentences'])
        chest_imagenome_anat_labels.append(group['labels'])
        assert len(group['sentences']) == len(group['labels'])
    chest_imagenome_anat_labels = np.concatenate(chest_imagenome_anat_labels, axis=0)
    logger.info(f'len(chest_imagenome_anat_sentences): {len(chest_imagenome_anat_sentences)}')
    logger.info(f'chest_imagenome_anat_labels.shape: {chest_imagenome_anat_labels.shape}')

    # find the intersection of observations and anatomical locations
    chest_imagenome_sentences = list(set(chest_imagenome_obs_sentences) & set(chest_imagenome_anat_sentences))
    chest_imagenome_obs_s2i = { s: i for i, s in enumerate(chest_imagenome_obs_sentences) }
    chest_imagenome_anat_s2i = { s: i for i, s in enumerate(chest_imagenome_anat_sentences) }
    chest_imagenome_obs_labels = chest_imagenome_obs_labels[[chest_imagenome_obs_s2i[s] for s in chest_imagenome_sentences]]
    chest_imagenome_anat_labels = chest_imagenome_anat_labels[[chest_imagenome_anat_s2i[s] for s in chest_imagenome_sentences]]
    chest_imagenome_labels = np.concatenate([chest_imagenome_obs_labels, chest_imagenome_anat_labels], axis=1)
    logger.info(f'len(chest_imagenome_sentences): {len(chest_imagenome_sentences)}')
    logger.info(f'chest_imagenome_labels.shape: {chest_imagenome_labels.shape}')

    # Rule 6: "Chest ImaGenome-based triplets"
    sample_observation_triplets__rule6(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence2index=sentence2index,
        index2sentence=sentences_list,
        sentence_embeddings=embeddings,
        sentences=chest_imagenome_sentences,
        labels=chest_imagenome_labels,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
        num_processes=args.num_processes,
    )

    # Rule 7: "RadGraph-based triplets"
    radgraph_index2cluster = { i:c for i, c in zip(radgraph_indices, radgraph_cluster_labels) }
    s2l = dict()
    for key in ('train', 'dev', 'test', 'chexpert', 'mimiccxr'):
        for s, l in radgraph_data[key].items():
            if s in s2l:
                s2l[s] |= l # union of sets
            else:
                s2l[s] = l
    radgraph_sentence_labels_pairs = [(s, l) for s, l in s2l.items()]
    sample_observation_triplets__rule7(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence_labels_pairs=radgraph_sentence_labels_pairs,
        sentence2index=sentence2index,
        index2cluster=radgraph_index2cluster,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 8: "Hard negative triplets generated by ChatGPT"
    sample_observation_triplets__rule8(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence2paraphrases=sentence2paraphrases,
        anchor2negatives=anchor2negatives,
        sentence2index=sentence2index,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 9: "Sample triplets of abnormal visual findings from Chest ImaGenome, MS-CXR, PadChest-GR
    # according to a hybrid sampling strategy"
    sample_observation_triplets__rule9(
        num_triplets_to_sample=args.num_train_triplets_per_rule,
        abnormal_visual_findings=abnormal_visual_findings,
        abnormal_visual_finding_cluster_ids=abnormal_visual_finding_cluster_labels,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        logger=logger,
        num_processes=args.num_processes,
        hybrid_alpha=args.hybrid_alpha,
        hybrid_beta=args.hybrid_beta,
        hybrid_gamma=args.hybrid_gamma,
        triplets_dict=triplets,
    )

    # ==================================================================================================
    # Extract triplets for validation and test sets
    logger.info('Extracting triplets for validation and test sets...')
    for key in ['anatomical_locations', 'observations']:
        if len(triplets['train'][key]) == 0:
            continue
        for x in triplets['train'][key]:
            indices = random.sample(range(len(x['triplets'])), args.num_val_triplets_per_rule + args.num_test_triplets_per_rule)
            indices_set = set(indices)
            sampled_triplets = [x['triplets'][i] for i in indices]
            val_triplets = np.array(sampled_triplets[:args.num_val_triplets_per_rule])
            test_triplets = np.array(sampled_triplets[args.num_val_triplets_per_rule:])
            train_triplets = [x['triplets'][i] for i in range(len(x['triplets'])) if i not in indices_set]
            train_triplets = np.array(train_triplets)
            x['triplets'] = train_triplets
            triplets['val'][key].append({
                'rule': x['rule'],
                'triplets': val_triplets,
            })
            triplets['test'][key].append({
                'rule': x['rule'],
                'triplets': test_triplets,
            })

    # Save triplets
    logger.info(f'Saving triplets to {triplets_save_path}...')
    save_pickle(triplets, triplets_save_path)
    logger.info(f'Done!')

if __name__ == '__main__':
    main()   