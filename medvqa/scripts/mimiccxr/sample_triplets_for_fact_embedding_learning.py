import argparse
import numpy as np
import os
import random
import math
from tqdm import tqdm
from Levenshtein import distance as levenshtein_distance
from sklearn.cluster import KMeans

from medvqa.datasets.mimiccxr import MIMICCXR_LARGE_FAST_CACHE_DIR
from medvqa.utils.logging import get_console_logger
from medvqa.utils.files import load_jsonl, load_pickle, save_pickle
from medvqa.models.huggingface_utils import (
    compute_text_embeddings_with_BiomedVLP_CXR_BERT_specialized,
    compute_text_embeddings_with_BiomedVLP_BioVilT,
)
from medvqa.utils.metrics import jaccard_score_between_sets

class _BERT_MODEL_NAMES:
    BiomedVLP_CXR_BERT_specialized = 'BiomedVLP_CXR_BERT_specialized'
    BiomedVLP_BioVilT = 'BiomedVLP_BioVilT'
    @staticmethod
    def get_all():
        return [
            _BERT_MODEL_NAMES.BiomedVLP_CXR_BERT_specialized,
            _BERT_MODEL_NAMES.BiomedVLP_BioVilT,
        ]

MAX_TRIES = 10

def _levenshtein_similarity(a, b):
    return 1 - levenshtein_distance(a, b) / max(len(a), len(b))

def _is_at_least_x_percent_better(a, b, x):
    return a > 0 and a > b and (a - b) / a >= x
    
def randomly_select_nonduplicate_from_list(L, x):
    while True:
        y = random.choice(L)
        if y != x:
            return y

def _find_negatives_and_add_triplest_for_AP(A, P, n_negs, sentence_embeddings, sentence_list, forbidden_idxs_set,
                                            n_clusters, cluster2indices, used_triplets, triplets_list,
                                            same_cluster_as_anchor=False, a_cid=None):
    if same_cluster_as_anchor:
        assert a_cid is not None
        
    AP_sim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
    AP_levd = levenshtein_distance(sentence_list[A], sentence_list[P])
    for _ in range(n_negs):
        for _ in range(MAX_TRIES):
            if same_cluster_as_anchor:
                N = random.choice(cluster2indices[a_cid])
            else:
                cid = random.randint(0, n_clusters-1) # randomly select a cluster
                N = random.choice(cluster2indices[cid]) # randomly select an index from the cluster
            if N == A or N in forbidden_idxs_set: # if same as anchor or forbidden -> skip
                continue
            triplet = (A, P, N)
            if triplet in used_triplets: # if already used -> skip
                continue
            AN_sim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
            AN_levd = levenshtein_distance(sentence_list[A], sentence_list[N])
            if AN_sim > AP_sim and AN_levd < AP_levd: # if both CXR-BERT and Leveinshtein disagree -> skip
                continue
            # if we reach here, then we have found a valid triplet
            used_triplets.add(triplet)
            triplets_list.append(triplet)
            break # break out of for loop

def _rule1_common(
    n_triplets_per_rule,
    sentence2paraphrases,
    n_clusters,
    cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    field_name,
):
    # Rule 1: "Rank paraphrases very highly"
    # dot(E(A), E(P)) > dot(E(A), E(N)) unless cos(A, P) < cos(A, N) AND leveinshtein(A, P) > leveinshtein(A, N)
    # Where:
    # A = anchor (anatomical location or observation)
    # P = positive = a paraphrased version of A
    # N = negative = randomly sampled anatomical location or observation
    # In words: include the triplet (A, P, N) (i.e. trust the paraphrase generated by GPT-3.5/4)
    #           except if both CXR-BERT and Leveinshtein disagree at the same time.

    logger.info('Rule 1: "Rank paraphrases very highly"')

    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(sentence2paraphrases))
    rule_triplets = []
    
    for sentence, paraphrases in tqdm(sentence2paraphrases.items(), mininterval=2):
        idx = sentence2index[sentence]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{sentence}". Skipping...')
            continue

        n_triplets_per_para = math.ceil(n_triplets_per_iter * 0.5 / len(para_idxs))

        # triplets where analoc_idx is anchor
        A = idx
        for P in para_idxs:
            _find_negatives_and_add_triplest_for_AP(A, P, n_triplets_per_para, sentence_embeddings, sentence_list,
                                                    para_idxs_set, n_clusters, cluster2indices,
                                                    used_triplets, rule_triplets)

        # triplets where paraphrases are anchors
        para_idxs_ = para_idxs.copy()
        para_idxs_.append(A) # add A to para_idxs_
        para_idxs_set.add(A) # add A to para_idxs_set
        for A in para_idxs:
            for _ in range(n_triplets_per_para):
                P = randomly_select_nonduplicate_from_list(para_idxs_, A)
                _find_negatives_and_add_triplest_for_AP(A, P, 1, sentence_embeddings, sentence_list,
                                                        para_idxs_set, n_clusters, cluster2indices,
                                                        used_triplets, rule_triplets)
    
    logger.info(f'Actual number of triplets sampled for Rule 1: "Rank paraphrases very highly": {len(rule_triplets)}')
    triplets_dict['train'][field_name].append({
        'rule': 'Rank paraphrases very highly',
        'triplets': rule_triplets,
    })

def _rule2_common(
    n_triplets_per_rule,
    sentence2paraphrases,
    n_clusters,
    cluster2indices,
    idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    field_name,
):
    # Rule 2: "Rank paraphrases very highly - Hard negative"
    # We will sample negatives from the same cluster as the anchor

    logger.info('Rule 2: "Rank paraphrases very highly - Hard negative"')
    
    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(sentence2paraphrases))
    rule_triplets = []
    
    for sentence, paraphrases in tqdm(sentence2paraphrases.items(), mininterval=2):
        idx = sentence2index[sentence]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{sentence}". Skipping...')
            continue

        n_triplets_per_para = math.ceil(n_triplets_per_iter * 0.5 / len(para_idxs))

        # triplets where analoc_idx is anchor
        A = idx
        A_cid = idx2cluster[A]
        for P in para_idxs:
            _find_negatives_and_add_triplest_for_AP(A, P, n_triplets_per_para, sentence_embeddings, sentence_list,
                                                    para_idxs_set, n_clusters, cluster2indices,
                                                    used_triplets, rule_triplets, same_cluster_as_anchor=True, a_cid=A_cid)

        # triplets where paraphrases are anchors
        para_idxs_ = para_idxs.copy()
        para_idxs_.append(A) # add A to para_idxs_
        para_idxs_set.add(A) # add A to para_idxs_set
        for A in para_idxs:
            for _ in range(n_triplets_per_para):
                P = randomly_select_nonduplicate_from_list(para_idxs_, A)
                _find_negatives_and_add_triplest_for_AP(A, P, 1, sentence_embeddings, sentence_list,
                                                        para_idxs_set, n_clusters, cluster2indices,
                                                        used_triplets, rule_triplets, same_cluster_as_anchor=True, a_cid=A_cid)
    
    logger.info(f'Actual number of triplets sampled for Rule 2: "Rank paraphrases very highly - Hard negative": {len(rule_triplets)}')
    triplets_dict['train'][field_name].append({
        'rule': 'Rank paraphrases very highly - Hard negative',
        'triplets': rule_triplets,
    })

def sample_anatomical_location_triplets__rule1(
    n_triplets_per_rule,
    anatloc2paraphrases,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):  
    _rule1_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=anatloc2paraphrases,
        n_clusters=n_anatloc_clusters,
        cluster2indices=anatloc_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='anatomical_locations',
    )

def sample_anatomical_location_triplets__rule2(
    n_triplets_per_rule,
    anatloc2paraphrases,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    anatloc_idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule2_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=anatloc2paraphrases,
        n_clusters=n_anatloc_clusters,
        cluster2indices=anatloc_cluster2indices,
        idx2cluster=anatloc_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='anatomical_locations',
    )

def sample_anatomical_location_triplets__rule3(
    n_triplets_per_rule,
    anatloc_indices,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    anatloc_idx2cluster,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    top_k=20, # top k sentences from the same cluster as A to consider for P
    n_samples_per_cluster=300, # number of random samples per cluster to look for P's
):
    # Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = anatomical location
    # P = positive = another anatomical location randomly sampled from cluster(A)
    # N = negative = randomly sampled anatomical location
    # In words: include the triplet (A, P, N) if both CXR-BERT and Leveinshtein agree at the same time.
    
    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(anatloc_indices))
    rule_triplets = []

    logger.info('Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus"')

    for A in tqdm(anatloc_indices, mininterval=2):
        cid_A = anatloc_idx2cluster[A]
        random_idxs_for_P = anatloc_cluster2indices[cid_A]
        if n_samples_per_cluster < len(random_idxs_for_P):
            random_idxs_for_P = random.sample(random_idxs_for_P, n_samples_per_cluster)
        scores = [_levenshtein_similarity(sentence_list[A], sentence_list[idx]) +\
                  np.dot(sentence_embeddings[A], sentence_embeddings[idx])\
                    for idx in random_idxs_for_P]
        sorted_idxs = np.argsort(scores)[::-1] # sort in descending order
        P_list = []
        for j, i in enumerate(sorted_idxs):
            s_idx = random_idxs_for_P[i]
            if scores[i] == 0:
                break
            if s_idx == A:
                continue
            P_list.append((s_idx, j)) # (sentence index, rank)
            if len(P_list) == top_k or len(P_list) == n_triplets_per_anchor:
                # We will consider at most the top 'top_k' sentences from the same cluster as A for P
                break
        assert len(P_list) <= n_triplets_per_anchor
        if len(P_list) == 0:
            continue
        n_triplets_per_positive = math.ceil(n_triplets_per_anchor / len(P_list))
        for P, rank in P_list:
            AP_score = scores[sorted_idxs[rank]]
            assert AP_score > 0
            for _ in range(n_triplets_per_positive):
                for _ in range(MAX_TRIES):
                    c = random.randint(0, n_anatloc_clusters-1) # randomly select a cluster
                    if c == cid_A:
                        continue # skip if same cluster as A
                    N = random.choice(anatloc_cluster2indices[c]) # randomly select an index from the cluster
                    assert N != A and N != P
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AN_embsim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AN_levsim = _levenshtein_similarity(sentence_list[A], sentence_list[N])
                    AN_score = AN_embsim + AN_levsim
                    if _is_at_least_x_percent_better(AP_score, AN_score, 0.5):
                        # both CXR-BERT and Leveinshtein agree
                        used_triplets.add(triplet) # add triplet to used triplets
                        rule_triplets.append(triplet)
                        break # break out of for loop
    
    logger.info(f'Actual number of triplets sampled for Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus": {len(rule_triplets)}')
    triplets_dict['train']['anatomical_locations'].append({
        'rule': 'Rank some triplets according to CXR-BERT and Leveinshtein consensus',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule1(
    n_triplets_per_rule,
    obs2paraphrases,
    n_obs_clusters,
    obs_cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule1_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=obs2paraphrases,
        n_clusters=n_obs_clusters,
        cluster2indices=obs_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='observations',
    )

def sample_observation_triplets__rule2(
    n_triplets_per_rule,
    obs2paraphrases,
    n_obs_clusters,
    obs_cluster2indices,
    obs_idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule2_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=obs2paraphrases,
        n_clusters=n_obs_clusters,
        cluster2indices=obs_cluster2indices,
        idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='observations',
    )

def sample_observation_triplets__rule3(
    n_triplets_per_rule,
    obs_idx_2_hsid_cid,
    hsid_cid_2_obs_idxs,
    n_obs_clusters,
    obs_cluster2indices,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    top_k=10, # top k sentences from the same cluster as A to consider for P
    n_samples_per_cluster=100, # number of random samples per cluster to look for P's
):
    # Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if HS(A) = HS(P) AND cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = observation
    # P = positive = another observation randomly sampled from cluster(A)
    # N = negative = randomly sampled observation
    # In words: include the triplet (A, P, N) if both CXR-BERT and Leveinshtein agree at the same time
    #           and if the anchor and positive share the same health status.

    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(obs_idx_2_hsid_cid))
    rule_triplets = []
    logger.info('Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status"')
    
    for A, hsid_cid_A in tqdm(obs_idx_2_hsid_cid.items(), mininterval=2):
        random_idxs_for_P = hsid_cid_2_obs_idxs[hsid_cid_A]
        if n_samples_per_cluster < len(random_idxs_for_P):
            random_idxs_for_P = random.sample(random_idxs_for_P, n_samples_per_cluster)
        scores = [_levenshtein_similarity(sentence_list[A], sentence_list[idx]) +\
                  np.dot(sentence_embeddings[A], sentence_embeddings[idx])\
                    for idx in random_idxs_for_P]
        sorted_idxs = np.argsort(scores)[::-1] # sort in descending order
        P_list = []
        for j, i in enumerate(sorted_idxs):
            if scores[i] == 0:
                break
            s_idx = random_idxs_for_P[i]
            if s_idx == A:
                continue
            P_list.append((s_idx, j)) # (sentence index, rank)
            if len(P_list) == top_k or len(P_list) == n_triplets_per_anchor:
                # We will consider at most the top 'top_k' sentences from the same cluster as A for P
                break
        assert len(P_list) <= n_triplets_per_anchor
        if len(P_list) == 0:
            continue # no sentences with non-zero Levenshtein similarity
        n_triplets_per_positive = math.ceil(n_triplets_per_anchor / len(P_list))
        A_cluster = hsid_cid_A[1]
        for P, rank in P_list:
            AP_score = scores[sorted_idxs[rank]]
            assert AP_score > 0
            for _ in range(n_triplets_per_positive):
                for _ in range(MAX_TRIES):
                    c = random.randint(0, n_obs_clusters-1) # randomly select a cluster
                    if c == A_cluster:
                        continue # skip if cluster is the same as A's cluster
                    N = random.choice(obs_cluster2indices[c]) # randomly select an index from the cluster
                    assert N != A and N != P
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AN_embsim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AN_levsim = _levenshtein_similarity(sentence_list[A], sentence_list[N])
                    AN_score = AN_embsim + AN_levsim
                    if _is_at_least_x_percent_better(AP_score, AN_score, 0.5):
                        # both CXR-BERT and Leveinshtein agree
                        used_triplets.add(triplet) # add triplet to used triplets
                        rule_triplets.append(triplet)
                        break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus,'
                f' while anchor and positive share the same health status": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule4(
    n_triplets_per_rule,
    obs2paraphrases,
    obs_idx_2_hsid,
    obs_cluster2indices,
    obs_idx2cluster,
    sentence2index,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 4: "Hard health-status-vs-others triplets"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if HS(A) == HS(P) AND HS(N) != HS(A)
    # Where:
    # A = anchor = observation
    # P = positive = paraphrase of A
    # N = negative = randomly sampled observation from the same cluster as A but with a different health status
    # In words: include the triplet (A, P, N) if the anchor and positive are paraphrases of each other
    #           and if the negative is from the same cluster as the anchor but with a different health status.
    
    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(obs2paraphrases))
    rule_triplets = []
    logger.info('Rule 4: "Hard health-status-vs-others triplets"')
    
    for obs, paraphrases in tqdm(obs2paraphrases.items(), mininterval=2):
        obs_idx = sentence2index[obs]
        try:
            hsid = obs_idx_2_hsid[obs_idx]
        except KeyError:
            logger.warning(f'Could not find health status for observation "{obs}". Skipping...')
            continue
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.add(obs_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) < 2:
            logger.warning(f'Not enough paraphrases found for "{obs}". Skipping...')
            continue
        cluster_idxs = obs_cluster2indices[obs_idx2cluster[obs_idx]]
        for _ in range(n_triplets_per_iter):
            for _ in range(MAX_TRIES):
                A, P = random.sample(para_idxs, 2)
                assert A != P
                N = random.choice(cluster_idxs)
                if obs_idx_2_hsid[N] == hsid:
                    continue
                if N in para_idxs_set:
                    continue
                triplet = (A, P, N)
                if triplet in used_triplets: # already used
                    continue
                used_triplets.add(triplet) # add triplet to used triplets
                rule_triplets.append(triplet)
                break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 4: "Hard health-status-vs-others triplets": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Hard health-status-vs-others triplets',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule5(
    n_triplets_per_rule,
    integrated_fact_metadata,
    obs2paraphrases,
    sentence2index,
    sentence_list,
    sentence_embeddings,
    obs_idx2cluster,
    obs_cluster2indices,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 5: "Short observation, detailed observation, and fact must be close to each other - Hard negative"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = short observation, detailed observation, or fact, or a paraphrase of one of these
    # P = positive = some paraphrase of the same group as A, probably of a different length
    # N = negative = randomly sampled observation from the same cluster as A
    # In words: include the triplet (A, P, N) if A and P are paraphrases of the same fact with different degrees of detail
    #           (e.g. "cardiomegaly" vs "cardiomegaly is present" vs "cardiomegaly is present, which is an enlarged heart")
    #           and if the negative is from the same cluster as the anchor, as long as CXR-BERT and Leveinshtein do not both disagree.

    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(integrated_fact_metadata))
    rule_triplets = []
    logger.info('Rule 5: "Short observation, detailed observation, and fact must be close to each other - Hard negative"')

    def _add_triplets(obs_idxs, i_min, i_max, j_min, j_max, n_triplets_per_obs):
        for i in range(i_min, i_max+1): # first half (or second half)
            A = obs_idxs[i]
            A_cid = obs_idx2cluster[A]
            A_cluster_idxs = obs_cluster2indices[A_cid]
            for _ in range(n_triplets_per_obs):
                for _ in range(MAX_TRIES):
                    j = random.randint(j_min, j_max) # second half (or first half)
                    P = obs_idxs[j]
                    assert A != P
                    N = random.choice(A_cluster_idxs)
                    if N in obs_idxs_set:
                        continue
                    assert A != N and P != N
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AP_sim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
                    AN_sim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AP_levd = levenshtein_distance(sentence_list[A], sentence_list[P])
                    AN_levd = levenshtein_distance(sentence_list[A], sentence_list[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # if both CXR-BERT and Leveinshtein disagree -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop

    fail_count = 0
    
    for row in tqdm(integrated_fact_metadata, mininterval=2):
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        obs_idxs_set = set()
        # Add fact and its paraphrases to obs_idxs_set
        fact_idx = sentence2index[fact]
        obs_idxs_set.add(fact_idx)
        if fact in obs2paraphrases:
            obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[fact])
        # Add detailed observation and its paraphrases to obs_idxs_set
        if det_obs:
            det_obs_idx = sentence2index[det_obs]
            if det_obs_idx != fact_idx:
                obs_idxs_set.add(det_obs_idx)
                if det_obs in obs2paraphrases:
                    obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[det_obs])
        else:
            det_obs_idx = None
        # Add short observation and its paraphrases to obs_idxs_set
        if short_obs:
            short_obs_idx = sentence2index[short_obs]
            if short_obs_idx != fact_idx and short_obs_idx != det_obs_idx:
                obs_idxs_set.add(short_obs_idx)
                if short_obs in obs2paraphrases:
                    obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[short_obs])
        # Skip if obs_idxs_set has less than 2 observations
        if len(obs_idxs_set) < 2:
            fail_count += 1
            continue
        
        # Sort obs_idxs by length
        obs_idxs = list(obs_idxs_set)
        obs_idxs.sort(key=lambda x: len(sentence_list[x]))
        
        # Sample triples where the anchor is in the first half and the positive is in the second half and vice versa
        n_triplets_per_obs = math.ceil(n_triplets_per_iter / len(obs_idxs))
        half = len(obs_idxs) // 2
        _add_triplets(obs_idxs, 0, half-1, half, len(obs_idxs)-1, n_triplets_per_obs)
        _add_triplets(obs_idxs, half, len(obs_idxs)-1, 0, half-1, n_triplets_per_obs)
        
    if fail_count > 0:
        logger.warning(f'Failed to find enough observations for {fail_count}/{len(integrated_fact_metadata)} integrated fact metadata rows.')

    logger.info(f'Actual number of triplets sampled for Rule 5: "Short observation, detailed observation, and fact'
                f' must be close to each other - Hard negative": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Short observation, detailed observation, and fact must be close to each other - Hard negative',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule6(
    n_triplets_per_rule,
    sentence2index,
    index2sentence,
    sentence_embeddings,
    sentences,
    labels,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 6: "Rank triplets according to Chest ImaGenome labels"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if F1(A, P) > F1(A, N), as long as CXR-BERT and Leveinshtein do not both disagree.
    # Where:
    # A = anchor = a sentence annotated with Chest ImaGenome labels
    # P = positive = another sentence annotated with Chest ImaGenome labels, sharing at least one label with A
    # N = negative = randomly sampled sentence annotated with Chest ImaGenome labels, not sharing any label with A
    # F1(X, Y) = F1 score between the labels of X and Y
    # In words: include the triplet (A, P, N) if A and P share at least one label and if the F1 score between the labels of A and P
    #           is higher than the F1 score between the labels of A and N, as long as CXR-BERT and Leveinshtein do not both disagree.
    
    logger.info('Rule 6: "Rank triplets according to Chest ImaGenome labels"')
    assert len(sentences) == len(labels), f'len(sentences)={len(sentences)} != len(labels)={len(labels)}'
    n, m = labels.shape
    rule_triplets = []
    
    # For each label, collect the indices of sentences that are annotated with that label
    label2idxs = [[] for _ in range(m + 1)] # +1 for the "no label" label
    idx2i = {}
    for i in tqdm(range(n), mininterval=2):
        s = sentences[i]
        s_idx = sentence2index[s]
        idx2i[s_idx] = i
        no_label = True
        for j in range(m):
            if labels[i, j] == 1:
                label2idxs[j].append(s_idx)
                no_label = False
        if no_label:
            label2idxs[m].append(s_idx)

    n_triplets_per_label = math.ceil(n_triplets_per_rule / m)
    for i in tqdm(range(m), mininterval=2):
        idxs = label2idxs[i]
        if n_triplets_per_label < len(idxs):
            idxs.sort(key=lambda x: len(index2sentence[x]))
            idxs = [idxs[i] for i in np.linspace(0, len(idxs)-1, n_triplets_per_label, dtype=int)]
            assert len(idxs) == n_triplets_per_label
        n_triplets_per_idx = math.ceil(n_triplets_per_label / len(idxs))
        for A in idxs:
            A_labels = labels[idx2i[A]]
            for _ in range(n_triplets_per_idx):
                for _ in range(MAX_TRIES):
                    P = random.choice(label2idxs[i])
                    if A == P:
                        continue
                    L = random.randint(0, m) # randomly select a label (including "no label")
                    if L < m and A_labels[L] == 1: # if A already has label L -> skip
                        continue
                    if len(label2idxs[L]) == 0: # if no sentence has label L -> skip
                        continue
                    N = random.choice(label2idxs[L])
                    N_labels = labels[idx2i[N]]
                    if np.any(A_labels & N_labels): # if A and N share at least one label -> skip
                        continue
                    assert A != N and P != N
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AP_sim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
                    AN_sim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AP_levd = levenshtein_distance(index2sentence[A], index2sentence[P])
                    AN_levd = levenshtein_distance(index2sentence[A], index2sentence[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # if both CXR-BERT and Leveinshtein disagree -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop
        
    logger.info(f'Actual number of triplets sampled for Rule 6: "Rank triplets according to Chest ImaGenome labels": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Rank triplets according to Chest ImaGenome labels',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule7(
    n_triplets_per_rule,
    sentence_labels_pairs,
    sentence2index,
    index2cluster,
    used_triplets,
    triplets_dict,
    logger,
    margin=0.5, # margin for Jaccard index
    top_k=30, # top k sentences from the same cluster as A to consider for P
    n_samples_per_cluster=300, # number of random samples per cluster to look for P's
):
    # Rule 7: "RadGraph-based triplets (Machine Annotations)"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if Jaccard(A, P) > Jaccard(A, N) + margin
    # Where:
    # A = anchor = a sentence with machine annotations from RadGraph
    # P = positive = another sentence with machine annotations from RadGraph, from the same cluster as A
    # N = negative = another sentence with machine annotations from RadGraph
    # Jaccard(X, Y) = Jaccard index between sets of RadGraph labels (entities and relations) of X and Y
    # In words: include the triplet (A, P, N) if the Jaccard index between the sets of RadGraph labels of A and P
    #           is higher than the Jaccard index between the sets of RadGraph labels of A and N.
    
    logger.info('Rule 7: "RadGraph-based triplets"')
    rule_triplets = []

    logger.info(f'len(sentence_labels_pairs)={len(sentence_labels_pairs)}')
    logger.info('Computing cluster2idxs...')
    cluster2idxs = dict()
    idx2i = {}
    for i, p in tqdm(enumerate(sentence_labels_pairs), mininterval=2):
        s = p[0]
        s_idx = sentence2index[s]
        idx2i[s_idx] = i
        c_idx = index2cluster[s_idx]
        if c_idx not in cluster2idxs:
            cluster2idxs[c_idx] = []
        cluster2idxs[c_idx].append(s_idx)
    cluster_list = list(cluster2idxs.keys())

    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(sentence_labels_pairs))
    logger.info('Sampling triplets...')
    logger.info(f'n_triplets_per_anchor={n_triplets_per_anchor}')
    for sentence, labels in tqdm(sentence_labels_pairs, mininterval=2):
        sentence_idx = sentence2index[sentence]
        sentence_cluster = index2cluster[sentence_idx]
        A = sentence_idx
        assert n_samples_per_cluster < len(cluster2idxs[sentence_cluster])
        random_idxs_for_P = random.sample(cluster2idxs[sentence_cluster], n_samples_per_cluster)
        scores = [jaccard_score_between_sets(labels, sentence_labels_pairs[idx2i[idx]][1]) for idx in random_idxs_for_P]
        sorted_idxs = np.argsort(scores)[::-1] # sort in descending order
        P_list = []
        for j, i in enumerate(sorted_idxs):
            if scores[i] == 0:
                break # if we reach a score of 0, then all subsequent scores will also be 0
            s_idx = random_idxs_for_P[i]
            if s_idx == A:
                continue
            assert index2cluster[s_idx] == sentence_cluster
            P_list.append((s_idx, j)) # (sentence index, rank)
            if len(P_list) == top_k or len(P_list) == n_triplets_per_anchor:
                # We will consider at most the top 'top_k' sentences from the same cluster as A for P
                break
        assert len(P_list) <= n_triplets_per_anchor
        if len(P_list) == 0:
            continue
        n_triplets_per_positive = math.ceil(n_triplets_per_anchor / len(P_list))
        for P, rank in P_list:
            P_score = scores[sorted_idxs[rank]]
            assert P_score > 0
            for _ in range(n_triplets_per_positive):
                for _ in range(MAX_TRIES):
                    c = random.choice(cluster_list)
                    if c == sentence_cluster:
                        continue
                    N = random.choice(cluster2idxs[c])
                    assert N != A and N != P
                    N_score = jaccard_score_between_sets(labels, sentence_labels_pairs[idx2i[N]][1])
                    if N_score > 0 and N_score + margin > P_score:
                        continue
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop

    logger.info(f'Actual number of triplets sampled for Rule 7: "RadGraph-based triplets": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'RadGraph-based triplets',
        'triplets': rule_triplets,
    })

def main():
    parser = argparse.ArgumentParser(description='Sample triplets for fact embedding learning')
    parser.add_argument('--paraphrased_anatomical_locations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--paraphrased_observations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--integrated_fact_metadata_filepath', type=str, required=True)
    parser.add_argument('--chest_imagenome_sentences_and_labels_filepath', type=str, required=True)
    parser.add_argument('--radgraph_sentences_and_labels_filepath', type=str, required=True)
    parser.add_argument('--num_anatloc_clusters', type=int, required=True)
    parser.add_argument('--num_obs_clusters', type=int, required=True)
    parser.add_argument('--num_radgraph_clusters', type=int, required=True)
    parser.add_argument('--num_train_triplets_per_rule', type=int, required=True)
    parser.add_argument('--num_val_triplets_per_rule', type=int, required=True)
    parser.add_argument('--num_test_triplets_per_rule', type=int, required=True)

    parser.add_argument('--precomputed_sentence_embeddings_filepath', type=str, default=None)
    parser.add_argument('--bert_model_name', type=str, default='BiomedVLP_CXR_BERT_specialized', choices=_BERT_MODEL_NAMES.get_all())
    
    parser.add_argument('--logging_level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'])
    parser.add_argument("--device", type=str, default="GPU", choices=["GPU", "CPU"])
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--num_kmeans_iterations', type=int, default=300)
    args = parser.parse_args()

    # Add validation and test triplets to training triplets (since they will be subtracted from the training triplets later)
    args.num_train_triplets_per_rule += args.num_val_triplets_per_rule
    args.num_train_triplets_per_rule += args.num_test_triplets_per_rule

    # Set up logging
    logger = get_console_logger(args.logging_level)
    
    # Collect all sentences, observations and anatomical locations from input files
    logger.info('Collecting all sentences, observations and anatomical locations from input files...')
    
    sentences_set = set()
    anatomical_locations_set = set()
    observations_set = set()
    radgraph_sentences_set = set()
    
    logger.info(f'Loading integrated fact metadata from {args.integrated_fact_metadata_filepath}...')
    integrated_fact_metadata = load_jsonl(args.integrated_fact_metadata_filepath)
    logger.info(f'Found {len(integrated_fact_metadata)} integrated fact metadata rows.')
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        anat_loc = row['metadata']['anatomical location']
        assert len(fact) > 0
        sentences_set.add(fact)
        observations_set.add(fact)
        if det_obs:
            sentences_set.add(det_obs)
            observations_set.add(det_obs)
        if short_obs:
            sentences_set.add(short_obs)
            observations_set.add(short_obs)
        if anat_loc:
            sentences_set.add(anat_loc)
            anatomical_locations_set.add(anat_loc)
    
    anatloc2paraphrases = {}
    for filepath in args.paraphrased_anatomical_locations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_anatomical_locations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_anatomical_locations)} paraphrased anatomical locations.')
        for row in paraphrased_anatomical_locations:
            anatloc = row['metadata']['anatomical location']
            paraphrases = row['parsed_response']
            sentences_set.add(anatloc)
            sentences_set.update(paraphrases)
            anatomical_locations_set.add(anatloc)
            anatomical_locations_set.update(paraphrases)
            if anatloc not in anatloc2paraphrases:
                anatloc2paraphrases[anatloc] = set()
            anatloc2paraphrases[anatloc].update(paraphrases)
    logger.info(f'Found {len(anatomical_locations_set)} unique anatomical locations.')
    anatomical_locations_list = list(anatomical_locations_set)
    anatomical_locations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    
    obs2paraphrases = {}
    for filepath in args.paraphrased_observations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_observations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_observations)} paraphrased observations.')
        for row in paraphrased_observations:
            try:
                observation = row['metadata']['query']
            except KeyError:
                observation = row['metadata']['observation'] # backward compatibility
            paraphrases = row['parsed_response']
            sentences_set.add(observation)
            sentences_set.update(paraphrases)
            observations_set.add(observation)
            observations_set.update(paraphrases)
            if observation not in obs2paraphrases:
                obs2paraphrases[observation] = set()
            obs2paraphrases[observation].update(paraphrases)
    logger.info(f'Found {len(observations_set)} unique observations.')
    observations_list = list(observations_set)
    observations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically

    logger.info(f'Loading Chest ImaGenome sentences and labels from {args.chest_imagenome_sentences_and_labels_filepath}...')
    chest_imagenome_data = load_pickle(args.chest_imagenome_sentences_and_labels_filepath)
    for group in chest_imagenome_data['groups']:
        sentences_set.update(group['sentences'])
        logger.info(f'len(group["sentences"])={len(group["sentences"])}')

    logger.info(f'Loading RadGraph sentences and labels from {args.radgraph_sentences_and_labels_filepath}...')
    radgraph_data = load_pickle(args.radgraph_sentences_and_labels_filepath)
    for key in ('train', 'dev', 'test', 'chexpert', 'mimiccxr'):
        logger.info(f'len(radgraph_data[{key}])={len(radgraph_data[key])}')
        for s in radgraph_data[key]:
            radgraph_sentences_set.add(s)
            sentences_set.add(s)
    
    logger.info(f'Found {len(sentences_set)} unique sentences.')
    sentences_list = list(sentences_set)
    sentences_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    sentence2index = {s: i for i, s in enumerate(sentences_list)} # Map sentence to index

    # Convert anatomical locations and observations to indices
    anatloc_indices = [sentence2index[s] for s in anatomical_locations_list]
    obs_indices = [sentence2index[s] for s in observations_list]

    # Print some examples
    logger.info('Examples of anatomical locations:')
    for i in np.linspace(0, len(anatomical_locations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {anatomical_locations_list[i]}')

    logger.info('Examples of observations:')
    for i in np.linspace(0, len(observations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {observations_list[i]}')

    # Precompute BERT-based embeddings for all sentences
    if args.bert_model_name == _BERT_MODEL_NAMES.BiomedVLP_CXR_BERT_specialized:
        embedding_extraction_function = compute_text_embeddings_with_BiomedVLP_CXR_BERT_specialized
    elif args.bert_model_name == _BERT_MODEL_NAMES.BiomedVLP_BioVilT:
        embedding_extraction_function = compute_text_embeddings_with_BiomedVLP_BioVilT
    else: assert False

    sentlen_sum = sum(len(x) for x in sentences_list)
    sentence_embeddings_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                                 f'sentence_embeddings({args.bert_model_name},{len(sentences_list)},{sentlen_sum}).pkl')
    if os.path.exists(sentence_embeddings_save_path):
        logger.info(f'Found precomputed sentence embeddings at {sentence_embeddings_save_path}. Loading...')
        sentences_and_embeddings = load_pickle(sentence_embeddings_save_path)
        assert len(sentences_list) == len(sentences_and_embeddings['sentences'])
        assert all(s1 == s2 for s1, s2 in zip(sentences_list, sentences_and_embeddings['sentences']))
        embeddings = sentences_and_embeddings['embeddings']
        assert embeddings.shape[0] == len(sentences_list)
    else:
        logger.info(f'Precomputing sentence embeddings for {len(sentences_list)} sentences...')

        if args.precomputed_sentence_embeddings_filepath is not None:
            assert os.path.exists(args.precomputed_sentence_embeddings_filepath)
            logger.info(f'Loading precomputed sentence embeddings from {args.precomputed_sentence_embeddings_filepath}...')
            precomputed_sentence_embeddings = load_pickle(args.precomputed_sentence_embeddings_filepath)
            _precomputed_embeddings = precomputed_sentence_embeddings['embeddings']
            _sentences_to_skip = set(precomputed_sentence_embeddings['sentences'])
            _sentences_to_process = [s for s in sentences_list if s not in _sentences_to_skip]

            embeddings = np.zeros((len(sentences_list), _precomputed_embeddings.shape[1]), dtype=np.float32)

            if len(_sentences_to_process) == 0:
                logger.info(f'Found {len(_sentences_to_skip)} precomputed sentence embeddings. No sentences to process.')
                for i, s in enumerate(precomputed_sentence_embeddings['sentences']):
                    if s in sentence2index:
                        embeddings[sentence2index[s]] = _precomputed_embeddings[i]
            else:
                logger.info(f'Found {len(_sentences_to_skip)} precomputed sentence embeddings. Computing embeddings for {len(_sentences_to_process)} sentences...')
                random.shuffle(_sentences_to_process) # shuffle to minimize the chance of having multiple long sentences in a batch
                _embeddings = embedding_extraction_function(_sentences_to_process,
                                                            device=args.device,
                                                            batch_size=args.batch_size,
                                                            num_workers=args.num_workers,
                                                            )
                logger.info(f'_precomputed_embeddings.shape: {_precomputed_embeddings.shape}')
                logger.info(f'_embeddings.shape: {_embeddings.shape}')
                logger.info('Merge precomputed embeddings with newly computed embeddings...')
                embeddings = np.empty((len(sentences_list), _embeddings.shape[1]), dtype=np.float32)
                for i, s in tqdm(enumerate(precomputed_sentence_embeddings['sentences']), mininterval=2):
                    if s in sentence2index:
                        embeddings[sentence2index[s]] = _precomputed_embeddings[i]
                for i, s in tqdm(enumerate(_sentences_to_process), mininterval=2):
                    embeddings[sentence2index[s]] = _embeddings[i]
        else:
            embeddings = embedding_extraction_function(sentences_list,
                                                        device=args.device,
                                                        batch_size=args.batch_size,
                                                        num_workers=args.num_workers,
                                                        )
        # Sanity checks
        assert embeddings.shape[0] == len(sentences_list)
        for i in tqdm(range(len(sentences_list)), mininterval=2):
            assert np.allclose(embeddings[i], embeddings[sentence2index[sentences_list[i]]])
            assert embeddings[i].min() + 1e-7 < embeddings[i].max() # make sure embeddings are not all the same
        
        logger.info(f'embeddings.shape: {embeddings.shape}')
        logger.info(f'Saving embeddings to {sentence_embeddings_save_path}...')
        sentences_and_embeddings = {
            'sentences': sentences_list,
            'embeddings': embeddings,
        }
        save_pickle(sentences_and_embeddings, sentence_embeddings_save_path)
        logger.info(f"sentences_and_embeddings['embeddings'].shape: {sentences_and_embeddings['embeddings'].shape}")

    # Precompute clusters for anatomical locations and observations
    clusters_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                        f'clusters({args.num_anatloc_clusters},{args.num_obs_clusters}'
                                        f'{len(anatomical_locations_list)},{len(observations_list)},'
                                        f'{len(sentences_list)},{sentlen_sum}).pkl')
    if os.path.exists(clusters_save_path):
        logger.info(f'Found precomputed clusters at {clusters_save_path}. Loading...')
        clusters = load_pickle(clusters_save_path)
    else:
        # Precompute clusters for anatomical locations
        logger.info(f'Precomputing clusters for {len(anatomical_locations_list)} anatomical locations...')
        anatloc_embeddings = embeddings[anatloc_indices]
        logger.info(f'anatloc_embeddings.shape: {anatloc_embeddings.shape}')
        anatloc_kmeans = KMeans(n_clusters=args.num_anatloc_clusters, init='k-means++', n_init='auto', verbose=1,
                                max_iter=args.num_kmeans_iterations).fit(anatloc_embeddings)
        anatloc_clusters = anatloc_kmeans.labels_
        logger.info(f'anatloc_clusters.shape: {anatloc_clusters.shape}')

        # Precompute clusters for observations
        logger.info(f'Precomputing clusters for {len(observations_list)} observations...')
        obs_embeddings = embeddings[obs_indices]
        logger.info(f'obs_embeddings.shape: {obs_embeddings.shape}')
        obs_kmeans = KMeans(n_clusters=args.num_obs_clusters, init='k-means++', n_init='auto', verbose=1,
                            max_iter=args.num_kmeans_iterations).fit(obs_embeddings)
        obs_clusters = obs_kmeans.labels_
        logger.info(f'obs_clusters.shape: {obs_clusters.shape}')

        # Save clusters
        logger.info(f'Saving clusters to {clusters_save_path}...')
        clusters = {
            'anatomical_locations': {
                'indices': anatloc_indices,
                'clusters': anatloc_clusters,
            },
            'observations': {
                'indices': obs_indices,
                'clusters': obs_clusters,
            },
        }
        save_pickle(clusters, clusters_save_path)
    
    # Convert clusters to a more convenient format
    
    obs_idx2cluster = {}
    obs_cluster2indices = [[] for _ in range(args.num_obs_clusters)]
    for i, c in zip(clusters['observations']['indices'], clusters['observations']['clusters']):
        obs_cluster2indices[c].append(i)
        obs_idx2cluster[i] = c
    n_obs_clusters = len(obs_cluster2indices)
    
    anatloc_idx2cluster = {}
    anatloc_cluster2indices = [[] for _ in range(args.num_anatloc_clusters)]
    for i, c in zip(clusters['anatomical_locations']['indices'], clusters['anatomical_locations']['clusters']):
        anatloc_cluster2indices[c].append(i)
        anatloc_idx2cluster[i] = c
    n_anatloc_clusters = len(anatloc_cluster2indices)

    # Precompute clusters for radgraph sentences
    radgraph_clusters_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                        f'radgraph_clusters({len(radgraph_sentences_set)},{sentlen_sum}).pkl')
    if os.path.exists(radgraph_clusters_save_path):
        logger.info(f'Found precomputed radgraph clusters at {radgraph_clusters_save_path}. Loading...')
        radgraph_clusters = load_pickle(radgraph_clusters_save_path)
    else:
        logger.info(f'Precomputing RadGraph clusters for {len(radgraph_sentences_set)} sentences...')
        _idxs = [sentence2index[s] for s in radgraph_sentences_set]
        _idxs.sort()
        _embeddings = embeddings[_idxs]
        logger.info(f'_embeddings.shape: {_embeddings.shape}')
        radgraph_kmeans = KMeans(n_clusters=args.num_radgraph_clusters, init='k-means++', n_init='auto', verbose=1,
                                max_iter=args.num_kmeans_iterations).fit(_embeddings)
        logger.info(f'radgraph_kmeans.labels_.shape: {radgraph_kmeans.labels_.shape}')

        # Save RadGraph clusters
        logger.info(f'Saving RadGraph clusters to {radgraph_clusters_save_path}...')
        radgraph_clusters = {
            'indices': _idxs,
            'clusters': radgraph_kmeans.labels_,
        }
        save_pickle(radgraph_clusters, radgraph_clusters_save_path)


    # =================
    # Triplet sampling
    # =================
    # -----------------------------------------------
    # Definitions:
    # -----------------------------------------------
    # dot(E(X), E(Y)) = dot product of embeddings of X and Y, where E(.) is the embedding function that we want to learn
    # cos(X, Y) = cosine similarity between X and Y, where X and Y are represented as vectors using a pre-trained
    #            sentence embedding model from Microsoft Research called CXR-BERT
    # leveinshtein(X, Y) = Leveinshtein distance between X and Y

    used_triplets = set()

    triplets = {
        'sentences': sentences_list,
        'train': {
            'anatomical_locations': [],
            'observations': [],
        },
        'val': {
            'anatomical_locations': [],
            'observations': [],
        },
        'test': {
            'anatomical_locations': [],
            'observations': [],
        },
    }

    triplets_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                      f'triplets({len(sentences_list)},{len(anatomical_locations_list)},{len(observations_list)},'
                                      f'{args.num_train_triplets_per_rule},{args.num_val_triplets_per_rule},{args.num_test_triplets_per_rule}).pkl')

    # --------------------------------------------
    # 1) Sample triplets for anatomical locations
    # --------------------------------------------
    
    logger.info('============================================================')
    logger.info('Sampling triplets for anatomical locations...')
    
    sample_anatomical_location_triplets__rule1(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc2paraphrases=anatloc2paraphrases,
        n_anatloc_clusters=n_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    sample_anatomical_location_triplets__rule2(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc2paraphrases=anatloc2paraphrases,
        n_anatloc_clusters=n_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        anatloc_idx2cluster=anatloc_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    sample_anatomical_location_triplets__rule3(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc_indices=anatloc_indices,
        n_anatloc_clusters=n_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        anatloc_idx2cluster=anatloc_idx2cluster,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # ------------------------------------
    # 2) Sample triplets for observations
    # ------------------------------------

    logger.info('============================================================')
    logger.info('Sampling triplets for observations...')

    # Rule 1
    sample_observation_triplets__rule1(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        n_obs_clusters=n_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 2
    sample_observation_triplets__rule2(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        n_obs_clusters=n_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        obs_idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )
    
    logger.info('---------------------------------------')

    # First, we need to map observations to health statuses
    hs2id = {}
    obs_idx_2_hsid = {}
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        health_status = row['metadata']['health status']
        try:
            hsid = hs2id[health_status]
        except KeyError:
            hsid = len(hs2id)
            hs2id[health_status] = hsid
        assert len(fact) > 0
        obs_idx_2_hsid[sentence2index[fact]] = hsid
        if det_obs:
            obs_idx_2_hsid[sentence2index[det_obs]] = hsid
        if short_obs:
            obs_idx_2_hsid[sentence2index[short_obs]] = hsid
    logger.info(f'len(hs2id): {len(hs2id)}')
    logger.info(f'hs2id: {hs2id}')
    hsid2hs = {v: k for k, v in hs2id.items()}
    count = 0
    for obs, paraphrases in obs2paraphrases.items():
        obs_idx = sentence2index[obs]
        if obs_idx in obs_idx_2_hsid:
            for para in paraphrases:
                para_idx = sentence2index[para]
                obs_idx_2_hsid[para_idx] = obs_idx_2_hsid[obs_idx]
        else:
            count += 1
    # Print number of observations for each health status
    hsid2count = {i: 0 for i in range(len(hs2id))}
    for obs_idx, hsid in obs_idx_2_hsid.items():
        hsid2count[hsid] += 1
    logger.info(f'Number of observations for each health status:')
    for hsid, _ in sorted(list(hsid2count.items()), key=lambda x: x[1], reverse=True):
        logger.info(f'{hsid2count[hsid]} observations with health status "{hsid2hs[hsid]}"')
    # Print warning if some observations do not have a health status
    if count > 0:
        logger.warning(f'Could not find health status for {count}/{len(obs2paraphrases)} paraphrased observations.')

    hsid_cid_2_obs_idxs = {} # map (hsid, cid) to list of obs_idxs
    obs_idx_2_hsid_cid = {} # map obs_idx to (hsid, cid)
    for obs_idx, hsid in obs_idx_2_hsid.items():
        cid = obs_idx2cluster[obs_idx]
        hsid_cid = (hsid, cid)
        obs_idx_2_hsid_cid[obs_idx] = hsid_cid
        try:
            hsid_cid_2_obs_idxs[hsid_cid].append(obs_idx)
        except KeyError:
            hsid_cid_2_obs_idxs[hsid_cid] = [obs_idx]
    logger.info(f'len(hsid_cid_2_obs_idxs): {len(hsid_cid_2_obs_idxs)}')
    logger.info(f'len(obs_idx_2_hsid_cid): {len(obs_idx_2_hsid_cid)}')
    logger.info('---------------------------------------')

    # Rule 3
    sample_observation_triplets__rule3(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs_idx_2_hsid_cid=obs_idx_2_hsid_cid,
        hsid_cid_2_obs_idxs=hsid_cid_2_obs_idxs,
        n_obs_clusters=n_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 4
    sample_observation_triplets__rule4(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        obs_idx_2_hsid=obs_idx_2_hsid,
        obs_cluster2indices=obs_cluster2indices,
        obs_idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 5
    sample_observation_triplets__rule5(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        integrated_fact_metadata=integrated_fact_metadata,
        obs2paraphrases=obs2paraphrases,
        sentence2index=sentence2index,
        sentence_list=sentences_list,
        sentence_embeddings=embeddings,
        obs_idx2cluster=obs_idx2cluster,
        obs_cluster2indices=obs_cluster2indices,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 6
    chest_imagenome_sentences = []
    chest_imagenome_labels = []
    for group in chest_imagenome_data['groups']:
        chest_imagenome_sentences.extend(group['sentences'])
        chest_imagenome_labels.append(group['labels'])
        assert len(group['sentences']) == len(group['labels'])
    chest_imagenome_labels = np.concatenate(chest_imagenome_labels, axis=0)
    logger.info(f'len(chest_imagenome_sentences): {len(chest_imagenome_sentences)}')
    logger.info(f'chest_imagenome_labels.shape: {chest_imagenome_labels.shape}')
    sample_observation_triplets__rule6(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence2index=sentence2index,
        index2sentence=sentences_list,
        sentence_embeddings=embeddings,
        sentences=chest_imagenome_sentences,
        labels=chest_imagenome_labels,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 7
    radgraph_index2cluster = { i:c for i, c in zip(radgraph_clusters['indices'], radgraph_clusters['clusters']) }
    s2l = dict()
    for key in ('train', 'dev', 'test', 'chexpert', 'mimiccxr'):
        for s, l in radgraph_data[key].items():
            if s in s2l:
                s2l[s] |= l # union of sets
            else:
                s2l[s] = l
    radgraph_sentence_labels_pairs = [(s, l) for s, l in s2l.items()]
    sample_observation_triplets__rule7(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence_labels_pairs=radgraph_sentence_labels_pairs,
        sentence2index=sentence2index,
        index2cluster=radgraph_index2cluster,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # ==================================================================================================
    # Extract triplets for validation and test sets
    logger.info('Extracting triplets for validation and test sets...')
    for key in ['anatomical_locations', 'observations']:
        if len(triplets['train'][key]) == 0:
            continue
        for x in triplets['train'][key]:
            indices = random.sample(range(len(x['triplets'])), args.num_val_triplets_per_rule + args.num_test_triplets_per_rule)
            indices_set = set(indices)
            sampled_triplets = [x['triplets'][i] for i in indices]
            val_triplets = np.array(sampled_triplets[:args.num_val_triplets_per_rule])
            test_triplets = np.array(sampled_triplets[args.num_val_triplets_per_rule:])
            train_triplets = [x['triplets'][i] for i in range(len(x['triplets'])) if i not in indices_set]
            train_triplets = np.array(train_triplets)
            x['triplets'] = train_triplets
            triplets['val'][key].append({
                'rule': x['rule'],
                'triplets': val_triplets,
            })
            triplets['test'][key].append({
                'rule': x['rule'],
                'triplets': test_triplets,
            })

    # Save triplets
    logger.info(f'Saving triplets to {triplets_save_path}...')
    save_pickle(triplets, triplets_save_path)
    logger.info(f'Done!')

if __name__ == '__main__':
    main()   