import argparse
import numpy as np
import os
import random
import math
from tqdm import tqdm
from Levenshtein import distance as levenshtein_distance

from medvqa.datasets.mimiccxr import MIMICCXR_LARGE_FAST_CACHE_DIR
from medvqa.utils.logging import get_console_logger
from medvqa.utils.files import load_jsonl, load_pickle, save_pickle
from medvqa.models.huggingface_utils import (
    CachedTextEmbeddingExtractor,
    SupportedHuggingfaceMedicalBERTModels,
)
from medvqa.utils.metrics import jaccard_between_sets
from medvqa.utils.hashing import hash_string

MAX_TRIES = 10

def _levenshtein_similarity(a, b):
    return 1 - levenshtein_distance(a, b) / max(len(a), len(b))

def _is_at_least_x_percent_better(a, b, x):
    return a > 0 and a > b and (a - b) / a >= x
    
def randomly_select_nonduplicate_from_list(L, x):
    while True:
        y = random.choice(L)
        if y != x:
            return y

def _find_negatives_and_add_triplest_for_AP(A, P, n_negs, sentence_embeddings, sentence_list, forbidden_idxs_set,
                                            n_clusters, cluster2indices, used_triplets, triplets_list,
                                            same_cluster_as_anchor=False, a_cid=None):
    if same_cluster_as_anchor:
        assert a_cid is not None
        
    AP_sim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
    AP_levd = levenshtein_distance(sentence_list[A], sentence_list[P])
    for _ in range(n_negs):
        for _ in range(MAX_TRIES):
            if same_cluster_as_anchor:
                N = random.choice(cluster2indices[a_cid])
            else:
                cid = random.randint(0, n_clusters-1) # randomly select a cluster
                N = random.choice(cluster2indices[cid]) # randomly select an index from the cluster
            if N == A or N in forbidden_idxs_set: # if same as anchor or forbidden -> skip
                continue
            triplet = (A, P, N)
            if triplet in used_triplets: # if already used -> skip
                continue
            AN_sim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
            AN_levd = levenshtein_distance(sentence_list[A], sentence_list[N])
            if AN_sim > AP_sim and AN_levd < AP_levd: # if both CXR-BERT and Leveinshtein disagree -> skip
                continue
            # if we reach here, then we have found a valid triplet
            used_triplets.add(triplet)
            triplets_list.append(triplet)
            break # break out of for loop


# -----------------------------------------------
# Definitions:
# -----------------------------------------------
# dot(E(X), E(Y)) = dot product of embeddings of X and Y, where E(.) is the embedding function that we want to learn
# cos(X, Y) = cosine similarity between X and Y, where X and Y are represented as vectors using a pre-trained
#            sentence embedding model, e.g. CXR BERT from Microsoft Research
# leveinshtein(X, Y) = Leveinshtein distance between X and Y

def _rule1_common(
    n_triplets_per_rule,
    sentence2paraphrases,
    n_clusters,
    cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    field_name,
):
    # Rule 1: "Rank paraphrases very highly"
    # dot(E(A), E(P)) > dot(E(A), E(N)) unless cos(A, P) < cos(A, N) AND leveinshtein(A, P) > leveinshtein(A, N)
    # Where:
    # A = anchor (anatomical location or observation)
    # P = positive = a paraphrased version of A
    # N = negative = randomly sampled anatomical location or observation
    # In words: include the triplet (A, P, N) (i.e. trust the paraphrase generated by GPT-3.5/4)
    #           except if both CXR-BERT and Leveinshtein disagree at the same time.

    logger.info('Rule 1: "Rank paraphrases very highly"')

    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(sentence2paraphrases))
    rule_triplets = []
    
    for sentence, paraphrases in tqdm(sentence2paraphrases.items(), mininterval=2):
        idx = sentence2index[sentence]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{sentence}". Skipping...')
            continue

        n_triplets_per_para = math.ceil(n_triplets_per_iter * 0.5 / len(para_idxs))

        # triplets where analoc_idx is anchor
        A = idx
        for P in para_idxs:
            _find_negatives_and_add_triplest_for_AP(A, P, n_triplets_per_para, sentence_embeddings, sentence_list,
                                                    para_idxs_set, n_clusters, cluster2indices,
                                                    used_triplets, rule_triplets)

        # triplets where paraphrases are anchors
        para_idxs_ = para_idxs.copy()
        para_idxs_.append(A) # add A to para_idxs_
        para_idxs_set.add(A) # add A to para_idxs_set
        for A in para_idxs:
            for _ in range(n_triplets_per_para):
                P = randomly_select_nonduplicate_from_list(para_idxs_, A)
                _find_negatives_and_add_triplest_for_AP(A, P, 1, sentence_embeddings, sentence_list,
                                                        para_idxs_set, n_clusters, cluster2indices,
                                                        used_triplets, rule_triplets)
    
    logger.info(f'Actual number of triplets sampled for Rule 1: "Rank paraphrases very highly": {len(rule_triplets)}')
    triplets_dict['train'][field_name].append({
        'rule': 'Rank paraphrases very highly',
        'triplets': rule_triplets,
    })

def _rule2_common(
    n_triplets_per_rule,
    sentence2paraphrases,
    n_clusters,
    cluster2indices,
    idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    field_name,
):
    # Rule 2: "Rank paraphrases very highly - Hard negative"
    # We will sample negatives from the same cluster as the anchor

    logger.info('Rule 2: "Rank paraphrases very highly - Hard negative"')
    
    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(sentence2paraphrases))
    rule_triplets = []
    
    for sentence, paraphrases in tqdm(sentence2paraphrases.items(), mininterval=2):
        idx = sentence2index[sentence]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{sentence}". Skipping...')
            continue

        n_triplets_per_para = math.ceil(n_triplets_per_iter * 0.5 / len(para_idxs))

        # triplets where analoc_idx is anchor
        A = idx
        A_cid = idx2cluster[A]
        for P in para_idxs:
            _find_negatives_and_add_triplest_for_AP(A, P, n_triplets_per_para, sentence_embeddings, sentence_list,
                                                    para_idxs_set, n_clusters, cluster2indices,
                                                    used_triplets, rule_triplets, same_cluster_as_anchor=True, a_cid=A_cid)

        # triplets where paraphrases are anchors
        para_idxs_ = para_idxs.copy()
        para_idxs_.append(A) # add A to para_idxs_
        para_idxs_set.add(A) # add A to para_idxs_set
        for A in para_idxs:
            for _ in range(n_triplets_per_para):
                P = randomly_select_nonduplicate_from_list(para_idxs_, A)
                _find_negatives_and_add_triplest_for_AP(A, P, 1, sentence_embeddings, sentence_list,
                                                        para_idxs_set, n_clusters, cluster2indices,
                                                        used_triplets, rule_triplets, same_cluster_as_anchor=True, a_cid=A_cid)
    
    logger.info(f'Actual number of triplets sampled for Rule 2: "Rank paraphrases very highly - Hard negative": {len(rule_triplets)}')
    triplets_dict['train'][field_name].append({
        'rule': 'Rank paraphrases very highly - Hard negative',
        'triplets': rule_triplets,
    })

def sample_anatomical_location_triplets__rule1(
    n_triplets_per_rule,
    anatloc2paraphrases,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):  
    _rule1_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=anatloc2paraphrases,
        n_clusters=n_anatloc_clusters,
        cluster2indices=anatloc_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='anatomical_locations',
    )

def sample_anatomical_location_triplets__rule2(
    n_triplets_per_rule,
    anatloc2paraphrases,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    anatloc_idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule2_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=anatloc2paraphrases,
        n_clusters=n_anatloc_clusters,
        cluster2indices=anatloc_cluster2indices,
        idx2cluster=anatloc_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='anatomical_locations',
    )

def sample_anatomical_location_triplets__rule3(
    n_triplets_per_rule,
    anatloc_indices,
    n_anatloc_clusters,
    anatloc_cluster2indices,
    anatloc_idx2cluster,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    top_k=20, # top k sentences from the same cluster as A to consider for P
    n_samples_per_cluster=300, # number of random samples per cluster to look for P's
):
    # Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = anatomical location
    # P = positive = another anatomical location randomly sampled from cluster(A)
    # N = negative = randomly sampled anatomical location
    # In words: include the triplet (A, P, N) if both CXR-BERT and Leveinshtein agree at the same time.
    
    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(anatloc_indices))
    rule_triplets = []

    logger.info('Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus"')

    for A in tqdm(anatloc_indices, mininterval=2):
        cid_A = anatloc_idx2cluster[A]
        random_idxs_for_P = anatloc_cluster2indices[cid_A]
        if n_samples_per_cluster < len(random_idxs_for_P):
            random_idxs_for_P = random.sample(random_idxs_for_P, n_samples_per_cluster)

        filtered_scores = []
        filtered_idxs = []
        for idx in random_idxs_for_P:
            levsim = _levenshtein_similarity(sentence_list[A], sentence_list[idx])
            dotsim = np.dot(sentence_embeddings[A], sentence_embeddings[idx])
            if levsim > 0 and dotsim > 0:
                filtered_scores.append(levsim + dotsim)
                filtered_idxs.append(idx)

        if len(filtered_idxs) == 0:
            continue

        filtered_scores_argsort = np.argsort(filtered_scores)[::-1] # sort in descending order
        P_list = []
        for rank, pos in enumerate(filtered_scores_argsort):
            s_idx = filtered_idxs[pos]
            if s_idx == A:
                continue
            P_list.append((s_idx, rank)) # (sentence index, rank)
            if len(P_list) == top_k or len(P_list) == n_triplets_per_anchor:
                # We will consider at most the top 'top_k' sentences from the same cluster as A for P
                break
        assert len(P_list) <= n_triplets_per_anchor
        if len(P_list) == 0:
            continue
        n_triplets_per_positive = math.ceil(n_triplets_per_anchor / len(P_list))
        for P, rank in P_list:
            AP_score = filtered_scores[filtered_scores_argsort[rank]]
            assert AP_score > 0
            for _ in range(n_triplets_per_positive):
                for _ in range(MAX_TRIES):
                    c = random.randint(0, n_anatloc_clusters-1) # randomly select a cluster
                    if c == cid_A:
                        continue # skip if same cluster as A
                    N = random.choice(anatloc_cluster2indices[c]) # randomly select an index from the cluster
                    assert N != A and N != P
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AN_embsim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AN_levsim = _levenshtein_similarity(sentence_list[A], sentence_list[N])
                    AN_score = AN_embsim + AN_levsim
                    if _is_at_least_x_percent_better(AP_score, AN_score, 0.5):
                        # both CXR-BERT and Leveinshtein agree
                        used_triplets.add(triplet) # add triplet to used triplets
                        rule_triplets.append(triplet)
                        break # break out of for loop
    
    logger.info(f'Actual number of triplets sampled for Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus": {len(rule_triplets)}')
    triplets_dict['train']['anatomical_locations'].append({
        'rule': 'Rank some triplets according to CXR-BERT and Leveinshtein consensus',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule1(
    n_triplets_per_rule,
    obs2paraphrases,
    n_obs_clusters,
    obs_cluster2indices,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule1_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=obs2paraphrases,
        n_clusters=n_obs_clusters,
        cluster2indices=obs_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='observations',
    )

def sample_observation_triplets__rule2(
    n_triplets_per_rule,
    obs2paraphrases,
    n_obs_clusters,
    obs_cluster2indices,
    obs_idx2cluster,
    sentence2index,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
):
    _rule2_common(
        n_triplets_per_rule=n_triplets_per_rule,
        sentence2paraphrases=obs2paraphrases,
        n_clusters=n_obs_clusters,
        cluster2indices=obs_cluster2indices,
        idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=sentence_embeddings,
        sentence_list=sentence_list,
        used_triplets=used_triplets,
        triplets_dict=triplets_dict,
        logger=logger,
        field_name='observations',
    )

def sample_observation_triplets__rule3(
    n_triplets_per_rule,
    obs_idx_2_hsid_cid,
    hsid_cid_2_obs_idxs,
    n_obs_clusters,
    obs_cluster2indices,
    sentence_embeddings,
    sentence_list,
    used_triplets,
    triplets_dict,
    logger,
    top_k=10, # top k sentences from the same cluster as A to consider for P
    n_samples_per_cluster=100, # number of random samples per cluster to look for P's
):
    # Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if HS(A) = HS(P) AND cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = observation
    # P = positive = another observation randomly sampled from cluster(A)
    # N = negative = randomly sampled observation
    # In words: include the triplet (A, P, N) if both CXR-BERT and Leveinshtein agree at the same time
    #           and if the anchor and positive share the same health status.

    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(obs_idx_2_hsid_cid))
    rule_triplets = []
    logger.info('Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status"')
    
    for A, hsid_cid_A in tqdm(obs_idx_2_hsid_cid.items(), mininterval=2):
        random_idxs_for_P = hsid_cid_2_obs_idxs[hsid_cid_A]
        if n_samples_per_cluster < len(random_idxs_for_P):
            random_idxs_for_P = random.sample(random_idxs_for_P, n_samples_per_cluster)

        filtered_scores = []
        filtered_idxs = []
        for idx in random_idxs_for_P:
            levsim = _levenshtein_similarity(sentence_list[A], sentence_list[idx])
            dotsim = np.dot(sentence_embeddings[A], sentence_embeddings[idx])
            if levsim > 0 and dotsim > 0:
                filtered_scores.append(levsim + dotsim)
                filtered_idxs.append(idx)

        if len(filtered_idxs) == 0:
            continue

        filtered_scores_argsort = np.argsort(filtered_scores)[::-1] # sort in descending order
        P_list = []
        for rank, pos in enumerate(filtered_scores_argsort):
            s_idx = filtered_idxs[pos]
            if s_idx == A:
                continue
            P_list.append((s_idx, rank)) # (sentence index, rank)
            if len(P_list) == top_k or len(P_list) == n_triplets_per_anchor:
                # We will consider at most the top 'top_k' sentences from the same cluster as A for P
                break
        assert len(P_list) <= n_triplets_per_anchor
        if len(P_list) == 0:
            continue # no sentences with non-zero Levenshtein similarity
        n_triplets_per_positive = math.ceil(n_triplets_per_anchor / len(P_list))
        A_cluster = hsid_cid_A[1]
        for P, rank in P_list:
            AP_score = filtered_scores[filtered_scores_argsort[rank]]
            assert AP_score > 0
            for _ in range(n_triplets_per_positive):
                for _ in range(MAX_TRIES):
                    c = random.randint(0, n_obs_clusters-1) # randomly select a cluster
                    if c == A_cluster:
                        continue # skip if cluster is the same as A's cluster
                    N = random.choice(obs_cluster2indices[c]) # randomly select an index from the cluster
                    assert N != A and N != P
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AN_embsim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AN_levsim = _levenshtein_similarity(sentence_list[A], sentence_list[N])
                    AN_score = AN_embsim + AN_levsim
                    if _is_at_least_x_percent_better(AP_score, AN_score, 0.5):
                        # both CXR-BERT and Leveinshtein agree
                        used_triplets.add(triplet) # add triplet to used triplets
                        rule_triplets.append(triplet)
                        break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 3: "Rank some triplets according to CXR-BERT and Leveinshtein consensus,'
                f' while anchor and positive share the same health status": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule4(
    n_triplets_per_rule,
    obs2paraphrases,
    obs_idx_2_hsid,
    obs_cluster2indices,
    obs_idx2cluster,
    sentence2index,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 4: "Hard health-status-vs-others triplets"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if HS(A) == HS(P) AND HS(N) != HS(A)
    # Where:
    # A = anchor = observation
    # P = positive = paraphrase of A
    # N = negative = randomly sampled observation from the same cluster as A but with a different health status
    # In words: include the triplet (A, P, N) if the anchor and positive are paraphrases of each other
    #           and if the negative is from the same cluster as the anchor but with a different health status.
    
    rule_triplets = []
    logger.info('Rule 4: "Hard health-status-vs-others triplets"')

    obs_with_hs_and_p = [ obs for obs in obs2paraphrases.keys() if sentence2index[obs] in obs_idx_2_hsid ]
    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(obs_with_hs_and_p))
    logger.info(f'Number of observations with health status and paraphrases: {len(obs_with_hs_and_p)}')
    logger.info(f'Number of observations with health status: {len(obs_idx_2_hsid)}')
    logger.info(f'Number of observations with paraphrases: {len(obs2paraphrases)}')
    logger.info(f'Number of triplets to sample per iteration: {n_triplets_per_iter}')
    
    for obs in tqdm(obs_with_hs_and_p, mininterval=2):
        paraphrases = obs2paraphrases[obs]
        obs_idx = sentence2index[obs]
        hsid = obs_idx_2_hsid[obs_idx]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.add(obs_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) < 2:
            logger.warning(f'Not enough paraphrases found for "{obs}". Skipping...')
            continue
        cluster_idxs = obs_cluster2indices[obs_idx2cluster[obs_idx]]
        for _ in range(n_triplets_per_iter):
            for _ in range(MAX_TRIES):
                A, P = random.sample(para_idxs, 2)
                assert A != P
                N = random.choice(cluster_idxs)
                if N not in obs_idx_2_hsid:
                    continue
                if obs_idx_2_hsid[N] == hsid:
                    continue
                if N in para_idxs_set:
                    continue
                triplet = (A, P, N)
                if triplet in used_triplets: # already used
                    continue
                used_triplets.add(triplet) # add triplet to used triplets
                rule_triplets.append(triplet)
                break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 4: "Hard health-status-vs-others triplets": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Hard health-status-vs-others triplets',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule5(
    n_triplets_per_rule,
    integrated_fact_metadata,
    obs2paraphrases,
    sentence2index,
    sentence_list,
    sentence_embeddings,
    obs_idx2cluster,
    obs_cluster2indices,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 5: "Short observation, detailed observation, and fact must be close to each other"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = short observation, detailed observation, or fact, or a paraphrase of one of these
    # P = positive = some paraphrase of the same group as A, probably of a different length
    # N = negative = randomly sampled observation from the same cluster as A
    # In words: include the triplet (A, P, N) if A and P are paraphrases of the same fact with different degrees of detail
    #           (e.g. "cardiomegaly" vs "cardiomegaly is present" vs "cardiomegaly is present, which is an enlarged heart")
    #           and if the negative is from the same cluster as the anchor, as long as CXR-BERT and Leveinshtein do not both disagree.

    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(integrated_fact_metadata))
    rule_triplets = []
    logger.info('Rule 5: "Short observation, detailed observation, and fact must be close to each other"')
    n_obs_clusters = len(obs_cluster2indices)

    def _add_triplets(obs_idxs, obs_idxs_set, i_min, i_max, j_min, j_max, n_triplets_per_obs):
        for i in range(i_min, i_max+1): # first half (or second half)
            A = obs_idxs[i]
            A_cid = obs_idx2cluster[A]
            for _ in range(n_triplets_per_obs):
                for _ in range(MAX_TRIES):
                    j = random.randint(j_min, j_max) # second half (or first half)
                    P = obs_idxs[j]
                    assert A != P
                    c = random.randint(0, n_obs_clusters-1) # randomly select a cluster
                    if c == A_cid:
                        continue # skip if cluster is the same as A's cluster
                    N = random.choice(obs_cluster2indices[c]) # randomly select an index from the cluster
                    if N in obs_idxs_set:
                        continue
                    assert A != N and P != N
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AP_sim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
                    AN_sim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AP_levd = levenshtein_distance(sentence_list[A], sentence_list[P])
                    AN_levd = levenshtein_distance(sentence_list[A], sentence_list[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # if both CXR-BERT and Leveinshtein disagree -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop

    fail_count = 0
    
    for row in tqdm(integrated_fact_metadata, mininterval=2):
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        obs_idxs_set = set()
        # Add fact and its paraphrases to obs_idxs_set
        fact_idx = sentence2index[fact]
        obs_idxs_set.add(fact_idx)
        if fact in obs2paraphrases:
            obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[fact])
        # Add detailed observation and its paraphrases to obs_idxs_set
        if det_obs:
            det_obs_idx = sentence2index[det_obs]
            if det_obs_idx != fact_idx:
                obs_idxs_set.add(det_obs_idx)
                if det_obs in obs2paraphrases:
                    obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[det_obs])
        else:
            det_obs_idx = None
        # Add short observation and its paraphrases to obs_idxs_set
        if short_obs:
            short_obs_idx = sentence2index[short_obs]
            if short_obs_idx != fact_idx and short_obs_idx != det_obs_idx:
                obs_idxs_set.add(short_obs_idx)
                if short_obs in obs2paraphrases:
                    obs_idxs_set.update(sentence2index[p] for p in obs2paraphrases[short_obs])
        # Skip if obs_idxs_set has less than 2 observations
        if len(obs_idxs_set) < 2:
            fail_count += 1
            continue
        
        # Sort obs_idxs by length
        obs_idxs = list(obs_idxs_set)
        obs_idxs.sort(key=lambda x: len(sentence_list[x]))
        
        # Sample triples where the anchor is in the first half and the positive is in the second half and vice versa
        n_triplets_per_obs = math.ceil(n_triplets_per_iter / len(obs_idxs))
        half = len(obs_idxs) // 2
        _add_triplets(obs_idxs, obs_idxs_set, 0, half-1, half, len(obs_idxs)-1, n_triplets_per_obs)
        _add_triplets(obs_idxs, obs_idxs_set, half, len(obs_idxs)-1, 0, half-1, n_triplets_per_obs)
        
    if fail_count > 0:
        logger.warning(f'Failed to find enough observations for {fail_count}/{len(integrated_fact_metadata)} integrated fact metadata rows.')

    logger.info(f'Actual number of triplets sampled for Rule 5: "Short observation, detailed observation, and fact'
                f' must be close to each other": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Short observation, detailed observation, and fact must be close to each other',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule6(
    n_triplets_per_rule,
    sentence2index,
    index2sentence,
    sentence_embeddings,
    sentences,
    labels,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 6: "Rank triplets according to Chest ImaGenome labels"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if acc(A, P) > 0 and acc(P, N) = acc(A, N) = 0, as long as CXR-BERT and Leveinshtein do not both disagree.
    # Where:
    # A = anchor = a sentence annotated with Chest ImaGenome labels
    # P = positive = another sentence annotated with Chest ImaGenome labels, sharing at least one label with A
    # N = negative = randomly sampled sentence annotated with Chest ImaGenome labels, not sharing any label with A and P
    # acc(X, Y) = accuracy between the labels of X and Y
    # In words: include the triplet (A, P, N) if A and P share at least one label and A/P and N share none, as long as CXR-BERT and Leveinshtein do not both disagree.
    
    logger.info('Rule 6: "Rank triplets according to Chest ImaGenome labels"')
    assert len(sentences) == len(labels), f'len(sentences)={len(sentences)} != len(labels)={len(labels)}'
    n, m = labels.shape
    rule_triplets = []
    
    # For each label, collect the indices of sentences that are annotated with that label
    label2idxs = [[] for _ in range(m + 1)] # +1 for the "no label" label
    idx2i = {}
    for i in tqdm(range(n), mininterval=2):
        s = sentences[i]
        s_idx = sentence2index[s]
        idx2i[s_idx] = i
        no_label = True
        for j in range(m):
            if labels[i, j] == 1:
                label2idxs[j].append(s_idx)
                no_label = False
        if no_label:
            label2idxs[m].append(s_idx)

    n_triplets_per_label = math.ceil(n_triplets_per_rule / m)
    for i in tqdm(range(m), mininterval=2):
        idxs = label2idxs[i]
        if n_triplets_per_label < len(idxs):
            idxs.sort(key=lambda x: len(index2sentence[x]))
            idxs = [idxs[i] for i in np.linspace(0, len(idxs)-1, n_triplets_per_label, dtype=int)]
            assert len(idxs) == n_triplets_per_label
        n_triplets_per_idx = math.ceil(n_triplets_per_label / len(idxs))
        for A in idxs:
            A_labels = labels[idx2i[A]]
            assert A_labels[i] == 1
            for _ in range(n_triplets_per_idx):
                for _ in range(MAX_TRIES):
                    P = random.choice(label2idxs[i])
                    if A == P:
                        continue
                    L = random.randint(0, m) # randomly select a label (including "no label")
                    if L < m and A_labels[L] == 1: # if A already has label L -> skip
                        continue
                    if len(label2idxs[L]) == 0: # if no sentence has label L -> skip
                        continue
                    N = random.choice(label2idxs[L])
                    N_labels = labels[idx2i[N]]
                    if np.any(A_labels & N_labels): # if A and N share at least one label -> skip
                        continue
                    P_labels = labels[idx2i[P]]
                    if np.any(P_labels & N_labels): # if P and N share at least one label -> skip
                        continue
                    assert P_labels[i] == 1
                    assert np.any(A_labels & P_labels) # A and P must share at least one label
                    assert A != N and P != N
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    AP_dotsim = np.dot(sentence_embeddings[A], sentence_embeddings[P])
                    AN_dotsim = np.dot(sentence_embeddings[A], sentence_embeddings[N])
                    AP_levsim = _levenshtein_similarity(index2sentence[A], index2sentence[P])
                    AN_levsim = _levenshtein_similarity(index2sentence[A], index2sentence[N])
                    if not(AP_dotsim > AN_dotsim and AP_levsim > AN_levsim): # if not both CXR-BERT and Leveinshtein agree -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop
        
    logger.info(f'Actual number of triplets sampled for Rule 6: "Rank triplets according to Chest ImaGenome labels": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Rank triplets according to Chest ImaGenome labels',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule7(
    n_triplets_per_rule,
    sentence_labels_pairs,
    sentence2index,
    index2cluster,
    used_triplets,
    triplets_dict,
    logger,
    margin=0.5, # margin for Jaccard index
    top_k=30, # top k sentences from the same cluster as A to consider for P
    n_samples_per_cluster=300, # number of random samples per cluster to look for P's
):
    # Rule 7: "RadGraph-based triplets"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if Jaccard(A, P) > Jaccard(A, N) + margin
    # Where:
    # A = anchor = a sentence with RadGraph annotations
    # P = positive = another sentence with RadGraph annotations, from the same cluster as A
    # N = negative = another sentence with RadGraph annotations
    # Jaccard(X, Y) = Jaccard index between sets of RadGraph labels (entities and relations) of X and Y
    # In words: include the triplet (A, P, N) if the Jaccard index between the sets of RadGraph labels of A and P
    #           is higher than the Jaccard index between the sets of RadGraph labels of A and N.
    
    logger.info('Rule 7: "RadGraph-based triplets"')
    rule_triplets = []

    logger.info(f'len(sentence_labels_pairs)={len(sentence_labels_pairs)}')
    logger.info('Computing cluster2idxs...')
    cluster2idxs = dict()
    idx2i = {}
    for i, p in tqdm(enumerate(sentence_labels_pairs), mininterval=2):
        s = p[0]
        s_idx = sentence2index[s]
        idx2i[s_idx] = i
        c_idx = index2cluster[s_idx]
        if c_idx not in cluster2idxs:
            cluster2idxs[c_idx] = []
        cluster2idxs[c_idx].append(s_idx)
    cluster_list = list(cluster2idxs.keys())

    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(sentence_labels_pairs))
    logger.info('Sampling triplets...')
    logger.info(f'n_triplets_per_anchor={n_triplets_per_anchor}')
    for sentence, labels in tqdm(sentence_labels_pairs, mininterval=2):
        sentence_idx = sentence2index[sentence]
        sentence_cluster = index2cluster[sentence_idx]
        A = sentence_idx
        assert n_samples_per_cluster < len(cluster2idxs[sentence_cluster])
        random_idxs_for_P = random.sample(cluster2idxs[sentence_cluster], n_samples_per_cluster)
        scores = [jaccard_between_sets(labels, sentence_labels_pairs[idx2i[idx]][1]) for idx in random_idxs_for_P]
        sorted_idxs = np.argsort(scores)[::-1] # sort in descending order
        P_list = []
        for j, i in enumerate(sorted_idxs):
            if scores[i] == 0:
                break # if we reach a score of 0, then all subsequent scores will also be 0
            s_idx = random_idxs_for_P[i]
            if s_idx == A:
                continue
            assert index2cluster[s_idx] == sentence_cluster
            P_list.append((s_idx, j)) # (sentence index, rank)
            if len(P_list) == top_k or len(P_list) == n_triplets_per_anchor:
                # We will consider at most the top 'top_k' sentences from the same cluster as A for P
                break
        assert len(P_list) <= n_triplets_per_anchor
        if len(P_list) == 0:
            continue
        n_triplets_per_positive = math.ceil(n_triplets_per_anchor / len(P_list))
        for P, rank in P_list:
            P_score = scores[sorted_idxs[rank]]
            assert P_score > 0
            for _ in range(n_triplets_per_positive):
                for _ in range(MAX_TRIES):
                    c = random.choice(cluster_list)
                    if c == sentence_cluster:
                        continue
                    N = random.choice(cluster2idxs[c])
                    assert N != A and N != P
                    N_score = jaccard_between_sets(labels, sentence_labels_pairs[idx2i[N]][1])
                    if N_score > 0 and N_score + margin > P_score:
                        continue
                    triplet = (A, P, N)
                    if triplet in used_triplets: # if already used -> skip
                        continue
                    # if we reach here, then we have found a valid triplet
                    used_triplets.add(triplet)
                    rule_triplets.append(triplet)
                    break # break out of for loop

    logger.info(f'Actual number of triplets sampled for Rule 7: "RadGraph-based triplets": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'RadGraph-based triplets',
        'triplets': rule_triplets,
    })

def sample_observation_triplets__rule8(
    n_triplets_per_rule,
    sentence2paraphrases,
    anchor2negatives,
    sentence2index,
    used_triplets,
    triplets_dict,
    logger,
):
    # Rule 8: "Hard negative triplets generated by ChatGPT"
    # dot(E(A), E(P)) > dot(E(A), E(N)) for each (A, P, N) generated by ChatGPT
    # Where:
    # A = anchor
    # P = positive = paraphrase of A (generated by ChatGPT)
    # N = negative = another sentence very similar to A, but with different semantics (generated by ChatGPT)
    
    logger.info('Rule 8: "Hard negative triplets generated by ChatGPT"')
    rule_triplets = []

    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(anchor2negatives))

    def get_triplets(A, P_list, N_list):
        assert len(P_list) > 0
        assert len(N_list) > 0
        if len(P_list) * len(N_list) <= n_triplets_per_anchor:
            for P in P_list:
                for N in N_list:
                    triplet = (A, P, N)
                    if triplet in used_triplets:
                        continue
                    yield triplet
        else:
            i, j = 0, 0
            for _ in range(n_triplets_per_anchor):
                for _ in range(MAX_TRIES):
                    P = P_list[i]
                    N = N_list[j]
                    triplet = (A, P, N)
                    if triplet in used_triplets:
                        if random.random() < 0.5:
                            i += 1
                            if i == len(P_list):
                                i = 0
                        else:
                            j += 1
                            if j == len(N_list):
                                j = 0
                        continue
                    yield triplet
                    i += 1
                    if i == len(P_list):
                        i = 0
                    j += 1
                    if j == len(N_list):
                        j = 0
                    break

    for anchor, negatives in tqdm(anchor2negatives.items(), mininterval=2):
        A = sentence2index[anchor]
        P_set = set(sentence2index[p] for p in sentence2paraphrases[anchor])
        P_set.discard(A)
        P_list = list(P_set)
        N_set = set()
        N_list_1 = []
        N_list_2 = []
        for n in negatives:
            N = sentence2index[n]
            if N == A or N in P_set or N in N_set:
                continue
            N_set.add(N)
            N_list_1.append(N)
            if n in sentence2paraphrases:
                for p in sentence2paraphrases[n]:
                    N = sentence2index[p]
                    if N == A or N in P_set or N in N_set:
                        continue
                    N_set.add(N)
                    N_list_2.append(N)
        N_list = N_list_1 + N_list_2
        assert A not in N_set
        for n in N_set: assert n not in P_set
        assert len(P_list) > 0
        assert len(N_list) > 0
        for triplet in get_triplets(A, P_list, N_list):
            used_triplets.add(triplet)
            rule_triplets.append(triplet)

    logger.info(f'Actual number of triplets sampled for Rule 8: "Hard negative triplets generated by ChatGPT": {len(rule_triplets)}')
    triplets_dict['train']['observations'].append({
        'rule': 'Hard negative triplets generated by ChatGPT',
        'triplets': rule_triplets,
    })

def main():
    parser = argparse.ArgumentParser(description='Sample triplets for fact embedding learning')
    parser.add_argument('--paraphrased_anatomical_locations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--paraphrased_observations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--integrated_fact_metadata_filepath', type=str, required=True)
    parser.add_argument('--chest_imagenome_sentences_and_labels_filepath', type=str, required=True)
    parser.add_argument('--radgraph_sentences_and_labels_filepath', type=str, required=True)
    parser.add_argument('--hard_triplets_from_facts_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--num_anatloc_clusters', type=int, required=True)
    parser.add_argument('--num_obs_clusters', type=int, required=True)
    parser.add_argument('--num_radgraph_clusters', type=int, required=True)
    parser.add_argument('--num_train_triplets_per_rule', type=int, required=True)
    parser.add_argument('--num_val_triplets_per_rule', type=int, required=True)
    parser.add_argument('--num_test_triplets_per_rule', type=int, required=True)

    parser.add_argument('--bert_model_name', type=str, default=SupportedHuggingfaceMedicalBERTModels.BiomedVLP_CXR_BERT_specialized,
                        choices=SupportedHuggingfaceMedicalBERTModels.get_all())
    parser.add_argument('--bert_checkpoint_folder_path', type=str, default=None)
    
    parser.add_argument('--logging_level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'])
    parser.add_argument("--device", type=str, default="GPU", choices=["GPU", "CPU"])
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--num_kmeans_iterations', type=int, default=300)
    args = parser.parse_args()

    # Add validation and test triplets to training triplets (since they will be subtracted from the training triplets later)
    args.num_train_triplets_per_rule += args.num_val_triplets_per_rule
    args.num_train_triplets_per_rule += args.num_test_triplets_per_rule

    # Set up logging
    logger = get_console_logger(args.logging_level)
    
    # Collect all sentences, observations and anatomical locations from input files
    logger.info('Collecting all sentences, observations and anatomical locations from input files...')
    
    sentences_set = set()
    anatomical_locations_set = set()
    observations_set = set()
    radgraph_sentences_set = set()
    
    logger.info(f'Loading integrated fact metadata from {args.integrated_fact_metadata_filepath}...')
    integrated_fact_metadata = load_jsonl(args.integrated_fact_metadata_filepath)
    logger.info(f'Found {len(integrated_fact_metadata)} integrated fact metadata rows.')
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        anat_loc = row['metadata']['anatomical location']
        assert len(fact) > 0
        sentences_set.add(fact)
        observations_set.add(fact)
        if det_obs:
            sentences_set.add(det_obs)
            observations_set.add(det_obs)
        if short_obs:
            sentences_set.add(short_obs)
            observations_set.add(short_obs)
        if anat_loc:
            sentences_set.add(anat_loc)
            anatomical_locations_set.add(anat_loc)

    sentence2paraphrases = {}
    
    for filepath in args.paraphrased_anatomical_locations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_anatomical_locations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_anatomical_locations)} paraphrased anatomical locations.')
        for row in paraphrased_anatomical_locations:
            anatloc = row['metadata']['anatomical location']
            paraphrases = row['parsed_response']
            sentences_set.add(anatloc)
            sentences_set.update(paraphrases)
            anatomical_locations_set.add(anatloc)
            anatomical_locations_set.update(paraphrases)
            if anatloc not in sentence2paraphrases:
                sentence2paraphrases[anatloc] = set()
            sentence2paraphrases[anatloc].update(paraphrases)
    
    for filepath in args.paraphrased_observations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_observations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_observations)} paraphrased observations.')
        for row in paraphrased_observations:
            try:
                observation = row['metadata']['query']
            except KeyError:
                observation = row['metadata']['observation'] # backward compatibility
            paraphrases = row['parsed_response']
            sentences_set.add(observation)
            sentences_set.update(paraphrases)
            observations_set.add(observation)
            observations_set.update(paraphrases)
            if observation not in sentence2paraphrases:
                sentence2paraphrases[observation] = set()
            sentence2paraphrases[observation].update(paraphrases)

    logger.info(f'Loading hard triplets from facts from {args.hard_triplets_from_facts_filepaths}...')
    anchor2negatives = {}
    for filepath in args.hard_triplets_from_facts_filepaths:
        rows = load_jsonl(filepath)
        logger.info(f'Loaded {len(rows)} rows from {filepath}.')
        for row in rows:
            anchor = row['metadata']['query']
            positives = row['parsed_response']['positives']
            negatives = row['parsed_response']['negatives']
            sentences_set.add(anchor)
            sentences_set.update(positives)
            sentences_set.update(negatives)
            if anchor not in sentence2paraphrases:
                sentence2paraphrases[anchor] = set()
            sentence2paraphrases[anchor].update(positives)
            if anchor not in anchor2negatives:
                anchor2negatives[anchor] = set()
            anchor2negatives[anchor].update(negatives)

    logger.info(f'Loading Chest ImaGenome sentences and labels from {args.chest_imagenome_sentences_and_labels_filepath}...')
    chest_imagenome_data = load_pickle(args.chest_imagenome_sentences_and_labels_filepath)
    for group in chest_imagenome_data['groups']:
        sentences_set.update(group['sentences'])
        logger.info(f'len(group["sentences"])={len(group["sentences"])}')

    logger.info(f'Loading RadGraph sentences and labels from {args.radgraph_sentences_and_labels_filepath}...')
    radgraph_data = load_pickle(args.radgraph_sentences_and_labels_filepath)
    for key in ('train', 'dev', 'test', 'chexpert', 'mimiccxr'):
        logger.info(f'len(radgraph_data[{key}])={len(radgraph_data[key])}')
        for s in radgraph_data[key]:
            radgraph_sentences_set.add(s)
            sentences_set.add(s)
    
    logger.info(f'Found {len(sentences_set)} unique sentences.')
    sentences_list = list(sentences_set)
    sentences_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    sentence2index = {s: i for i, s in enumerate(sentences_list)} # Map sentence to index

    anatloc2paraphrases = { s:ps for s, ps in sentence2paraphrases.items() if s in anatomical_locations_set }
    for ps in anatloc2paraphrases.values():
        anatomical_locations_set.update(ps)
    anatomical_locations_list = list(anatomical_locations_set)
    anatomical_locations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    
    logger.info(f'Found {len(anatomical_locations_set)} unique anatomical locations.')
    logger.info(f'len(anatloc2paraphrases)={len(anatloc2paraphrases)}')

    obs2paraphrases = { s:ps for s, ps in sentence2paraphrases.items() if s in observations_set }
    for ps in obs2paraphrases.values():
        observations_set.update(ps)
    observations_list = list(observations_set)
    observations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically

    logger.info(f'Found {len(observations_set)} unique observations.')
    logger.info(f'len(obs2paraphrases)={len(obs2paraphrases)}')

    # Convert anatomical locations and observations to indices
    anatloc_indices = [sentence2index[s] for s in anatomical_locations_list]
    obs_indices = [sentence2index[s] for s in observations_list]

    # Print some examples
    logger.info('Examples of anatomical locations:')
    for i in np.linspace(0, len(anatomical_locations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {anatomical_locations_list[i]}')

    logger.info('Examples of observations:')
    for i in np.linspace(0, len(observations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {observations_list[i]}')

    # Precompute BERT-based embeddings for all sentences
    emb_extractor = CachedTextEmbeddingExtractor(
        model_name=args.bert_model_name,
        model_checkpoint_folder_path=args.bert_checkpoint_folder_path,
        device=args.device,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
    )
    embeddings = emb_extractor.compute_text_embeddings(sentences_list)
    assert embeddings.shape[0] == len(sentences_list)
    logger.info(f"embeddings.shape: {embeddings.shape}")

    # Precompute clusters for anatomical locations and observations
    anatloc_clusters = emb_extractor.compute_kmeans_labels(
        texts=anatomical_locations_list,
        n_clusters=args.num_anatloc_clusters,
        num_iterations=args.num_kmeans_iterations,
    )
    obs_clusters = emb_extractor.compute_kmeans_labels(
        texts=observations_list,
        n_clusters=args.num_obs_clusters,
        num_iterations=args.num_kmeans_iterations,
    )

    # Convert clusters to a more convenient format    
    obs_idx2cluster = {}
    obs_cluster2indices = [[] for _ in range(args.num_obs_clusters)]
    for i, c in zip(obs_indices, obs_clusters):
        obs_cluster2indices[c].append(i)
        obs_idx2cluster[i] = c
    assert all(len(x) > 0 for x in obs_cluster2indices)
    
    anatloc_idx2cluster = {}
    anatloc_cluster2indices = [[] for _ in range(args.num_anatloc_clusters)]
    for i, c in zip(anatloc_indices, anatloc_clusters):
        anatloc_cluster2indices[c].append(i)
        anatloc_idx2cluster[i] = c
    assert all(len(x) > 0 for x in anatloc_cluster2indices)

    # Precompute clusters for radgraph sentences
    radgraph_indices = [sentence2index[s] for s in radgraph_sentences_set]
    radgraph_indices.sort()
    radgraph_sentences = [sentences_list[i] for i in radgraph_indices]
    radgraph_clusters = emb_extractor.compute_kmeans_labels(
        texts=radgraph_sentences,
        n_clusters=args.num_radgraph_clusters,
        num_iterations=args.num_kmeans_iterations,
    )

    # =================
    # Triplet sampling
    # =================

    used_triplets = set()

    triplets = {
        'sentences': sentences_list,
        'train': {
            'anatomical_locations': [],
            'observations': [],
        },
        'val': {
            'anatomical_locations': [],
            'observations': [],
        },
        'test': {
            'anatomical_locations': [],
            'observations': [],
        },
    }

    config_string = ';'.join([
        f'num_anatloc_clusters={args.num_anatloc_clusters}',
        f'num_obs_clusters={args.num_obs_clusters}',
        f'num_radgraph_clusters={args.num_radgraph_clusters}',
        f'num_train_triplets_per_rule={args.num_train_triplets_per_rule}',
        f'num_val_triplets_per_rule={args.num_val_triplets_per_rule}',
        f'num_test_triplets_per_rule={args.num_test_triplets_per_rule}',
        f'num_kmeans_iterations={args.num_kmeans_iterations}',
        f'bert_model_name={args.bert_model_name}',
        f'bert_checkpoint_folder_path={args.bert_checkpoint_folder_path}',
    ])
    config_hash = hash_string(config_string)
    triplets_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                      f'triplets({len(sentences_list)},{len(anatomical_locations_list)},{len(observations_list)},'
                                      f'{args.num_train_triplets_per_rule},{args.num_val_triplets_per_rule},{args.num_test_triplets_per_rule},'
                                      f'{config_hash[0]},{config_hash[1]}).pkl')

    # --------------------------------------------
    # 1) Sample triplets for anatomical locations
    # --------------------------------------------
    
    logger.info('============================================================')
    logger.info('Sampling triplets for anatomical locations...')
    
    sample_anatomical_location_triplets__rule1(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc2paraphrases=anatloc2paraphrases,
        n_anatloc_clusters=args.num_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    sample_anatomical_location_triplets__rule2(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc2paraphrases=anatloc2paraphrases,
        n_anatloc_clusters=args.num_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        anatloc_idx2cluster=anatloc_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    sample_anatomical_location_triplets__rule3(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        anatloc_indices=anatloc_indices,
        n_anatloc_clusters=args.num_anatloc_clusters,
        anatloc_cluster2indices=anatloc_cluster2indices,
        anatloc_idx2cluster=anatloc_idx2cluster,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # ------------------------------------
    # 2) Sample triplets for observations
    # ------------------------------------

    logger.info('============================================================')
    logger.info('Sampling triplets for observations...')

    # Rule 1
    sample_observation_triplets__rule1(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        n_obs_clusters=args.num_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 2
    sample_observation_triplets__rule2(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        n_obs_clusters=args.num_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        obs_idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )
    
    logger.info('---------------------------------------')

    # First, we need to map observations to health statuses
    hs2id = {}
    obs_idx_2_hsid = {}
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        health_status = row['metadata']['health status']
        try:
            hsid = hs2id[health_status]
        except KeyError:
            hsid = len(hs2id)
            hs2id[health_status] = hsid
        assert len(fact) > 0
        obs_idx_2_hsid[sentence2index[fact]] = hsid
        if det_obs:
            obs_idx_2_hsid[sentence2index[det_obs]] = hsid
        if short_obs:
            obs_idx_2_hsid[sentence2index[short_obs]] = hsid
    logger.info(f'len(hs2id): {len(hs2id)}')
    logger.info(f'hs2id: {hs2id}')
    hsid2hs = {v: k for k, v in hs2id.items()}
    count = 0
    for obs, paraphrases in obs2paraphrases.items():
        obs_idx = sentence2index[obs]
        if obs_idx in obs_idx_2_hsid:
            obs_hsid = obs_idx_2_hsid[obs_idx]
            for para in paraphrases:
                para_idx = sentence2index[para]
                obs_idx_2_hsid[para_idx] = obs_hsid
        else:
            count += 1
    # Print number of observations for each health status
    hsid2count = {i: 0 for i in range(len(hs2id))}
    for obs_idx, hsid in obs_idx_2_hsid.items():
        hsid2count[hsid] += 1
    logger.info(f'Number of observations for each health status:')
    for hsid, _ in sorted(list(hsid2count.items()), key=lambda x: x[1], reverse=True):
        logger.info(f'{hsid2count[hsid]} observations with health status "{hsid2hs[hsid]}"')
    # Print warning if some observations do not have a health status
    if count > 0:
        logger.warning(f'Could not find health status for {count} paraphrased observations.')

    hsid_cid_2_obs_idxs = {} # map (hsid, cid) to list of obs_idxs
    obs_idx_2_hsid_cid = {} # map obs_idx to (hsid, cid)
    for obs_idx, hsid in obs_idx_2_hsid.items():
        cid = obs_idx2cluster[obs_idx]
        hsid_cid = (hsid, cid)
        obs_idx_2_hsid_cid[obs_idx] = hsid_cid
        try:
            hsid_cid_2_obs_idxs[hsid_cid].append(obs_idx)
        except KeyError:
            hsid_cid_2_obs_idxs[hsid_cid] = [obs_idx]
    logger.info(f'len(hsid_cid_2_obs_idxs): {len(hsid_cid_2_obs_idxs)}')
    logger.info(f'len(obs_idx_2_hsid_cid): {len(obs_idx_2_hsid_cid)}')
    logger.info('---------------------------------------')

    # Rule 3
    sample_observation_triplets__rule3(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs_idx_2_hsid_cid=obs_idx_2_hsid_cid,
        hsid_cid_2_obs_idxs=hsid_cid_2_obs_idxs,
        n_obs_clusters=args.num_obs_clusters,
        obs_cluster2indices=obs_cluster2indices,
        sentence_embeddings=embeddings,
        sentence_list=sentences_list,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 4
    sample_observation_triplets__rule4(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        obs2paraphrases=obs2paraphrases,
        obs_idx_2_hsid=obs_idx_2_hsid,
        obs_cluster2indices=obs_cluster2indices,
        obs_idx2cluster=obs_idx2cluster,
        sentence2index=sentence2index,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 5
    sample_observation_triplets__rule5(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        integrated_fact_metadata=integrated_fact_metadata,
        obs2paraphrases=obs2paraphrases,
        sentence2index=sentence2index,
        sentence_list=sentences_list,
        sentence_embeddings=embeddings,
        obs_idx2cluster=obs_idx2cluster,
        obs_cluster2indices=obs_cluster2indices,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 6
    chest_imagenome_sentences = []
    chest_imagenome_labels = []
    for group in chest_imagenome_data['groups']:
        chest_imagenome_sentences.extend(group['sentences'])
        chest_imagenome_labels.append(group['labels'])
        assert len(group['sentences']) == len(group['labels'])
    chest_imagenome_labels = np.concatenate(chest_imagenome_labels, axis=0)
    logger.info(f'len(chest_imagenome_sentences): {len(chest_imagenome_sentences)}')
    logger.info(f'chest_imagenome_labels.shape: {chest_imagenome_labels.shape}')
    sample_observation_triplets__rule6(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence2index=sentence2index,
        index2sentence=sentences_list,
        sentence_embeddings=embeddings,
        sentences=chest_imagenome_sentences,
        labels=chest_imagenome_labels,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 7
    radgraph_index2cluster = { i:c for i, c in zip(radgraph_indices, radgraph_clusters) }
    s2l = dict()
    for key in ('train', 'dev', 'test', 'chexpert', 'mimiccxr'):
        for s, l in radgraph_data[key].items():
            if s in s2l:
                s2l[s] |= l # union of sets
            else:
                s2l[s] = l
    radgraph_sentence_labels_pairs = [(s, l) for s, l in s2l.items()]
    sample_observation_triplets__rule7(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence_labels_pairs=radgraph_sentence_labels_pairs,
        sentence2index=sentence2index,
        index2cluster=radgraph_index2cluster,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # Rule 8
    sample_observation_triplets__rule8(
        n_triplets_per_rule=args.num_train_triplets_per_rule,
        sentence2paraphrases=sentence2paraphrases,
        anchor2negatives=anchor2negatives,
        sentence2index=sentence2index,
        used_triplets=used_triplets,
        triplets_dict=triplets,
        logger=logger,
    )

    # ==================================================================================================
    # Extract triplets for validation and test sets
    logger.info('Extracting triplets for validation and test sets...')
    for key in ['anatomical_locations', 'observations']:
        if len(triplets['train'][key]) == 0:
            continue
        for x in triplets['train'][key]:
            indices = random.sample(range(len(x['triplets'])), args.num_val_triplets_per_rule + args.num_test_triplets_per_rule)
            indices_set = set(indices)
            sampled_triplets = [x['triplets'][i] for i in indices]
            val_triplets = np.array(sampled_triplets[:args.num_val_triplets_per_rule])
            test_triplets = np.array(sampled_triplets[args.num_val_triplets_per_rule:])
            train_triplets = [x['triplets'][i] for i in range(len(x['triplets'])) if i not in indices_set]
            train_triplets = np.array(train_triplets)
            x['triplets'] = train_triplets
            triplets['val'][key].append({
                'rule': x['rule'],
                'triplets': val_triplets,
            })
            triplets['test'][key].append({
                'rule': x['rule'],
                'triplets': test_triplets,
            })

    # Save triplets
    logger.info(f'Saving triplets to {triplets_save_path}...')
    save_pickle(triplets, triplets_save_path)
    logger.info(f'Done!')

if __name__ == '__main__':
    main()   