import argparse
import numpy as np
import os
import random
import math
from tqdm import tqdm
from Levenshtein import distance as levenshtein_distance
from sklearn.cluster import KMeans

from medvqa.datasets.mimiccxr import MIMICCXR_LARGE_FAST_CACHE_DIR
from medvqa.utils.logging import get_console_logger
from medvqa.utils.files import load_jsonl, load_pickle, save_pickle
from medvqa.models.huggingface_utils import compute_text_embeddings_with_BiomedVLP_CXR_BERT_specialized

MAX_TRIES = 20

def _levenshtein_similarity(a, b):
    return 1 - levenshtein_distance(a, b) / max(len(a), len(b))

def _is_at_least_x_percent_better(a, b, x, reverse=False):
    if reverse: # a is better than b if a is smaller than b
        return a < b and (b - a) / b >= x
    else: # a is better than b if a is larger than b
        return a > b and (a - b) / a >= x

if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Sample triplets for fact embedding learning')
    parser.add_argument('--paraphrased_anatomical_locations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--paraphrased_observations_filepaths', type=str, nargs='+', required=True)
    parser.add_argument('--integrated_fact_metadata_filepath', type=str, required=True)
    parser.add_argument('--num_anatloc_clusters', type=int, required=True,
                        help='Number of clusters to be formed for anatomical locations. Used for sampling triplets.')
    parser.add_argument('--num_obs_clusters', type=int, required=True,
                        help='Number of clusters to be formed for observations. Used for sampling triplets.')
    parser.add_argument('--num_train_anatloc_triplets', type=int, required=True)
    parser.add_argument('--num_train_obs_triplets', type=int, required=True)
    parser.add_argument('--num_val_anatloc_triplets', type=int, required=True)
    parser.add_argument('--num_val_obs_triplets', type=int, required=True)
    
    parser.add_argument('--logging_level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'])
    parser.add_argument("--device", type=str, default="GPU", choices=["GPU", "CPU"])
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--num_kmeans_iterations', type=int, default=300)
    args = parser.parse_args()

    # Add validation triplets to training triplets (since the validation triplets will be subtracted from the training triplets later)
    args.num_train_anatloc_triplets += args.num_val_anatloc_triplets 
    args.num_train_obs_triplets += args.num_val_obs_triplets

    # -----------------------------------------------
    # Definitions:
    # -----------------------------------------------
    # dot(E(X), E(Y)) = dot product of embeddings of X and Y, where E(.) is the embedding function that we want to learn
    # cos(X, Y) = cosine similarity between X and Y, where X and Y are represented as vectors using a pre-trained
    #            sentence embedding model from Microsoft Research called CXR-BERT
    # leveinshtein(X, Y) = Leveinshtein distance between X and Y

    # ------------------------------------------------------
    # Rules for sampling triplets for anatomical locations:
    # ------------------------------------------------------
    
    # Rule 1: "Rank paraphrases very highly"
    # dot(E(A), E(P)) > dot(E(A), E(N)) unless cos(A, P) < cos(A, N) AND leveinshtein(A, P) > leveinshtein(A, N)
    # Where:
    # A = anchor = anatomical location
    # P = positive = paraphrased anatomical location
    # N = negative = randomly sampled anatomical location    
    # In words: include the triplet (A, P, N) (i.e. trust the paraphrase generated by GPT-3.5)
    #           except if both CXR-BERT and Leveinshtein disagree at the same time.

    # Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus"
    # dot(E(A), E(P)) > dot(E(A), E(N)) if cos(A, P) > cos(A, N) AND leveinshtein(A, P) < leveinshtein(A, N)
    # Where:
    # A = anchor = anatomical location
    # P = positive = another anatomical location randomly sampled from cluster(A)
    # N = negative = randomly sampled anatomical location
    # In words: include the triplet (A, P, N) if both CXR-BERT and Leveinshtein agree at the same time.

    # -----------------------------------------------
    # Rules for sampling triplets for observations:
    # -----------------------------------------------

    # Rule 1: "Rank paraphrases very highly"
    # dot(E(O), E(P)) > dot(E(O), E(N)) unless cos(O, P) < cos(O, N) AND leveinshtein(O, P) > leveinshtein(O, N)
    # Where:
    # O = anchor = observation
    # P = positive = paraphrased observation
    # N = negative = randomly sampled observation
    # In words: include the triplet (O, P, N) (i.e. trust the paraphrase generated by GPT-3.5)
    #           except if both CXR-BERT and Leveinshtein disagree at the same time.

    # Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, making sure that anchor and positive share the same health status"
    # dot(E(O), E(P)) > dot(E(O), E(N)) if HS(O) = HS(P) AND cos(O, P) > cos(O, N) AND leveinshtein(O, P) < leveinshtein(O, N)
    # Where:
    # O = anchor = observation
    # P = positive = another observation randomly sampled from cluster(O)
    # N = negative = randomly sampled observation
    # In words: include the triplet (O, P, N) if both CXR-BERT and Leveinshtein agree at the same time
    #           and if the anchor and positive share the same health status.

    # Rule 3: "Hard health-status-vs-others triplets"
    # dot(E(O), E(P)) > dot(E(O), E(N)) if HS(O) == HS(P) AND HS(N) != HS(O)
    # Where:
    # O = anchor = observation
    # P = positive = paraphrase of O
    # N = negative = randomly sampled observation from the same cluster as O but with a different health status
    # In words: include the triplet (O, P, N) if the anchor and positive are paraphrases of each other
    #           and if the negative is from the same cluster as the anchor but with a different health status.

    # -----------------------------------------------

    # Set up logging
    logger = get_console_logger(args.logging_level)
    
    # Collect all sentences, observations and anatomical locations from input files
    logger.info('Collecting all sentences, observations and anatomical locations from input files...')
    
    sentences_set = set()
    anatomical_locations_set = set()
    observations_set = set()
    
    logger.info(f'Loading integrated fact metadata from {args.integrated_fact_metadata_filepath}...')
    integrated_fact_metadata = load_jsonl(args.integrated_fact_metadata_filepath)
    logger.info(f'Found {len(integrated_fact_metadata)} integrated fact metadata rows.')
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        anat_loc = row['metadata']['anatomical location']
        assert len(fact) > 0
        sentences_set.add(fact)
        observations_set.add(fact)
        if det_obs:
            sentences_set.add(det_obs)
            observations_set.add(det_obs)
        if short_obs:
            sentences_set.add(short_obs)
            observations_set.add(short_obs)
        if anat_loc:
            sentences_set.add(anat_loc)
            anatomical_locations_set.add(anat_loc)
    
    anatloc_paraphrases = []
    for filepath in args.paraphrased_anatomical_locations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_anatomical_locations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_anatomical_locations)} paraphrased anatomical locations.')
        for row in paraphrased_anatomical_locations:
            anatloc = row['metadata']['anatomical location']
            paraphrases = row['parsed_response']
            sentences_set.add(anatloc)
            sentences_set.update(paraphrases)
            anatomical_locations_set.add(anatloc)
            anatomical_locations_set.update(paraphrases)
            anatloc_paraphrases.append((anatloc, paraphrases))
    logger.info(f'Found {len(anatomical_locations_set)} unique anatomical locations.')
    anatomical_locations_list = list(anatomical_locations_set)
    anatomical_locations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    
    observation_paraphrases = []
    for filepath in args.paraphrased_observations_filepaths:
        logger.info(f'Collecting strings from {filepath}...')
        paraphrased_observations = load_jsonl(filepath)
        logger.info(f'Found {len(paraphrased_observations)} paraphrased observations.')
        for row in paraphrased_observations:
            observation = row['metadata']['observation']
            paraphrases = row['parsed_response']
            sentences_set.add(observation)
            sentences_set.update(paraphrases)
            observations_set.add(observation)
            observations_set.update(paraphrases)
            observation_paraphrases.append((observation, paraphrases))
    logger.info(f'Found {len(observations_set)} unique observations.')
    observations_list = list(observations_set)
    observations_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically

    logger.info(f'Found {len(sentences_set)} unique sentences.')
    sentences_list = list(sentences_set)
    sentences_list.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically
    sentence2index = {s: i for i, s in enumerate(sentences_list)} # Map sentence to index

    # Convert anatomical locations and observations to indices
    anatloc_indices = [sentence2index[s] for s in anatomical_locations_list]
    obs_indices = [sentence2index[s] for s in observations_list]

    # Print some examples
    logger.info('Examples of anatomical locations:')
    for i in np.linspace(0, len(anatomical_locations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {anatomical_locations_list[i]}')

    logger.info('Examples of observations:')
    for i in np.linspace(0, len(observations_list)-1, 10, dtype=int):
        logger.info(f'{i}: {observations_list[i]}')

    # Precompute CXR-BERT embeddings for all sentences
    sentlen_sum = sum(len(x) for x in sentences_list)
    sentence_embeddings_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                                 f'cxr_bert_sentence_embeddings({len(sentences_list)},{sentlen_sum}).pkl')
    if os.path.exists(sentence_embeddings_save_path):
        logger.info(f'Found precomputed sentence embeddings at {sentence_embeddings_save_path}. Loading...')
        sentences_and_embeddings = load_pickle(sentence_embeddings_save_path)
        assert len(sentences_list) == len(sentences_and_embeddings['sentences'])
        assert all(s1 == s2 for s1, s2 in zip(sentences_list, sentences_and_embeddings['sentences']))
        embeddings = sentences_and_embeddings['embeddings']
        assert embeddings.shape[0] == len(sentences_list)
    else:
        logger.info(f'Precomputing sentence embeddings for {len(sentences_list)} sentences...')
        embeddings = compute_text_embeddings_with_BiomedVLP_CXR_BERT_specialized(sentences_list,
                                                                                 device=args.device,
                                                                                 logger=logger,
                                                                                 batch_size=args.batch_size,
                                                                                 num_workers=args.num_workers,
                                                                                 )
        logger.info(f'embeddings.shape: {embeddings.shape}')
        logger.info(f'Saving embeddings to {sentence_embeddings_save_path}...')
        sentences_and_embeddings = {
            'sentences': sentences_list,
            'embeddings': embeddings,
        }
        save_pickle(sentences_and_embeddings, sentence_embeddings_save_path)
        logger.info(f"sentences_and_embeddings['embeddings'].shape: {sentences_and_embeddings['embeddings'].shape}")

    # Precompute clusters for anatomical locations and observations
    clusters_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                        f'clusters({args.num_anatloc_clusters},{args.num_obs_clusters}'
                                        f'{len(anatomical_locations_list)},{len(observations_list)}).pkl')
    if os.path.exists(clusters_save_path):
        logger.info(f'Found precomputed clusters at {clusters_save_path}. Loading...')
        clusters = load_pickle(clusters_save_path)
    else:
        # Precompute clusters for anatomical locations
        logger.info(f'Precomputing clusters for {len(anatomical_locations_list)} anatomical locations...')
        anatloc_embeddings = embeddings[anatloc_indices]
        logger.info(f'anatloc_embeddings.shape: {anatloc_embeddings.shape}')
        anatloc_kmeans = KMeans(n_clusters=args.num_anatloc_clusters, init='k-means++', n_init='auto', verbose=2,
                                max_iter=args.num_kmeans_iterations).fit(anatloc_embeddings)
        anatloc_clusters = anatloc_kmeans.labels_
        logger.info(f'anatloc_clusters.shape: {anatloc_clusters.shape}')

        # Precompute clusters for observations
        logger.info(f'Precomputing clusters for {len(observations_list)} observations...')
        obs_embeddings = embeddings[obs_indices]
        logger.info(f'obs_embeddings.shape: {obs_embeddings.shape}')
        obs_kmeans = KMeans(n_clusters=args.num_obs_clusters, init='k-means++', n_init='auto', verbose=2,
                            max_iter=args.num_kmeans_iterations).fit(obs_embeddings)
        obs_clusters = obs_kmeans.labels_
        logger.info(f'obs_clusters.shape: {obs_clusters.shape}')

        # Save clusters
        logger.info(f'Saving clusters to {clusters_save_path}...')
        clusters = {
            'anatomical_locations': {
                'indices': anatloc_indices,
                'clusters': anatloc_clusters,
            },
            'observations': {
                'indices': obs_indices,
                'clusters': obs_clusters,
            },
        }
        save_pickle(clusters, clusters_save_path)
    
    # Convert clusters to a more convenient format
    obs_idx2cluster = {}
    obs_cluster2indices = [[] for _ in range(args.num_obs_clusters)]
    for i, c in zip(clusters['observations']['indices'], clusters['observations']['clusters']):
        obs_cluster2indices[c].append(i)
        obs_idx2cluster[i] = c
    anatloc_idx2cluster = {}
    anatloc_cluster2indices = [[] for _ in range(args.num_anatloc_clusters)]
    for i, c in zip(clusters['anatomical_locations']['indices'], clusters['anatomical_locations']['clusters']):
        anatloc_cluster2indices[c].append(i)
        anatloc_idx2cluster[i] = c

    # =================
    # Triplet sampling
    # =================
    used_triplets = set()

    triplets = {
        'sentences': sentences_list,
        'train': {
            'anatomical_locations': [],
            'observations': [],
        },
        'val': {
            'anatomical_locations': [],
            'observations': [],
        },
    }

    triplets_save_path = os.path.join(MIMICCXR_LARGE_FAST_CACHE_DIR,
                                      f'triplets({len(sentences_list)},{len(anatomical_locations_list)},{len(observations_list)},'
                                      f'{args.num_train_anatloc_triplets},{args.num_train_obs_triplets},'
                                      f'{args.num_val_anatloc_triplets},{args.num_val_obs_triplets}).pkl')

    # --------------------------------------------
    # 1) Sample triplets for anatomical locations
    # --------------------------------------------
    
    logger.info('============================================================')
    logger.info('Sampling triplets for anatomical locations...')

    
    logger.info('---------------------------------------')
    n_triplets_per_rule = math.ceil(args.num_train_anatloc_triplets / 3) # 3 rules: 1, 1.1, 2
    
    # Rule 1: "Rank paraphrases very highly"
    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(anatloc_paraphrases))
    tmp = []
    print('Rule 1: "Rank paraphrases very highly"')
    for anatloc, paraphrases in tqdm(anatloc_paraphrases, mininterval=2):
        anatloc_idx = sentence2index[anatloc]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(anatloc_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{anatloc}". Skipping...')
            continue
        A = anatloc_idx
        n_triplets_per_para = math.ceil(n_triplets_per_anchor / len(para_idxs))
        for P in para_idxs:
            AP_sim = np.dot(embeddings[A], embeddings[P])
            AP_levd = levenshtein_distance(sentences_list[A], sentences_list[P])
            for i in range(n_triplets_per_para):
                for _ in range(MAX_TRIES):
                    N = random.choice(anatloc_indices)
                    if N == A or N in para_idxs_set: # same as anchor or in paraphrases
                        continue
                    triplet = (A, P, N)
                    if triplet in used_triplets: # already used
                        continue
                    AN_sim = np.dot(embeddings[A], embeddings[N])
                    AN_levd = levenshtein_distance(sentences_list[A], sentences_list[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # both CXR-BERT and Leveinshtein disagree 
                        continue
                    used_triplets.add(triplet) # add triplet to used triplets
                    tmp.append(triplet)
                    break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 1: "Rank paraphrases very highly": {len(tmp)}')
    triplets['train']['anatomical_locations'].append({
        'rule': 'Rank paraphrases very highly',
        'triplets': tmp,
    })

    # Rule 1.1: "Rank paraphrases very highly - Hard negative"
    # We will sample negatives from the same cluster as the anchor
    logger.info('---------------------------------------')
    tmp = []
    print('Rule 1.1: "Rank paraphrases very highly - Hard negative"')
    for anatloc, paraphrases in tqdm(anatloc_paraphrases, mininterval=2):
        anatloc_idx = sentence2index[anatloc]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(anatloc_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{anatloc}". Skipping...')
            continue
        A = anatloc_idx
        cid_A = anatloc_idx2cluster[A]
        n_triplets_per_para = math.ceil(n_triplets_per_anchor / len(para_idxs))
        for P in para_idxs:
            AP_sim = np.dot(embeddings[A], embeddings[P])
            AP_levd = levenshtein_distance(sentences_list[A], sentences_list[P])
            for i in range(n_triplets_per_para):
                for _ in range(MAX_TRIES):
                    N = random.choice(anatloc_cluster2indices[cid_A])
                    if N == A or N in para_idxs_set: # same as anchor or in paraphrases
                        continue
                    triplet = (A, P, N)
                    if triplet in used_triplets: # already used
                        continue
                    AN_sim = np.dot(embeddings[A], embeddings[N])
                    AN_levd = levenshtein_distance(sentences_list[A], sentences_list[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # both CXR-BERT and Leveinshtein disagree 
                        continue
                    used_triplets.add(triplet) # add triplet to used triplets
                    tmp.append(triplet)
                    break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 1.1: "Rank paraphrases very highly - Hard negative": {len(tmp)}')
    triplets['train']['anatomical_locations'].append({
        'rule': 'Rank paraphrases very highly - Hard negative',
        'triplets': tmp,
    })

    # Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus"
    logger.info('---------------------------------------')
    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(anatloc_indices))
    tmp = []
    print('Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus"')
    for A in tqdm(anatloc_indices, mininterval=2):
        cid_A = anatloc_idx2cluster[A]
        for i in range(n_triplets_per_anchor):
            for _ in range(MAX_TRIES):
                P = random.choice(anatloc_cluster2indices[cid_A])
                if P == A:
                    continue
                N = random.choice(anatloc_indices)
                triplet = (A, P, N)
                if triplet in used_triplets: # already used
                    continue
                AP_sim = np.dot(embeddings[A], embeddings[P])
                AN_sim = np.dot(embeddings[A], embeddings[N])
                AP_levsim = _levenshtein_similarity(sentences_list[A], sentences_list[P])
                AN_levsim = _levenshtein_similarity(sentences_list[A], sentences_list[N])
                if _is_at_least_x_percent_better(AP_sim, AN_sim, 0.5) and _is_at_least_x_percent_better(AP_levsim, AN_levsim, 0.4):
                    # both CXR-BERT and Leveinshtein agree
                    used_triplets.add(triplet) # add triplet to used triplets
                    tmp.append(triplet)
                    break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus": {len(tmp)}')
    triplets['train']['anatomical_locations'].append({
        'rule': 'Rank some triplets according to CXR-BERT and Leveinshtein consensus',
        'triplets': tmp,
    })

    # ------------------------------------
    # 2) Sample triplets for observations
    # ------------------------------------

    logger.info('============================================================')
    logger.info('Sampling triplets for observations...')

    n_triplets_per_rule = math.ceil(args.num_train_obs_triplets / 4) # 4 rules: 1, 1.1, 2, 3
    
    # Rule 1: "Rank paraphrases very highly"
    logger.info('---------------------------------------')
    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(observation_paraphrases))
    tmp = []
    print('Rule 1: "Rank paraphrases very highly"')
    for obs, paraphrases in tqdm(observation_paraphrases, mininterval=2):
        obs_idx = sentence2index[obs]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(obs_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{obs}". Skipping...')
            continue
        A = obs_idx
        n_triplets_per_para = math.ceil(n_triplets_per_anchor / len(para_idxs))
        for P in para_idxs:
            AP_sim = np.dot(embeddings[A], embeddings[P])
            AP_levd = levenshtein_distance(sentences_list[A], sentences_list[P])
            for i in range(n_triplets_per_para):
                for _ in range(MAX_TRIES):
                    N = random.choice(obs_indices)
                    if N == A or N in para_idxs_set: # same as anchor or in paraphrases
                        continue
                    triplet = (A, P, N)
                    if triplet in used_triplets: # already used
                        continue
                    AN_sim = np.dot(embeddings[A], embeddings[N])
                    AN_levd = levenshtein_distance(sentences_list[A], sentences_list[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # both CXR-BERT and Leveinshtein disagree 
                        continue
                    used_triplets.add(triplet) # add triplet to used triplets
                    tmp.append(triplet)
                    break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 1: "Rank paraphrases very highly": {len(tmp)}')
    triplets['train']['observations'].append({
        'rule': 'Rank paraphrases very highly',
        'triplets': tmp,
    })

    # Rule 1.1: "Rank paraphrases very highly - Hard negative"
    # We will sample negatives from the same cluster as the anchor
    logger.info('---------------------------------------')
    tmp = []
    print('Rule 1.1: "Rank paraphrases very highly - Hard negative"')
    for obs, paraphrases in tqdm(observation_paraphrases, mininterval=2):
        obs_idx = sentence2index[obs]
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.discard(obs_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) == 0:
            logger.warning(f'No paraphrases found for "{obs}". Skipping...')
            continue
        A = obs_idx
        cid_A = obs_idx2cluster[A]
        n_triplets_per_para = math.ceil(n_triplets_per_anchor / len(para_idxs))
        for P in para_idxs:
            AP_sim = np.dot(embeddings[A], embeddings[P])
            AP_levd = levenshtein_distance(sentences_list[A], sentences_list[P])
            for i in range(n_triplets_per_para):
                for _ in range(MAX_TRIES):
                    N = random.choice(obs_cluster2indices[cid_A])
                    if N == A or N in para_idxs_set: # same as anchor or in paraphrases
                        continue
                    triplet = (A, P, N)
                    if triplet in used_triplets: # already used
                        continue
                    AN_sim = np.dot(embeddings[A], embeddings[N])
                    AN_levd = levenshtein_distance(sentences_list[A], sentences_list[N])
                    if AN_sim > AP_sim and AN_levd < AP_levd: # both CXR-BERT and Leveinshtein disagree 
                        continue
                    used_triplets.add(triplet) # add triplet to used triplets
                    tmp.append(triplet)
                    break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 1.1: "Rank paraphrases very highly - Hard negative": {len(tmp)}')
    triplets['train']['observations'].append({
        'rule': 'Rank paraphrases very highly - Hard negative',
        'triplets': tmp,
    })

    # Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status"
    
    logger.info('---------------------------------------')

    # First, we need to map observations to health statuses
    hs2id = {}
    obs_idx_2_hsid = {}
    for row in integrated_fact_metadata:
        fact = row['fact']
        det_obs = row['metadata']['detailed observation']
        short_obs = row['metadata']['short observation']
        health_status = row['metadata']['health status']
        try:
            hsid = hs2id[health_status]
        except KeyError:
            hsid = len(hs2id)
            hs2id[health_status] = hsid
        assert len(fact) > 0
        obs_idx_2_hsid[sentence2index[fact]] = hsid
        if det_obs:
            obs_idx_2_hsid[sentence2index[det_obs]] = hsid
        if short_obs:
            obs_idx_2_hsid[sentence2index[short_obs]] = hsid
    logger.info(f'len(hs2id): {len(hs2id)}')
    logger.info(f'hs2id: {hs2id}')
    hsid2hs = {v: k for k, v in hs2id.items()}
    count = 0
    for obs, paraphrases in observation_paraphrases:
        obs_idx = sentence2index[obs]
        if obs_idx in obs_idx_2_hsid:
            for para in paraphrases:
                para_idx = sentence2index[para]
                obs_idx_2_hsid[para_idx] = obs_idx_2_hsid[obs_idx]
        else:
            count += 1
    # Print number of observations for each health status
    hsid2count = {i: 0 for i in range(len(hs2id))}
    for obs_idx, hsid in obs_idx_2_hsid.items():
        hsid2count[hsid] += 1
    logger.info(f'Number of observations for each health status:')
    for hsid, _ in sorted(list(hsid2count.items()), key=lambda x: x[1], reverse=True):
        logger.info(f'{hsid2count[hsid]} observations with health status "{hsid2hs[hsid]}"')
    # Print warning if some observations do not have a health status
    if count > 0:
        logger.warning(f'Could not find health status for {count}/{len(observation_paraphrases)} paraphrased observations.')

    hsid_cid_2_obs_idxs = {} # map (hsid, cid) to list of obs_idxs
    obs_idx_2_hsid_cid = {} # map obs_idx to (hsid, cid)
    for obs_idx, hsid in obs_idx_2_hsid.items():
        cid = obs_idx2cluster[obs_idx]
        hsid_cid = (hsid, cid)
        obs_idx_2_hsid_cid[obs_idx] = hsid_cid
        try:
            hsid_cid_2_obs_idxs[hsid_cid].append(obs_idx)
        except KeyError:
            hsid_cid_2_obs_idxs[hsid_cid] = [obs_idx]
    logger.info(f'len(hsid_cid_2_obs_idxs): {len(hsid_cid_2_obs_idxs)}')
    logger.info(f'len(obs_idx_2_hsid_cid): {len(obs_idx_2_hsid_cid)}')

    n_triplets_per_anchor = math.ceil(n_triplets_per_rule / len(obs_idx_2_hsid_cid))
    tmp = []
    print('Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status"')
    for A, hsid_cid_A in tqdm(obs_idx_2_hsid_cid.items(), mininterval=2):
        pos_idxs = hsid_cid_2_obs_idxs[hsid_cid_A]
        for i in range(n_triplets_per_anchor):
            for _ in range(MAX_TRIES):
                P = random.choice(pos_idxs)
                if P == A:
                    continue
                N = random.choice(obs_indices)
                triplet = (A, P, N)
                if triplet in used_triplets: # already used
                    continue
                AP_sim = np.dot(embeddings[A], embeddings[P])
                AN_sim = np.dot(embeddings[A], embeddings[N])
                AP_levsim = _levenshtein_similarity(sentences_list[A], sentences_list[P])
                AN_levsim = _levenshtein_similarity(sentences_list[A], sentences_list[N])
                if _is_at_least_x_percent_better(AP_sim, AN_sim, 0.5) and _is_at_least_x_percent_better(AP_levsim, AN_levsim, 0.4):
                    # both CXR-BERT and Leveinshtein agree
                    used_triplets.add(triplet) # add triplet to used triplets
                    tmp.append(triplet)
                    break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 2: "Rank some triplets according to CXR-BERT and Leveinshtein consensus,'
                f' while anchor and positive share the same health status": {len(tmp)}')
    triplets['train']['observations'].append({
        'rule': 'Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status',
        'triplets': tmp,
    })

    # Rule 3: "Hard health-status-vs-others triplets"
    logger.info('---------------------------------------')
    n_triplets_per_iter = math.ceil(n_triplets_per_rule / len(observation_paraphrases))
    tmp = []
    print('Rule 3: "Hard health-status-vs-others triplets"')
    print('n_triplets_per_iter:', n_triplets_per_iter)
    for obs, paraphrases in tqdm(observation_paraphrases, mininterval=2):
        obs_idx = sentence2index[obs]
        try:
            hsid = obs_idx_2_hsid[obs_idx]
        except KeyError:
            logger.warning(f'Could not find health status for observation "{obs}". Skipping...')
            continue
        para_idxs_set = set(sentence2index[p] for p in paraphrases)
        para_idxs_set.add(obs_idx)
        para_idxs = list(para_idxs_set)
        if len(para_idxs) < 2:
            logger.warning(f'Not enough paraphrases found for "{obs}". Skipping...')
            continue
        cluster_idxs = obs_cluster2indices[obs_idx2cluster[obs_idx]]
        for i in range(n_triplets_per_iter):
            for _ in range(MAX_TRIES):
                A, P = random.sample(para_idxs, 2)
                assert A != P
                N = random.choice(cluster_idxs)
                if obs_idx_2_hsid[N] == hsid:
                    continue
                if N in para_idxs_set:
                    continue
                triplet = (A, P, N)
                if triplet in used_triplets: # already used
                    continue
                used_triplets.add(triplet) # add triplet to used triplets
                tmp.append(triplet)
                break # break out of for loop
    logger.info(f'Actual number of triplets sampled for Rule 3: "Hard health-status-vs-others triplets": {len(tmp)}')
    triplets['train']['observations'].append({
        'rule': 'Hard health-status-vs-others triplets',
        'triplets': tmp,
    })

    # Extract triplets for validation set
    logger.info('Extracting triplets for validation set...')
    for key, num_triplets in zip(
            ['anatomical_locations', 'observations'],
            [args.num_val_anatloc_triplets, args.num_val_obs_triplets],
        ):
        if len(triplets['train'][key]) == 0:
            continue
        num_triplets_per_rule = math.ceil(num_triplets / len(triplets['train'][key]))
        for x in triplets['train'][key]:
            indices = random.sample(range(len(x['triplets'])), num_triplets_per_rule)
            indices_set = set(indices)
            val_triplets = [x['triplets'][i] for i in indices]
            val_triplets = np.array(val_triplets)
            train_triplets = [x['triplets'][i] for i in range(len(x['triplets'])) if i not in indices_set]
            train_triplets = np.array(train_triplets)
            x['triplets'] = train_triplets
            triplets['val'][key].append({
                'rule': x['rule'],
                'triplets': val_triplets,
            })

    # Save triplets
    logger.info(f'Saving triplets to {triplets_save_path}...')
    save_pickle(triplets, triplets_save_path)
    logger.info(f'Done!')