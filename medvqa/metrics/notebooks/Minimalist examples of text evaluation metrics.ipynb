{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bdc6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "     \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "]\n",
    "\n",
    "gen_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The fast brown fox leaped over the lazy dog.\",\n",
    "    \"Being or not being, that's what's being questioned.\",\n",
    "    \"Everything that sparkles isn't necessarily gold.\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient has no cardiomegaly\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabfe03",
   "metadata": {},
   "source": [
    "### Cider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed317537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.cider import cider_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d142817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 4.7064514948768945\n",
      "scores: [10.          3.20705125  0.74208407  0.73021884 10.          3.5593548 ]\n"
     ]
    }
   ],
   "source": [
    "scorer = cider_scorer.CiderScorer(n=4)\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    scorer += (gen, [gt])\n",
    "mean_score, scores = scorer.compute_score()\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0a692",
   "metadata": {},
   "source": [
    "### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57df0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.rouge import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf76279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 0.7060052418323793\n",
      "scores: [1.0, 0.7777777777777778, 0.21785714285714283, 0.3333333333333333, 1.0, 0.9070631970260222]\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge.Rouge()\n",
    "scores = []\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    score = scorer.calc_score([gen], [gt])\n",
    "    scores.append(score)\n",
    "mean_score = sum(scores) / len(scores)\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59083425",
   "metadata": {},
   "source": [
    "### Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32cefa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu import bleu_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "663e829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: [0.6664715669720498, 0.578358029419633, 0.5130914126445008, 0.46274151089507487]\n",
      "scores:\n",
      "b1: 1.0000, b2: 1.0000, b3: 1.0000, b4: 1.0000, b: 1.0000\n",
      "b1: 0.7778, b2: 0.6236, b3: 0.4807, b4: 0.3689, b: 0.5628\n",
      "b1: 0.1947, b2: 0.1472, b3: 0.0000, b4: 0.0000, b: 0.0855\n",
      "b1: 0.3333, b2: 0.0000, b3: 0.0000, b4: 0.0000, b: 0.0833\n",
      "b1: 1.0000, b2: 1.0000, b3: 1.0000, b4: 1.0000, b: 1.0000\n",
      "b1: 0.8000, b2: 0.6325, b3: 0.5109, b4: 0.0001, b: 0.4859\n"
     ]
    }
   ],
   "source": [
    "scorer = bleu_scorer.BleuScorer(n=4)\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    scorer += (gen, [gt])\n",
    "mean_score, scores = scorer.compute_score()\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:')\n",
    "for i in range(len(gen_texts)):\n",
    "    b1 = scores[0][i]\n",
    "    b2 = scores[1][i]\n",
    "    b3 = scores[2][i]\n",
    "    b4 = scores[3][i]\n",
    "    b = (b1+b2+b3+b4)/4\n",
    "    print(f'b1: {b1:.4f}, b2: {b2:.4f}, b3: {b3:.4f}, b4: {b4:.4f}, b: {b:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa238419",
   "metadata": {},
   "source": [
    "### Meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be764c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c103393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 0.8226083930326539\n",
      "scores: [0.9995, 0.9995, 0.5852864583333333, 0.4445422535211268, 0.9921875, 0.9146341463414633]\n"
     ]
    }
   ],
   "source": [
    "tokenized_gt_texts = [word_tokenize(x) for x in gt_texts]\n",
    "tokenized_gen_texts = [word_tokenize(x) for x in gen_texts]\n",
    "scores = []\n",
    "for gen, gt in zip(tokenized_gen_texts, tokenized_gt_texts):\n",
    "    score = meteor_score([gt], gen)\n",
    "    scores.append(score)\n",
    "mean_score = sum(scores) / len(scores)\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0d3d0",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a872668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ae5bdb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71866766c08243ffb0aa399f7b7c0167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d028c6c30634463b98b5d1217636308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.05 seconds, 112.79 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = bert_score(gen_texts, gt_texts, lang='en', verbose=True, use_fast_tokenizer=True)\n",
    "P = P.cpu().numpy()\n",
    "R = R.cpu().numpy()\n",
    "F1 = F1.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac62a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [1.         0.9806174  0.93713343 0.9434316  1.         0.9791807 ]\n",
      "R: [1.         0.98459214 0.9486927  0.95515096 1.         0.98743534]\n",
      "F1: [1.         0.98260075 0.9428776  0.9492551  1.         0.98329073]\n"
     ]
    }
   ],
   "source": [
    "print('P:', P)\n",
    "print('R:', R)\n",
    "print('F1:', F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cdb43",
   "metadata": {},
   "source": [
    "### CheXbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53f76465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f1chexbert import F1CheXbert\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from medvqa.utils.hashing import hash_string\n",
    "from medvqa.utils.common import CACHE_DIR\n",
    "from medvqa.utils.files import get_cached_pickle_file\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9073b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEXBERT_LABELS = [\n",
    "    \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\", \"Lung Lesion\", \"Edema\",\n",
    "    \"Consolidation\", \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "    \"Fracture\", \"Support Devices\", \"No Finding\",\n",
    "]\n",
    "\n",
    "def merge_labels(labels_list):        \n",
    "    merged = np.zeros((len(CHEXBERT_LABELS),), np.int8)\n",
    "    merged[-1] = 1 # default to no findings\n",
    "    for labels in labels_list:\n",
    "        if labels[-1] == 0: # there is a finding\n",
    "            merged[-1] = 0\n",
    "        for i in range(0, len(labels)-1): # iterate over all labels except the last one\n",
    "            if labels[i] == 1:\n",
    "                merged[i] = 1\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ef2a537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXbertLabeler(F1CheXbert):\n",
    "\n",
    "    def __init__(self, device=None, verbose=False):\n",
    "        super().__init__(device=device)\n",
    "        self.cache_path = os.path.join(CACHE_DIR, 'chexbert_labeler_cache.pkl')\n",
    "        self.cache = get_cached_pickle_file(self.cache_path)\n",
    "        self.verbose = verbose\n",
    "        if self.cache is None:\n",
    "            self.cache = dict()\n",
    "        elif verbose:\n",
    "            print(f'Cache successfully loaded from {self.cache_path}')\n",
    "\n",
    "    def get_labels(self, texts, update_cache_on_disk=False):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'(*) Chexbert: labeling {len(texts)} texts ...')\n",
    "\n",
    "        output_labels = [None] * len(texts)\n",
    "        dirty_count = 0\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), mininterval=2):\n",
    "            text_hash = hash_string(text)\n",
    "            if text_hash in self.cache:\n",
    "                output_labels[i] = self.cache[text_hash]\n",
    "                continue\n",
    "            sentences = sent_tokenize(text)\n",
    "            sentence_labels = []\n",
    "            for sentence in sentences:\n",
    "                hash = hash_string(sentence)\n",
    "                labels = self.cache.get(hash, None)\n",
    "                if labels is None:\n",
    "                    labels = self.get_label(sentence)\n",
    "                    self.cache[hash] = labels\n",
    "                    dirty_count += 1\n",
    "                sentence_labels.append(labels)\n",
    "            output_labels[i] = merge_labels(sentence_labels)\n",
    "            self.cache[text_hash] = output_labels[i]            \n",
    "\n",
    "        if dirty_count > 0:\n",
    "            if self.verbose:\n",
    "                print(f'Done labeling: {dirty_count} new labels found and cached')\n",
    "            if update_cache_on_disk:\n",
    "                save_pickle(self.cache, self.cache_path)\n",
    "                if self.verbose:\n",
    "                    print(f'Cache successfully updated and saved to {self.cache_path}')\n",
    "        elif self.verbose:\n",
    "            print('All labels found in cache, no need to invoke chexbert labeler')\n",
    "        \n",
    "        return np.array(output_labels)\n",
    "    \n",
    "    def get_embedding(self, text):\n",
    "        assert type(text) == str\n",
    "        text = pd.Series([text])\n",
    "        out = tokenize(text, self.tokenizer)\n",
    "        batch = torch.LongTensor([o for o in out])\n",
    "        src_len = [b.shape[0] for b in batch]\n",
    "        attn_mask = generate_attention_masks(batch, src_len, self.device)\n",
    "        final_hidden = self.model.bert(batch.to(self.device), attention_mask=attn_mask)[0]\n",
    "        cls_hidden = final_hidden[:, 0, :].squeeze(dim=1)\n",
    "        return cls_hidden.cpu().detach().numpy().squeeze()\n",
    "    \n",
    "    def get_embeddings(self, texts):\n",
    "        assert type(texts) == list or type(texts) == np.ndarray\n",
    "        return np.array([self.get_embedding(text) for text in tqdm(texts, mininterval=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79956acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chexbert_f1_scores(labeler, gen_texts, gt_texts):\n",
    "    gen_labels = labeler.get_labels(gen_texts)\n",
    "    gt_labels = labeler.get_labels(gt_texts)\n",
    "    f1_scores = np.zeros(len(gen_texts))\n",
    "    for i in range(len(gen_texts)):\n",
    "        tp = np.sum((gen_labels[i] == 1) & (gt_labels[i] == 1))\n",
    "        fp = np.sum((gen_labels[i] == 1) & (gt_labels[i] == 0))\n",
    "        fn = np.sum((gen_labels[i] == 0) & (gt_labels[i] == 1))\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1_scores[i] = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "012e8fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexbert_labeler_cache.pkl\n",
      "(*) Chexbert: labeling 6 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 222.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done labeling: 2 new labels found and cached\n",
      "(*) Chexbert: labeling 6 texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 20712.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels found in cache, no need to invoke chexbert labeler\n",
      "f1: [1. 1. 0. 1. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labeler = CheXbertLabeler(verbose=True)\n",
    "f1_scores = compute_chexbert_f1_scores(labeler, gen_texts, gt_texts)\n",
    "print('f1:', f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd9dc0",
   "metadata": {},
   "source": [
    "### RadGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "927ff2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radgraph import RadGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ad9fb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_relations(data):\n",
    "    entities = data['entities']\n",
    "    n = len(entities)\n",
    "    e_strings = [None] * n\n",
    "    hash2count = dict()\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        e_strings[i] = f\"{e['tokens']}|{e['label']}\" # tokens|label\n",
    "        h = hash_string(e_strings[i])\n",
    "        hash2count[h] = hash2count.get(h, 0) + 1\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        for r in e['relations']:\n",
    "            j = int(r[1])-1\n",
    "            rel_s1 = f\"{e_strings[i]}|{r[0]}|{e_strings[j]}\" # e1|rel|e2\n",
    "            rel_s2 = f\"{e_strings[i]}|{e_strings[j]}\" # e1|e2\n",
    "            h1 = hash_string(rel_s1)\n",
    "            h2 = hash_string(rel_s2)\n",
    "            hash2count[h1] = hash2count.get(h1, 0) + 1\n",
    "            hash2count[h2] = hash2count.get(h2, 0) + 1\n",
    "    return hash2count\n",
    "\n",
    "def jaccard_between_dicts(d1, d2):\n",
    "    if len(d1) == 0 and len(d2) == 0:\n",
    "        return 1 # both dicts are empty -> perfect match\n",
    "    inters_size = 0\n",
    "    for k, c in d1.items():\n",
    "        inters_size += min(c, d2.get(k, 0))\n",
    "    union_size = sum(d1.values()) + sum(d2.values()) - inters_size\n",
    "    assert union_size > 0\n",
    "    return inters_size / union_size\n",
    "\n",
    "def f1_between_dicts(gt_dict, pred_dict):\n",
    "    if len(gt_dict) == 0 and len(pred_dict) == 0:\n",
    "        return 1 # both dicts are empty -> perfect match\n",
    "    inters_size = 0\n",
    "    for k, c in gt_dict.items():\n",
    "        inters_size += min(c, pred_dict.get(k, 0))\n",
    "    pred_sum = sum(pred_dict.values())\n",
    "    gt_sum = sum(gt_dict.values())\n",
    "    p = inters_size / pred_sum if pred_sum > 0 else 0\n",
    "    r = inters_size / gt_sum if gt_sum > 0 else 0\n",
    "    return 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "def eval_radgraph(gen_texts, gt_texts):\n",
    "    # Create instance of RadGraph\n",
    "    radgraph = RadGraph()\n",
    "    \n",
    "    # Collect unique sentences\n",
    "    sentences = set()\n",
    "    sentences.update(gen_texts)\n",
    "    sentences.update(gt_texts)\n",
    "    sentences = list(sentences)\n",
    "    s2idx = {s: i for i, s in enumerate(sentences)} # Map sentence to index\n",
    "\n",
    "    # Compute RadGraph annotations\n",
    "    annotations = radgraph(sentences)\n",
    "\n",
    "    # Extract entities and relations\n",
    "    ent_rel_dicts = [None] * len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        ent_rel_dicts[i] = extract_entities_and_relations(annotations[str(i)])\n",
    "    \n",
    "    # Compute F1 score and Jaccard score\n",
    "    f1_scores = []\n",
    "    jaccard_scores = []\n",
    "    for gt, gen in zip(gt_texts, gen_texts):\n",
    "        gt_idx = s2idx[gt]\n",
    "        gen_idx = s2idx[gen]\n",
    "        f1_scores.append(f1_between_dicts(ent_rel_dicts[gt_idx], ent_rel_dicts[gen_idx]))\n",
    "        jaccard_scores.append(jaccard_between_dicts(ent_rel_dicts[gt_idx], ent_rel_dicts[gen_idx]))\n",
    "    \n",
    "    # Return\n",
    "    return f1_scores, jaccard_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ecded7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores, jaccard_scores = eval_radgraph(gen_texts, gt_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "353e104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_scores: [1.0, 0.4444444444444444, 1, 0, 1.0, 0]\n",
      "jaccard_scores: [1.0, 0.2857142857142857, 1, 0.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print('f1_scores:', f1_scores)\n",
    "print('jaccard_scores:', jaccard_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
