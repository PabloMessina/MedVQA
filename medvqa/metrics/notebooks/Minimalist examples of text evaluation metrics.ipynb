{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e4ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient\",\n",
    "]\n",
    "\n",
    "gen_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The fast brown fox leaped over the lazy dog.\",\n",
    "    \"Being or not being, that's what's being questioned.\",\n",
    "    \"Everything that sparkles isn't necessarily gold.\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient has no cardiomegaly\",\n",
    "    \"The subject\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c035ed5",
   "metadata": {},
   "source": [
    "### Cider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcac9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.cider import cider_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad3239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 4.050422474728564\n",
      "scores: [10.          3.30879368  0.74854162  0.73773126 10.          3.40067913\n",
      "  0.15721163]\n"
     ]
    }
   ],
   "source": [
    "scorer = cider_scorer.CiderScorer(n=4)\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    scorer += (gen, [gt])\n",
    "mean_score, scores = scorer.compute_score()\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d8812",
   "metadata": {},
   "source": [
    "### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689fca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.rouge import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c66ddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 0.6765759215706109\n",
      "scores: [1.0, 0.7777777777777778, 0.21785714285714283, 0.3333333333333333, 1.0, 0.9070631970260222, 0.5]\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge.Rouge()\n",
    "scores = []\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    score = scorer.calc_score([gen], [gt])\n",
    "    scores.append(score)\n",
    "mean_score = sum(scores) / len(scores)\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2061a",
   "metadata": {},
   "source": [
    "### Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93975c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu import bleu_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d895698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: [0.6589154193334064, 0.5673484877926362, 0.506750741961736, 0.45857604536602137]\n",
      "scores:\n",
      "b1: 1.0000, b2: 1.0000, b3: 1.0000, b4: 1.0000, b: 1.0000\n",
      "b1: 0.7778, b2: 0.6236, b3: 0.4807, b4: 0.3689, b: 0.5628\n",
      "b1: 0.1947, b2: 0.1472, b3: 0.0000, b4: 0.0000, b: 0.0855\n",
      "b1: 0.3333, b2: 0.0000, b3: 0.0000, b4: 0.0000, b: 0.0833\n",
      "b1: 1.0000, b2: 1.0000, b3: 1.0000, b4: 1.0000, b: 1.0000\n",
      "b1: 0.8000, b2: 0.6325, b3: 0.5109, b4: 0.0001, b: 0.4859\n",
      "b1: 0.5000, b2: 0.0000, b3: 0.0000, b4: 0.0000, b: 0.1250\n"
     ]
    }
   ],
   "source": [
    "scorer = bleu_scorer.BleuScorer(n=4)\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    scorer += (gen, [gt])\n",
    "mean_score, scores = scorer.compute_score()\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:')\n",
    "for i in range(len(gen_texts)):\n",
    "    b1 = scores[0][i]\n",
    "    b2 = scores[1][i]\n",
    "    b3 = scores[2][i]\n",
    "    b4 = scores[3][i]\n",
    "    b = (b1+b2+b3+b4)/4\n",
    "    print(f'b1: {b1:.4f}, b2: {b2:.4f}, b3: {b3:.4f}, b4: {b4:.4f}, b: {b:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317d9c6",
   "metadata": {},
   "source": [
    "### Meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a8eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371b3d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 0.7408071940279891\n",
      "scores: [0.9995, 0.9995, 0.5852864583333333, 0.4445422535211268, 0.9921875, 0.9146341463414633, 0.25]\n"
     ]
    }
   ],
   "source": [
    "tokenized_gt_texts = [word_tokenize(x) for x in gt_texts]\n",
    "tokenized_gen_texts = [word_tokenize(x) for x in gen_texts]\n",
    "scores = []\n",
    "for gen, gt in zip(tokenized_gen_texts, tokenized_gt_texts):\n",
    "    score = meteor_score([gt], gen)\n",
    "    scores.append(score)\n",
    "mean_score = sum(scores) / len(scores)\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a1aac",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c46f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38dba60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertScore(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertScore, self).__init__()\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(model_type='distilbert-base-uncased',\n",
    "                                          num_layers=5,\n",
    "                                          batch_size=64,\n",
    "                                          nthreads=4,\n",
    "                                          all_layers=False,\n",
    "                                          idf=False,\n",
    "                                          device=None,\n",
    "                                          lang='en',\n",
    "                                          rescale_with_baseline=True,\n",
    "                                          baseline_path=None)\n",
    "\n",
    "    def forward(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=64,\n",
    "        )\n",
    "        return torch.mean(f).item(), f.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c7b784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a2f1eb3a9c4d9082b0c890cea7b12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ada9db76f243aea5795ba9a2a85e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c080719b795449ad9c16db6462abb02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe11eb753a2e4ce8ba9a00b266dcbdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_score = BertScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccee4a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802782416343689\n",
      "[0.9999998211860657, 0.9180418848991394, 0.5307456851005554, 0.7448107004165649, 1.0, 0.9339528679847717, 0.491926372051239]\n"
     ]
    }
   ],
   "source": [
    "x, y = bert_score(gen_texts, gt_texts)\n",
    "print(x) # average f1\n",
    "print(y) # instance f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e51a9c",
   "metadata": {},
   "source": [
    "### CheXbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1650738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f1chexbert import F1CheXbert\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aced057",
   "metadata": {},
   "outputs": [],
   "source": [
    "_B = 1777771\n",
    "_M = [999727999, 1070777777]\n",
    "_MASK32 = (1 << 32) - 1\n",
    "_MAXLEN = 1000000\n",
    "_POW = [[None] * _MAXLEN, [None] * _MAXLEN] # _POW[k][i] = _B^i mod _M[k]\n",
    "for k in range(2):\n",
    "    _POW[k][0] = 1\n",
    "    for i in range(1, _MAXLEN):\n",
    "        _POW[k][i] = (_POW[k][i - 1] * _B) % _M[k]\n",
    "\n",
    "def hash_string(s):\n",
    "    hh = 0\n",
    "    for k in range(2):\n",
    "        h = 0\n",
    "        for c in s:\n",
    "            h = (h * _B + ord(c)) % _M[k]\n",
    "        hh = (hh << 32) | h\n",
    "    return (len(s), hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07ef075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEXBERT_LABELS = [\n",
    "    \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\", \"Lung Lesion\", \"Edema\",\n",
    "    \"Consolidation\", \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "    \"Fracture\", \"Support Devices\", \"No Finding\",\n",
    "]\n",
    "\n",
    "def merge_labels(labels_list):        \n",
    "    merged = np.zeros((len(CHEXBERT_LABELS),), np.int8)\n",
    "    merged[-1] = 1 # default to no findings\n",
    "    for labels in labels_list:\n",
    "        if labels[-1] == 0: # there is a finding\n",
    "            merged[-1] = 0\n",
    "        for i in range(0, len(labels)-1): # iterate over all labels except the last one\n",
    "            if labels[i] == 1:\n",
    "                merged[i] = 1\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a0a754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXbertLabeler(F1CheXbert):\n",
    "\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__(device=device)\n",
    "        self.cache = dict()\n",
    "\n",
    "    def get_labels(self, texts):\n",
    "\n",
    "        output_labels = [None] * len(texts)\n",
    "        dirty_count = 0\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), total=len(texts), mininterval=2):\n",
    "            text_hash = hash_string(text)\n",
    "            if text_hash in self.cache:\n",
    "                output_labels[i] = self.cache[text_hash]\n",
    "                continue\n",
    "            sentences = sent_tokenize(text)\n",
    "            sentence_labels = []\n",
    "            for sentence in sentences:\n",
    "                hash = hash_string(sentence)\n",
    "                labels = self.cache.get(hash, None)\n",
    "                if labels is None:\n",
    "                    labels = self.get_label(sentence)\n",
    "                    self.cache[hash] = labels\n",
    "                    dirty_count += 1\n",
    "                sentence_labels.append(labels)\n",
    "            output_labels[i] = merge_labels(sentence_labels)\n",
    "            self.cache[text_hash] = output_labels[i]\n",
    "        \n",
    "        return np.array(output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1073bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(gt_binary_labels, gen_binary_labels):\n",
    "    tp = np.sum((gen_binary_labels == 1) & (gt_binary_labels == 1))\n",
    "    fp = np.sum((gen_binary_labels == 1) & (gt_binary_labels == 0))\n",
    "    fn = np.sum((gen_binary_labels == 0) & (gt_binary_labels == 1))\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def compute_chexbert_f1_scores(labeler, gen_texts, gt_texts):\n",
    "    gen_labels = labeler.get_labels(gen_texts)\n",
    "    gt_labels = labeler.get_labels(gt_texts)\n",
    "    print(gen_labels.shape)\n",
    "    print(gt_labels.shape)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    f1_scores = np.zeros(len(gen_texts))\n",
    "    for i in range(len(gen_texts)):\n",
    "        f1_scores[i] = f1(gt_labels[i], gen_labels[i])\n",
    "    metrics['instance_f1s'] = f1_scores\n",
    "    \n",
    "    metrics['sample_avg_f1'] = f1_scores.mean()\n",
    "    \n",
    "    metrics['micro_avg_f1'] = f1(gt_labels.flatten(), gen_labels.flatten())\n",
    "    \n",
    "    metrics['class_f1s'] = {}\n",
    "    class_f1s = np.empty((len(CHEXBERT_LABELS),))\n",
    "    for i in range(len(CHEXBERT_LABELS)):\n",
    "        class_f1s[i] = f1(gt_labels.T[i], gen_labels.T[i])   \n",
    "        metrics['class_f1s'][CHEXBERT_LABELS[i]] = class_f1s[i]\n",
    "        \n",
    "    metrics['macro_avg_f1'] = class_f1s.mean()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f79e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 85.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 193.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 14)\n",
      "(7, 14)\n",
      "{'class_f1s': {'Atelectasis': 0.0,\n",
      "               'Cardiomegaly': 0.6666666666666666,\n",
      "               'Consolidation': 0.0,\n",
      "               'Edema': 0.0,\n",
      "               'Enlarged Cardiomediastinum': 0.0,\n",
      "               'Fracture': 0.0,\n",
      "               'Lung Lesion': 0.0,\n",
      "               'Lung Opacity': 0.0,\n",
      "               'No Finding': 1.0,\n",
      "               'Pleural Effusion': 0.0,\n",
      "               'Pleural Other': 0.0,\n",
      "               'Pneumonia': 0.0,\n",
      "               'Pneumothorax': 0.0,\n",
      "               'Support Devices': 0.0},\n",
      " 'instance_f1s': array([1., 1., 0., 1., 1., 0., 1.]),\n",
      " 'macro_avg_f1': 0.11904761904761904,\n",
      " 'micro_avg_f1': 0.9090909090909091,\n",
      " 'sample_avg_f1': 0.7142857142857143}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labeler = CheXbertLabeler()\n",
    "f1_scores = compute_chexbert_f1_scores(labeler, gen_texts, gt_texts)\n",
    "pprint(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f700af",
   "metadata": {},
   "source": [
    "### RadGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca10a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radgraph import RadGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f226dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_relations(data):\n",
    "    entities = data['entities']\n",
    "    n = len(entities)\n",
    "    e_strings = [None] * n\n",
    "    hash2count = dict()\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        e_strings[i] = f\"{e['tokens']}|{e['label']}\" # tokens|label\n",
    "        h = hash_string(e_strings[i])\n",
    "        hash2count[h] = hash2count.get(h, 0) + 1\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        for r in e['relations']:\n",
    "            j = int(r[1])-1\n",
    "            rel_s1 = f\"{e_strings[i]}|{r[0]}|{e_strings[j]}\" # e1|rel|e2\n",
    "            rel_s2 = f\"{e_strings[i]}|{e_strings[j]}\" # e1|e2\n",
    "            h1 = hash_string(rel_s1)\n",
    "            h2 = hash_string(rel_s2)\n",
    "            hash2count[h1] = hash2count.get(h1, 0) + 1\n",
    "            hash2count[h2] = hash2count.get(h2, 0) + 1\n",
    "    return hash2count\n",
    "\n",
    "def jaccard_between_dicts(d1, d2):\n",
    "    if len(d1) == 0 and len(d2) == 0:\n",
    "        return 1 # both dicts are empty -> perfect match\n",
    "    inters_size = 0\n",
    "    for k, c in d1.items():\n",
    "        inters_size += min(c, d2.get(k, 0))\n",
    "    union_size = sum(d1.values()) + sum(d2.values()) - inters_size\n",
    "    assert union_size > 0\n",
    "    return inters_size / union_size\n",
    "\n",
    "def f1_between_dicts(gt_dict, pred_dict):\n",
    "    if len(gt_dict) == 0 and len(pred_dict) == 0:\n",
    "        return 1 # both dicts are empty -> perfect match\n",
    "    inters_size = 0\n",
    "    for k, c in gt_dict.items():\n",
    "        inters_size += min(c, pred_dict.get(k, 0))\n",
    "    pred_sum = sum(pred_dict.values())\n",
    "    gt_sum = sum(gt_dict.values())\n",
    "    p = inters_size / pred_sum if pred_sum > 0 else 0\n",
    "    r = inters_size / gt_sum if gt_sum > 0 else 0\n",
    "    return 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "def eval_radgraph(gen_texts, gt_texts):\n",
    "    # Create instance of RadGraph\n",
    "    radgraph = RadGraph()\n",
    "    \n",
    "    # Collect unique sentences\n",
    "    sentences = set()\n",
    "    sentences.update(gen_texts)\n",
    "    sentences.update(gt_texts)\n",
    "    sentences = list(sentences)\n",
    "    s2idx = {s: i for i, s in enumerate(sentences)} # Map sentence to index\n",
    "\n",
    "    # Compute RadGraph annotations\n",
    "    annotations = radgraph(sentences)\n",
    "\n",
    "    # Extract entities and relations\n",
    "    ent_rel_dicts = [None] * len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        ent_rel_dicts[i] = extract_entities_and_relations(annotations[str(i)])\n",
    "    \n",
    "    # Compute F1 score and Jaccard score\n",
    "    f1_scores = []\n",
    "    jaccard_scores = []\n",
    "    for gt, gen in zip(gt_texts, gen_texts):\n",
    "        gt_idx = s2idx[gt]\n",
    "        gen_idx = s2idx[gen]\n",
    "        f1_scores.append(f1_between_dicts(ent_rel_dicts[gt_idx], ent_rel_dicts[gen_idx]))\n",
    "        jaccard_scores.append(jaccard_between_dicts(ent_rel_dicts[gt_idx], ent_rel_dicts[gen_idx]))\n",
    "    \n",
    "    # Return\n",
    "    return f1_scores, jaccard_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "611470c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "f1_scores, jaccard_scores = eval_radgraph(gen_texts, gt_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cd1ac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_scores: [1.0, 0.4444444444444444, 1, 0, 1.0, 0, 1]\n",
      "jaccard_scores: [1.0, 0.2857142857142857, 1, 0.0, 1.0, 0.0, 1]\n"
     ]
    }
   ],
   "source": [
    "print('f1_scores:', f1_scores)\n",
    "print('jaccard_scores:', jaccard_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
