{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4468,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medvqa\n",
    "from medvqa.metrics.medical.med_completeness import (\n",
    "    MedicalCompleteness,\n",
    "    WeightedMedicalCompleteness,\n",
    "    MEDICAL_TERMS_PATH, \n",
    "    MEDICAL_SYNONYMS_PATH,\n",
    ")\n",
    "from medvqa.datasets.tokenizer import Tokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'medvqa.datasets.tokenizer' from '/home/pamessina/medvqa/medvqa/datasets/tokenizer.py'>"
      ]
     },
     "execution_count": 4467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(medvqa.metrics.medical.med_completeness)\n",
    "reload(medvqa.datasets.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "ground truth\n",
      "-----\n",
      "1-grams (16): no, free, air, below, hemidiaphragm, low, lung, volume, no, acute, process, no, evidence, free, peritoneal, air\n",
      "ignored (10): <s>, there, is, the, ., but, and, of, ., </s>\n",
      "------------------\n",
      "generated\n",
      "-----\n",
      "1-grams (6): no, free, air, below, right, hemidiaphragm\n",
      "ignored (6): <s>, the, is, seen, ., </s>\n",
      "======\n",
      "intersec_size = [5, 3, 2, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2754419191919192"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gt_s = 'there is no free air under the hemidiaphragms . low lung volumes but no acute process and no evidence of free peritoneal air .'\n",
    "_gt_ids = tokenizer.string2ids(_gt_s)\n",
    "_gen_s = 'no free air below the right hemidiaphragm is seen .'\n",
    "_gen_ids = tokenizer.string2ids(_gen_s)\n",
    "medcomp.score__debug(_gt_ids, _gen_ids, tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.datasets.mimiccxr import MIMICCXR_QA_ADAPTED_REPORTS_JSON_PATH\n",
    "from medvqa.datasets.iuxray import IUXRAY_QA_ADAPTED_REPORTS_JSON_PATH\n",
    "from medvqa.utils.files import load_json_file\n",
    "from medvqa.datasets.preprocessing import get_sentences\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "iuxray_qa_reports = load_json_file(IUXRAY_QA_ADAPTED_REPORTS_JSON_PATH)\n",
    "mimiccxr_qa_reports = load_json_file(MIMICCXR_QA_ADAPTED_REPORTS_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [None] * (10 * (len(iuxray_qa_reports['reports']) + len(mimiccxr_qa_reports['reports'])))\n",
    "for i, s in enumerate(get_sentences([iuxray_qa_reports, mimiccxr_qa_reports])):\n",
    "    sentences[i] = s\n",
    "sentences = sentences[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [wordpunct_tokenize(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idxs = dict()\n",
    "for i, s in enumerate(tokenized_sentences):\n",
    "    for w in s:\n",
    "        try:\n",
    "            word2idxs[w].add(i)\n",
    "        except KeyError:\n",
    "            word2idxs[w] = {i}\n",
    "for w in word2idxs.keys():\n",
    "    word2idxs[w] = list(word2idxs[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11214"
      ]
     },
     "execution_count": 4264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_medical_terms_file():\n",
    "    with open('./medical_terms__aux.txt') as in_f:\n",
    "        medical_terms = [x.strip() for x in in_f.readlines()]\n",
    "        medical_terms = sorted(list(set(medical_terms)))\n",
    "        for x in medical_terms:\n",
    "            assert x in word2idxs, x\n",
    "    with open('./medical_terms.txt', 'w') as out_f:\n",
    "        for x in medical_terms:\n",
    "            out_f.write(x + '\\n')\n",
    "\n",
    "def clean_medical_synonyms_file():\n",
    "    with open('./medical_synonyms__aux.txt') as in_f:\n",
    "        medical_synonyms = [x.strip().split() for x in in_f.readlines()]\n",
    "        terms = set()\n",
    "        for x in medical_synonyms:            \n",
    "            for y in x:\n",
    "                assert y in word2idxs, y\n",
    "                size_bef = len(terms)\n",
    "                terms.add(y)\n",
    "                try:\n",
    "                    assert len(terms) == size_bef + 1\n",
    "                except AssertionError:\n",
    "                    print(y)\n",
    "                    raise\n",
    "        for i in range(len(medical_synonyms)):\n",
    "            medical_synonyms[i].sort()\n",
    "        medical_synonyms.sort(key=lambda x:x[0])\n",
    "    with open('./medical_synonyms.txt', 'w') as out_f:\n",
    "        for x in medical_synonyms:\n",
    "            out_f.write(f'{\" \".join(x)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4481,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_medical_terms_file()\n",
    "clean_medical_synonyms_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1150867it [00:06, 167813.31it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer('vocab/test.pkl', [mimiccxr_qa_reports, iuxray_qa_reports], min_freq=5, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5013"
      ]
     },
     "execution_count": 4289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '\"',\n",
       " \"'\",\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " '>',\n",
       " 'CHF',\n",
       " 'COPD',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'abandoned',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'aberrant',\n",
       " 'ability',\n",
       " 'ablation']"
      ]
     },
     "execution_count": 4261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id2token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4482,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "medcomp = MedicalCompleteness(tokenizer, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 'possible subtle fracture along anterolateral right third rib.')"
      ]
     },
     "execution_count": 4487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word = 'third'\n",
    "len(word2idxs[_word]), sentences[random.choice(word2idxs[_word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentence(s):\n",
    "    return sum(1 for token in s if token in medcomp.medical_terms)\n",
    "    \n",
    "def sample_sentences(n_samples=1000, top_k=5):\n",
    "    idxs = random.sample(range(len(sentences)), n_samples)\n",
    "    idxs.sort(key=lambda i : rank_sentence(tokenized_sentences[i]), reverse=True)\n",
    "    for i in range(top_k):\n",
    "        print('-' * 50)\n",
    "        print(sentences[idxs[i]])\n",
    "        print(len(tokenized_sentences[idxs[i]]), rank_sentence(tokenized_sentences[idxs[i]]))\n",
    "        print([x for x in tokenized_sentences[idxs[i]] if x in medcomp.medical_terms])\n",
    "        print([x for x in tokenized_sentences[idxs[i]] if x not in medcomp.medical_terms])\n",
    "        print(tokenizer.ids2string(tokenizer.string2ids(sentences[idxs[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "exam is otherwise remarkable for a large mass-like area of opacification above the right hilum with adjacent surgical sutures, as well as right upper lobe volume loss and asymmetrical right apical thickening.\n",
      "36 20\n",
      "['remarkable', 'large', 'mass', 'area', 'opacification', 'above', 'right', 'hilum', 'adjacent', 'surgical', 'sutures', 'right', 'upper', 'lobe', 'volume', 'loss', 'asymmetrical', 'right', 'apical', 'thickening']\n",
      "['exam', 'is', 'otherwise', 'for', 'a', '-', 'like', 'of', 'the', 'with', ',', 'as', 'well', 'as', 'and', '.']\n",
      "<s> exam is otherwise remarkable for a large mass like area of opacification above the right hilum with adjacent surgical sutures , as well as right upper lobe volume loss and asymmetrical right apical thickening . </s>\n",
      "--------------------------------------------------\n",
      "the final radiograph in the series shows repositioning of the right pic line from a right internal jugular vein to the estimated location of the right superior cavoatrial junction alongside the indwelling right internal jugular catheter.\n",
      "37 17\n",
      "['repositioning', 'right', 'pic', 'line', 'right', 'internal', 'jugular', 'vein', 'right', 'superior', 'cavoatrial', 'junction', 'indwelling', 'right', 'internal', 'jugular', 'catheter']\n",
      "['the', 'final', 'radiograph', 'in', 'the', 'series', 'shows', 'of', 'the', 'from', 'a', 'to', 'the', 'estimated', 'location', 'of', 'the', 'alongside', 'the', '.']\n",
      "<s> the final radiograph in the series shows repositioning of the right pic line from a right internal jugular vein to the estimated location of the right superior cavoatrial junction alongside the indwelling right internal jugular catheter . </s>\n",
      "--------------------------------------------------\n",
      "standard thoracostomy tube has replaced the left pigtail pleural drainage catheter, curving over the apex of the lung, having decrease the pneumothorax from moderate to slightly smaller.\n",
      "30 16\n",
      "['standard', 'thoracostomy', 'tube', 'replaced', 'left', 'pigtail', 'pleural', 'drainage', 'catheter', 'curving', 'apex', 'lung', 'decrease', 'pneumothorax', 'moderate', 'smaller']\n",
      "['has', 'the', ',', 'over', 'the', 'of', 'the', ',', 'having', 'the', 'from', 'to', 'slightly', '.']\n",
      "<s> standard thoracostomy tube has replaced the left pigtail pleural drainage catheter , curving over the apex of the lung , having decrease the pneumothorax from moderate to slightly smaller . </s>\n",
      "--------------------------------------------------\n",
      "increased heterogeneous, right greater than left, lung opacities in association with thickening of the right horizontal fissure and pulmonary vascular congestion are consistent with pulmonary edema.\n",
      "29 16\n",
      "['increased', 'heterogeneous', 'right', 'greater', 'left', 'lung', 'opacities', 'thickening', 'right', 'horizontal', 'fissure', 'pulmonary', 'vascular', 'congestion', 'pulmonary', 'edema']\n",
      "[',', 'than', ',', 'in', 'association', 'with', 'of', 'the', 'and', 'are', 'consistent', 'with', '.']\n",
      "<s> increased heterogeneous , right greater than left , lung opacities in association with thickening of the right horizontal fissure and pulmonary vascular congestion are consistent with pulmonary edema . </s>\n",
      "--------------------------------------------------\n",
      "triple-lead left-sided aicd is again seen, unchanged in position, with leads extending in the expected positions of the right atrium, right ventricle, and coronary sinus.\n",
      "34 15\n",
      "['triple', 'lead', 'left', 'sided', 'aicd', 'position', 'leads', 'extending', 'positions', 'right', 'atrium', 'right', 'ventricle', 'coronary', 'sinus']\n",
      "['-', '-', 'is', 'again', 'seen', ',', 'unchanged', 'in', ',', 'with', 'in', 'the', 'expected', 'of', 'the', ',', ',', 'and', '.']\n",
      "<s> triple lead left sided aicd is again seen , unchanged in position , with leads extending in the expected positions of the right atrium , right ventricle , and coronary sinus . </s>\n"
     ]
    }
   ],
   "source": [
    "sample_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(i = None, j = None):\n",
    "    word = random.choice(tokenizer.id2token)\n",
    "    _i, _j = random.choices(word2idxs[word], k=2)\n",
    "    if i is None: i = _i\n",
    "    if j is None: j = _j\n",
    "    si = sentences[i]\n",
    "    sj = sentences[j]\n",
    "    print('***', i, si)\n",
    "    print('----')\n",
    "    print('***', j, sj)\n",
    "    score_debug = medcomp.score__debug(\n",
    "        tokenizer.string2ids(si),\n",
    "        tokenizer.string2ids(sj),\n",
    "        tokenizer, verbose=False)\n",
    "    score = medcomp.score(\n",
    "        tokenizer.string2ids(si),\n",
    "        tokenizer.string2ids(sj))\n",
    "    assert score_debug == score\n",
    "    print('score_debug =', score_debug, 'score =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 889075 improving subcutaneous and intramuscular air.\n",
      "----\n",
      "*** 889070 there is also improvement of the subcutaneous and intramuscular gas seen on the right side of the body.\n",
      "------------------\n",
      "ground truth\n",
      "------------------\n",
      "1-grams (3): subcutaneous, intramuscular, air\n",
      "ignored (5): <s>, improving, and, ., </s>\n",
      "------------------\n",
      "generated\n",
      "------------------\n",
      "1-grams (7): amelioration, subcutaneous, intramuscular, gas, right, side, bodies\n",
      "ignored (14): <s>, there, is, also, of, the, and, seen, on, the, of, the, ., </s>\n",
      "======\n",
      "inter_size = [2, 1, 0]\n",
      "score_debug = 0.21666666666666667 score = 0.21666666666666667\n"
     ]
    }
   ],
   "source": [
    "evaluate_pair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./medical_terms__aux.txt', 'a') as f:\n",
    "#     for w in tokenizer.id2token:\n",
    "#         f.write(f'{w}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights():\n",
    "    \n",
    "    # load medical terms\n",
    "    with open(MEDICAL_TERMS_PATH) as f:\n",
    "        medical_terms = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # load medical synonyms\n",
    "    with open(MEDICAL_SYNONYMS_PATH) as f:\n",
    "        medical_synonyms = [line.strip().split() for line in f.readlines()]\n",
    "    term2synonym = { x:x for x in medical_terms }\n",
    "    for row in medical_synonyms:\n",
    "        term0 = row[0]\n",
    "        for i in range(1, len(row)):\n",
    "            term2synonym[row[i]] = term0\n",
    "    \n",
    "    # compute frequencies\n",
    "    freqs = [dict() for _ in range(4)]\n",
    "    counts = [0] * 4\n",
    "    for sentence in tokenized_sentences:\n",
    "        medical_sentence = []\n",
    "        for token in sentence:\n",
    "            synonym = term2synonym.get(token, None)\n",
    "            if synonym is not None:\n",
    "                medical_sentence.append(synonym)\n",
    "        for k in range(min(4, len(medical_sentence))):\n",
    "            f_k = freqs[k]\n",
    "            for i in range(len(medical_sentence) - k):\n",
    "                if k == 0:\n",
    "                    key = medical_sentence[i]\n",
    "                else:\n",
    "                    key = tuple(medical_sentence[i:i+k+1])\n",
    "                f_k[key] = f_k.get(key, 0) + 1\n",
    "                counts[k] += 1\n",
    "    \n",
    "    # compute weights\n",
    "    weights = [\n",
    "        { key : math.log(counts[k] / freqs[k][key]) for key in freqs[k].keys() }\n",
    "        for k in range(4)\n",
    "    ]\n",
    "    return freqs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4398,
   "metadata": {},
   "outputs": [],
   "source": [
    "_freqs, _weights = compute_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.files import save_to_pickle\n",
    "from medvqa.utils.common import CACHE_DIR\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4400,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(_weights, os.path.join(CACHE_DIR, 'medical_terms_weights.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4422,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weightmedcomp = WeightedMedicalCompleteness(tokenizer, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair__weighted(i = None, j = None):\n",
    "    word = random.choice(tokenizer.id2token)\n",
    "    _i, _j = random.choices(word2idxs[word], k=2)\n",
    "    if i is None: i = _i\n",
    "    if j is None: j = _j\n",
    "    si = sentences[i]\n",
    "    sj = sentences[j]\n",
    "    print('***', i, si)\n",
    "    print('----')\n",
    "    print('***', j, sj)\n",
    "    score = weightmedcomp.score(\n",
    "        tokenizer.string2ids(si),\n",
    "        tokenizer.string2ids(sj))\n",
    "    print('score =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 120631 post-surgical changes in the right lower lobe are again seen with chain sutures abutting the oblique fissure.\n",
      "----\n",
      "*** 245980 chain sutures within the left upper and lower lung fields are compatible with prior wedge resections.\n",
      "[71.08388006555843, 94.5753593236537, 100.64032808444145, 97.54173147961983]\n",
      "[59.7469520233614, 72.22699991291432, 79.0491533630988, 74.94881956078274]\n",
      "[22.037562117814087, 9.19059404597967, 0, 0]\n",
      "score = 0.11177094542986725\n"
     ]
    }
   ],
   "source": [
    "# evaluate_pair__weighted(1023895, 376450)\n",
    "evaluate_pair__weighted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 120631 post-surgical changes in the right lower lobe are again seen with chain sutures abutting the oblique fissure.\n",
      "----\n",
      "*** 245980 chain sutures within the left upper and lower lung fields are compatible with prior wedge resections.\n",
      "------------------\n",
      "ground truth\n",
      "-----\n",
      "1-grams (10): post, surgeries, right, low, lobe, chain, suture, abut, oblique, fissure\n",
      "ignored (11): <s>, changes, in, the, are, again, seen, with, the, ., </s>\n",
      "------------------\n",
      "generated\n",
      "-----\n",
      "1-grams (9): chain, suture, left, theupper, low, lung, field, wedge, resected\n",
      "ignored (10): <s>, within, the, and, are, compatible, with, prior, ., </s>\n",
      "======\n",
      "intersec_size = [3, 1, 0, 0]\n",
      "score = 0.10835913312693499\n"
     ]
    }
   ],
   "source": [
    "evaluate_pair(120631 , 245980 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
