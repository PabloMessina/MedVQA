{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 20\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: None\n",
      "   mimiccxr_qa_adapted_reports_filename: qa_adapted_reports__20220904_095810.json\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: densenet-121\n",
      "   image_local_feat_size: 1024\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: None\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: v2\n",
      "   num_regions: 64\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,4,1e-4,16,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 140\n",
      "   iters_to_accumulate: 2\n",
      "   num_workers: 0\n",
      "   device: GPU\n",
      "   img_aug_mode: None\n",
      "   image_size: [256, 256]\n",
      "   mimiccxr_weight: 1.0\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: chest_imagenome\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_amp: True\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230208_231030_mim+mim(chex)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,1.0_medtok_chx_amp\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: True\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: False\n",
      "   vinbig_training_data_mode: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: bce\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: True\n",
      "   clamp_bboxes_chest_imagenome: True\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/chest_imagenome_train_average_bbox_coords(qa_adapted_reports__20220904_095810.json,clamped.pkl...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of MultiPurposeVisualModel ...\u001b[0m\n",
      "create_densenet121_feature_extractor()\n",
      "   drop_rate: 0.0\n",
      "  self.global_feat_size = 2048\n",
      "BoundingBoxRegressor_v2:\n",
      "  local_feat_dim: 1024\n",
      "  global_feat_dim: 2048\n",
      "  hidden_dim: 128\n",
      "  num_classes: 36\n",
      "  num_regions: 64\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,4,1e-4,16,1e-6\n",
      "1e-06 4 0.0001 16 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "train_image_transform_kwargs: {'image_size': [256, 256], 'augmentation_mode': None, 'use_clip_transform': False, 'clip_version': None, 'use_huggingface_vitmodel_transform': False, 'huggingface_vitmodel_name': None}\n",
      "Using default transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [256, 256], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "val_image_transform_kwargs: {'image_size': [256, 256], 'augmentation_mode': None, 'use_clip_transform': False, 'clip_version': None, 'use_huggingface_vitmodel_transform': False, 'huggingface_vitmodel_name': None}\n",
      "Using default transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [256, 256], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR visual module trainer ...\u001b[0m\n",
      "Loading cached detailed metadata from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/qa_adapted_reports__20220904_095810.json__detailed_metadata.pkl\n",
      "Loaded 240530 non-gold DICOM IDs from Chest Imagenome\n",
      "227835it [00:00, 488528.81it/s]\n",
      "Loading Chest Imagenome bounding boxes...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_17_chf1+chf1+chf1+chf1+chf1+cD+ema+wmmp=0.4304.pt', 'checkpoint_34_chf1+chf1+chf1+chf1+chf1+cD+ema+wmmp=0.5019.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230208_231030_mim+mim(chex)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,1.0_medtok_chx_amp/checkpoint_34_chf1+chf1+chf1+chf1+chf1+cD+ema+wmmp=0.5019.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m15) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.01151, chestimgbbmf1 0.82055, chestimgbb_loss 0.01867, chestimgbbiou 0.90743, chestimgbbmae 0.03177, 420.85 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.79640, chestimgbbiou 0.90104, chestimgbbmae 0.03452, 17.79 secs\n",
      "Adjusting learning rate of group 0 to 3.1623e-06.\n",
      "\u001b[1m---- Epoch 2/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.01125, chestimgbbmf1 0.82205, chestimgbb_loss 0.01779, chestimgbbiou 0.90779, chestimgbbmae 0.03163, 422.72 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.79975, chestimgbbiou 0.90165, chestimgbbmae 0.03431, 17.51 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "\u001b[1m---- Epoch 3/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.01504, chestimgbbmf1 0.82323, chestimgbb_loss 0.01836, chestimgbbiou 0.90789, chestimgbbmae 0.03161, 421.57 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.80372, chestimgbbiou 0.90196, chestimgbbmae 0.03419, 17.74 secs\n",
      "Adjusting learning rate of group 0 to 3.1623e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 4/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.01865, chestimgbbmf1 0.82239, chestimgbb_loss 0.01900, chestimgbbiou 0.90766, chestimgbbmae 0.03172, 419.88 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.80932, chestimgbbiou 0.90292, chestimgbbmae 0.03383, 17.24 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[1m---- Epoch 5/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.01817, chestimgbbmf1 0.79337, chestimgbb_loss 0.01972, chestimgbbiou 0.90323, chestimgbbmae 0.03329, 419.69 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.81612, chestimgbbiou 0.90409, chestimgbbmae 0.03336, 17.25 secs\n",
      "Adjusting learning rate of group 0 to 7.4989e-05.\n",
      "\u001b[1m---- Epoch 6/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000075) ...\n",
      "loss 0.01065, chestimgbbmf1 0.85239, chestimgbb_loss 0.01838, chestimgbbiou 0.91300, chestimgbbmae 0.02986, 416.59 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.84492, chestimgbbiou 0.90940, chestimgbbmae 0.03157, 17.20 secs\n",
      "Adjusting learning rate of group 0 to 5.6234e-05.\n",
      "\u001b[1m---- Epoch 7/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.01439, chestimgbbmf1 0.86918, chestimgbb_loss 0.01670, chestimgbbiou 0.91667, chestimgbbmae 0.02852, 416.99 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85537, chestimgbbiou 0.91149, chestimgbbmae 0.03082, 17.26 secs\n",
      "Adjusting learning rate of group 0 to 4.2170e-05.\n",
      "\u001b[1m---- Epoch 8/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000042) ...\n",
      "loss 0.02045, chestimgbbmf1 0.87749, chestimgbb_loss 0.01629, chestimgbbiou 0.91844, chestimgbbmae 0.02792, 417.81 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86481, chestimgbbiou 0.91361, chestimgbbmae 0.03007, 17.24 secs\n",
      "Adjusting learning rate of group 0 to 3.1623e-05.\n",
      "\u001b[1m---- Epoch 9/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.01757, chestimgbbmf1 0.88341, chestimgbb_loss 0.01613, chestimgbbiou 0.92001, chestimgbbmae 0.02739, 418.24 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86994, chestimgbbiou 0.91537, chestimgbbmae 0.02945, 17.35 secs\n",
      "Adjusting learning rate of group 0 to 2.3714e-05.\n",
      "\u001b[1m---- Epoch 10/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n",
      "loss 0.01703, chestimgbbmf1 0.88761, chestimgbb_loss 0.01659, chestimgbbiou 0.92081, chestimgbbmae 0.02714, 423.90 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87564, chestimgbbiou 0.91680, chestimgbbmae 0.02895, 17.57 secs\n",
      "Adjusting learning rate of group 0 to 1.7783e-05.\n",
      "\u001b[1m---- Epoch 11/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.00777, chestimgbbmf1 0.89387, chestimgbb_loss 0.01497, chestimgbbiou 0.92281, chestimgbbmae 0.02637, 422.25 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87938, chestimgbbiou 0.91733, chestimgbbmae 0.02877, 17.78 secs\n",
      "Adjusting learning rate of group 0 to 1.3335e-05.\n",
      "\u001b[1m---- Epoch 12/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.01509, chestimgbbmf1 0.89908, chestimgbb_loss 0.01294, chestimgbbiou 0.92432, chestimgbbmae 0.02577, 422.45 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88085, chestimgbbiou 0.91763, chestimgbbmae 0.02866, 17.54 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "\u001b[1m---- Epoch 13/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.00871, chestimgbbmf1 0.89879, chestimgbb_loss 0.01211, chestimgbbiou 0.92421, chestimgbbmae 0.02578, 419.30 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88323, chestimgbbiou 0.91844, chestimgbbmae 0.02839, 17.47 secs\n",
      "Adjusting learning rate of group 0 to 7.4989e-06.\n",
      "\u001b[1m---- Epoch 14/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "^C iteration 4000\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"../train_visual_module.py\", line 1295, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"../train_visual_module.py\", line 1192, in train_from_scratch\n",
      "    debug=debug)\n",
      "  File \"../train_visual_module.py\", line 807, in train_model\n",
      "    epoch_length = batches_per_epoch)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 701, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 774, in _internal_run\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 469, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 744, in _internal_run\n",
      "    time_taken = self._run_once_on_dataset()\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 834, in _run_once_on_dataset\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/vqa.py\", line 698, in step_fn\n",
      "    output = step_fn__mimiccxr_iuxray(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/vqa.py\", line 175, in step_fn__mimiccxr_iuxray\n",
      "    model_output = model(**model_kwargs)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/vision/visual_modules.py\", line 362, in forward\n",
      "    padchest_forward=False,\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torchvision/models/densenet.py\", line 127, in forward\n",
      "    new_features = layer(features)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torchvision/models/densenet.py\", line 94, in forward\n",
      "    new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "        --pretrained-checkpoint-folder-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230208_231030_mim+mim(chex)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,1.0_medtok_chx_amp\" \\\n",
    "        --epochs 20 \\\n",
    "        --batches-per-epoch 300 \\\n",
    "        --batch-size 140 \\\n",
    "        --num-workers 0 \\\n",
    "        --iters-to-accumulate 2 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,4,1e-4,16,1e-6\" \\\n",
    "        --use-mimiccxr \\\n",
    "        --mimiccxr-weight 1.0 \\\n",
    "        --mimiccxr-qa-adapted-reports-filename \"qa_adapted_reports__20220904_095810.json\" \\\n",
    "        --mimiccxr-view-mode \"chest_imagenome\" \\\n",
    "        --predict-bboxes-chest-imagenome \\\n",
    "        --clamp-bboxes-chest-imagenome \\\n",
    "        --chest-imagenome-bbox-regressor-version \"v2\" \\\n",
    "        --raw-image-encoding \"densenet-121\" \\\n",
    "        --image-size 256 256 \\\n",
    "        --image-local-feat-size 1024 \\\n",
    "        --num-regions 64 \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 20\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: models/visual_module/20230215_124027_mim_dense121\n",
      "   mimiccxr_qa_adapted_reports_filename: None\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: densenet-121\n",
      "   image_local_feat_size: 1024\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: None\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: v1\n",
      "   num_regions: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,3,1e-4,17,8e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 140\n",
      "   iters_to_accumulate: 2\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   img_aug_mode: None\n",
      "   image_size: (256, 256)\n",
      "   mimiccxr_weight: 1\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: any_single\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_amp: False\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   save: True\n",
      "   override_lr: True\n",
      "   train_mimiccxr: False\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: False\n",
      "   vinbig_training_data_mode: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: bce\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: False\n",
      "   clamp_bboxes_chest_imagenome: False\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Resuming training ------\u001b[0m\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of MultiPurposeVisualModel ...\u001b[0m\n",
      "create_densenet121_feature_extractor()\n",
      "   drop_rate: 0.0\n",
      "  self.global_feat_size = 2048\n",
      "BoundingBoxRegressor_v2:\n",
      "  local_feat_dim: 1024\n",
      "  global_feat_dim: 2048\n",
      "  hidden_dim: 128\n",
      "  num_classes: 36\n",
      "  num_regions: 64\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,3,1e-4,17,8e-6\n",
      "1e-06 3 0.0001 17 8e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "train_image_transform_kwargs: {'image_size': [256, 256], 'augmentation_mode': None, 'use_clip_transform': False, 'clip_version': None, 'use_huggingface_vitmodel_transform': False, 'huggingface_vitmodel_name': None}\n",
      "Using default transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [256, 256], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "val_image_transform_kwargs: {'image_size': [256, 256], 'augmentation_mode': None, 'use_clip_transform': False, 'clip_version': None, 'use_huggingface_vitmodel_transform': False, 'huggingface_vitmodel_name': None}\n",
      "Using default transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [256, 256], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR visual module trainer ...\u001b[0m\n",
      "Loading cached detailed metadata from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/qa_adapted_reports__20220904_095810.json__detailed_metadata.pkl\n",
      "Loaded 240530 non-gold DICOM IDs from Chest Imagenome\n",
      "227835it [00:00, 455018.01it/s]\n",
      "Loading Chest Imagenome bounding boxes...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_13_chestimgbbmf1=0.8848.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121/checkpoint_13_chestimgbbmf1=0.8848.pt\n",
      "Loading model and epoch only\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 14/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.00793, chestimgbbmf1 0.90149, chestimgbb_loss 0.01030, chestimgbbiou 0.92572, chestimgbbmae 0.02514, 398.25 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88381, chestimgbbiou 0.91874, chestimgbbmae 0.02830, 16.86 secs\n",
      "Adjusting learning rate of group 0 to 4.6416e-06.\n",
      "\u001b[1m---- Epoch 15/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.01294, chestimgbbmf1 0.89952, chestimgbb_loss 0.01173, chestimgbbiou 0.92440, chestimgbbmae 0.02571, 395.29 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88174, chestimgbbiou 0.91792, chestimgbbmae 0.02859, 16.65 secs\n",
      "Adjusting learning rate of group 0 to 2.1544e-05.\n",
      "\u001b[1m---- Epoch 16/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.01030, chestimgbbmf1 0.89540, chestimgbb_loss 0.01163, chestimgbbiou 0.92341, chestimgbbmae 0.02602, 394.82 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87205, chestimgbbiou 0.91551, chestimgbbmae 0.02945, 17.04 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[1m---- Epoch 17/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.00766, chestimgbbmf1 0.83671, chestimgbb_loss 0.01602, chestimgbbiou 0.91082, chestimgbbmae 0.03058, 397.78 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.84574, chestimgbbiou 0.91115, chestimgbbmae 0.03099, 16.69 secs\n",
      "Adjusting learning rate of group 0 to 8.6194e-05.\n",
      "\u001b[1m---- Epoch 18/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000086) ...\n",
      "loss 0.00686, chestimgbbmf1 0.87595, chestimgbb_loss 0.01623, chestimgbbiou 0.91841, chestimgbbmae 0.02794, 395.70 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86547, chestimgbbiou 0.91446, chestimgbbmae 0.02977, 16.71 secs\n",
      "Adjusting learning rate of group 0 to 7.4294e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 19/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000074) ...\n",
      "loss 0.01346, chestimgbbmf1 0.88901, chestimgbb_loss 0.01452, chestimgbbiou 0.92182, chestimgbbmae 0.02669, 397.82 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87360, chestimgbbiou 0.91652, chestimgbbmae 0.02910, 16.99 secs\n",
      "Adjusting learning rate of group 0 to 6.4037e-05.\n",
      "\u001b[1m---- Epoch 20/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000064) ...\n",
      "loss 0.01625, chestimgbbmf1 0.89440, chestimgbb_loss 0.01315, chestimgbbiou 0.92334, chestimgbbmae 0.02611, 394.51 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87730, chestimgbbiou 0.91729, chestimgbbmae 0.02883, 16.81 secs\n",
      "Adjusting learning rate of group 0 to 5.5195e-05.\n",
      "\u001b[1m---- Epoch 21/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000055) ...\n",
      "loss 0.02296, chestimgbbmf1 0.89747, chestimgbb_loss 0.01326, chestimgbbiou 0.92394, chestimgbbmae 0.02593, 396.80 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87696, chestimgbbiou 0.91719, chestimgbbmae 0.02886, 16.84 secs\n",
      "Adjusting learning rate of group 0 to 4.7575e-05.\n",
      "\u001b[1m---- Epoch 22/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000048) ...\n",
      "loss 0.01060, chestimgbbmf1 0.90195, chestimgbb_loss 0.01201, chestimgbbiou 0.92581, chestimgbbmae 0.02522, 395.34 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88722, chestimgbbiou 0.92043, chestimgbbmae 0.02772, 16.81 secs\n",
      "Adjusting learning rate of group 0 to 4.1007e-05.\n",
      "\u001b[1m---- Epoch 23/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000041) ...\n",
      "loss 0.01654, chestimgbbmf1 0.90485, chestimgbb_loss 0.01244, chestimgbbiou 0.92645, chestimgbbmae 0.02504, 396.75 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88177, chestimgbbiou 0.91816, chestimgbbmae 0.02858, 17.17 secs\n",
      "Adjusting learning rate of group 0 to 3.5345e-05.\n",
      "\u001b[1m---- Epoch 24/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000035) ...\n",
      "loss 0.00742, chestimgbbmf1 0.90696, chestimgbb_loss 0.01147, chestimgbbiou 0.92746, chestimgbbmae 0.02463, 399.01 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89092, chestimgbbiou 0.92123, chestimgbbmae 0.02745, 17.47 secs\n",
      "Adjusting learning rate of group 0 to 3.0465e-05.\n",
      "\u001b[1m---- Epoch 25/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 0.01584, chestimgbbmf1 0.91196, chestimgbb_loss 0.00917, chestimgbbiou 0.92913, chestimgbbmae 0.02396, 398.42 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89296, chestimgbbiou 0.92191, chestimgbbmae 0.02720, 16.71 secs\n",
      "Adjusting learning rate of group 0 to 2.6259e-05.\n",
      "\u001b[1m---- Epoch 26/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000026) ...\n",
      "loss 0.00561, chestimgbbmf1 0.91378, chestimgbb_loss 0.00800, chestimgbbiou 0.93012, chestimgbbmae 0.02354, 391.71 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89460, chestimgbbiou 0.92264, chestimgbbmae 0.02697, 16.91 secs\n",
      "Adjusting learning rate of group 0 to 2.2634e-05.\n",
      "\u001b[1m---- Epoch 27/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.00364, chestimgbbmf1 0.91394, chestimgbb_loss 0.00810, chestimgbbiou 0.93001, chestimgbbmae 0.02360, 395.26 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89728, chestimgbbiou 0.92343, chestimgbbmae 0.02666, 17.47 secs\n",
      "Adjusting learning rate of group 0 to 1.9509e-05.\n",
      "\u001b[1m---- Epoch 28/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 0.00650, chestimgbbmf1 0.91535, chestimgbb_loss 0.00772, chestimgbbiou 0.93061, chestimgbbmae 0.02338, 396.94 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89562, chestimgbbiou 0.92317, chestimgbbmae 0.02675, 17.17 secs\n",
      "Adjusting learning rate of group 0 to 1.6816e-05.\n",
      "\u001b[1m---- Epoch 29/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.00943, chestimgbbmf1 0.91531, chestimgbb_loss 0.00778, chestimgbbiou 0.93076, chestimgbbmae 0.02333, 405.96 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89898, chestimgbbiou 0.92389, chestimgbbmae 0.02651, 17.84 secs\n",
      "Adjusting learning rate of group 0 to 1.4494e-05.\n",
      "\u001b[1m---- Epoch 30/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.00483, chestimgbbmf1 0.91558, chestimgbb_loss 0.00767, chestimgbbiou 0.93099, chestimgbbmae 0.02324, 410.11 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89878, chestimgbbiou 0.92404, chestimgbbmae 0.02647, 17.83 secs\n",
      "Adjusting learning rate of group 0 to 1.2493e-05.\n",
      "\u001b[1m---- Epoch 31/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.00288, chestimgbbmf1 0.91849, chestimgbb_loss 0.00632, chestimgbbiou 0.93223, chestimgbbmae 0.02274, 411.28 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89947, chestimgbbiou 0.92440, chestimgbbmae 0.02635, 17.72 secs\n",
      "Adjusting learning rate of group 0 to 1.0768e-05.\n",
      "\u001b[1m---- Epoch 32/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.00452, chestimgbbmf1 0.91961, chestimgbb_loss 0.00625, chestimgbbiou 0.93253, chestimgbbmae 0.02264, 412.20 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90017, chestimgbbiou 0.92435, chestimgbbmae 0.02638, 17.83 secs\n",
      "Adjusting learning rate of group 0 to 9.2814e-06.\n",
      "\u001b[1m---- Epoch 33/33\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.00782, chestimgbbmf1 0.91964, chestimgbb_loss 0.00623, chestimgbbiou 0.93259, chestimgbbmae 0.02262, 408.79 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90198, chestimgbbiou 0.92538, chestimgbbmae 0.02600, 17.69 secs\n",
      "Adjusting learning rate of group 0 to 8.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "        --checkpoint-folder \"models/visual_module/20230215_124027_mim_dense121\" \\\n",
    "        --epochs 20 \\\n",
    "        --batches-per-epoch 300 \\\n",
    "        --batch-size 140 \\\n",
    "        --num-workers 3 \\\n",
    "        --iters-to-accumulate 2 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,3,1e-4,17,8e-6\" \\\n",
    "        --override-lr \\\n",
    "        --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 30\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: None\n",
      "   mimiccxr_qa_adapted_reports_filename: qa_adapted_reports__20220904_095810.json\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: densenet-121\n",
      "   image_local_feat_size: 1024\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: None\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: v2\n",
      "   torchxrayvision_weights_name: None\n",
      "   num_regions: 64\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,1e-4,25,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 140\n",
      "   iters_to_accumulate: 2\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   image_size: [256, 256]\n",
      "   mimiccxr_weight: 1.0\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: chest_imagenome\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_amp: True\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: True\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: False\n",
      "   vinbig_training_data_mode: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: bce\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: True\n",
      "   clamp_bboxes_chest_imagenome: True\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/chest_imagenome_train_average_bbox_coords(qa_adapted_reports__20220904_095810.json,clamped.pkl...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of MultiPurposeVisualModel ...\u001b[0m\n",
      "create_densenet121_feature_extractor()\n",
      "   drop_rate: 0.0\n",
      "  self.global_feat_size = 2048\n",
      "BoundingBoxRegressor_v2:\n",
      "  local_feat_dim: 1024\n",
      "  global_feat_dim: 2048\n",
      "  hidden_dim: 128\n",
      "  num_classes: 36\n",
      "  num_regions: 64\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,1e-4,25,1e-6\n",
      "1e-06 5 0.0001 25 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mCreating MIMIC-CXR visual module trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    len(_augmented_bbox_transforms) = 2\n",
      "    augmentation_mode = random-color-and-spatial\n",
      "    default_prob = 0.3\n",
      "    Returning augmented transforms with mode random-color-and-spatial\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "Loading cached detailed metadata from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/qa_adapted_reports__20220904_095810.json__detailed_metadata.pkl\n",
      "Loaded 240530 non-gold DICOM IDs from Chest Imagenome\n",
      "227835it [00:00, 480520.04it/s]\n",
      "len(self.train_indices) = 235209\n",
      "len(self.val_indices) = 1945\n",
      "Loading Chest Imagenome bounding boxes...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230216_131111_mim_dn121\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230216_131111_mim_dn121/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_13_chestimgbbmf1=0.8848.pt', 'checkpoint_33_chestimgbbmf1=0.9038.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121/checkpoint_33_chestimgbbmf1=0.9038.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230216_131111_mim_dn121/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.13499, chestimgbbmf1 0.75716, chestimgbb_loss 0.08512, chestimgbbiou 0.88972, chestimgbbmae 0.04101, 402.50 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.84675, chestimgbbiou 0.91179, chestimgbbmae 0.03107, 16.43 secs\n",
      "Adjusting learning rate of group 0 to 2.5119e-06.\n",
      "\u001b[1m---- Epoch 2/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.07164, chestimgbbmf1 0.78965, chestimgbb_loss 0.06475, chestimgbbiou 0.89463, chestimgbbmae 0.03893, 401.88 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85136, chestimgbbiou 0.91153, chestimgbbmae 0.03111, 16.47 secs\n",
      "Adjusting learning rate of group 0 to 6.3096e-06.\n",
      "\u001b[1m---- Epoch 3/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.04396, chestimgbbmf1 0.79957, chestimgbb_loss 0.04831, chestimgbbiou 0.89582, chestimgbbmae 0.03829, 398.98 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85205, chestimgbbiou 0.91119, chestimgbbmae 0.03118, 16.65 secs\n",
      "Adjusting learning rate of group 0 to 1.5849e-05.\n",
      "\u001b[1m---- Epoch 4/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.03553, chestimgbbmf1 0.81352, chestimgbb_loss 0.04149, chestimgbbiou 0.89868, chestimgbbmae 0.03707, 403.91 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85876, chestimgbbiou 0.91258, chestimgbbmae 0.03064, 16.60 secs\n",
      "Adjusting learning rate of group 0 to 3.9811e-05.\n",
      "\u001b[1m---- Epoch 5/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 0.02794, chestimgbbmf1 0.81961, chestimgbb_loss 0.03786, chestimgbbiou 0.90036, chestimgbbmae 0.03629, 402.29 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestimgbbmf1 0.85505, chestimgbbiou 0.91167, chestimgbbmae 0.03091, 16.76 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[1m---- Epoch 6/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.03304, chestimgbbmf1 0.79855, chestimgbb_loss 0.03610, chestimgbbiou 0.89677, chestimgbbmae 0.03748, 402.16 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.84878, chestimgbbiou 0.91029, chestimgbbmae 0.03138, 16.70 secs\n",
      "Adjusting learning rate of group 0 to 8.3176e-05.\n",
      "\u001b[1m---- Epoch 7/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000083) ...\n",
      "loss 0.03732, chestimgbbmf1 0.83045, chestimgbb_loss 0.03433, chestimgbbiou 0.90329, chestimgbbmae 0.03505, 401.68 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.84159, chestimgbbiou 0.90931, chestimgbbmae 0.03175, 16.49 secs\n",
      "Adjusting learning rate of group 0 to 6.9183e-05.\n",
      "\u001b[1m---- Epoch 8/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000069) ...\n",
      "loss 0.03826, chestimgbbmf1 0.84262, chestimgbb_loss 0.03336, chestimgbbiou 0.90584, chestimgbbmae 0.03410, 401.15 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86179, chestimgbbiou 0.91431, chestimgbbmae 0.02993, 16.65 secs\n",
      "Adjusting learning rate of group 0 to 5.7544e-05.\n",
      "\u001b[1m---- Epoch 9/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000058) ...\n",
      "loss 0.02444, chestimgbbmf1 0.85367, chestimgbb_loss 0.03219, chestimgbbiou 0.90845, chestimgbbmae 0.03313, 404.46 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87380, chestimgbbiou 0.91639, chestimgbbmae 0.02928, 16.64 secs\n",
      "Adjusting learning rate of group 0 to 4.7863e-05.\n",
      "\u001b[1m---- Epoch 10/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000048) ...\n",
      "loss 0.02728, chestimgbbmf1 0.86035, chestimgbb_loss 0.03239, chestimgbbiou 0.91020, chestimgbbmae 0.03248, 402.83 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87632, chestimgbbiou 0.91727, chestimgbbmae 0.02888, 16.70 secs\n",
      "Adjusting learning rate of group 0 to 3.9811e-05.\n",
      "\u001b[1m---- Epoch 11/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 0.02307, chestimgbbmf1 0.86577, chestimgbb_loss 0.03206, chestimgbbiou 0.91148, chestimgbbmae 0.03203, 402.24 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.87576, chestimgbbiou 0.91729, chestimgbbmae 0.02895, 16.81 secs\n",
      "Adjusting learning rate of group 0 to 3.3113e-05.\n",
      "\u001b[1m---- Epoch 12/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000033) ...\n",
      "loss 0.02712, chestimgbbmf1 0.87089, chestimgbb_loss 0.03044, chestimgbbiou 0.91321, chestimgbbmae 0.03132, 399.80 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88295, chestimgbbiou 0.91892, chestimgbbmae 0.02840, 16.98 secs\n",
      "Adjusting learning rate of group 0 to 2.7542e-05.\n",
      "\u001b[1m---- Epoch 13/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.03810, chestimgbbmf1 0.87324, chestimgbb_loss 0.02994, chestimgbbiou 0.91388, chestimgbbmae 0.03108, 402.14 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88601, chestimgbbiou 0.92000, chestimgbbmae 0.02798, 16.73 secs\n",
      "Adjusting learning rate of group 0 to 2.2909e-05.\n",
      "\u001b[1m---- Epoch 14/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.02866, chestimgbbmf1 0.87516, chestimgbb_loss 0.03043, chestimgbbiou 0.91417, chestimgbbmae 0.03101, 402.45 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88911, chestimgbbiou 0.92120, chestimgbbmae 0.02757, 16.73 secs\n",
      "Adjusting learning rate of group 0 to 1.9055e-05.\n",
      "\u001b[1m---- Epoch 15/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.03108, chestimgbbmf1 0.87705, chestimgbb_loss 0.03048, chestimgbbiou 0.91455, chestimgbbmae 0.03089, 402.95 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.88961, chestimgbbiou 0.92094, chestimgbbmae 0.02766, 16.70 secs\n",
      "Adjusting learning rate of group 0 to 1.5849e-05.\n",
      "\u001b[1m---- Epoch 16/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.03937, chestimgbbmf1 0.87975, chestimgbb_loss 0.03010, chestimgbbiou 0.91533, chestimgbbmae 0.03059, 401.73 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89148, chestimgbbiou 0.92177, chestimgbbmae 0.02738, 16.69 secs\n",
      "Adjusting learning rate of group 0 to 1.3183e-05.\n",
      "\u001b[1m---- Epoch 17/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.02718, chestimgbbmf1 0.87926, chestimgbb_loss 0.03001, chestimgbbiou 0.91542, chestimgbbmae 0.03057, 401.97 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89085, chestimgbbiou 0.92177, chestimgbbmae 0.02739, 16.74 secs\n",
      "Adjusting learning rate of group 0 to 1.0965e-05.\n",
      "\u001b[1m---- Epoch 18/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.03576, chestimgbbmf1 0.88224, chestimgbb_loss 0.02974, chestimgbbiou 0.91627, chestimgbbmae 0.03023, 402.49 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89365, chestimgbbiou 0.92218, chestimgbbmae 0.02724, 16.68 secs\n",
      "Adjusting learning rate of group 0 to 9.1201e-06.\n",
      "\u001b[1m---- Epoch 19/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.02362, chestimgbbmf1 0.88354, chestimgbb_loss 0.02949, chestimgbbiou 0.91663, chestimgbbmae 0.03011, 403.91 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89588, chestimgbbiou 0.92304, chestimgbbmae 0.02694, 16.79 secs\n",
      "Adjusting learning rate of group 0 to 7.5858e-06.\n",
      "\u001b[1m---- Epoch 20/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.02067, chestimgbbmf1 0.88454, chestimgbb_loss 0.02982, chestimgbbiou 0.91681, chestimgbbmae 0.03006, 403.43 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89598, chestimgbbiou 0.92319, chestimgbbmae 0.02686, 16.93 secs\n",
      "Adjusting learning rate of group 0 to 6.3096e-06.\n",
      "\u001b[1m---- Epoch 21/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.02154, chestimgbbmf1 0.88454, chestimgbb_loss 0.03015, chestimgbbiou 0.91666, chestimgbbmae 0.03014, 403.36 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89708, chestimgbbiou 0.92368, chestimgbbmae 0.02670, 17.03 secs\n",
      "Adjusting learning rate of group 0 to 5.2481e-06.\n",
      "\u001b[1m---- Epoch 22/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.02580, chestimgbbmf1 0.88588, chestimgbb_loss 0.02873, chestimgbbiou 0.91758, chestimgbbmae 0.02974, 403.33 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89839, chestimgbbiou 0.92396, chestimgbbmae 0.02661, 16.73 secs\n",
      "Adjusting learning rate of group 0 to 4.3652e-06.\n",
      "\u001b[1m---- Epoch 23/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.02159, chestimgbbmf1 0.88780, chestimgbb_loss 0.02930, chestimgbbiou 0.91782, chestimgbbmae 0.02967, 402.52 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89751, chestimgbbiou 0.92395, chestimgbbmae 0.02661, 16.70 secs\n",
      "Adjusting learning rate of group 0 to 3.6308e-06.\n",
      "\u001b[1m---- Epoch 24/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.03173, chestimgbbmf1 0.88606, chestimgbb_loss 0.02959, chestimgbbiou 0.91725, chestimgbbmae 0.02991, 404.41 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89784, chestimgbbiou 0.92403, chestimgbbmae 0.02658, 16.92 secs\n",
      "Adjusting learning rate of group 0 to 3.0200e-06.\n",
      "\u001b[1m---- Epoch 25/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.03361, chestimgbbmf1 0.88723, chestimgbb_loss 0.02905, chestimgbbiou 0.91787, chestimgbbmae 0.02964, 402.99 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89930, chestimgbbiou 0.92440, chestimgbbmae 0.02644, 16.73 secs\n",
      "Adjusting learning rate of group 0 to 2.5119e-06.\n",
      "\u001b[1m---- Epoch 26/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.03663, chestimgbbmf1 0.88869, chestimgbb_loss 0.02879, chestimgbbiou 0.91811, chestimgbbmae 0.02957, 403.52 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89889, chestimgbbiou 0.92428, chestimgbbmae 0.02649, 16.89 secs\n",
      "Adjusting learning rate of group 0 to 2.0893e-06.\n",
      "\u001b[1m---- Epoch 27/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02658, chestimgbbmf1 0.88692, chestimgbb_loss 0.02879, chestimgbbiou 0.91772, chestimgbbmae 0.02971, 402.08 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89931, chestimgbbiou 0.92452, chestimgbbmae 0.02641, 16.66 secs\n",
      "Adjusting learning rate of group 0 to 1.7378e-06.\n",
      "\u001b[1m---- Epoch 28/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02691, chestimgbbmf1 0.88813, chestimgbb_loss 0.02886, chestimgbbiou 0.91806, chestimgbbmae 0.02958, 400.95 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89875, chestimgbbiou 0.92436, chestimgbbmae 0.02646, 16.73 secs\n",
      "Adjusting learning rate of group 0 to 1.4454e-06.\n",
      "\u001b[1m---- Epoch 29/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.04603, chestimgbbmf1 0.88829, chestimgbb_loss 0.02907, chestimgbbiou 0.91814, chestimgbbmae 0.02954, 403.17 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89844, chestimgbbiou 0.92432, chestimgbbmae 0.02649, 16.69 secs\n",
      "Adjusting learning rate of group 0 to 1.2023e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 30/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.03208, chestimgbbmf1 0.88863, chestimgbb_loss 0.02817, chestimgbbiou 0.91850, chestimgbbmae 0.02936, 401.74 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89851, chestimgbbiou 0.92426, chestimgbbmae 0.02651, 16.70 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "        --pretrained-checkpoint-folder-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230215_124027_mim_dense121\" \\\n",
    "        --epochs 30 \\\n",
    "        --batches-per-epoch 300 \\\n",
    "        --batch-size 140 \\\n",
    "        --num-workers 3 \\\n",
    "        --iters-to-accumulate 2 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,1e-4,25,1e-6\" \\\n",
    "        --use-mimiccxr \\\n",
    "        --mimiccxr-weight 1.0 \\\n",
    "        --mimiccxr-qa-adapted-reports-filename \"qa_adapted_reports__20220904_095810.json\" \\\n",
    "        --mimiccxr-view-mode \"chest_imagenome\" \\\n",
    "        --predict-bboxes-chest-imagenome \\\n",
    "        --clamp-bboxes-chest-imagenome \\\n",
    "        --chest-imagenome-bbox-regressor-version \"v2\" \\\n",
    "        --raw-image-encoding \"densenet-121\" \\\n",
    "        --image-size 256 256 \\\n",
    "        --image-local-feat-size 1024 \\\n",
    "        --num-regions 64 \\\n",
    "        --img-aug-mode \"random-color-and-spatial\" \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 15\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: models/visual_module/20230216_131111_mim_dn121\n",
      "   mimiccxr_qa_adapted_reports_filename: None\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: densenet-121\n",
      "   image_local_feat_size: 1024\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: None\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: v1\n",
      "   torchxrayvision_weights_name: None\n",
      "   num_regions: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,4,1e-5,10,2e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 140\n",
      "   iters_to_accumulate: 2\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   img_aug_mode: None\n",
      "   image_size: (256, 256)\n",
      "   mimiccxr_weight: 1\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: any_single\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_amp: False\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   save: True\n",
      "   override_lr: True\n",
      "   train_mimiccxr: False\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: False\n",
      "   vinbig_training_data_mode: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: bce\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: False\n",
      "   clamp_bboxes_chest_imagenome: False\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Resuming training ------\u001b[0m\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230216_131111_mim_dn121/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of MultiPurposeVisualModel ...\u001b[0m\n",
      "create_densenet121_feature_extractor()\n",
      "   drop_rate: 0.0\n",
      "  self.global_feat_size = 2048\n",
      "BoundingBoxRegressor_v2:\n",
      "  local_feat_dim: 1024\n",
      "  global_feat_dim: 2048\n",
      "  hidden_dim: 128\n",
      "  num_classes: 36\n",
      "  num_regions: 64\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,4,1e-5,10,2e-6\n",
      "1e-06 4 1e-05 10 2e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mCreating MIMIC-CXR visual module trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    len(_augmented_bbox_transforms) = 2\n",
      "    augmentation_mode = random-color-and-spatial\n",
      "    default_prob = 0.5\n",
      "    Returning augmented transforms with mode random-color-and-spatial\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "Loading cached detailed metadata from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/qa_adapted_reports__20220904_095810.json__detailed_metadata.pkl\n",
      "Loaded 240530 non-gold DICOM IDs from Chest Imagenome\n",
      "227835it [00:00, 493939.91it/s]\n",
      "len(self.train_indices) = 235209\n",
      "len(self.val_indices) = 1945\n",
      "Loading Chest Imagenome bounding boxes...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_25_chestimgbbmf1=0.8981.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230216_131111_mim_dn121/checkpoint_25_chestimgbbmf1=0.8981.pt\n",
      "Loading model and epoch only\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230216_131111_mim_dn121/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 26/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.02363, chestimgbbmf1 0.89579, chestimgbb_loss 0.02416, chestimgbbiou 0.92162, chestimgbbmae 0.02789, 299.26 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90010, chestimgbbiou 0.92485, chestimgbbmae 0.02629, 12.39 secs\n",
      "Adjusting learning rate of group 0 to 1.7783e-06.\n",
      "\u001b[1m---- Epoch 27/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.01402, chestimgbbmf1 0.89599, chestimgbb_loss 0.02397, chestimgbbiou 0.92140, chestimgbbmae 0.02801, 296.39 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90011, chestimgbbiou 0.92497, chestimgbbmae 0.02624, 12.58 secs\n",
      "Adjusting learning rate of group 0 to 3.1623e-06.\n",
      "\u001b[1m---- Epoch 28/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.02639, chestimgbbmf1 0.89612, chestimgbb_loss 0.02417, chestimgbbiou 0.92157, chestimgbbmae 0.02791, 294.28 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90050, chestimgbbiou 0.92500, chestimgbbmae 0.02621, 12.52 secs\n",
      "Adjusting learning rate of group 0 to 5.6234e-06.\n",
      "\u001b[1m---- Epoch 29/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.01705, chestimgbbmf1 0.89487, chestimgbb_loss 0.02313, chestimgbbiou 0.92122, chestimgbbmae 0.02800, 294.27 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90024, chestimgbbiou 0.92486, chestimgbbmae 0.02628, 12.54 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "\u001b[1m---- Epoch 30/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.03075, chestimgbbmf1 0.89524, chestimgbb_loss 0.02309, chestimgbbiou 0.92149, chestimgbbmae 0.02787, 294.52 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89681, chestimgbbiou 0.92352, chestimgbbmae 0.02676, 12.57 secs\n",
      "Adjusting learning rate of group 0 to 8.5134e-06.\n",
      "\u001b[1m---- Epoch 31/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.02168, chestimgbbmf1 0.89316, chestimgbb_loss 0.02379, chestimgbbiou 0.92087, chestimgbbmae 0.02810, 292.34 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89786, chestimgbbiou 0.92377, chestimgbbmae 0.02669, 12.43 secs\n",
      "Adjusting learning rate of group 0 to 7.2478e-06.\n",
      "\u001b[1m---- Epoch 32/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.01526, chestimgbbmf1 0.89450, chestimgbb_loss 0.02384, chestimgbbiou 0.92086, chestimgbbmae 0.02817, 292.71 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.89945, chestimgbbiou 0.92442, chestimgbbmae 0.02645, 12.52 secs\n",
      "Adjusting learning rate of group 0 to 6.1703e-06.\n",
      "\u001b[1m---- Epoch 33/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.02457, chestimgbbmf1 0.89428, chestimgbb_loss 0.02306, chestimgbbiou 0.92142, chestimgbbmae 0.02790, 293.86 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90079, chestimgbbiou 0.92489, chestimgbbmae 0.02625, 12.54 secs\n",
      "Adjusting learning rate of group 0 to 5.2531e-06.\n",
      "\u001b[1m---- Epoch 34/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.03847, chestimgbbmf1 0.89729, chestimgbb_loss 0.02327, chestimgbbiou 0.92213, chestimgbbmae 0.02766, 294.92 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90025, chestimgbbiou 0.92478, chestimgbbmae 0.02631, 12.49 secs\n",
      "Adjusting learning rate of group 0 to 4.4721e-06.\n",
      "\u001b[1m---- Epoch 35/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.01822, chestimgbbmf1 0.89745, chestimgbb_loss 0.02310, chestimgbbiou 0.92227, chestimgbbmae 0.02762, 294.23 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90101, chestimgbbiou 0.92489, chestimgbbmae 0.02627, 12.54 secs\n",
      "Adjusting learning rate of group 0 to 3.8073e-06.\n",
      "\u001b[1m---- Epoch 36/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.02163, chestimgbbmf1 0.89659, chestimgbb_loss 0.02363, chestimgbbiou 0.92193, chestimgbbmae 0.02778, 293.83 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90195, chestimgbbiou 0.92536, chestimgbbmae 0.02612, 12.58 secs\n",
      "Adjusting learning rate of group 0 to 3.2413e-06.\n",
      "\u001b[1m---- Epoch 37/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.01674, chestimgbbmf1 0.89698, chestimgbb_loss 0.02277, chestimgbbiou 0.92229, chestimgbbmae 0.02759, 294.35 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90216, chestimgbbiou 0.92533, chestimgbbmae 0.02613, 12.43 secs\n",
      "Adjusting learning rate of group 0 to 2.7595e-06.\n",
      "\u001b[1m---- Epoch 38/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.02901, chestimgbbmf1 0.89725, chestimgbb_loss 0.02347, chestimgbbiou 0.92219, chestimgbbmae 0.02767, 293.96 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90271, chestimgbbiou 0.92544, chestimgbbmae 0.02610, 12.50 secs\n",
      "Adjusting learning rate of group 0 to 2.3492e-06.\n",
      "\u001b[1m---- Epoch 39/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02234, chestimgbbmf1 0.89750, chestimgbb_loss 0.02331, chestimgbbiou 0.92213, chestimgbbmae 0.02768, 293.59 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90246, chestimgbbiou 0.92567, chestimgbbmae 0.02600, 12.65 secs\n",
      "Adjusting learning rate of group 0 to 2.0000e-06.\n",
      "\u001b[1m---- Epoch 40/40\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02420, chestimgbbmf1 0.89756, chestimgbb_loss 0.02287, chestimgbbiou 0.92250, chestimgbbmae 0.02752, 292.62 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.90259, chestimgbbiou 0.92557, chestimgbbmae 0.02605, 12.59 secs\n",
      "Adjusting learning rate of group 0 to 1.7027e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "        --checkpoint-folder \"models/visual_module/20230216_131111_mim_dn121\" \\\n",
    "        --epochs 15 \\\n",
    "        --batches-per-epoch 300 \\\n",
    "        --batch-size 140 \\\n",
    "        --num-workers 3 \\\n",
    "        --iters-to-accumulate 2 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,4,1e-5,10,2e-6\" \\\n",
    "        --override-lr \\\n",
    "        --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
