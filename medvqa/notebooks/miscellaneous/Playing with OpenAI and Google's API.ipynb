{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75bb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reloadb\n",
    "import medvqab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90856182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'medvqa.utils.openai_api_utils' from '/home/pamessina/medvqa/medvqa/utils/openai_api_utils.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(medvqa.utils.openai_api_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464dc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.openai_api_utils import run_common_boilerplate_for_api_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffdc67a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-15 17:29:52,024 - INFO - root\u001b[0m - Logging configured (Color: True).\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:52,025 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving API requests to tmp/openai/api_requests_20250515_172952.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:52,025 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving API responses to tmp/openai/api_responses_20250515_172952.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:52,025 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving results to tmp/openai/api_responses_20250515_172952.jsonl\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,025 - DEBUG - asyncio\u001b[0m - Using selector: EpollSelector\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,026 - DEBUG - httpx\u001b[0m - load_ssl_context verify=True cert=None trust_env=True http2=False\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,028 - DEBUG - httpx\u001b[0m - load_verify_locations cafile='/home/pamessina/venv2/lib/python3.10/site-packages/certifi/cacert.pem'\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,035 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - OpenAI client initialized with default OpenAI base URL.\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,036 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Initialization complete.\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,036 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - File opened. Entering main loop\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,036 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Reading request 0: APIRequest(task_id=0, request_json={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': 'Translate the following text to French.'}, {'role': 'user', 'content': 'The quick brown fox jumps over the lazy dog.'}], 'temperature': 0, 'frequency_penalty': 0, 'presence_penalty': 0, 'max_tokens': None}, token_consumption=0, attempts_left=5, metadata={'query': 'The quick brown fox jumps over the lazy dog.'}, result=[])\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:52,036 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Starting request #0\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,234 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Read file exhausted\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,240 - DEBUG - openai._base_client\u001b[0m - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Translate the following text to French.'}, {'role': 'user', 'content': 'The quick brown fox jumps over the lazy dog.'}], 'model': 'gpt-3.5-turbo', 'frequency_penalty': 0, 'max_tokens': None, 'presence_penalty': 0, 'temperature': 0}, 'idempotency_key': 'stainless-python-retry-1e3ae636-74bd-4618-8d5f-1119e8168931'}\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,241 - DEBUG - openai._base_client\u001b[0m - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,246 - DEBUG - httpcore.connection\u001b[0m - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,357 - DEBUG - httpcore.connection\u001b[0m - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f2de104b7f0>\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,358 - DEBUG - httpcore.connection\u001b[0m - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f2de174cac0> server_hostname='api.openai.com' timeout=5.0\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,371 - DEBUG - httpcore.connection\u001b[0m - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f2de172af50>\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,372 - DEBUG - httpcore.http11\u001b[0m - send_request_headers.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,373 - DEBUG - httpcore.http11\u001b[0m - send_request_headers.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,373 - DEBUG - httpcore.http11\u001b[0m - send_request_body.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,374 - DEBUG - httpcore.http11\u001b[0m - send_request_body.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:52,374 - DEBUG - httpcore.http11\u001b[0m - receive_response_headers.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,697 - DEBUG - httpcore.http11\u001b[0m - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 15 May 2025 21:29:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-c5jfc4nuhiv4622mvzops6f1'), (b'openai-processing-ms', b'441'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'445'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'50000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'49999975'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_24cd4d7bcad24c26d937e1d04ddc3142'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=wZbD1oB1hPMlB0ssUdczLRWqjB0BYTS5s9PTtYMYJK8-1747344593-1.0.1.1-8GqSHqXv9HPQpFlo__aAqQBFErbSH86Vk18piqxUTxWZqtsMdVit9i0aRN2eOSI3Sfk1oLG0ZdRpU20iLCNJrk4XQ1bLjLEXtHFkgKT17sY; path=/; expires=Thu, 15-May-25 21:59:53 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=Q13EXELS3yHr0T2FQW5XGfVaMAVrjZ4A90L2quhQ97M-1747344593704-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9405bbb6696b8937-SCL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:53,701 - INFO - httpx\u001b[0m - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,701 - DEBUG - httpcore.http11\u001b[0m - receive_response_body.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,703 - DEBUG - httpcore.http11\u001b[0m - receive_response_body.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,704 - DEBUG - httpcore.http11\u001b[0m - response_closed.started\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,704 - DEBUG - httpcore.http11\u001b[0m - response_closed.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,705 - DEBUG - openai._base_client\u001b[0m - HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 15 May 2025 21:29:53 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-c5jfc4nuhiv4622mvzops6f1'), ('openai-processing-ms', '441'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '445'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '50000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '49999975'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '0s'), ('x-request-id', 'req_24cd4d7bcad24c26d937e1d04ddc3142'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=wZbD1oB1hPMlB0ssUdczLRWqjB0BYTS5s9PTtYMYJK8-1747344593-1.0.1.1-8GqSHqXv9HPQpFlo__aAqQBFErbSH86Vk18piqxUTxWZqtsMdVit9i0aRN2eOSI3Sfk1oLG0ZdRpU20iLCNJrk4XQ1bLjLEXtHFkgKT17sY; path=/; expires=Thu, 15-May-25 21:59:53 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=Q13EXELS3yHr0T2FQW5XGfVaMAVrjZ4A90L2quhQ97M-1747344593704-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '9405bbb6696b8937-SCL'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,705 - DEBUG - openai._base_client\u001b[0m - request_id: req_24cd4d7bcad24c26d937e1d04ddc3142\u001b[0m\n",
      "\u001b[36m2025-05-15 17:29:53,739 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Request 0 succeeded and saved to tmp/openai/api_responses_20250515_172952.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:53,739 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Parallel processing complete. Results saved to tmp/openai/api_responses_20250515_172952.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:53,740 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Loading API responses from tmp/openai/api_responses_20250515_172952.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 17:29:53,740 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Succesfully processed 1 of 1 API responses.\n",
      "                    0 of 1 API responses could not be processed.\n",
      "                    Saving processed texts to translation_results.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/miscellaneous/play_with_openai_and_google_api.py \\\n",
    "--test_case \"mini_test_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d49b693f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-15 19:23:23,051 - INFO - root\u001b[0m - Logging configured (Color: True).\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:23,052 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving API requests to tmp/gemini/api_requests_20250515_192323.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:23,053 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving API responses to tmp/gemini/api_responses_20250515_192323.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:23,053 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving results to tmp/gemini/api_responses_20250515_192323.jsonl\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,053 - DEBUG - asyncio\u001b[0m - Using selector: EpollSelector\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,083 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - OpenAI client initialized with base_url: https://generativelanguage.googleapis.com/v1beta/openai/\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,083 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Initialization complete.\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,083 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - File opened. Entering main loop\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,084 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Reading request 0: APIRequest(api_type='gemini', task_id=0, request_json={'model': 'gemini-2.0-flash', 'messages': [{'role': 'system', 'content': 'Summarize the following text in one sentence.'}, {'role': 'user', 'content': 'Artificial intelligence is transforming the world in many exciting ways.'}], 'temperature': 0}, token_consumption=0, attempts_left=5, metadata={'query': 'Artificial intelligence is transforming the world in many exciting ways.'}, result=[])\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:23,084 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Starting request #0\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,283 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Read file exhausted\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,289 - DEBUG - openai._base_client\u001b[0m - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Summarize the following text in one sentence.'}, {'role': 'user', 'content': 'Artificial intelligence is transforming the world in many exciting ways.'}], 'model': 'gemini-2.0-flash', 'temperature': 0}, 'idempotency_key': 'stainless-python-retry-fd043079-2a7a-4857-890e-28721b4e3a51'}\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,289 - DEBUG - openai._base_client\u001b[0m - Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,293 - DEBUG - httpcore.connection\u001b[0m - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,306 - DEBUG - httpcore.connection\u001b[0m - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f6ecdc316c0>\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,306 - DEBUG - httpcore.connection\u001b[0m - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f6ece231740> server_hostname='generativelanguage.googleapis.com' timeout=5.0\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,316 - DEBUG - httpcore.connection\u001b[0m - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f6ecde07f70>\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,316 - DEBUG - httpcore.http11\u001b[0m - send_request_headers.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,318 - DEBUG - httpcore.http11\u001b[0m - send_request_headers.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,318 - DEBUG - httpcore.http11\u001b[0m - send_request_body.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,319 - DEBUG - httpcore.http11\u001b[0m - send_request_body.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:23,319 - DEBUG - httpcore.http11\u001b[0m - receive_response_headers.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,465 - DEBUG - httpcore.http11\u001b[0m - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Thu, 15 May 2025 23:23:24 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1144'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:24,468 - INFO - httpx\u001b[0m - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,472 - DEBUG - httpcore.http11\u001b[0m - receive_response_body.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,474 - DEBUG - httpcore.http11\u001b[0m - receive_response_body.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,475 - DEBUG - httpcore.http11\u001b[0m - response_closed.started\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,475 - DEBUG - httpcore.http11\u001b[0m - response_closed.complete\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,476 - DEBUG - openai._base_client\u001b[0m - HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"200 OK\" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Thu, 15 May 2025 23:23:24 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=1144'), ('alt-svc', 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), ('transfer-encoding', 'chunked')])\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,476 - DEBUG - openai._base_client\u001b[0m - request_id: None\u001b[0m\n",
      "\u001b[36m2025-05-15 19:23:24,508 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Request 0 succeeded and saved to tmp/gemini/api_responses_20250515_192323.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:24,508 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Parallel processing complete. Results saved to tmp/gemini/api_responses_20250515_192323.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:24,509 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Loading API responses from tmp/gemini/api_responses_20250515_192323.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-15 19:23:24,509 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Succesfully processed 1 of 1 API responses. 0 of 1 API responses could not be processed. Saving processed texts to \u001b[1;35mgemini_results.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/miscellaneous/play_with_openai_and_google_api.py \\\n",
    "--test_case \"mini_test_gemini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "245ae969",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-16 15:53:29,996 - INFO - root\u001b[0m - Logging configured (Color: True).\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:29,998 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving API requests to tmp/gemini/api_requests_20250516_155329.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:29,998 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving API responses to tmp/gemini/api_responses_20250516_155329.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:29,998 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Saving results to tmp/gemini/api_responses_20250516_155329.jsonl\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:29,998 - DEBUG - asyncio\u001b[0m - Using selector: EpollSelector\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,028 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - OpenAI client initialized with base_url: https://generativelanguage.googleapis.com/v1beta/openai/\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,028 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Initialization complete.\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,028 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - File opened. Entering main loop\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,028 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - api_endpoint: chat/completions\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,028 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - token_consumption: 48\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,028 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Reading request 0: APIRequest(api_type='gemini', task_id=0, request_json={'model': 'gemini-2.0-flash', 'messages': [{'role': 'system', 'content': 'Summarize the following text in one sentence.'}, {'role': 'user', 'content': 'Artificial intelligence is transforming the world in many exciting ways.'}], 'temperature': 0}, token_consumption=48, attempts_left=5, metadata={'query': 'Artificial intelligence is transforming the world in many exciting ways.'}, result=[])\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:30,028 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Starting request #0\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,230 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Read file exhausted\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,236 - DEBUG - openai._base_client\u001b[0m - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Summarize the following text in one sentence.'}, {'role': 'user', 'content': 'Artificial intelligence is transforming the world in many exciting ways.'}], 'model': 'gemini-2.0-flash', 'temperature': 0}, 'idempotency_key': 'stainless-python-retry-8436bd7b-b228-4c20-b874-a0c910759d2e'}\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,237 - DEBUG - openai._base_client\u001b[0m - Sending HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,241 - DEBUG - httpcore.connection\u001b[0m - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=5.0 socket_options=None\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,252 - DEBUG - httpcore.connection\u001b[0m - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fc2cea29810>\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,252 - DEBUG - httpcore.connection\u001b[0m - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fc2cf025840> server_hostname='generativelanguage.googleapis.com' timeout=5.0\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,259 - DEBUG - httpcore.connection\u001b[0m - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fc2cf03f970>\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,259 - DEBUG - httpcore.http11\u001b[0m - send_request_headers.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,260 - DEBUG - httpcore.http11\u001b[0m - send_request_headers.complete\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,260 - DEBUG - httpcore.http11\u001b[0m - send_request_body.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,260 - DEBUG - httpcore.http11\u001b[0m - send_request_body.complete\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:30,261 - DEBUG - httpcore.http11\u001b[0m - receive_response_headers.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,476 - DEBUG - httpcore.http11\u001b[0m - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Fri, 16 May 2025 19:53:31 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1211'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:31,478 - INFO - httpx\u001b[0m - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,483 - DEBUG - httpcore.http11\u001b[0m - receive_response_body.started request=<Request [b'POST']>\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,485 - DEBUG - httpcore.http11\u001b[0m - receive_response_body.complete\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,486 - DEBUG - httpcore.http11\u001b[0m - response_closed.started\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,486 - DEBUG - httpcore.http11\u001b[0m - response_closed.complete\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,487 - DEBUG - openai._base_client\u001b[0m - HTTP Response: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"200 OK\" Headers([('content-type', 'application/json'), ('vary', 'Origin'), ('vary', 'X-Origin'), ('vary', 'Referer'), ('content-encoding', 'gzip'), ('date', 'Fri, 16 May 2025 19:53:31 GMT'), ('server', 'scaffolding on HTTPServer2'), ('x-xss-protection', '0'), ('x-frame-options', 'SAMEORIGIN'), ('x-content-type-options', 'nosniff'), ('server-timing', 'gfet4t7; dur=1211'), ('alt-svc', 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), ('transfer-encoding', 'chunked')])\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,488 - DEBUG - openai._base_client\u001b[0m - request_id: None\u001b[0m\n",
      "\u001b[36m2025-05-16 15:53:31,516 - DEBUG - medvqa.utils.openai_api_utils\u001b[0m - Request 0 succeeded and saved to tmp/gemini/api_responses_20250516_155329.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:31,516 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Parallel processing complete. Results saved to tmp/gemini/api_responses_20250516_155329.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:31,517 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Loading API responses from tmp/gemini/api_responses_20250516_155329.jsonl\u001b[0m\n",
      "\u001b[32m2025-05-16 15:53:31,517 - INFO - medvqa.utils.openai_api_utils\u001b[0m - Succesfully processed 1 of 1 API responses. 0 of 1 API responses could not be processed. Saving processed texts to \u001b[1;35mtmp/gemini/gemini_results.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/miscellaneous/play_with_openai_and_google_api.py \\\n",
    "--test_case \"mini_test_gemini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43b00980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Generative AI refers to a category of artificial intelligence algorithms that can generate new content, such as text, images, music, and videos.  It learns the patterns and structure of its training data and then uses that knowledge to create new data that has similar characteristics.\n",
       "\n",
       "Here's a breakdown of key aspects:\n",
       "\n",
       "*   **Generative Models:** These are the core of generative AI. They learn the underlying distribution of the data they are trained on. Common types include:\n",
       "    *   **Generative Adversarial Networks (GANs):** GANs involve two neural networks, a generator, and a discriminator. The generator creates new data, and the discriminator tries to distinguish between the generated data and real data. They compete in a game-like scenario, leading the generator to produce increasingly realistic outputs.\n",
       "    *   **Variational Autoencoders (VAEs):** VAEs learn a compressed representation of the data (latent space). They can then sample from this latent space and decode it back into new, similar data.\n",
       "    *   **Transformers:** Transformers, particularly large language models (LLMs), have proven highly effective in generative tasks, especially with text. They use attention mechanisms to weigh the importance of different parts of the input data.\n",
       "\n",
       "*   **Training Data:** The quality and quantity of the training data are crucial.  Generative AI models learn from massive datasets. For example, a model that generates images might be trained on millions of images.  A text-generating model will be trained on a massive text corpus.\n",
       "\n",
       "*   **Applications:** The applications of generative AI are diverse and rapidly expanding:\n",
       "    *   **Text Generation:** Writing articles, creating marketing copy, generating code, chatbots, summarizing text, translating languages.\n",
       "    *   **Image Generation:** Creating photorealistic images from text descriptions, generating variations of existing images, creating art.\n",
       "    *   **Music Generation:** Composing new music in various styles, generating sound effects.\n",
       "    *   **Video Generation:** Creating short video clips, generating special effects.\n",
       "    *   **Drug Discovery:** Designing new molecules with desired properties.\n",
       "    *   **Materials Science:** Discovering new materials with specific characteristics.\n",
       "    *   **Game Development:** Generating game assets, creating storylines.\n",
       "\n",
       "*   **Key Concepts:**\n",
       "    *   **Latent Space:** A compressed, abstract representation of the data learned by the model.  Navigating the latent space allows for generating variations of the original data.\n",
       "    *   **Sampling:**  The process of selecting values from the latent space or from the model's output distribution to create new data.\n",
       "    *   **Creativity (Artificial):** While generative AI can produce novel outputs, it doesn't possess genuine creativity in the human sense. It's creating based on the patterns and knowledge it has learned from the training data.\n",
       "\n",
       "*   **Limitations and Challenges:**\n",
       "    *   **Bias:** Generative AI can inherit biases present in the training data, leading to biased or discriminatory outputs.\n",
       "    *   **Lack of Understanding:**  These models don't truly \"understand\" the content they are generating. They are simply manipulating patterns.\n",
       "    *   **Control and Fine-Tuning:**  Controlling the specific characteristics of the generated output can be challenging.\n",
       "    *   **Copyright and Ownership:** The legal and ethical implications of using AI to generate content are still being debated.  Who owns the copyright to an image created by an AI, for instance?\n",
       "    *   **Misinformation and Deepfakes:** The ability to generate realistic fake content poses a risk of spreading misinformation and creating deepfakes.\n",
       "\n",
       "In summary, generative AI is a powerful technology that allows computers to create new content by learning from existing data.  It has the potential to revolutionize many industries, but it's important to be aware of its limitations and ethical considerations.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "prompt = \"What is generative AI?\" # @param\n",
    "\n",
    "MODEL=\"gemini-2.0-flash\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
