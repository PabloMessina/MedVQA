{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 250\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_135046_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 20\n",
      "   override_lr: False\n",
      "   task_name: multitask\n",
      "   experiment_name: s2f+f2m+f2c+s2co+s2cal+nli+mlm\n",
      "   multitask_name_list: ['nli', 'sentence2facts', 'fact2metadata', 'fact2comparison', 'sentence2chestimagenome_observations', 'sentence2chestimagenome_anatomical_locations', 'mlm']\n",
      "   task2weight: {'sentence2facts': 1.0, 'fact2metadata': 1.0, 'fact2comparison': 0.3, 'sentence2chestimagenome_observations': 1.0, 'sentence2chestimagenome_anatomical_locations': 1.0, 'nli': 7.0, 'mlm': 5.0}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_phrases2labels_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl']\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl']\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl']\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   use_sentence2facts_for_nli: True\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   only_validate_nli: True\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 20, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9891 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mB\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mbilateral\u001b[0m\n",
      "\u001b[1m\u001b[35mboth sides\u001b[0m\n",
      "\u001b[1m\u001b[35mboth\u001b[0m\n",
      "\u001b[1m\u001b[35mon both sides\u001b[0m\n",
      "\u001b[1m\u001b[35msymmetrical\u001b[0m\n",
      "\u001b[1m\u001b[35mequally on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mpresent on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mseen on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35moccurring on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mappearing on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mfound on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mseen bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mobserved bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mdetected bilaterally\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9890 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilus\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the hilum on the left side\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 8511 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary vasculature\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral lung vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary blood vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mlung periphery vasculature\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature of the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels in the outer regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular network in the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary circulation\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels supplying the outer areas of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature in the peripheral regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular system in the outer regions of the lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 1864 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35ma\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mone\u001b[0m\n",
      "\u001b[1m\u001b[35msingle\u001b[0m\n",
      "\u001b[1m\u001b[35msole\u001b[0m\n",
      "\u001b[1m\u001b[35monly\u001b[0m\n",
      "\u001b[1m\u001b[35mindividual\u001b[0m\n",
      "\u001b[1m\u001b[35mlone\u001b[0m\n",
      "\u001b[1m\u001b[35msolitary\u001b[0m\n",
      "\u001b[1m\u001b[35munique\u001b[0m\n",
      "\u001b[1m\u001b[35mdistinct\u001b[0m\n",
      "\u001b[1m\u001b[35msingular\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 10000 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9999 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9997 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9994 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9996 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome not detected\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome is not observed\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant constriction\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of notable narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of substantial narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of notable constriction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19943 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of wheezing\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing detected\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing observed\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing present\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing found\u001b[0m\n",
      "--------\n",
      "\u001b[1mNumber of unique inputs: 190085\u001b[0m\n",
      "\u001b[1mNumber of total paraphrases: 2093173\u001b[0m\n",
      "Number of medical sentences from paraphrases: 2039914\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2facts dataset\u001b[0m\n",
      "Loaded 9999 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19971 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\n",
      "Loaded 14990 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\n",
      "Loaded 14991 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2F: Small to moderate -sized bilateral pleural effusions with bibasilar patchy opacities, potentially compressive atelectasis, but infection or aspiration cannot be excluded.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"small to moderate-sized bilateral pleural effusions\", \"bibasilar patchy opacities\", \"potential compressive atelectasis\", \"infection cannot be excluded\", \"aspiration cannot be excluded\"]\u001b[0m\n",
      "Number of train examples: 84935\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing nli dataset\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "Number of sources: 5\n",
      "----\n",
      "\u001b[1mLoading sentence2facts input/output pairs for NLI...\u001b[0m\n",
      "Number of entailment samples added: 368966\n",
      "----\n",
      "\u001b[1mLoading general domain datasets...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset...\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: contradiction -> 181976 (5334.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The heart appears borderline in size. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe heart is significantly enlarged.\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: contradiction -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: She also reported a \"large amount\" of hematuria x 1 episode yesterday. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient has no RBCs in her urine. \u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: contradiction -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Anxiety 12. h/o Anemia - hemolytic anemia after Keflex 13. #Hypothesis:  No history of anemia\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: contradiction -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Nasolaryngoscopy exam by ED resident showed arytenoid, vocal chord, and supraglottic edema. #Hypothesis:  Patient has normal endoscopy findings\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: contradiction -> 212 (461.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The left lung is clear. #Hypothesis: Left lower lobe atelectasis is unchanged.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: anli | Label: contradiction -> 88178 (4433.68)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: How to downgrade your avg subscription on windows<br>Turn on your computer. Press the power button on your computer to turn it on. [substeps] The location of the power button will depend on your pc. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35meveryone knows where the power button is on their system\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: contradiction -> 274712 (5897.92)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: and he just whips out you know i saw him yesterday morning over there and and he paid his monthly bills and he just you know wrote all the checks on the screen and hit print and it printed out like ten checks and he just you know they're perforated and he just ripped them off they go through just a continuous thing on the printer and #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mit's useful for many things, but it can't print checks\u001b[0m\n",
      "\u001b[1mSource: snli | Label: contradiction -> 379404 (6365.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: 2 men stand in line at a restaurant. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe two men are at a drive through in their car.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: entailment -> 43334 (3654.55)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Cardiomediastinal and hilar silhouettes are normal size. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe patient's heart and mediastinal structures are not enlarged.\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: entailment -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: She was readmitted and had a complete febrile work-up that included CT abdomene/pelvis, chest x-ray, blood culture's, urine cultures, all of which were unremarkable. #Hypothesis:  She has a fever\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: entailment -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: - Myelodysplastic syndrome first diagnosed [**3213-7-9**], treated with danazol and aranesp. #Hypothesis:  The patient has cancer\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: entailment -> 946 (966.10)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: He was admitted for thoracoscopy with evacuation of pleural effusion, takedown of adhesions and partial lung decortication. #Hypothesis:  He was treated for lung abnormalities\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: entailment -> 186 (428.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is no focal consolidation, pleural effusion, pulmonary edema, or pneumothorax. #Hypothesis: No pleural effusion or pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSource: s2f | Label: entailment -> 368966 (6324.57)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Surgical clips are noted over the mid abdomen.Irregular opacity previously seen in the mid abdomen is no longer visualized. #Hypothesis: surgical clips over the mid abdomen\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: anli | Label: entailment -> 108502 (4680.39)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The New Zealand Maori Queen, Te Arikinui Dame Te Atairangikaahu died on Tuesday at 5.32 p.m. (NZST) following a long illness. She was 75 years old. The Maori Queen holds no constitutional function in New Zealand, but Dame Te Atairangikaahu was an avid supporter of cultural and sporting events; the 40th anniversary of her coronation was celebrated in May. She was awarded an Honorary Doctorate from Waikato University in 1973, and an Honorary Doctor of Laws from Victoria University in 1999. She was one of the first inductees of the Order of New Zealand when it was established in 1987. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe Order of New Zealand was established more than a decade after  Dame Te Atairangikaahu  was awarded an Honorary Doctorate from Waikato University\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: entailment -> 275682 (5902.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Saturday in Massachusetts, just off I-91, in a restaurant, in the men's room, on the red plastic screen that covers the urinal drain, were emblazoned these  Say No To Drugs. #Hypothesis: There is a restaurant off I-91 in Massachusetts. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: snli | Label: entailment -> 380226 (6369.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A person is riding a unicycle on the street with many other people around. #Hypothesis: A person is outside.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: neutral -> 69708 (4164.76)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Right-sided dual lead pacemaker is unchanged in appearance. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMild pulmonary edema, mild cardiomegaly, mild bibasilar atelectasis.\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: neutral -> 7486 (2131.74)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No organomegaly or masses. #Hypothesis:  Abdominal exam is normal \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: neutral -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The patient was unable to offer a review of systems. #Hypothesis:  Patient has AF\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: neutral -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Multiple consultations were performed. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m Patient has multiple disease\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: neutral -> 562 (762.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Elevation of the right hemidiaphragm is unchanged. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is a dobhoff coursing below the diaphragm, however the tip is not visualized.\u001b[0m\n",
      "\u001b[1mSource: anli | Label: neutral -> 141850 (5012.51)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Suntaragaali (Kannada: ಸುಂಟರಗಾಳಿ ) is a 2006 Indian Kannada romantic action film directed by Sadhu Kokila and written by Ranganath. Besides direction, Sadhu Kokila has composed the music and also enacted in a supporting role. The main cast includes Darshan, Rakshita and Ashish Vidyarthi besides Seetha, Umashree and Rangayana Raghu in other pivotal roles. #Hypothesis: Ranganath spent one and a half years writing for the film.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: neutral -> 274304 (5895.82)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: This was a reference to Dukakis' search for a vice-presidential candidate. #Hypothesis: Dukakis was searching for a Vice Presidential candidate. He has not found one he backs 100%.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: snli | Label: neutral -> 378436 (6362.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A woman in a yellow jacket holding a red umbrella. #Hypothesis: The woman shields herself from the rain with a red umbrella.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "----\n",
      "Number of RadNLI test samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No focal consolidation, pleural effusion or pneumothorax is identified. #Hypothesis: No focal infiltrate is identified.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The cardiomediastinal and hilar contours are normal. #Hypothesis: The cardiomediastinal and hilar contours are unchanged.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: No overt pulmonary edema is seen. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo bony abnormalities are seen.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The lungs are mostly clear aside from mild lower lung atelectasis. #Hypothesis: The lungs are clear bilaterally.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: mild interstitial edema has worsened. #Hypothesis: mild interstitial edema has improved.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset...\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2metadata dataset\u001b[0m\n",
      "Loaded 19989 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\n",
      "Loaded 19948 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2M: left PICC line terminating at left subclavian/brachiocephalic vein\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"anatomical location\": \"left subclavian/brachiocephalic vein\", \"detailed observation\": \"PICC line terminating at left subclavian/brachiocephalic vein\", \"short observation\": \"PICC line terminating at left subclavian/brachiocephalic vein\", \"category\": \"tubes and lines\", \"health status\": \"unknown\", \"prev_study_comparison?\": \"no\", \"comparison status\": \"\"}\u001b[0m\n",
      "Number of train examples: 59921\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2comparison dataset\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Added 341589 paraphrased inputs (total 443344)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft-sided pneumothorax of moderate size\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2C: Prominent osteophytic bridging in the thoracic region of the spine\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "Counter:\n",
      "Counter({'no comparison': 276665, 'stable/unchanged': 46121, 'worsened': 27557, 'resolved': 26871, 'improved': 18519, 'progressed': 10316, 'new finding': 8625, 'larger': 5687, 'increase': 5642, 'decrease': 5399, 'smaller': 4471, 'unclear comparison': 3230, 'position changed': 2691, 'reappeared': 1522, 'other': 28})\n",
      "Output: no comparison, train size: 276665, weight: 326.80615211646756\n",
      "Output: worsened, train size: 27557, weight: 217.56637091187034\n",
      "Output: resolved, train size: 26871, weight: 216.4948037154779\n",
      "Output: larger, train size: 5687, weight: 155.5870070153868\n",
      "Output: improved, train size: 18519, weight: 200.97934960420258\n",
      "Output: smaller, train size: 4471, weight: 147.0491363001147\n",
      "Output: progressed, train size: 10316, weight: 177.7581176401765\n",
      "Output: increase, train size: 5642, weight: 155.30121845247666\n",
      "Output: decrease, train size: 5399, weight: 153.7222195569894\n",
      "Output: unclear comparison, train size: 3230, weight: 135.8930734349483\n",
      "Output: new finding, train size: 8625, weight: 170.93754906650108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: stable/unchanged, train size: 46121, weight: 240.03726849161214\n",
      "Output: reappeared, train size: 1522, weight: 111.7619539554242\n",
      "Output: other, train size: 28, weight: 23.11066134663147\n",
      "Output: position changed, train size: 2691, weight: 129.8215650898703\n",
      "Number of train examples: 443344\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_observations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:20<00:00, 27622.07it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mAssociated consolidative opacification of the left lung base could be compressive atelectasis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"consolidation\", \"lung opacity\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mSmall amount of subcutaneous gas is seen in soft tissues.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"subcutaneous air\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNew trans subclavian right atrial right ventricular pacer leads continuous from the new left pectoral generator.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac pacer and wires\"]\u001b[0m\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\n",
      "Loaded 14859 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\n",
      "Loaded 19952 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\n",
      "Loaded 19959 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\n",
      "Loaded 9977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\n",
      "Loaded 9974 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\n",
      "Loaded 9987 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\n",
      "Added 103692 paraphrased inputs (total 749511)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mOngoing subtle opacification observed in the right lung base on the lateral aspect\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\"]\u001b[0m\n",
      "100%|███████████████████████████████| 749511/749511 [00:03<00:00, 211545.84it/s]\n",
      "Number of total examples: 749511\n",
      "Number of examples without labels: 160864\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 4498\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  metastatic tumor\u001b[0m\n",
      "\u001b[93m\u001b[1m  right upper lung lobe\u001b[0m\n",
      "\u001b[93m\u001b[1m  tube dislodgement\u001b[0m\n",
      "\u001b[93m\u001b[1m  vascular displacement\u001b[0m\n",
      "\u001b[93m\u001b[1m  Key decision makers\u001b[0m\n",
      "\u001b[93m\u001b[1m  pleural scarring\u001b[0m\n",
      "\u001b[93m\u001b[1m  Key risk indicators\u001b[0m\n",
      "\u001b[93m\u001b[1m  contrast agent not visualized\u001b[0m\n",
      "\u001b[93m\u001b[1m  nodular opacity\u001b[0m\n",
      "\u001b[93m\u001b[1m  superior vena cava catheter placement\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: lung opacity, train size: 306147, weight: 6052.314258424769\n",
      "Label: None, train size: 160864, weight: 5173.661459041586\n",
      "Label: atelectasis, train size: 94030, weight: 4509.162252814013\n",
      "Label: pleural effusion, train size: 93022, weight: 4496.442365546256\n",
      "Label: pneumonia, train size: 46693, weight: 3731.7501716078773\n",
      "Label: pulmonary edema/hazy opacity, train size: 43454, weight: 3657.3900232811834\n",
      "Label: enlarged cardiac silhouette, train size: 35280, weight: 3447.4421996052774\n",
      "Label: enteric tube, train size: 27597, weight: 3210.498555548174\n",
      "Label: vascular congestion, train size: 25952, weight: 3152.9573291446445\n",
      "Label: consolidation, train size: 25260, weight: 3127.8725669115815\n",
      "Label: lung lesion, train size: 23572, weight: 3064.285857407827\n",
      "Label: endotracheal tube, train size: 21905, weight: 2997.8020284788163\n",
      "Label: pleural/parenchymal scarring, train size: 18682, weight: 2856.8572521836454\n",
      "Label: low lung volumes, train size: 18588, weight: 2852.463878874611\n",
      "Label: pneumothorax, train size: 16796, weight: 2765.12204817844\n",
      "Label: mass/nodule (not otherwise specified), train size: 14471, weight: 2640.017501159535\n",
      "Label: picc, train size: 14283, weight: 2629.221328708754\n",
      "Label: cardiac pacer and wires, train size: 13389, weight: 2576.2889815362387\n",
      "Label: ij line, train size: 12405, weight: 2514.6981737246747\n",
      "Label: lobar/segmental collapse, train size: 12371, weight: 2512.5021455815854\n",
      "Label: aspiration, train size: 12238, weight: 2503.8658589102106\n",
      "Label: linear/patchy atelectasis, train size: 12101, weight: 2494.8922039241074\n",
      "Label: chest tube, train size: 11749, weight: 2471.46349448124\n",
      "Label: airspace opacity, train size: 11717, weight: 2469.306345996421\n",
      "Label: enlarged hilum, train size: 11714, weight: 2469.103875848375\n",
      "Label: rib fracture, train size: 9956, weight: 2342.7541486499636\n",
      "Label: hyperaeration, train size: 8180, weight: 2195.9279342345876\n",
      "Label: mediastinal widening, train size: 7463, weight: 2129.5314346971195\n",
      "Label: chest port, train size: 7009, weight: 2084.8845125773137\n",
      "Label: copd/emphysema, train size: 6983, weight: 2082.260542425887\n",
      "Label: costophrenic angle blunting, train size: 6952, weight: 2079.1220606210954\n",
      "Label: vascular calcification, train size: 6944, weight: 2078.310371948699\n",
      "Label: tortuous aorta, train size: 6438, weight: 2025.4354983318656\n",
      "Label: fluid overload/heart failure, train size: 6402, weight: 2021.5528120628337\n",
      "Label: elevated hemidiaphragm, train size: 6133, weight: 1991.9943635549255\n",
      "Label: infiltration, train size: 5736, weight: 1946.4900159268673\n",
      "Label: mediastinal displacement, train size: 5220, weight: 1883.5769616626842\n",
      "Label: subcutaneous air, train size: 5140, weight: 1873.4003520541203\n",
      "Label: increased reticular markings/ild pattern, train size: 5109, weight: 1869.4242864055232\n",
      "Label: multiple masses/nodules, train size: 4868, weight: 1837.866541377119\n",
      "Label: spinal fracture, train size: 4604, weight: 1801.8953640063596\n",
      "Label: subclavian line, train size: 4440, weight: 1778.7494591002173\n",
      "Label: interstitial lung disease, train size: 4264, weight: 1753.173688697151\n",
      "Label: pigtail catheter, train size: 4257, weight: 1752.1399805275955\n",
      "Label: spinal degenerative changes, train size: 4162, weight: 1737.9816177147438\n",
      "Label: superior mediastinal mass/enlargement, train size: 4157, weight: 1737.2296621177059\n",
      "Label: hernia, train size: 3762, weight: 1675.527028301087\n",
      "Label: vascular redistribution, train size: 3623, weight: 1652.6453368828752\n",
      "Label: calcified nodule, train size: 3469, weight: 1626.5059586547704\n",
      "Label: sub-diaphragmatic air, train size: 3423, weight: 1618.5283906143927\n",
      "Label: tracheostomy tube, train size: 3391, weight: 1612.9309556389703\n",
      "Label: bone lesion, train size: 3358, weight: 1607.116732365291\n",
      "Label: scoliosis, train size: 3061, weight: 1552.7507585367864\n",
      "Label: granulomatous disease, train size: 2943, weight: 1530.0471242872911\n",
      "Label: swan-ganz catheter, train size: 2894, weight: 1520.4184683791714\n",
      "Label: prosthetic valve, train size: 2572, weight: 1453.9094023139187\n",
      "Label: lung cancer, train size: 2555, weight: 1450.2289229189291\n",
      "Label: alveolar hemorrhage, train size: 2530, weight: 1444.7831652398547\n",
      "Label: rotated, train size: 2258, weight: 1382.7784896739577\n",
      "Label: shoulder osteoarthritis, train size: 2196, weight: 1367.8758349752577\n",
      "Label: bronchiectasis, train size: 2124, weight: 1350.1735399073136\n",
      "Label: cabg grafts, train size: 1959, weight: 1307.8676309222412\n",
      "Label: pericardial effusion, train size: 1814, weight: 1268.4657612483188\n",
      "Label: hydropneumothorax, train size: 1631, weight: 1215.294185273704\n",
      "Label: artifact, train size: 1623, weight: 1212.872238143207\n",
      "Label: breast/nipple shadows, train size: 1557, weight: 1192.5514618266216\n",
      "Label: clavicle fracture, train size: 1556, weight: 1192.2387865976527\n",
      "Label: cyst/bullae, train size: 1475, weight: 1166.411761557198\n",
      "Label: pneumomediastinum, train size: 1358, weight: 1127.223892522407\n",
      "Label: intra-aortic balloon pump, train size: 931, weight: 959.3546703073852\n",
      "Label: goiter, train size: 906, weight: 947.9406351817632\n",
      "Label: mediastinal drain, train size: 904, weight: 947.0179422691563\n",
      "Label: aortic graft/repair, train size: 775, weight: 884.1976644686547\n",
      "Label: diaphragmatic eventration (benign), train size: 406, weight: 650.6631467873593\n",
      "Label: skin fold, train size: 311, weight: 567.8220950583348\n",
      "--------\n",
      "Number of train examples: 1270062\n",
      "Number of train datasets: 75\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: deficit in all components of the blood\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Widespread parenchymal opacities appear to be unchanged/slightly worsened since the prior study, most likely consistent with pulmonary edema, continued surveillance is recommended.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"airspace opacity\", \"lung opacity\", \"pulmonary edema/hazy opacity\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Right-sided pleural effusion is now large, slightly increased.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\", \"pleural effusion\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Cardiac contour cannot be assessed due to obscuration of the left heart border but appears stable.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_anatomical_locations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:12<00:00, 45886.58it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mLung volume is high consistent with COPD.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mInterval worsening of diffuse bilateral pulmonary opacifications consistent with the clinical diagnosis of pneumonia.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mPleural opacity posteriorly on the lateral radiograph likely corresponds to an area of known malignant pleural thickening on recent CT torso of ___.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "Loaded 24929 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\n",
      "Loaded 24951 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\n",
      "Loaded 24988 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\n",
      "Added 352702 paraphrased inputs (total 983681)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mfluid buildup in the space between the lungs and chest wall\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left costophrenic angle\", \"right costophrenic angle\"]\u001b[0m\n",
      "100%|███████████████████████████████| 983681/983681 [00:05<00:00, 194584.60it/s]\n",
      "Number of total examples: 983681\n",
      "Number of examples without labels: 96958\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 24482\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  pulmonary veins\u001b[0m\n",
      "\u001b[93m\u001b[1m  valve\u001b[0m\n",
      "\u001b[93m\u001b[1m  high image quality\u001b[0m\n",
      "\u001b[93m\u001b[1m  left mediastinum\u001b[0m\n",
      "\u001b[93m\u001b[1m  left upper quadrant\u001b[0m\n",
      "\u001b[93m\u001b[1m  chest radiograph\u001b[0m\n",
      "\u001b[93m\u001b[1m  lower mid abdomen\u001b[0m\n",
      "\u001b[93m\u001b[1m  clinical setting\u001b[0m\n",
      "\u001b[93m\u001b[1m  addendum\u001b[0m\n",
      "\u001b[93m\u001b[1m  PA\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: right lung, train size: 349137, weight: 6243.158334871754\n",
      "Label: left lung, train size: 347284, weight: 6235.3525090961175\n",
      "Label: mediastinum, train size: 144031, weight: 5031.875282788735\n",
      "Label: left lower lung zone, train size: 139395, weight: 4990.409355647508\n",
      "Label: right lower lung zone, train size: 133961, weight: 4940.320692958888\n",
      "Label: right hilar structures, train size: 104637, weight: 4636.605540005738\n",
      "Label: left hilar structures, train size: 99507, weight: 4576.371609488107\n",
      "Label: cardiac silhouette, train size: 97597, weight: 4553.288558553895\n",
      "Label: None, train size: 96958, weight: 4545.482695622845\n",
      "Label: left costophrenic angle, train size: 87782, weight: 4428.425926894693\n",
      "Label: right costophrenic angle, train size: 85867, weight: 4402.731970876394\n",
      "Label: abdomen, train size: 69685, weight: 4164.391638360652\n",
      "Label: neck, train size: 68283, weight: 4141.664068017979\n",
      "Label: right mid lung zone, train size: 42058, weight: 3623.944216201201\n",
      "Label: right chest wall, train size: 39937, weight: 3571.3605151829925\n",
      "Label: left chest wall, train size: 39710, weight: 3565.5994137333387\n",
      "Label: left mid lung zone, train size: 39072, weight: 3549.263144482792\n",
      "Label: trachea, train size: 37858, weight: 3517.5713767379216\n",
      "Label: spine, train size: 36967, weight: 3493.7833910652507\n",
      "Label: right upper lung zone, train size: 31930, weight: 3349.8346375050337\n",
      "Label: upper mediastinum, train size: 31127, weight: 3325.2150888629853\n",
      "Label: left hemidiaphragm, train size: 26182, weight: 3161.175801242095\n",
      "Label: left upper lung zone, train size: 24852, weight: 3112.8230688218005\n",
      "Label: right hemidiaphragm, train size: 22859, weight: 3036.325975604692\n",
      "Label: svc, train size: 22448, weight: 3019.8900361258507\n",
      "Label: aortic arch, train size: 21171, weight: 2967.237668771257\n",
      "Label: right apical zone, train size: 20033, weight: 2918.1322541597774\n",
      "Label: left apical zone, train size: 17367, weight: 2793.7252100239602\n",
      "Label: carina, train size: 17254, weight: 2788.1246491411166\n",
      "Label: right atrium, train size: 15526, weight: 2698.6228163775186\n",
      "Label: right shoulder, train size: 13652, weight: 2592.1436672609016\n",
      "Label: right clavicle, train size: 12056, weight: 2491.9272027167312\n",
      "Label: left clavicle, train size: 11934, weight: 2483.844699572688\n",
      "Label: left shoulder, train size: 10455, weight: 2380.2892980476154\n",
      "Label: cavoatrial junction, train size: 7297, weight: 2113.457361673451\n",
      "Label: left arm, train size: 6546, weight: 2036.9838564325023\n",
      "Label: right arm, train size: 6398, weight: 2021.1203630444604\n",
      "Label: left breast, train size: 3000, weight: 1541.097763435584\n",
      "Label: right breast, train size: 2328, weight: 1399.2439717598722\n",
      "--------\n",
      "Number of train examples: 2384141\n",
      "Number of train datasets: 39\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Significant nodular lesions in the lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: fibrotic string in the left costophrenic angle\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left costophrenic angle\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: FINDINGS: In comparison with study of ___, mild enlargement of the cardiac silhouette persists, though there has been substantial decrease in the degree of pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"left hilar structures\", \"left lung\", \"right hilar structures\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: bilateral linear markings present on the chest X-ray\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing mlm dataset\u001b[0m\n",
      "Number of general sentences: 1377099\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 1377099\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:06<00:00, 207534.24it/s]\n",
      "\tlen(valid_tokens): 21522\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:10<00:00, 137224.29it/s]\n",
      "Number of sentences: 1348333\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: a [tok1] performing street music .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] man\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: If the request [tok1] only one House , [tok2] will work with the requester to seek support for the [tok3] from either ( 1 ) the senior leaders of the affected House or\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] affects [tok2] GAO [tok3] request\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: The best of [tok1] 's spicy cuisine , including saltfish and ackee , rice and peas , fish in coconut [tok2] , and Escovitch fish .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] Jamaica [tok2] milk\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: [tok1] hey sure oh absolutely\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] oh\u001b[0m\n",
      "Number of medical sentences: 2923224\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 2923224\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 2923224/2923224 [00:11<00:00, 250152.06it/s]\n",
      "\tlen(valid_tokens): 8333\n",
      "100%|█████████████████████████████| 2923224/2923224 [00:19<00:00, 148743.13it/s]\n",
      "Number of sentences: 2894485\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: There is a persistent small to moderate pleural effusion which is [tok1] stable in size from ___ CT [tok2] .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] overall [tok2] scan\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: mild mediastinal [tok1] likely related to [tok2] lung volumes\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] widening [tok2] low\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: [tok1] 's central reference point\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] neck\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A vague rounded opacity in the right base is [tok1] with a nipple [tok2] .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] consistent [tok2] shadow\u001b[0m\n",
      "----------------------------------------\n",
      "Number of train datasets: 7\n",
      "Number of val datasets: 1\n",
      "----------------------------------------\n",
      "Examples of val datasets:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Cardiac silhouette size is normal. #Hypothesis: The cardiac silhouette remains enlarged.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: stable size of right apical pneumothorax. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35munchanged size of right apical pneumothorax.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Right mid and lower lung consolidations are unchanged. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere has been worsening of the consolidation involving the right mid and lower lung fields.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: right lower lobe consolidation has only slightly improved. #Hypothesis: right lower lobe consolidation has only slightly increased.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_trainer.name =  multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_222821_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_222821_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_80_s2s_loss=0.5498.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_135046_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/checkpoint_80_s2s_loss=0.5498.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_222821_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.75518, s2s_loss 0.84936, 60.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86005, 2.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.5379.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.70597, s2s_loss 0.74470, 59.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85945, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.5413.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.83635, s2s_loss 0.59864, 58.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86158, 2.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.5460.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.45561, s2s_loss 0.54428, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86543, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_s2s_loss=0.5472.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.36701, s2s_loss 0.53387, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86414, 2.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_s2s_loss=0.5480.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.61253, s2s_loss 0.53313, 59.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86354, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.5482.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.44642, s2s_loss 0.53036, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86273, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.5485.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.43836, s2s_loss 0.52949, 59.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86300, 2.43 secs\n",
      "\u001b[1m---- Epoch 9/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.51549, s2s_loss 0.53515, 59.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86282, 2.46 secs\n",
      "\u001b[1m---- Epoch 10/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.81726, s2s_loss 0.53682, 43.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86288, 2.46 secs\n",
      "\u001b[1m---- Epoch 11/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.70425, s2s_loss 0.54591, 54.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86289, 2.37 secs\n",
      "\u001b[1m---- Epoch 12/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43437, s2s_loss 0.52392, 58.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86289, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_s2s_loss=0.5487.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.65118, s2s_loss 0.52143, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86078, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_s2s_loss=0.5494.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.58045, s2s_loss 0.53240, 58.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86177, 2.44 secs\n",
      "\u001b[1m---- Epoch 15/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.49442, s2s_loss 0.53445, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86040, 2.49 secs\n",
      "\u001b[1m---- Epoch 16/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.64495, s2s_loss 0.52544, 59.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86090, 2.46 secs\n",
      "\u001b[1m---- Epoch 17/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.37274, s2s_loss 0.52995, 58.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86101, 2.48 secs\n",
      "\u001b[1m---- Epoch 18/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.43731, s2s_loss 0.53049, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86097, 2.48 secs\n",
      "\u001b[1m---- Epoch 19/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.73822, s2s_loss 0.52291, 59.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86097, 2.43 secs\n",
      "\u001b[1m---- Epoch 20/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.83780, s2s_loss 0.52452, 59.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86106, 2.43 secs\n",
      "\u001b[1m---- Epoch 21/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.56564, s2s_loss 0.52485, 58.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86136, 2.38 secs\n",
      "\u001b[1m---- Epoch 22/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.68201, s2s_loss 0.51467, 58.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86271, 2.43 secs\n",
      "\u001b[1m---- Epoch 23/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.69224, s2s_loss 0.51739, 59.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86389, 2.45 secs\n",
      "\u001b[1m---- Epoch 24/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.46889, s2s_loss 0.52184, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86395, 2.44 secs\n",
      "\u001b[1m---- Epoch 25/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.53197, s2s_loss 0.53757, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86307, 2.43 secs\n",
      "\u001b[1m---- Epoch 26/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.53821, s2s_loss 0.52046, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86324, 2.42 secs\n",
      "\u001b[1m---- Epoch 27/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.72719, s2s_loss 0.52964, 58.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86316, 2.45 secs\n",
      "\u001b[1m---- Epoch 28/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.54878, s2s_loss 0.52175, 58.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86316, 2.45 secs\n",
      "\u001b[1m---- Epoch 29/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37748, s2s_loss 0.52732, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86010, 2.43 secs\n",
      "\u001b[1m---- Epoch 30/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.50423, s2s_loss 0.52134, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86001, 2.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_s2s_loss=0.5496.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.61022, s2s_loss 0.52625, 59.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85744, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_s2s_loss=0.5501.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.90235, s2s_loss 0.52378, 59.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85845, 2.43 secs\n",
      "\u001b[1m---- Epoch 33/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.44029, s2s_loss 0.52321, 58.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85828, 2.45 secs\n",
      "\u001b[1m---- Epoch 34/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.33780, s2s_loss 0.52734, 59.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85865, 2.44 secs\n",
      "\u001b[1m---- Epoch 35/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.84378, s2s_loss 0.52401, 59.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85896, 2.44 secs\n",
      "\u001b[1m---- Epoch 36/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.53177, s2s_loss 0.51294, 58.64 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.85901, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_36_s2s_loss=0.5502.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 37/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.63460, s2s_loss 0.50989, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86539, 2.46 secs\n",
      "\u001b[1m---- Epoch 38/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29641, s2s_loss 0.51276, 59.37 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86377, 2.44 secs\n",
      "\u001b[1m---- Epoch 39/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.79106, s2s_loss 0.52666, 59.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86250, 2.46 secs\n",
      "\u001b[1m---- Epoch 40/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35947, s2s_loss 0.51964, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86219, 2.47 secs\n",
      "\u001b[1m---- Epoch 41/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.28551, s2s_loss 0.51948, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86169, 2.44 secs\n",
      "\u001b[1m---- Epoch 42/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.58601, s2s_loss 0.51504, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86154, 2.43 secs\n",
      "\u001b[1m---- Epoch 43/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.25416, s2s_loss 0.52549, 59.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86136, 2.44 secs\n",
      "\u001b[1m---- Epoch 44/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.61829, s2s_loss 0.52132, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86145, 2.43 secs\n",
      "\u001b[1m---- Epoch 45/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.30429, s2s_loss 0.51631, 59.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86717, 2.43 secs\n",
      "\u001b[1m---- Epoch 46/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63283, s2s_loss 0.52779, 58.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86317, 2.45 secs\n",
      "\u001b[1m---- Epoch 47/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.73272, s2s_loss 0.53068, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86302, 2.45 secs\n",
      "\u001b[1m---- Epoch 48/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.64634, s2s_loss 0.51821, 59.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86226, 2.48 secs\n",
      "\u001b[1m---- Epoch 49/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.34349, s2s_loss 0.52390, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86255, 2.42 secs\n",
      "\u001b[1m---- Epoch 50/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.52880, s2s_loss 0.52241, 59.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86236, 2.44 secs\n",
      "\u001b[1m---- Epoch 51/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.72609, s2s_loss 0.51287, 59.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86235, 2.44 secs\n",
      "\u001b[1m---- Epoch 52/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43354, s2s_loss 0.52852, 59.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86227, 2.44 secs\n",
      "\u001b[1m---- Epoch 53/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37992, s2s_loss 0.51102, 58.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85823, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_53_s2s_loss=0.5505.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 54/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30533, s2s_loss 0.51519, 58.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86174, 2.42 secs\n",
      "\u001b[1m---- Epoch 55/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.54148, s2s_loss 0.51264, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86085, 2.42 secs\n",
      "\u001b[1m---- Epoch 56/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55594, s2s_loss 0.52119, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86058, 2.46 secs\n",
      "\u001b[1m---- Epoch 57/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.31004, s2s_loss 0.51517, 58.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85990, 2.43 secs\n",
      "\u001b[1m---- Epoch 58/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41839, s2s_loss 0.51000, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86016, 2.46 secs\n",
      "\u001b[1m---- Epoch 59/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.33879, s2s_loss 0.51574, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86013, 2.46 secs\n",
      "\u001b[1m---- Epoch 60/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.68248, s2s_loss 0.51974, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86017, 2.42 secs\n",
      "\u001b[1m---- Epoch 61/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.70226, s2s_loss 0.51298, 59.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86740, 2.45 secs\n",
      "\u001b[1m---- Epoch 62/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63135, s2s_loss 0.51776, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86575, 2.48 secs\n",
      "\u001b[1m---- Epoch 63/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.44553, s2s_loss 0.50439, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86418, 2.44 secs\n",
      "\u001b[1m---- Epoch 64/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.36841, s2s_loss 0.51152, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86454, 2.44 secs\n",
      "\u001b[1m---- Epoch 65/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.58206, s2s_loss 0.52306, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86409, 2.44 secs\n",
      "\u001b[1m---- Epoch 66/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.61149, s2s_loss 0.51230, 59.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86346, 2.46 secs\n",
      "\u001b[1m---- Epoch 67/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.78121, s2s_loss 0.51041, 59.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86321, 2.43 secs\n",
      "\u001b[1m---- Epoch 68/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46779, s2s_loss 0.51604, 59.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86317, 2.45 secs\n",
      "\u001b[1m---- Epoch 69/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.71181, s2s_loss 0.50718, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86330, 2.43 secs\n",
      "\u001b[1m---- Epoch 70/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.84594, s2s_loss 0.52099, 58.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85929, 2.46 secs\n",
      "\u001b[1m---- Epoch 71/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.62040, s2s_loss 0.51554, 58.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85837, 2.45 secs\n",
      "\u001b[1m---- Epoch 72/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55998, s2s_loss 0.50221, 59.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85823, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_72_s2s_loss=0.5509.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 73/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.53124, s2s_loss 0.50242, 59.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85830, 2.43 secs\n",
      "\u001b[1m---- Epoch 74/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.67144, s2s_loss 0.50886, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85828, 2.44 secs\n",
      "\u001b[1m---- Epoch 75/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.52243, s2s_loss 0.50950, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85830, 2.44 secs\n",
      "\u001b[1m---- Epoch 76/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.45547, s2s_loss 0.50983, 58.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85826, 2.45 secs\n",
      "\u001b[1m---- Epoch 77/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.57368, s2s_loss 0.51891, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85711, 2.45 secs\n",
      "\u001b[1m---- Epoch 78/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.53093, s2s_loss 0.52437, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85721, 2.45 secs\n",
      "\u001b[1m---- Epoch 79/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.52845, s2s_loss 0.51695, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85823, 2.43 secs\n",
      "\u001b[1m---- Epoch 80/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.53788, s2s_loss 0.50745, 58.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85859, 2.46 secs\n",
      "\u001b[1m---- Epoch 81/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.31182, s2s_loss 0.50571, 58.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85826, 2.43 secs\n",
      "\u001b[1m---- Epoch 82/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32123, s2s_loss 0.51049, 59.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85849, 2.45 secs\n",
      "\u001b[1m---- Epoch 83/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.51811, s2s_loss 0.50631, 59.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85856, 2.45 secs\n",
      "\u001b[1m---- Epoch 84/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40386, s2s_loss 0.50147, 44.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85853, 2.46 secs\n",
      "\u001b[1m---- Epoch 85/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.68002, s2s_loss 0.51911, 41.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85990, 2.43 secs\n",
      "\u001b[1m---- Epoch 86/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.65647, s2s_loss 0.50811, 41.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85852, 2.43 secs\n",
      "\u001b[1m---- Epoch 87/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41849, s2s_loss 0.50133, 51.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85858, 2.44 secs\n",
      "\u001b[1m---- Epoch 88/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.39358, s2s_loss 0.50747, 57.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85857, 2.45 secs\n",
      "\u001b[1m---- Epoch 89/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.67676, s2s_loss 0.50114, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85846, 2.46 secs\n",
      "\u001b[1m---- Epoch 90/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.56400, s2s_loss 0.50858, 58.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85845, 2.43 secs\n",
      "\u001b[1m---- Epoch 91/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32885, s2s_loss 0.50496, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85816, 2.45 secs\n",
      "\u001b[1m---- Epoch 92/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37164, s2s_loss 0.50033, 58.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85810, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_92_s2s_loss=0.5510.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 93/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.58648, s2s_loss 0.51823, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85530, 2.44 secs\n",
      "\u001b[1m---- Epoch 94/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.45163, s2s_loss 0.51098, 58.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85575, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_94_s2s_loss=0.5512.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 95/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.61115, s2s_loss 0.50302, 54.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85755, 2.46 secs\n",
      "\u001b[1m---- Epoch 96/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55693, s2s_loss 0.52014, 59.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85680, 2.42 secs\n",
      "\u001b[1m---- Epoch 97/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.74206, s2s_loss 0.52244, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85633, 2.44 secs\n",
      "\u001b[1m---- Epoch 98/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.39239, s2s_loss 0.50781, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85602, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_98_s2s_loss=0.5512.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 99/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.45978, s2s_loss 0.50260, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85588, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_99_s2s_loss=0.5515.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 100/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46379, s2s_loss 0.52021, 48.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85582, 2.46 secs\n",
      "\u001b[1m---- Epoch 101/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.26729, s2s_loss 0.50841, 59.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85258, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_101_s2s_loss=0.5521.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 102/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31662, s2s_loss 0.51468, 59.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85401, 2.46 secs\n",
      "\u001b[1m---- Epoch 103/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.69202, s2s_loss 0.50744, 58.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85551, 2.44 secs\n",
      "\u001b[1m---- Epoch 104/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.52425, s2s_loss 0.51087, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85507, 2.45 secs\n",
      "\u001b[1m---- Epoch 105/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.59452, s2s_loss 0.50780, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85532, 2.46 secs\n",
      "\u001b[1m---- Epoch 106/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31243, s2s_loss 0.51410, 59.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85513, 2.43 secs\n",
      "\u001b[1m---- Epoch 107/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46594, s2s_loss 0.51433, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85485, 2.44 secs\n",
      "\u001b[1m---- Epoch 108/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43263, s2s_loss 0.50781, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85481, 2.45 secs\n",
      "\u001b[1m---- Epoch 109/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.39096, s2s_loss 0.50729, 41.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85705, 2.45 secs\n",
      "\u001b[1m---- Epoch 110/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.44261, s2s_loss 0.51572, 41.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85728, 2.48 secs\n",
      "\u001b[1m---- Epoch 111/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.23914, s2s_loss 0.50546, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85505, 2.44 secs\n",
      "\u001b[1m---- Epoch 112/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.34353, s2s_loss 0.50738, 58.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85669, 2.46 secs\n",
      "\u001b[1m---- Epoch 113/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54001, s2s_loss 0.50440, 58.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85643, 2.42 secs\n",
      "\u001b[1m---- Epoch 114/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50833, s2s_loss 0.50259, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85649, 2.43 secs\n",
      "\u001b[1m---- Epoch 115/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.29537, s2s_loss 0.51116, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85628, 2.44 secs\n",
      "\u001b[1m---- Epoch 116/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.70244, s2s_loss 0.51853, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85629, 2.59 secs\n",
      "\u001b[1m---- Epoch 117/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44931, s2s_loss 0.51553, 59.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85563, 2.45 secs\n",
      "\u001b[1m---- Epoch 118/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43964, s2s_loss 0.49921, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85434, 2.46 secs\n",
      "\u001b[1m---- Epoch 119/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.34970, s2s_loss 0.49275, 59.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85571, 2.45 secs\n",
      "\u001b[1m---- Epoch 120/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.49136, s2s_loss 0.50082, 58.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85538, 2.45 secs\n",
      "\u001b[1m---- Epoch 121/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.45245, s2s_loss 0.50108, 58.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85519, 2.46 secs\n",
      "\u001b[1m---- Epoch 122/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.58959, s2s_loss 0.50097, 58.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85538, 2.45 secs\n",
      "\u001b[1m---- Epoch 123/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.25833, s2s_loss 0.50928, 58.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85523, 2.46 secs\n",
      "\u001b[1m---- Epoch 124/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38461, s2s_loss 0.49991, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85513, 2.44 secs\n",
      "\u001b[1m---- Epoch 125/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.50824, s2s_loss 0.50801, 53.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85234, 2.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_125_s2s_loss=0.5522.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 126/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.55151, s2s_loss 0.50805, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85005, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_126_s2s_loss=0.5528.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 127/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.42724, s2s_loss 0.50059, 58.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85185, 2.46 secs\n",
      "\u001b[1m---- Epoch 128/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48556, s2s_loss 0.50984, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85221, 2.43 secs\n",
      "\u001b[1m---- Epoch 129/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39056, s2s_loss 0.50913, 51.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85246, 2.46 secs\n",
      "\u001b[1m---- Epoch 130/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.57450, s2s_loss 0.50138, 48.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85233, 2.43 secs\n",
      "\u001b[1m---- Epoch 131/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.85847, s2s_loss 0.49532, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85225, 2.47 secs\n",
      "\u001b[1m---- Epoch 132/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.30699, s2s_loss 0.51342, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85231, 2.44 secs\n",
      "\u001b[1m---- Epoch 133/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.58126, s2s_loss 0.50143, 58.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85094, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_133_s2s_loss=0.5528.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 134/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33607, s2s_loss 0.50477, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84711, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_134_s2s_loss=0.5537.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 135/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.33502, s2s_loss 0.50382, 58.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84900, 2.45 secs\n",
      "\u001b[1m---- Epoch 136/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.69583, s2s_loss 0.49916, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85056, 2.44 secs\n",
      "\u001b[1m---- Epoch 137/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.52100, s2s_loss 0.49105, 49.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85044, 2.54 secs\n",
      "\u001b[1m---- Epoch 138/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.28526, s2s_loss 0.50971, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85044, 2.44 secs\n",
      "\u001b[1m---- Epoch 139/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31315, s2s_loss 0.50450, 59.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85028, 2.48 secs\n",
      "\u001b[1m---- Epoch 140/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.49908, s2s_loss 0.50889, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85019, 2.43 secs\n",
      "\u001b[1m---- Epoch 141/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.46675, s2s_loss 0.49454, 59.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84897, 2.47 secs\n",
      "\u001b[1m---- Epoch 142/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37025, s2s_loss 0.50722, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85351, 2.40 secs\n",
      "\u001b[1m---- Epoch 143/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.66260, s2s_loss 0.50709, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85194, 2.45 secs\n",
      "\u001b[1m---- Epoch 144/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.56658, s2s_loss 0.49996, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85099, 2.45 secs\n",
      "\u001b[1m---- Epoch 145/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.56376, s2s_loss 0.50629, 57.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85076, 2.42 secs\n",
      "\u001b[1m---- Epoch 146/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.47155, s2s_loss 0.51342, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85084, 2.44 secs\n",
      "\u001b[1m---- Epoch 147/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.52936, s2s_loss 0.50615, 58.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85085, 2.45 secs\n",
      "\u001b[1m---- Epoch 148/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.33175, s2s_loss 0.50430, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85084, 2.45 secs\n",
      "\u001b[1m---- Epoch 149/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.28528, s2s_loss 0.50419, 59.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85179, 2.46 secs\n",
      "\u001b[1m---- Epoch 150/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.52246, s2s_loss 0.50914, 58.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85175, 2.43 secs\n",
      "\u001b[1m---- Epoch 151/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.35152, s2s_loss 0.49334, 58.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85213, 2.44 secs\n",
      "\u001b[1m---- Epoch 152/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.60927, s2s_loss 0.51081, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85317, 2.39 secs\n",
      "\u001b[1m---- Epoch 153/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.51320, s2s_loss 0.50502, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85379, 2.44 secs\n",
      "\u001b[1m---- Epoch 154/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45707, s2s_loss 0.50682, 58.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85390, 2.42 secs\n",
      "\u001b[1m---- Epoch 155/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.47150, s2s_loss 0.51381, 59.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85395, 2.46 secs\n",
      "\u001b[1m---- Epoch 156/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.54746, s2s_loss 0.50941, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85399, 2.42 secs\n",
      "\u001b[1m---- Epoch 157/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.48618, s2s_loss 0.50762, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85627, 2.43 secs\n",
      "\u001b[1m---- Epoch 158/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.26256, s2s_loss 0.50124, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85030, 2.45 secs\n",
      "\u001b[1m---- Epoch 159/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.46429, s2s_loss 0.50250, 44.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84812, 2.43 secs\n",
      "\u001b[1m---- Epoch 160/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29058, s2s_loss 0.49971, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84934, 2.46 secs\n",
      "\u001b[1m---- Epoch 161/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.55797, s2s_loss 0.50219, 58.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84978, 2.44 secs\n",
      "\u001b[1m---- Epoch 162/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45056, s2s_loss 0.50230, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84999, 2.47 secs\n",
      "\u001b[1m---- Epoch 163/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32055, s2s_loss 0.51142, 59.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85013, 2.44 secs\n",
      "\u001b[1m---- Epoch 164/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.57972, s2s_loss 0.51128, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85020, 2.44 secs\n",
      "\u001b[1m---- Epoch 165/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.29354, s2s_loss 0.48942, 58.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85282, 2.46 secs\n",
      "\u001b[1m---- Epoch 166/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.81878, s2s_loss 0.50497, 58.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85204, 2.42 secs\n",
      "\u001b[1m---- Epoch 167/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.46037, s2s_loss 0.50564, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84991, 2.42 secs\n",
      "\u001b[1m---- Epoch 168/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.38130, s2s_loss 0.50436, 58.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84964, 2.46 secs\n",
      "\u001b[1m---- Epoch 169/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.66099, s2s_loss 0.48925, 59.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84940, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_169_s2s_loss=0.5538.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 170/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.02233, s2s_loss 0.50346, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84945, 2.46 secs\n",
      "\u001b[1m---- Epoch 171/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.56917, s2s_loss 0.50443, 58.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84966, 2.45 secs\n",
      "\u001b[1m---- Epoch 172/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41402, s2s_loss 0.49912, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84964, 2.44 secs\n",
      "\u001b[1m---- Epoch 173/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40383, s2s_loss 0.49506, 58.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85225, 2.48 secs\n",
      "\u001b[1m---- Epoch 174/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37784, s2s_loss 0.49162, 59.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85180, 2.47 secs\n",
      "\u001b[1m---- Epoch 175/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.75776, s2s_loss 0.49806, 56.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84810, 2.45 secs\n",
      "\u001b[1m---- Epoch 176/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55150, s2s_loss 0.50490, 52.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84874, 2.46 secs\n",
      "\u001b[1m---- Epoch 177/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.95413, s2s_loss 0.50600, 59.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84845, 2.45 secs\n",
      "\u001b[1m---- Epoch 178/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.46608, s2s_loss 0.49690, 58.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84863, 2.44 secs\n",
      "\u001b[1m---- Epoch 179/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63163, s2s_loss 0.50244, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84871, 2.48 secs\n",
      "\u001b[1m---- Epoch 180/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.58862, s2s_loss 0.50864, 59.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84867, 2.44 secs\n",
      "\u001b[1m---- Epoch 181/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.48261, s2s_loss 0.49961, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84256, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_181_s2s_loss=0.5551.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 182/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.73200, s2s_loss 0.49529, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84737, 2.43 secs\n",
      "\u001b[1m---- Epoch 183/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.35270, s2s_loss 0.49794, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84874, 2.46 secs\n",
      "\u001b[1m---- Epoch 184/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.47070, s2s_loss 0.50066, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84811, 2.44 secs\n",
      "\u001b[1m---- Epoch 185/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.28834, s2s_loss 0.50513, 58.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84888, 2.44 secs\n",
      "\u001b[1m---- Epoch 186/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.52995, s2s_loss 0.49719, 58.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84929, 2.43 secs\n",
      "\u001b[1m---- Epoch 187/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.30430, s2s_loss 0.50123, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84901, 2.45 secs\n",
      "\u001b[1m---- Epoch 188/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.20995, s2s_loss 0.50230, 59.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84893, 2.49 secs\n",
      "\u001b[1m---- Epoch 189/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.66552, s2s_loss 0.49147, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84537, 2.46 secs\n",
      "\u001b[1m---- Epoch 190/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.44585, s2s_loss 0.50243, 58.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84474, 2.51 secs\n",
      "\u001b[1m---- Epoch 191/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41988, s2s_loss 0.48894, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84577, 2.49 secs\n",
      "\u001b[1m---- Epoch 192/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.67252, s2s_loss 0.50103, 56.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84621, 2.43 secs\n",
      "\u001b[1m---- Epoch 193/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41385, s2s_loss 0.49411, 41.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84581, 2.43 secs\n",
      "\u001b[1m---- Epoch 194/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.51755, s2s_loss 0.50060, 41.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84593, 2.43 secs\n",
      "\u001b[1m---- Epoch 195/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.51189, s2s_loss 0.50055, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84593, 2.43 secs\n",
      "\u001b[1m---- Epoch 196/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.92614, s2s_loss 0.49754, 59.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84582, 2.44 secs\n",
      "\u001b[1m---- Epoch 197/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.52144, s2s_loss 0.49931, 58.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84697, 2.45 secs\n",
      "\u001b[1m---- Epoch 198/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.80886, s2s_loss 0.50523, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84575, 2.42 secs\n",
      "\u001b[1m---- Epoch 199/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.38315, s2s_loss 0.50251, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84779, 2.48 secs\n",
      "\u001b[1m---- Epoch 200/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.66860, s2s_loss 0.49896, 59.37 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84860, 2.44 secs\n",
      "\u001b[1m---- Epoch 201/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.59300, s2s_loss 0.49661, 58.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84811, 2.45 secs\n",
      "\u001b[1m---- Epoch 202/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62332, s2s_loss 0.49067, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84815, 2.45 secs\n",
      "\u001b[1m---- Epoch 203/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28016, s2s_loss 0.50562, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84818, 2.43 secs\n",
      "\u001b[1m---- Epoch 204/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37576, s2s_loss 0.49537, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84819, 2.45 secs\n",
      "\u001b[1m---- Epoch 205/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.69551, s2s_loss 0.49412, 59.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84880, 2.45 secs\n",
      "\u001b[1m---- Epoch 206/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33046, s2s_loss 0.49153, 58.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84845, 2.44 secs\n",
      "\u001b[1m---- Epoch 207/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.29421, s2s_loss 0.49672, 52.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84660, 2.46 secs\n",
      "\u001b[1m---- Epoch 208/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.52712, s2s_loss 0.49137, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84804, 2.47 secs\n",
      "\u001b[1m---- Epoch 209/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47795, s2s_loss 0.49724, 43.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84812, 2.43 secs\n",
      "\u001b[1m---- Epoch 210/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.36002, s2s_loss 0.49458, 49.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84809, 2.43 secs\n",
      "\u001b[1m---- Epoch 211/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.47716, s2s_loss 0.49586, 41.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84823, 2.43 secs\n",
      "\u001b[1m---- Epoch 212/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.66954, s2s_loss 0.49531, 54.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84821, 2.44 secs\n",
      "\u001b[1m---- Epoch 213/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.68223, s2s_loss 0.49846, 59.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84459, 2.45 secs\n",
      "\u001b[1m---- Epoch 214/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40405, s2s_loss 0.50161, 59.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84487, 2.44 secs\n",
      "\u001b[1m---- Epoch 215/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41276, s2s_loss 0.49320, 59.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84569, 2.49 secs\n",
      "\u001b[1m---- Epoch 216/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.52772, s2s_loss 0.49625, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84609, 2.45 secs\n",
      "\u001b[1m---- Epoch 217/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63471, s2s_loss 0.48920, 58.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84611, 2.50 secs\n",
      "\u001b[1m---- Epoch 218/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.26034, s2s_loss 0.48918, 57.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84607, 2.43 secs\n",
      "\u001b[1m---- Epoch 219/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43589, s2s_loss 0.50037, 59.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84617, 2.46 secs\n",
      "\u001b[1m---- Epoch 220/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32473, s2s_loss 0.50094, 58.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84619, 2.44 secs\n",
      "\u001b[1m---- Epoch 221/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.52518, s2s_loss 0.49363, 58.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84911, 2.42 secs\n",
      "\u001b[1m---- Epoch 222/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.28955, s2s_loss 0.49310, 59.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84990, 2.44 secs\n",
      "\u001b[1m---- Epoch 223/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.43952, s2s_loss 0.49142, 58.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84959, 2.42 secs\n",
      "\u001b[1m---- Epoch 224/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.59908, s2s_loss 0.49747, 59.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84834, 2.46 secs\n",
      "\u001b[1m---- Epoch 225/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.56023, s2s_loss 0.48967, 59.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84892, 2.46 secs\n",
      "\u001b[1m---- Epoch 226/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.68515, s2s_loss 0.49817, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84850, 2.46 secs\n",
      "\u001b[1m---- Epoch 227/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40946, s2s_loss 0.49975, 59.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84827, 2.44 secs\n",
      "\u001b[1m---- Epoch 228/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63060, s2s_loss 0.48593, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84823, 2.44 secs\n",
      "\u001b[1m---- Epoch 229/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.61666, s2s_loss 0.50142, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84343, 2.44 secs\n",
      "\u001b[1m---- Epoch 230/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40536, s2s_loss 0.49476, 58.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84317, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_230_s2s_loss=0.5552.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 231/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.43067, s2s_loss 0.50090, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84426, 2.44 secs\n",
      "\u001b[1m---- Epoch 232/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.67399, s2s_loss 0.49093, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84360, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_232_s2s_loss=0.5552.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 233/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "loss 0.44867, s2s_loss 0.49573, 59.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84439, 2.44 secs\n",
      "\u001b[1m---- Epoch 234/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35603, s2s_loss 0.50499, 58.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84441, 2.45 secs\n",
      "\u001b[1m---- Epoch 235/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.66409, s2s_loss 0.49323, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84444, 2.43 secs\n",
      "\u001b[1m---- Epoch 236/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.30357, s2s_loss 0.49825, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84456, 2.46 secs\n",
      "\u001b[1m---- Epoch 237/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.31890, s2s_loss 0.49247, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85257, 2.44 secs\n",
      "\u001b[1m---- Epoch 238/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.83763, s2s_loss 0.49740, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85148, 2.42 secs\n",
      "\u001b[1m---- Epoch 239/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.61761, s2s_loss 0.49308, 58.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84746, 2.46 secs\n",
      "\u001b[1m---- Epoch 240/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.71398, s2s_loss 0.49236, 58.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84835, 2.43 secs\n",
      "\u001b[1m---- Epoch 241/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.59533, s2s_loss 0.49203, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84901, 2.46 secs\n",
      "\u001b[1m---- Epoch 242/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63394, s2s_loss 0.49366, 53.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84868, 2.44 secs\n",
      "\u001b[1m---- Epoch 243/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.21413, s2s_loss 0.49543, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84829, 2.46 secs\n",
      "\u001b[1m---- Epoch 244/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.35124, s2s_loss 0.49677, 59.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84827, 2.49 secs\n",
      "\u001b[1m---- Epoch 245/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35603, s2s_loss 0.49285, 58.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84746, 2.42 secs\n",
      "\u001b[1m---- Epoch 246/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30137, s2s_loss 0.49928, 58.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84498, 2.43 secs\n",
      "\u001b[1m---- Epoch 247/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.37060, s2s_loss 0.49613, 42.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84102, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_247_s2s_loss=0.5557.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 248/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.50054, s2s_loss 0.49608, 59.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84178, 2.43 secs\n",
      "\u001b[1m---- Epoch 249/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39373, s2s_loss 0.49248, 58.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84249, 2.48 secs\n",
      "\u001b[1m---- Epoch 250/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63120, s2s_loss 0.49549, 56.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84233, 2.47 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_135046_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\" \\\n",
    "--epochs 250 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 20 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--task_name \"multitask\" \\\n",
    "--experiment_name \"s2f+f2m+f2c+s2co+s2cal+nli+mlm\" \\\n",
    "--multitask_name_list \\\n",
    "\"nli\" \\\n",
    "\"sentence2facts\" \\\n",
    "\"fact2metadata\" \\\n",
    "\"fact2comparison\" \\\n",
    "\"sentence2chestimagenome_observations\" \\\n",
    "\"sentence2chestimagenome_anatomical_locations\" \\\n",
    "\"mlm\" \\\n",
    "--task2weight '{\"sentence2facts\": 1.0, \"fact2metadata\": 1.0, \"fact2comparison\": 0.3, \"sentence2chestimagenome_observations\": 1.0, \"sentence2chestimagenome_anatomical_locations\": 1.0, \"nli\": 7.0, \"mlm\": 5.0}' \\\n",
    "--sentence_to_facts_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\"\\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\" \\\n",
    "--fact_to_metadata_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--fact_to_comparison_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "--chest_imagenome_phrases2labels_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\" \\\n",
    "--chest_imagenome_obs_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--chest_imagenome_anatloc_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--use_sentence2facts_for_nli \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--paraphrased_inputs_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--only_validate_nli \\\n",
    "--seq2seq_model_name \"t5\" \\\n",
    "--t5_model_name \"t5-small\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 300\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_222821_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 20\n",
      "   override_lr: False\n",
      "   task_name: multitask\n",
      "   experiment_name: s2f+f2m+f2c+s2co+s2cal+nli+mlm\n",
      "   multitask_name_list: ['nli', 'sentence2facts', 'fact2metadata', 'fact2comparison', 'sentence2chestimagenome_observations', 'sentence2chestimagenome_anatomical_locations', 'mlm']\n",
      "   task2weight: {'sentence2facts': 1.0, 'fact2metadata': 1.0, 'fact2comparison': 0.3, 'sentence2chestimagenome_observations': 1.0, 'sentence2chestimagenome_anatomical_locations': 1.0, 'nli': 7.0, 'mlm': 5.0}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_phrases2labels_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl']\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl']\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl']\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(188008,24487305).jsonl\n",
      "   use_sentence2facts_for_nli: True\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   only_validate_nli: True\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 20, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9891 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mB\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mbilateral\u001b[0m\n",
      "\u001b[1m\u001b[35mboth sides\u001b[0m\n",
      "\u001b[1m\u001b[35mboth\u001b[0m\n",
      "\u001b[1m\u001b[35mon both sides\u001b[0m\n",
      "\u001b[1m\u001b[35msymmetrical\u001b[0m\n",
      "\u001b[1m\u001b[35mequally on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mpresent on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mseen on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35moccurring on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mappearing on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mfound on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mseen bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mobserved bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mdetected bilaterally\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9890 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilus\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the hilum on the left side\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 8511 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary vasculature\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral lung vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary blood vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mlung periphery vasculature\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature of the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels in the outer regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular network in the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary circulation\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels supplying the outer areas of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature in the peripheral regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular system in the outer regions of the lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 1864 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35ma\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mone\u001b[0m\n",
      "\u001b[1m\u001b[35msingle\u001b[0m\n",
      "\u001b[1m\u001b[35msole\u001b[0m\n",
      "\u001b[1m\u001b[35monly\u001b[0m\n",
      "\u001b[1m\u001b[35mindividual\u001b[0m\n",
      "\u001b[1m\u001b[35mlone\u001b[0m\n",
      "\u001b[1m\u001b[35msolitary\u001b[0m\n",
      "\u001b[1m\u001b[35munique\u001b[0m\n",
      "\u001b[1m\u001b[35mdistinct\u001b[0m\n",
      "\u001b[1m\u001b[35msingular\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 10000 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9999 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9997 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9994 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9996 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome not detected\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome is not observed\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant constriction\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of notable narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of substantial narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of notable constriction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19943 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of wheezing\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing detected\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing observed\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing present\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing found\u001b[0m\n",
      "--------\n",
      "\u001b[1mNumber of unique inputs: 190085\u001b[0m\n",
      "\u001b[1mNumber of total paraphrases: 2093173\u001b[0m\n",
      "Number of medical sentences from paraphrases: 2039914\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2facts dataset\u001b[0m\n",
      "Loaded 9999 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19971 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\n",
      "Loaded 14990 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\n",
      "Loaded 14991 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2F: The extent of leftward displacement of the trachea at the thoracic inlet is comparable to earlier studies, for example due to a tortuous innominate artery.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"extent of leftward displacement of the trachea at the thoracic inlet\", \"extent of leftward displacement of the trachea comparable to earlier studies\", \"extent of leftward displacement of the trachea due to a tortuous innominate artery\"]\u001b[0m\n",
      "Number of train examples: 84935\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing nli dataset\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(188008,24487305).jsonl...\n",
      "Number of samples: 188008\n",
      "Number of sources: 7\n",
      "----\n",
      "\u001b[1mLoading sentence2facts input/output pairs for NLI...\u001b[0m\n",
      "Number of entailment samples added: 368966\n",
      "----\n",
      "\u001b[1mLoading general domain datasets...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset...\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: contradiction -> 181976 (5334.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The feeding tube tip is clearly visible in the chest X-ray. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mFeeding tube tip well below diaphragm, not included on the radiograph.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: contradiction -> 9286 (2289.97)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Moderate loculated left pleural effusion unchanged since . #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mTrace left pleural effusion appears chronic.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: contradiction -> 4376 (1769.54)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: No obvious pneumothorax identified. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mA small left pneumothorax is unchanged since the prior examination.\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: contradiction -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Catheterization showed 90% proximal LAD and was status post Hepacoat stent. #Hypothesis:  No history of cardiac disease \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: contradiction -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Head CT negative for acute bleed or mass. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m Head CT showed hemorrhage \u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: contradiction -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A MRA prior to discharge showed increased ........  of single and rector spinal muscles at T3-4 adjacent to facets and anterior within the right psoas. #Hypothesis:  the patient has a normal spine\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: contradiction -> 212 (461.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No pleural abnormality is seen. #Hypothesis: There are probable bilateral pleural effusions, right greater left along with right-sided atelectasis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: anli | Label: contradiction -> 88178 (4433.68)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Perhaps those who argue that the Nisga'a final agreement cannot be given full effect without first amending the Constitution of Canada just do not understand the process and do not understand the value of a negotiated reconciliation of aboriginal rights within the Canadian federation. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCanada is an empire, not a federation.\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: contradiction -> 274712 (5897.92)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The balance of outbound and inbound accounts would change from -$44. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThese types of accounts would be required for pandas as well.\u001b[0m\n",
      "\u001b[1mSource: snli | Label: contradiction -> 379404 (6365.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Four women are standing together, posing for a picture. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mWomen washing hair\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: entailment -> 43334 (3654.55)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No left-sided pleural effusion is seen, and there is no pneumothorax. #Hypothesis: No left-sided pleural effusion or pneumothorax is identified.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: entailment -> 6380 (2019.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: No pulmonary vascular congestion is identified. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe pulmonary vasculature does not show signs of congestion.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: entailment -> 3540 (1638.66)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Slight improvement of multifocal consolidations. #Hypothesis: Slight improvement of multifocal consolidations.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: entailment -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Wheezes at the time were noted which decreased with Combivent nebulizer treatment in the Emergency Room. #Hypothesis:  The patient has symptoms of a asthma exacerbation. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: entailment -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: 47yo M with PMHx metastatic colon CA to liver and lungs, hereditary telangiectasias complicated by chronic nosebleeds, a TIA, and a large pulmonary AVM repaired in [**1-/3145**] who presented on [**3151-4-7**] with chest and calf pain and fever. #Hypothesis:  Patient has colon neoplasm\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: entailment -> 946 (966.10)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: He developed tachycardia again the morning of [**4-18**] (which was treated with 5 mg IV metoprolol) and po diltiazem was also started. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m Patient has been given a beta blocker\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: entailment -> 186 (428.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: There is no pneumothorax. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mSpecifically, no evidence of appreciable pneumothorax.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSource: s2f | Label: entailment -> 368966 (6324.57)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The lungs are well expanded and clear with linear opacities in the right lower lung which is unchanged and may reflect an accessory fissure with surrounding atelectasis. #Hypothesis: clear lungs\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: anli | Label: entailment -> 108502 (4680.39)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: good idea<br>Melody had been given a gift card for her birthday. She could not decide if she should use it or sell it for cash. She really did not need the gift card for anything. She decided to hold on to the card. Later, she used the gift card to buy a gift for someone else. #Hypothesis: Melody did not use her birthday present right away\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: entailment -> 275682 (5902.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: On a governmentwide basis, Congress, under the bi-partisan leadership of this Committee and the House Government Reform Committee, has established a statutory framework consisting of requirements for goal-setting and performance measurement, financial management, and information technology management, all aimed at improving the performance, management, and accountability of the federal government. #Hypothesis: Congress has established a statutory framework in order to improve the overall performance of the federal government.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: snli | Label: entailment -> 380226 (6369.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Man on stage in shirt that says fun performing. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThis man is on a stage\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: neutral -> 69708 (4164.76)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Since the right PIC line has been withdrawn to the origin of the right brachiocephalic vein. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mKnown extensive right lung cancer with subsequent atelectasis and consolidation.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: neutral -> 20426 (2935.34)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No pneumothorax or pleural effusion is evident. #Hypothesis: The lungs appear clear bilaterally without focal consolidation, effusion, or pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: neutral -> 7936 (2173.86)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is no pleural effusion elsewhere. #Hypothesis: No pneumothorax or appreciable pleural effusion is seen.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: neutral -> 7486 (2131.74)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: He denies currently chest pain, dyspnea, dizziness. #Hypothesis:  He has normal cardiac function\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: neutral -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: She progressed to a vaginal delivery. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient had an epidural. \u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: neutral -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: HISTORY OF PRESENT ILLNESS:  The patient is a 54 year old male with endstage renal disease secondary to type 1 diabetes who presents for kidney transplant from wife. #Hypothesis:  patient has diabetic neuropathy\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: neutral -> 562 (762.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is no evidence of pneumothorax. #Hypothesis: There is no pleural effusion, pneumothorax, or pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: anli | Label: neutral -> 141850 (5012.51)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Nuance was an American dance music/freestyle group. It was formed by the producer and arranger, Ron Dean Miller, and featured Vikki Love on vocals. They charted three hits on the US \"Billboard\" Hot Dance Music/Club Play chart in the 1980s, including \"Loveride,\" which hit #1 in 1985. The same track peaked at #59 in the UK Singles Chart in January 1985. #Hypothesis: Nuance was an American dance music/freestyle group that hated their manager\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: neutral -> 274304 (5895.82)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: You can enter Temple Bar from Dame Street, or from Fleet Street (off Westmoreland), or you can walk through Merchants Arch, opposite the picturesque Ha'penny Bridge, into Temple Bar Square. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe only way to get to Temple Bar Square is via Merchants' Arch.\u001b[0m\n",
      "\u001b[1mSource: snli | Label: neutral -> 378436 (6362.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A child in red and black winter clothes hiding behind a snow fort. #Hypothesis: A child is playing outside and is cold.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "----\n",
      "Number of RadNLI test samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There are no acute osseous abnormalities. #Hypothesis: No acute bony abnormalities are seen.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: No pneumothorax or pleural effusion is present. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is no pulmonary edema or left pleural effusion.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: subsequent decrease of and atelectasis at the right lung bases. #Hypothesis: subsequent worsening of and atelectasis at the right lung bases.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: moderate left lower lobe atelectasis has worsened. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mmoderate left lower lobe atelectasis has increased.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No pneumothorax seen. #Hypothesis: No focal consolidation, pleural effusion, or pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset...\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2metadata dataset\u001b[0m\n",
      "Loaded 19989 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\n",
      "Loaded 19948 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2M: worsening of the airspace features in the bilateral lower lobe compared to the prior radiographs taken on the same day at 02:34\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"anatomical location\": \"bilateral lower lobe\", \"detailed observation\": \"worsening of the airspace features\", \"short observation\": \"worsening of airspace features\", \"category\": \"anatomical finding\", \"health status\": \"abnormal\", \"prev_study_comparison?\": \"yes\", \"comparison status\": \"worsened\"}\u001b[0m\n",
      "Number of train examples: 59921\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2comparison dataset\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Added 341589 paraphrased inputs (total 443344)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is a minimal amount of air trapped in the pleural space on the lateral aspect of the chest\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2C: Blurry heightened radiopacity in the region of the right hilum\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "Counter:\n",
      "Counter({'no comparison': 276665, 'stable/unchanged': 46121, 'worsened': 27557, 'resolved': 26871, 'improved': 18519, 'progressed': 10316, 'new finding': 8625, 'larger': 5687, 'increase': 5642, 'decrease': 5399, 'smaller': 4471, 'unclear comparison': 3230, 'position changed': 2691, 'reappeared': 1522, 'other': 28})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: no comparison, train size: 276665, weight: 326.80615211646756\n",
      "Output: worsened, train size: 27557, weight: 217.56637091187034\n",
      "Output: resolved, train size: 26871, weight: 216.4948037154779\n",
      "Output: larger, train size: 5687, weight: 155.5870070153868\n",
      "Output: improved, train size: 18519, weight: 200.97934960420258\n",
      "Output: smaller, train size: 4471, weight: 147.0491363001147\n",
      "Output: progressed, train size: 10316, weight: 177.7581176401765\n",
      "Output: increase, train size: 5642, weight: 155.30121845247666\n",
      "Output: decrease, train size: 5399, weight: 153.7222195569894\n",
      "Output: unclear comparison, train size: 3230, weight: 135.8930734349483\n",
      "Output: new finding, train size: 8625, weight: 170.93754906650108\n",
      "Output: stable/unchanged, train size: 46121, weight: 240.03726849161214\n",
      "Output: reappeared, train size: 1522, weight: 111.7619539554242\n",
      "Output: other, train size: 28, weight: 23.11066134663147\n",
      "Output: position changed, train size: 2691, weight: 129.8215650898703\n",
      "Number of train examples: 443344\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_observations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:20<00:00, 27547.09it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mA subtle patchy opacity is present in the right infrahilar region, obscuring a very small portion of the right heart border and associated with a corresponding opacity overlying the heart on the lateral view.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mRight-sided chest tube is unchanged in position terminating in the right basal pleural space.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"chest tube\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mRe- demonstrated linear opacity extending laterally from the left hilum, most consistent with atelectasis and/or scarring.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"linear/patchy atelectasis\", \"lung opacity\", \"pleural/parenchymal scarring\"]\u001b[0m\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\n",
      "Loaded 14859 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\n",
      "Loaded 19952 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\n",
      "Loaded 19959 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\n",
      "Loaded 9977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\n",
      "Loaded 9974 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\n",
      "Loaded 9987 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\n",
      "Added 103692 paraphrased inputs (total 749511)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCardiac failure with edema, consistent with the previous report\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"fluid overload/heart failure\"]\u001b[0m\n",
      "100%|███████████████████████████████| 749511/749511 [00:03<00:00, 211775.10it/s]\n",
      "Number of total examples: 749511\n",
      "Number of examples without labels: 160864\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 4498\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  shunt displacement\u001b[0m\n",
      "\u001b[93m\u001b[1m  Key performance indicators\u001b[0m\n",
      "\u001b[93m\u001b[1m  pulmonary edema\u001b[0m\n",
      "\u001b[93m\u001b[1m  cavoatrial junction\u001b[0m\n",
      "\u001b[93m\u001b[1m  right upper lobe\u001b[0m\n",
      "\u001b[93m\u001b[1m  skin infection\u001b[0m\n",
      "\u001b[93m\u001b[1m  post-operative changes\u001b[0m\n",
      "\u001b[93m\u001b[1m  central venous catheter repositioning\u001b[0m\n",
      "\u001b[93m\u001b[1m  shoulder injury\u001b[0m\n",
      "\u001b[93m\u001b[1m  right lower hemithorax\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: lung opacity, train size: 306147, weight: 6052.314258424769\n",
      "Label: None, train size: 160864, weight: 5173.661459041586\n",
      "Label: atelectasis, train size: 94030, weight: 4509.162252814013\n",
      "Label: pleural effusion, train size: 93022, weight: 4496.442365546256\n",
      "Label: pneumonia, train size: 46693, weight: 3731.7501716078773\n",
      "Label: pulmonary edema/hazy opacity, train size: 43454, weight: 3657.3900232811834\n",
      "Label: enlarged cardiac silhouette, train size: 35280, weight: 3447.4421996052774\n",
      "Label: enteric tube, train size: 27597, weight: 3210.498555548174\n",
      "Label: vascular congestion, train size: 25952, weight: 3152.9573291446445\n",
      "Label: consolidation, train size: 25260, weight: 3127.8725669115815\n",
      "Label: lung lesion, train size: 23572, weight: 3064.285857407827\n",
      "Label: endotracheal tube, train size: 21905, weight: 2997.8020284788163\n",
      "Label: pleural/parenchymal scarring, train size: 18682, weight: 2856.8572521836454\n",
      "Label: low lung volumes, train size: 18588, weight: 2852.463878874611\n",
      "Label: pneumothorax, train size: 16796, weight: 2765.12204817844\n",
      "Label: mass/nodule (not otherwise specified), train size: 14471, weight: 2640.017501159535\n",
      "Label: picc, train size: 14283, weight: 2629.221328708754\n",
      "Label: cardiac pacer and wires, train size: 13389, weight: 2576.2889815362387\n",
      "Label: ij line, train size: 12405, weight: 2514.6981737246747\n",
      "Label: lobar/segmental collapse, train size: 12371, weight: 2512.5021455815854\n",
      "Label: aspiration, train size: 12238, weight: 2503.8658589102106\n",
      "Label: linear/patchy atelectasis, train size: 12101, weight: 2494.8922039241074\n",
      "Label: chest tube, train size: 11749, weight: 2471.46349448124\n",
      "Label: airspace opacity, train size: 11717, weight: 2469.306345996421\n",
      "Label: enlarged hilum, train size: 11714, weight: 2469.103875848375\n",
      "Label: rib fracture, train size: 9956, weight: 2342.7541486499636\n",
      "Label: hyperaeration, train size: 8180, weight: 2195.9279342345876\n",
      "Label: mediastinal widening, train size: 7463, weight: 2129.5314346971195\n",
      "Label: chest port, train size: 7009, weight: 2084.8845125773137\n",
      "Label: copd/emphysema, train size: 6983, weight: 2082.260542425887\n",
      "Label: costophrenic angle blunting, train size: 6952, weight: 2079.1220606210954\n",
      "Label: vascular calcification, train size: 6944, weight: 2078.310371948699\n",
      "Label: tortuous aorta, train size: 6438, weight: 2025.4354983318656\n",
      "Label: fluid overload/heart failure, train size: 6402, weight: 2021.5528120628337\n",
      "Label: elevated hemidiaphragm, train size: 6133, weight: 1991.9943635549255\n",
      "Label: infiltration, train size: 5736, weight: 1946.4900159268673\n",
      "Label: mediastinal displacement, train size: 5220, weight: 1883.5769616626842\n",
      "Label: subcutaneous air, train size: 5140, weight: 1873.4003520541203\n",
      "Label: increased reticular markings/ild pattern, train size: 5109, weight: 1869.4242864055232\n",
      "Label: multiple masses/nodules, train size: 4868, weight: 1837.866541377119\n",
      "Label: spinal fracture, train size: 4604, weight: 1801.8953640063596\n",
      "Label: subclavian line, train size: 4440, weight: 1778.7494591002173\n",
      "Label: interstitial lung disease, train size: 4264, weight: 1753.173688697151\n",
      "Label: pigtail catheter, train size: 4257, weight: 1752.1399805275955\n",
      "Label: spinal degenerative changes, train size: 4162, weight: 1737.9816177147438\n",
      "Label: superior mediastinal mass/enlargement, train size: 4157, weight: 1737.2296621177059\n",
      "Label: hernia, train size: 3762, weight: 1675.527028301087\n",
      "Label: vascular redistribution, train size: 3623, weight: 1652.6453368828752\n",
      "Label: calcified nodule, train size: 3469, weight: 1626.5059586547704\n",
      "Label: sub-diaphragmatic air, train size: 3423, weight: 1618.5283906143927\n",
      "Label: tracheostomy tube, train size: 3391, weight: 1612.9309556389703\n",
      "Label: bone lesion, train size: 3358, weight: 1607.116732365291\n",
      "Label: scoliosis, train size: 3061, weight: 1552.7507585367864\n",
      "Label: granulomatous disease, train size: 2943, weight: 1530.0471242872911\n",
      "Label: swan-ganz catheter, train size: 2894, weight: 1520.4184683791714\n",
      "Label: prosthetic valve, train size: 2572, weight: 1453.9094023139187\n",
      "Label: lung cancer, train size: 2555, weight: 1450.2289229189291\n",
      "Label: alveolar hemorrhage, train size: 2530, weight: 1444.7831652398547\n",
      "Label: rotated, train size: 2258, weight: 1382.7784896739577\n",
      "Label: shoulder osteoarthritis, train size: 2196, weight: 1367.8758349752577\n",
      "Label: bronchiectasis, train size: 2124, weight: 1350.1735399073136\n",
      "Label: cabg grafts, train size: 1959, weight: 1307.8676309222412\n",
      "Label: pericardial effusion, train size: 1814, weight: 1268.4657612483188\n",
      "Label: hydropneumothorax, train size: 1631, weight: 1215.294185273704\n",
      "Label: artifact, train size: 1623, weight: 1212.872238143207\n",
      "Label: breast/nipple shadows, train size: 1557, weight: 1192.5514618266216\n",
      "Label: clavicle fracture, train size: 1556, weight: 1192.2387865976527\n",
      "Label: cyst/bullae, train size: 1475, weight: 1166.411761557198\n",
      "Label: pneumomediastinum, train size: 1358, weight: 1127.223892522407\n",
      "Label: intra-aortic balloon pump, train size: 931, weight: 959.3546703073852\n",
      "Label: goiter, train size: 906, weight: 947.9406351817632\n",
      "Label: mediastinal drain, train size: 904, weight: 947.0179422691563\n",
      "Label: aortic graft/repair, train size: 775, weight: 884.1976644686547\n",
      "Label: diaphragmatic eventration (benign), train size: 406, weight: 650.6631467873593\n",
      "Label: skin fold, train size: 311, weight: 567.8220950583348\n",
      "--------\n",
      "Number of train examples: 1270062\n",
      "Number of train datasets: 75\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: aortic calcifications may be found in the medial region\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"vascular calcification\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: A similar volume of peribronchial opacification in the left lower lobe has been present since at least ___, probably scarring or at least chronic atelectasis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"lung opacity\", \"pleural/parenchymal scarring\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: hazier right middle nodule\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\", \"mass/nodule (not otherwise specified)\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Bibasilar opacities have improved on the right and worsened on the left consistent with aspiration, pleural effusions and increasing atelectasis in the left lower lobe, also contribute to the radiologic findings.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"lung opacity\", \"pleural effusion\", \"aspiration\"]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_anatomical_locations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:12<00:00, 45946.78it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mRight lower lobe consolidation has slightly improved and is likely due to provided history of pneumonia.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"right lower lung zone\", \"right lung\", \"right mid lung zone\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mFINDINGS: As compared to the previous radiograph, there is unchanged cardiomegaly and unchanged appearance of the right internal jugular vein catheter.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"mediastinum\", \"neck\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe hilar and mediastinal contours are somewhat obscured by hazy patchy opacities which extend across both lungs, predominantly at the bases.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"left hilar structures\", \"left lower lung zone\", \"left lung\", \"mediastinum\", \"right hilar structures\", \"right lower lung zone\", \"right lung\"]\u001b[0m\n",
      "Loaded 24929 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\n",
      "Loaded 24951 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\n",
      "Loaded 24988 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\n",
      "Added 352702 paraphrased inputs (total 983681)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mthird wire from the top on the sternum\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"right chest wall\", \"left chest wall\"]\u001b[0m\n",
      "100%|███████████████████████████████| 983681/983681 [00:05<00:00, 193643.38it/s]\n",
      "Number of total examples: 983681\n",
      "Number of examples without labels: 96958\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 24482\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  right svc\u001b[0m\n",
      "\u001b[93m\u001b[1m  infracarinal area\u001b[0m\n",
      "\u001b[93m\u001b[1m  good resolution\u001b[0m\n",
      "\u001b[93m\u001b[1m  right heart\u001b[0m\n",
      "\u001b[93m\u001b[1m  mitral annulus\u001b[0m\n",
      "\u001b[93m\u001b[1m  retrosternal region\u001b[0m\n",
      "\u001b[93m\u001b[1m  left upper abdomen\u001b[0m\n",
      "\u001b[93m\u001b[1m  hiatal hernia\u001b[0m\n",
      "\u001b[93m\u001b[1m  respiratory insufficiency\u001b[0m\n",
      "\u001b[93m\u001b[1m  pleural gas\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: right lung, train size: 349137, weight: 6243.158334871754\n",
      "Label: left lung, train size: 347284, weight: 6235.3525090961175\n",
      "Label: mediastinum, train size: 144031, weight: 5031.875282788735\n",
      "Label: left lower lung zone, train size: 139395, weight: 4990.409355647508\n",
      "Label: right lower lung zone, train size: 133961, weight: 4940.320692958888\n",
      "Label: right hilar structures, train size: 104637, weight: 4636.605540005738\n",
      "Label: left hilar structures, train size: 99507, weight: 4576.371609488107\n",
      "Label: cardiac silhouette, train size: 97597, weight: 4553.288558553895\n",
      "Label: None, train size: 96958, weight: 4545.482695622845\n",
      "Label: left costophrenic angle, train size: 87782, weight: 4428.425926894693\n",
      "Label: right costophrenic angle, train size: 85867, weight: 4402.731970876394\n",
      "Label: abdomen, train size: 69685, weight: 4164.391638360652\n",
      "Label: neck, train size: 68283, weight: 4141.664068017979\n",
      "Label: right mid lung zone, train size: 42058, weight: 3623.944216201201\n",
      "Label: right chest wall, train size: 39937, weight: 3571.3605151829925\n",
      "Label: left chest wall, train size: 39710, weight: 3565.5994137333387\n",
      "Label: left mid lung zone, train size: 39072, weight: 3549.263144482792\n",
      "Label: trachea, train size: 37858, weight: 3517.5713767379216\n",
      "Label: spine, train size: 36967, weight: 3493.7833910652507\n",
      "Label: right upper lung zone, train size: 31930, weight: 3349.8346375050337\n",
      "Label: upper mediastinum, train size: 31127, weight: 3325.2150888629853\n",
      "Label: left hemidiaphragm, train size: 26182, weight: 3161.175801242095\n",
      "Label: left upper lung zone, train size: 24852, weight: 3112.8230688218005\n",
      "Label: right hemidiaphragm, train size: 22859, weight: 3036.325975604692\n",
      "Label: svc, train size: 22448, weight: 3019.8900361258507\n",
      "Label: aortic arch, train size: 21171, weight: 2967.237668771257\n",
      "Label: right apical zone, train size: 20033, weight: 2918.1322541597774\n",
      "Label: left apical zone, train size: 17367, weight: 2793.7252100239602\n",
      "Label: carina, train size: 17254, weight: 2788.1246491411166\n",
      "Label: right atrium, train size: 15526, weight: 2698.6228163775186\n",
      "Label: right shoulder, train size: 13652, weight: 2592.1436672609016\n",
      "Label: right clavicle, train size: 12056, weight: 2491.9272027167312\n",
      "Label: left clavicle, train size: 11934, weight: 2483.844699572688\n",
      "Label: left shoulder, train size: 10455, weight: 2380.2892980476154\n",
      "Label: cavoatrial junction, train size: 7297, weight: 2113.457361673451\n",
      "Label: left arm, train size: 6546, weight: 2036.9838564325023\n",
      "Label: right arm, train size: 6398, weight: 2021.1203630444604\n",
      "Label: left breast, train size: 3000, weight: 1541.097763435584\n",
      "Label: right breast, train size: 2328, weight: 1399.2439717598722\n",
      "--------\n",
      "Number of train examples: 2384141\n",
      "Number of train datasets: 39\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: FINDINGS: As compared to the previous radiograph, there is a very subtle but noticeable increase in interstitial structures.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: On the lateral view, there is a small rectangular-shaped radiopacity overlying the mid thoracic spine posteriorly, likely representing an overlying rib.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Milde congestive heart failure\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: unchanged area of consolidation at the base of the left lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lower lung zone\"]\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing mlm dataset\u001b[0m\n",
      "Number of general sentences: 1377099\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 1377099\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:06<00:00, 209898.37it/s]\n",
      "\tlen(valid_tokens): 21522\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:10<00:00, 137345.05it/s]\n",
      "Number of sentences: 1348333\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: The men are [tok1] a footrace .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] running\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A [tok1] dressed in [tok2] walking past a building that has a poster on display .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] woman [tok2] white\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Juliana Hatfield performed [tok1] on the album with her own music using various instruments that she played herself including [tok2] , piano , [tok3] , and guitar as well as the vocals for different voices .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] personally [tok2] drums [tok3] flute\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: The [tok1] has text on it\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] banner\u001b[0m\n",
      "Number of medical sentences: 2927736\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 2927736\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 2927736/2927736 [00:11<00:00, 247053.04it/s]\n",
      "\tlen(valid_tokens): 8333\n",
      "100%|█████████████████████████████| 2927736/2927736 [00:19<00:00, 149327.45it/s]\n",
      "Number of sentences: 2898987\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: costal margin of the diaphragm at the lateral [tok1]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] side\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: 9-mm pneumothorax at the left [tok1]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] apex\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: right supradiaphragmatic [tok1]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] area\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Smaller heart size observed [tok1] the presence of a large [tok2] effusion\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] following [tok2] pericardial\u001b[0m\n",
      "----------------------------------------\n",
      "Number of train datasets: 7\n",
      "Number of val datasets: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Examples of val datasets:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The chest is hyperinflated. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe lungs are clear.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: unchanged, tiny left apical pneumothorax. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew, tiny left apical pneumothorax.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The mediastinal and hilar contours are normal. #Hypothesis: Cardiac and mediastinal silhouettes are unremarkable.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Cardiomediastinal silhouette is stable. #Hypothesis: The cardiomediastinal silhouette is within normal limits.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "seq2seq_trainer.name =  multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_123953_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_123953_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_247_s2s_loss=0.5557.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_222821_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/checkpoint_247_s2s_loss=0.5557.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_123953_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.61690, s2s_loss 0.50841, 60.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84081, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.5552.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.44177, s2s_loss 0.50327, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83949, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.5558.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.70976, s2s_loss 0.50814, 58.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83585, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.5565.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.62567, s2s_loss 0.50603, 58.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83197, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_s2s_loss=0.5577.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.45558, s2s_loss 0.50786, 58.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83269, 2.45 secs\n",
      "\u001b[1m---- Epoch 6/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.48650, s2s_loss 0.51461, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82855, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.5582.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.38919, s2s_loss 0.51114, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82769, 2.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.5586.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.39144, s2s_loss 0.51942, 58.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82698, 2.44 secs\n",
      "\u001b[1m---- Epoch 9/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.72700, s2s_loss 0.51710, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82652, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.5587.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50557, s2s_loss 0.51553, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82621, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_s2s_loss=0.5588.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44156, s2s_loss 0.50577, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82609, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_s2s_loss=0.5593.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.35156, s2s_loss 0.50618, 58.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82612, 2.43 secs\n",
      "\u001b[1m---- Epoch 13/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.73803, s2s_loss 0.50259, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82349, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_s2s_loss=0.5601.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.50248, s2s_loss 0.50302, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82452, 2.43 secs\n",
      "\u001b[1m---- Epoch 15/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.44014, s2s_loss 0.50708, 57.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82161, 2.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_s2s_loss=0.5604.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.39689, s2s_loss 0.50662, 51.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82001, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_s2s_loss=0.5609.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.80066, s2s_loss 0.49513, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81983, 2.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_s2s_loss=0.5614.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32300, s2s_loss 0.49972, 58.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81965, 2.37 secs\n",
      "\u001b[1m---- Epoch 19/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.65284, s2s_loss 0.50234, 58.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81958, 2.41 secs\n",
      "\u001b[1m---- Epoch 20/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.52379, s2s_loss 0.49905, 59.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81952, 2.43 secs\n",
      "\u001b[1m---- Epoch 21/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44148, s2s_loss 0.50986, 46.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82782, 2.45 secs\n",
      "\u001b[1m---- Epoch 22/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38522, s2s_loss 0.50453, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82184, 2.44 secs\n",
      "\u001b[1m---- Epoch 23/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.60525, s2s_loss 0.51263, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81967, 2.40 secs\n",
      "\u001b[1m---- Epoch 24/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.51428, s2s_loss 0.51061, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81854, 2.39 secs\n",
      "\u001b[1m---- Epoch 25/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.45401, s2s_loss 0.50900, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81750, 2.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_s2s_loss=0.5615.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.56832, s2s_loss 0.50584, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81715, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_s2s_loss=0.5617.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.53463, s2s_loss 0.50779, 57.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81726, 2.40 secs\n",
      "\u001b[1m---- Epoch 28/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.47165, s2s_loss 0.50525, 58.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81731, 2.41 secs\n",
      "\u001b[1m---- Epoch 29/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.67928, s2s_loss 0.51035, 58.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81588, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_29_s2s_loss=0.5618.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 30/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.47336, s2s_loss 0.49436, 58.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81239, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_s2s_loss=0.5635.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.46368, s2s_loss 0.51303, 58.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81115, 2.45 secs\n",
      "\u001b[1m---- Epoch 32/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.50072, s2s_loss 0.50684, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81148, 2.42 secs\n",
      "\u001b[1m---- Epoch 33/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.33328, s2s_loss 0.50930, 58.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81191, 2.40 secs\n",
      "\u001b[1m---- Epoch 34/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.74935, s2s_loss 0.49307, 43.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81205, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_34_s2s_loss=0.5637.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 35/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.39239, s2s_loss 0.50443, 54.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81221, 2.41 secs\n",
      "\u001b[1m---- Epoch 36/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.53020, s2s_loss 0.50274, 58.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81228, 2.44 secs\n",
      "\u001b[1m---- Epoch 37/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.45033, s2s_loss 0.50854, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81721, 2.43 secs\n",
      "\u001b[1m---- Epoch 38/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.34771, s2s_loss 0.51037, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81581, 2.43 secs\n",
      "\u001b[1m---- Epoch 39/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.71577, s2s_loss 0.50070, 58.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81502, 2.42 secs\n",
      "\u001b[1m---- Epoch 40/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.44151, s2s_loss 0.50055, 59.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81385, 2.43 secs\n",
      "\u001b[1m---- Epoch 41/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.72858, s2s_loss 0.50755, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81345, 2.42 secs\n",
      "\u001b[1m---- Epoch 42/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45048, s2s_loss 0.51005, 58.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81323, 2.42 secs\n",
      "\u001b[1m---- Epoch 43/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "loss 0.51297, s2s_loss 0.51016, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81317, 2.42 secs\n",
      "\u001b[1m---- Epoch 44/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.30158, s2s_loss 0.50343, 58.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81302, 2.42 secs\n",
      "\u001b[1m---- Epoch 45/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40451, s2s_loss 0.50221, 58.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81116, 2.43 secs\n",
      "\u001b[1m---- Epoch 46/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.42203, s2s_loss 0.50959, 58.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80894, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_46_s2s_loss=0.5638.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 47/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.50076, s2s_loss 0.51618, 58.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80658, 2.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_s2s_loss=0.5641.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55716, s2s_loss 0.51690, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80754, 2.43 secs\n",
      "\u001b[1m---- Epoch 49/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.23990, s2s_loss 0.50986, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80773, 2.47 secs\n",
      "\u001b[1m---- Epoch 50/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.28806, s2s_loss 0.50432, 58.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80771, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_50_s2s_loss=0.5643.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 51/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.53601, s2s_loss 0.50623, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80764, 2.41 secs\n",
      "\u001b[1m---- Epoch 52/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.47111, s2s_loss 0.50271, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80753, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_52_s2s_loss=0.5645.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 53/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.48212, s2s_loss 0.50901, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80915, 2.41 secs\n",
      "\u001b[1m---- Epoch 54/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.49311, s2s_loss 0.50572, 59.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80632, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_54_s2s_loss=0.5647.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 55/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.45885, s2s_loss 0.51332, 57.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80609, 2.43 secs\n",
      "\u001b[1m---- Epoch 56/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48646, s2s_loss 0.49498, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80489, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_56_s2s_loss=0.5655.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 57/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39387, s2s_loss 0.49630, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80471, 2.37 secs\n",
      "\u001b[1m---- Epoch 58/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.34977, s2s_loss 0.50179, 58.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80479, 2.47 secs\n",
      "\u001b[1m---- Epoch 59/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.51622, s2s_loss 0.50405, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80484, 2.48 secs\n",
      "\u001b[1m---- Epoch 60/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46733, s2s_loss 0.49958, 57.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80495, 2.44 secs\n",
      "\u001b[1m---- Epoch 61/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.57603, s2s_loss 0.50262, 58.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81053, 2.48 secs\n",
      "\u001b[1m---- Epoch 62/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.34821, s2s_loss 0.51197, 59.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80621, 2.43 secs\n",
      "\u001b[1m---- Epoch 63/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.50717, s2s_loss 0.49702, 58.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80605, 2.44 secs\n",
      "\u001b[1m---- Epoch 64/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29241, s2s_loss 0.49627, 58.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80449, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_s2s_loss=0.5656.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38853, s2s_loss 0.50137, 59.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80460, 2.43 secs\n",
      "\u001b[1m---- Epoch 66/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45902, s2s_loss 0.49784, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80469, 2.43 secs\n",
      "\u001b[1m---- Epoch 67/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62979, s2s_loss 0.49984, 55.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80474, 2.45 secs\n",
      "\u001b[1m---- Epoch 68/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37372, s2s_loss 0.50199, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80475, 2.44 secs\n",
      "\u001b[1m---- Epoch 69/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33690, s2s_loss 0.50780, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80949, 2.45 secs\n",
      "\u001b[1m---- Epoch 70/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43866, s2s_loss 0.49567, 58.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80379, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_70_s2s_loss=0.5658.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 71/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.32273, s2s_loss 0.50220, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80015, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_71_s2s_loss=0.5665.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 72/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.85461, s2s_loss 0.50317, 56.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80002, 2.44 secs\n",
      "\u001b[1m---- Epoch 73/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.51759, s2s_loss 0.49998, 58.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80008, 2.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_73_s2s_loss=0.5666.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 74/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.37532, s2s_loss 0.50540, 58.74 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.80009, 2.43 secs\n",
      "\u001b[1m---- Epoch 75/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.42263, s2s_loss 0.50172, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79987, 2.44 secs\n",
      "\u001b[1m---- Epoch 76/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37111, s2s_loss 0.50298, 58.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79979, 2.46 secs\n",
      "\u001b[1m---- Epoch 77/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.74073, s2s_loss 0.49840, 56.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79961, 2.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_77_s2s_loss=0.5668.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 78/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.50820, s2s_loss 0.49891, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79626, 2.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_78_s2s_loss=0.5678.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 79/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.45675, s2s_loss 0.49866, 58.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79984, 2.42 secs\n",
      "\u001b[1m---- Epoch 80/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.58365, s2s_loss 0.51209, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79966, 2.45 secs\n",
      "\u001b[1m---- Epoch 81/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.50506, s2s_loss 0.49765, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80037, 2.43 secs\n",
      "\u001b[1m---- Epoch 82/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.12743, s2s_loss 0.48917, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80045, 2.47 secs\n",
      "\u001b[1m---- Epoch 83/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.73178, s2s_loss 0.49688, 55.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80021, 2.46 secs\n",
      "\u001b[1m---- Epoch 84/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63245, s2s_loss 0.48815, 59.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80016, 2.40 secs\n",
      "\u001b[1m---- Epoch 85/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.51604, s2s_loss 0.48916, 57.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79137, 2.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_85_s2s_loss=0.5696.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 86/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.35477, s2s_loss 0.49796, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79584, 2.44 secs\n",
      "\u001b[1m---- Epoch 87/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.45419, s2s_loss 0.50469, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79707, 2.39 secs\n",
      "\u001b[1m---- Epoch 88/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.57121, s2s_loss 0.49323, 58.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79734, 2.48 secs\n",
      "\u001b[1m---- Epoch 89/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38375, s2s_loss 0.50433, 59.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79698, 2.45 secs\n",
      "\u001b[1m---- Epoch 90/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.91324, s2s_loss 0.50059, 56.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79705, 2.44 secs\n",
      "\u001b[1m---- Epoch 91/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46395, s2s_loss 0.49436, 58.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79702, 2.47 secs\n",
      "\u001b[1m---- Epoch 92/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.86701, s2s_loss 0.50400, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79703, 2.43 secs\n",
      "\u001b[1m---- Epoch 93/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.58095, s2s_loss 0.49248, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79779, 2.42 secs\n",
      "\u001b[1m---- Epoch 94/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.59363, s2s_loss 0.50392, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79832, 2.44 secs\n",
      "\u001b[1m---- Epoch 95/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.77755, s2s_loss 0.51652, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79818, 2.40 secs\n",
      "\u001b[1m---- Epoch 96/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55957, s2s_loss 0.50088, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79789, 2.42 secs\n",
      "\u001b[1m---- Epoch 97/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54629, s2s_loss 0.50139, 58.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79780, 2.38 secs\n",
      "\u001b[1m---- Epoch 98/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.47807, s2s_loss 0.50848, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79741, 2.44 secs\n",
      "\u001b[1m---- Epoch 99/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.37314, s2s_loss 0.50764, 58.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79728, 2.38 secs\n",
      "\u001b[1m---- Epoch 100/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.66818, s2s_loss 0.49806, 53.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79732, 2.50 secs\n",
      "\u001b[1m---- Epoch 101/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35033, s2s_loss 0.49967, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79737, 2.43 secs\n",
      "\u001b[1m---- Epoch 102/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.46351, s2s_loss 0.50955, 58.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79655, 2.47 secs\n",
      "\u001b[1m---- Epoch 103/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.57414, s2s_loss 0.50022, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79648, 2.41 secs\n",
      "\u001b[1m---- Epoch 104/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.54185, s2s_loss 0.50179, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79673, 2.48 secs\n",
      "\u001b[1m---- Epoch 105/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.66562, s2s_loss 0.49629, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79570, 2.41 secs\n",
      "\u001b[1m---- Epoch 106/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.61501, s2s_loss 0.49737, 56.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79496, 2.44 secs\n",
      "\u001b[1m---- Epoch 107/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.34265, s2s_loss 0.50564, 56.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79471, 2.51 secs\n",
      "\u001b[1m---- Epoch 108/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.33590, s2s_loss 0.49504, 57.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79468, 2.45 secs\n",
      "\u001b[1m---- Epoch 109/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38309, s2s_loss 0.49503, 58.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79588, 2.50 secs\n",
      "\u001b[1m---- Epoch 110/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.39679, s2s_loss 0.50268, 57.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79663, 2.50 secs\n",
      "\u001b[1m---- Epoch 111/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.47407, s2s_loss 0.50304, 57.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79404, 2.42 secs\n",
      "\u001b[1m---- Epoch 112/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.49915, s2s_loss 0.50608, 57.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79405, 2.47 secs\n",
      "\u001b[1m---- Epoch 113/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38713, s2s_loss 0.50247, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79406, 2.50 secs\n",
      "\u001b[1m---- Epoch 114/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50016, s2s_loss 0.49914, 56.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79382, 2.42 secs\n",
      "\u001b[1m---- Epoch 115/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.33676, s2s_loss 0.50516, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79367, 2.47 secs\n",
      "\u001b[1m---- Epoch 116/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.56756, s2s_loss 0.49635, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79370, 2.48 secs\n",
      "\u001b[1m---- Epoch 117/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42823, s2s_loss 0.49352, 58.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79619, 2.50 secs\n",
      "\u001b[1m---- Epoch 118/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43861, s2s_loss 0.49838, 58.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79506, 2.46 secs\n",
      "\u001b[1m---- Epoch 119/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.32299, s2s_loss 0.50758, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79448, 2.47 secs\n",
      "\u001b[1m---- Epoch 120/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.31255, s2s_loss 0.50055, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79305, 2.47 secs\n",
      "\u001b[1m---- Epoch 121/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.65637, s2s_loss 0.51591, 57.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79302, 2.42 secs\n",
      "\u001b[1m---- Epoch 122/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.73792, s2s_loss 0.50097, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79306, 2.52 secs\n",
      "\u001b[1m---- Epoch 123/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.38130, s2s_loss 0.49987, 57.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79274, 2.46 secs\n",
      "\u001b[1m---- Epoch 124/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.27509, s2s_loss 0.49909, 54.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79269, 2.57 secs\n",
      "\u001b[1m---- Epoch 125/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.48692, s2s_loss 0.49617, 57.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79263, 2.54 secs\n",
      "\u001b[1m---- Epoch 126/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.52296, s2s_loss 0.49985, 57.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79065, 2.53 secs\n",
      "\u001b[1m---- Epoch 127/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41885, s2s_loss 0.50128, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78987, 2.47 secs\n",
      "\u001b[1m---- Epoch 128/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.50068, s2s_loss 0.49762, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79010, 2.41 secs\n",
      "\u001b[1m---- Epoch 129/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.68490, s2s_loss 0.49438, 56.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78956, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_129_s2s_loss=0.5698.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 130/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50074, s2s_loss 0.49939, 56.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78933, 2.45 secs\n",
      "\u001b[1m---- Epoch 131/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.68810, s2s_loss 0.49693, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78933, 2.42 secs\n",
      "\u001b[1m---- Epoch 132/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37061, s2s_loss 0.49680, 58.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78948, 2.42 secs\n",
      "\u001b[1m---- Epoch 133/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.66361, s2s_loss 0.49873, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79161, 2.48 secs\n",
      "\u001b[1m---- Epoch 134/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.52253, s2s_loss 0.50866, 59.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79106, 2.43 secs\n",
      "\u001b[1m---- Epoch 135/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40028, s2s_loss 0.51226, 55.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79016, 2.43 secs\n",
      "\u001b[1m---- Epoch 136/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.71966, s2s_loss 0.50665, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78881, 2.39 secs\n",
      "\u001b[1m---- Epoch 137/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47658, s2s_loss 0.49288, 58.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78890, 2.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_137_s2s_loss=0.5701.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 138/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32209, s2s_loss 0.49378, 56.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78889, 2.44 secs\n",
      "\u001b[1m---- Epoch 139/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.85245, s2s_loss 0.49989, 52.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78886, 2.42 secs\n",
      "\u001b[1m---- Epoch 140/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.67197, s2s_loss 0.49855, 58.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78880, 2.43 secs\n",
      "\u001b[1m---- Epoch 141/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38767, s2s_loss 0.49801, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79294, 2.43 secs\n",
      "\u001b[1m---- Epoch 142/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63221, s2s_loss 0.49434, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79363, 2.46 secs\n",
      "\u001b[1m---- Epoch 143/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.51718, s2s_loss 0.49970, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79008, 2.48 secs\n",
      "\u001b[1m---- Epoch 144/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.24139, s2s_loss 0.49423, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78937, 2.43 secs\n",
      "\u001b[1m---- Epoch 145/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.60693, s2s_loss 0.50733, 58.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78932, 2.46 secs\n",
      "\u001b[1m---- Epoch 146/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.55162, s2s_loss 0.48796, 58.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78886, 2.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_146_s2s_loss=0.5703.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 147/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31631, s2s_loss 0.49136, 58.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78868, 2.40 secs\n",
      "\u001b[1m---- Epoch 148/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.29915, s2s_loss 0.48806, 58.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78866, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_148_s2s_loss=0.5704.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 149/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44252, s2s_loss 0.49311, 58.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78544, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_149_s2s_loss=0.5711.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 150/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.68660, s2s_loss 0.50756, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78794, 2.42 secs\n",
      "\u001b[1m---- Epoch 151/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.54245, s2s_loss 0.50877, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78824, 2.43 secs\n",
      "\u001b[1m---- Epoch 152/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.70614, s2s_loss 0.49364, 56.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78771, 2.45 secs\n",
      "\u001b[1m---- Epoch 153/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.36826, s2s_loss 0.49781, 58.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78807, 2.43 secs\n",
      "\u001b[1m---- Epoch 154/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.61428, s2s_loss 0.49910, 59.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78803, 2.46 secs\n",
      "\u001b[1m---- Epoch 155/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.55434, s2s_loss 0.49526, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78817, 2.43 secs\n",
      "\u001b[1m---- Epoch 156/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.48279, s2s_loss 0.49136, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78806, 2.43 secs\n",
      "\u001b[1m---- Epoch 157/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.54662, s2s_loss 0.50891, 56.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78370, 2.43 secs\n",
      "\u001b[1m---- Epoch 158/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43781, s2s_loss 0.48952, 50.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78710, 2.43 secs\n",
      "\u001b[1m---- Epoch 159/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.60789, s2s_loss 0.49106, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78554, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_159_s2s_loss=0.5711.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 160/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.54368, s2s_loss 0.49195, 51.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78527, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_160_s2s_loss=0.5712.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 161/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.51182, s2s_loss 0.49158, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78512, 2.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_161_s2s_loss=0.5712.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 162/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.34817, s2s_loss 0.49497, 57.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78498, 2.41 secs\n",
      "\u001b[1m---- Epoch 163/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.30209, s2s_loss 0.50180, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78494, 2.45 secs\n",
      "\u001b[1m---- Epoch 164/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.52233, s2s_loss 0.50100, 58.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78489, 2.42 secs\n",
      "\u001b[1m---- Epoch 165/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.47256, s2s_loss 0.50415, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78565, 2.43 secs\n",
      "\u001b[1m---- Epoch 166/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43975, s2s_loss 0.49308, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78702, 2.42 secs\n",
      "\u001b[1m---- Epoch 167/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.24643, s2s_loss 0.49750, 56.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78512, 2.43 secs\n",
      "\u001b[1m---- Epoch 168/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55244, s2s_loss 0.49967, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78459, 2.41 secs\n",
      "\u001b[1m---- Epoch 169/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.49561, s2s_loss 0.49414, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78448, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_169_s2s_loss=0.5713.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 170/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.17393, s2s_loss 0.49949, 58.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78449, 2.44 secs\n",
      "\u001b[1m---- Epoch 171/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.32370, s2s_loss 0.49622, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78443, 2.36 secs\n",
      "\u001b[1m---- Epoch 172/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.18738, s2s_loss 0.49468, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78441, 2.44 secs\n",
      "\u001b[1m---- Epoch 173/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40331, s2s_loss 0.49844, 58.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78518, 2.46 secs\n",
      "\u001b[1m---- Epoch 174/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38469, s2s_loss 0.49885, 58.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78853, 2.45 secs\n",
      "\u001b[1m---- Epoch 175/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.79055, s2s_loss 0.49713, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78666, 2.42 secs\n",
      "\u001b[1m---- Epoch 176/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.46090, s2s_loss 0.49822, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78571, 2.42 secs\n",
      "\u001b[1m---- Epoch 177/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.64838, s2s_loss 0.48408, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78644, 2.43 secs\n",
      "\u001b[1m---- Epoch 178/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.52345, s2s_loss 0.49857, 58.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78629, 2.43 secs\n",
      "\u001b[1m---- Epoch 179/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.22026, s2s_loss 0.51082, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78627, 2.43 secs\n",
      "\u001b[1m---- Epoch 180/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55195, s2s_loss 0.49419, 58.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78608, 2.41 secs\n",
      "\u001b[1m---- Epoch 181/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42708, s2s_loss 0.49922, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78698, 2.43 secs\n",
      "\u001b[1m---- Epoch 182/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.70256, s2s_loss 0.48944, 57.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78502, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_182_s2s_loss=0.5713.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 183/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.48533, s2s_loss 0.48934, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78501, 2.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_183_s2s_loss=0.5713.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 184/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.65228, s2s_loss 0.50284, 58.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78500, 2.42 secs\n",
      "\u001b[1m---- Epoch 185/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47016, s2s_loss 0.48743, 59.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78488, 2.51 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_185_s2s_loss=0.5715.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 186/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.52091, s2s_loss 0.49633, 56.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78497, 2.42 secs\n",
      "\u001b[1m---- Epoch 187/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62229, s2s_loss 0.48683, 58.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78509, 2.43 secs\n",
      "\u001b[1m---- Epoch 188/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.56041, s2s_loss 0.48670, 58.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78504, 2.44 secs\n",
      "\u001b[1m---- Epoch 189/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.82855, s2s_loss 0.50041, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78764, 2.47 secs\n",
      "\u001b[1m---- Epoch 190/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.46804, s2s_loss 0.48813, 50.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78516, 2.41 secs\n",
      "\u001b[1m---- Epoch 191/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40901, s2s_loss 0.48880, 59.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78421, 2.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_191_s2s_loss=0.5716.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 192/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.46903, s2s_loss 0.48816, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78391, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_192_s2s_loss=0.5717.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 193/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.52562, s2s_loss 0.50094, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78378, 2.42 secs\n",
      "\u001b[1m---- Epoch 194/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.74898, s2s_loss 0.50346, 56.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78360, 2.46 secs\n",
      "\u001b[1m---- Epoch 195/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.37223, s2s_loss 0.48693, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78369, 2.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_195_s2s_loss=0.5718.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 196/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38107, s2s_loss 0.49415, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78372, 2.44 secs\n",
      "\u001b[1m---- Epoch 197/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.62214, s2s_loss 0.49315, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78429, 2.44 secs\n",
      "\u001b[1m---- Epoch 198/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.21459, s2s_loss 0.49723, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78529, 2.41 secs\n",
      "\u001b[1m---- Epoch 199/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.30630, s2s_loss 0.50186, 57.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78465, 2.47 secs\n",
      "\u001b[1m---- Epoch 200/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48601, s2s_loss 0.49449, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78441, 2.41 secs\n",
      "\u001b[1m---- Epoch 201/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.48072, s2s_loss 0.49869, 57.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78477, 2.42 secs\n",
      "\u001b[1m---- Epoch 202/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.54036, s2s_loss 0.49906, 58.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78440, 2.56 secs\n",
      "\u001b[1m---- Epoch 203/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43619, s2s_loss 0.49179, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78431, 2.42 secs\n",
      "\u001b[1m---- Epoch 204/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.51502, s2s_loss 0.49609, 56.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78431, 2.42 secs\n",
      "\u001b[1m---- Epoch 205/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.54623, s2s_loss 0.48962, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78086, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_205_s2s_loss=0.5725.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 206/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33517, s2s_loss 0.49890, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78192, 2.43 secs\n",
      "\u001b[1m---- Epoch 207/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.83806, s2s_loss 0.49232, 57.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78293, 2.43 secs\n",
      "\u001b[1m---- Epoch 208/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.61377, s2s_loss 0.49292, 57.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78179, 2.52 secs\n",
      "\u001b[1m---- Epoch 209/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.62471, s2s_loss 0.49203, 57.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78155, 2.45 secs\n",
      "\u001b[1m---- Epoch 210/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.53230, s2s_loss 0.49286, 56.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78157, 2.49 secs\n",
      "\u001b[1m---- Epoch 211/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.39517, s2s_loss 0.49478, 55.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78146, 2.44 secs\n",
      "\u001b[1m---- Epoch 212/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.84211, s2s_loss 0.49093, 57.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78148, 2.42 secs\n",
      "\u001b[1m---- Epoch 213/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.47958, s2s_loss 0.49234, 58.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78182, 2.45 secs\n",
      "\u001b[1m---- Epoch 214/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.39397, s2s_loss 0.48335, 58.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78273, 2.48 secs\n",
      "\u001b[1m---- Epoch 215/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.49409, s2s_loss 0.48330, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78190, 2.48 secs\n",
      "\u001b[1m---- Epoch 216/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.30407, s2s_loss 0.48511, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78199, 2.53 secs\n",
      "\u001b[1m---- Epoch 217/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.52183, s2s_loss 0.49283, 56.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78148, 2.43 secs\n",
      "\u001b[1m---- Epoch 218/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.53466, s2s_loss 0.49175, 57.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78131, 2.44 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 219/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63914, s2s_loss 0.49428, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78120, 2.47 secs\n",
      "\u001b[1m---- Epoch 220/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.36320, s2s_loss 0.48476, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78112, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_220_s2s_loss=0.5727.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 221/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.45984, s2s_loss 0.49609, 56.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78265, 2.48 secs\n",
      "\u001b[1m---- Epoch 222/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "^C iteration 132950\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 536, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 447, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 304, in train_model\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 95, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 81, in step_fn\n",
      "    gradient_accumulator.step(batch_loss, model)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 29, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240101_222821_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\" \\\n",
    "--epochs 300 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 20 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--task_name \"multitask\" \\\n",
    "--experiment_name \"s2f+f2m+f2c+s2co+s2cal+nli+mlm\" \\\n",
    "--multitask_name_list \\\n",
    "\"nli\" \\\n",
    "\"sentence2facts\" \\\n",
    "\"fact2metadata\" \\\n",
    "\"fact2comparison\" \\\n",
    "\"sentence2chestimagenome_observations\" \\\n",
    "\"sentence2chestimagenome_anatomical_locations\" \\\n",
    "\"mlm\" \\\n",
    "--task2weight '{\"sentence2facts\": 1.0, \"fact2metadata\": 1.0, \"fact2comparison\": 0.3, \"sentence2chestimagenome_observations\": 1.0, \"sentence2chestimagenome_anatomical_locations\": 1.0, \"nli\": 7.0, \"mlm\": 5.0}' \\\n",
    "--sentence_to_facts_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\"\\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\" \\\n",
    "--fact_to_metadata_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--fact_to_comparison_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "--chest_imagenome_phrases2labels_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\" \\\n",
    "--chest_imagenome_obs_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--chest_imagenome_anatloc_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(188008,24487305).jsonl\" \\\n",
    "--use_sentence2facts_for_nli \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--paraphrased_inputs_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--only_validate_nli \\\n",
    "--seq2seq_model_name \"t5\" \\\n",
    "--t5_model_name \"t5-small\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_162937_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 20\n",
      "   override_lr: False\n",
      "   task_name: multitask\n",
      "   experiment_name: s2f+f2m+f2c+s2co+s2cal+nli+mlm\n",
      "   multitask_name_list: ['nli', 'sentence2facts', 'fact2metadata', 'fact2comparison', 'sentence2chestimagenome_observations', 'sentence2chestimagenome_anatomical_locations', 'mlm']\n",
      "   task2weight: {'sentence2facts': 1.0, 'fact2metadata': 1.0, 'fact2comparison': 0.3, 'sentence2chestimagenome_observations': 1.0, 'sentence2chestimagenome_anatomical_locations': 1.0, 'nli': 8.0, 'mlm': 5.0}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_phrases2labels_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl']\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl']\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl']\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(157742,20319191).jsonl\n",
      "   use_sentence2facts_for_nli: True\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   only_validate_nli: True\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 20, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9891 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mB\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mbilateral\u001b[0m\n",
      "\u001b[1m\u001b[35mboth sides\u001b[0m\n",
      "\u001b[1m\u001b[35mboth\u001b[0m\n",
      "\u001b[1m\u001b[35mon both sides\u001b[0m\n",
      "\u001b[1m\u001b[35msymmetrical\u001b[0m\n",
      "\u001b[1m\u001b[35mequally on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mpresent on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mseen on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35moccurring on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mappearing on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mfound on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mseen bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mobserved bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mdetected bilaterally\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9890 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilus\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the hilum on the left side\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 8511 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary vasculature\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral lung vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary blood vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mlung periphery vasculature\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature of the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels in the outer regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular network in the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary circulation\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels supplying the outer areas of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature in the peripheral regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular system in the outer regions of the lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 1864 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35ma\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mone\u001b[0m\n",
      "\u001b[1m\u001b[35msingle\u001b[0m\n",
      "\u001b[1m\u001b[35msole\u001b[0m\n",
      "\u001b[1m\u001b[35monly\u001b[0m\n",
      "\u001b[1m\u001b[35mindividual\u001b[0m\n",
      "\u001b[1m\u001b[35mlone\u001b[0m\n",
      "\u001b[1m\u001b[35msolitary\u001b[0m\n",
      "\u001b[1m\u001b[35munique\u001b[0m\n",
      "\u001b[1m\u001b[35mdistinct\u001b[0m\n",
      "\u001b[1m\u001b[35msingular\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 10000 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9999 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9997 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9994 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9996 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome not detected\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome is not observed\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant constriction\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of notable narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of substantial narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of notable constriction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19943 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of wheezing\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing detected\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing observed\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing present\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing found\u001b[0m\n",
      "--------\n",
      "\u001b[1mNumber of unique inputs: 190085\u001b[0m\n",
      "\u001b[1mNumber of total paraphrases: 2093173\u001b[0m\n",
      "Number of medical sentences from paraphrases: 2039914\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2facts dataset\u001b[0m\n",
      "Loaded 9999 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19971 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\n",
      "Loaded 14990 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\n",
      "Loaded 14991 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2F: The known FDG-avid nodules seen on recent PET-CT are below the threshold of detection on chest radiograph.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"known FDG-avid nodules below the threshold of detection on chest radiograph\"]\u001b[0m\n",
      "Number of train examples: 84935\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing nli dataset\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(157742,20319191).jsonl...\n",
      "Number of samples: 157742\n",
      "Number of sources: 7\n",
      "----\n",
      "\u001b[1mLoading sentence2facts input/output pairs for NLI...\u001b[0m\n",
      "Number of entailment samples added: 368966\n",
      "----\n",
      "\u001b[1mLoading general domain datasets...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset...\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: contradiction -> 158860 (5157.45)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Extensive spinal fixation hardware is noted. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mSpinal fixation hardware is not present.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: contradiction -> 16332 (2741.30)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: There is upper zone re-distribution, vascular plethora and increased interstitial markings consistent with interval development of CHF with interstitial edema. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe degree of pulmonary vascular congestion and bilateral diffuse interstitial edema has mildly improved.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: contradiction -> 8340 (2210.12)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is small right pleural effusion with right basilar atelectasis. #Hypothesis: There is no pleural effusion or in pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: contradiction -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: He also had right posterior pleuritic chest pain consistent with prior episodes of pneumonia. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient has never had a pulmonary infection. \u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: contradiction -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: CRI (baseline Cr 1.5-1.7) 2. #Hypothesis:  the patient has normal kidney function\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: contradiction -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: 79 yo M [**Country **] speaking only w/HTN, DM, Hyperlipidemia, T-Ao aneurysm w/moderate AR, p/w productive cough and SOB. #Hypothesis:  Patient has no medical conditions\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: contradiction -> 212 (461.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Patchy opacities in the right lung are stable as well. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mAdditionaly, patchy opacities at the lung bases is more conspicuous on this study.\u001b[0m\n",
      "\u001b[1mSource: anli | Label: contradiction -> 88178 (4433.68)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: This new paragraph clarifies for organizations the circumstances under which they may accede at their discretion to the legitimate requests of government institutions for personal information, for national security, law enforcement and administrative purposes where they have lawful authority. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is no paragraph that clarifies circumstances under which organizations may accede to law enforcement requests.\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: contradiction -> 274712 (5897.92)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Mrs. Vandemeyer lifted her lids. #Hypothesis: The lids remained shut.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: snli | Label: contradiction -> 379404 (6365.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A group of men and women with face paint on are huddled around a table as one woman pulls up a cup, with a band with various instruments playing music in the background. #Hypothesis: One man sits alone in the park feeding pigeons.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: entailment -> 26370 (3167.85)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Right lower opacity, a combination of atelectasis and effusion has minimally improved. #Hypothesis: A right lower lung opacity is a combination of a layering effusion and atelectasis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: entailment -> 6380 (2019.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: There is no appreciable effusion or pneumothorax. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo visible pleural effusion or pneumothorax.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: entailment -> 3540 (1638.66)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: However, the lungs are otherwise grossly clear. #Hypothesis: The remaining visualized lungs are clear.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: entailment -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Prenatal screens:  Blood type A positive, antibody negative, Rubella immune, RPR nonreactive; hepatitis B surface antigen negative. #Hypothesis:  The patient does not have syphilis. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: entailment -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: He remained stable there for 2 days and is transferred to the medical floors for further management.  cx. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m His required further medical treatment\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: entailment -> 946 (966.10)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The patient was sent to the HD unit prior to coming to the floor for workup of fever. #Hypothesis:  The patient has elevated temperature  \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: entailment -> 186 (428.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The mediastinal and hilar contours appear within normal limits. #Hypothesis: The mediastinal contours are normal.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSource: s2f | Label: entailment -> 368966 (6324.57)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: This finding has markedly improved and only a very mild degree of perivascular haze can be appreciated. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mmild degree of perivascular haze\u001b[0m\n",
      "\u001b[1mSource: anli | Label: entailment -> 108502 (4680.39)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Gemayel's assassination is almost certain to deepen the political crisis in Lebanon, which started after the resignation of six pro-Syrian ministers, two from Hezbollah, hours after all-party round table talks collapsed. #Hypothesis: Lebanon has a political crisis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: entailment -> 275682 (5902.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: but there's so many more people that are homeless and yeah #Hypothesis: The amount of homelessness is increasing.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: snli | Label: entailment -> 380226 (6369.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A young lady being comforted by a man after she has seen something sad and it has made her cry. #Hypothesis: A young lady is crying.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: neutral -> 38246 (3527.79)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Frontal and lateral chest radiographs demonstrate stable cardiomediastinal contours. #Hypothesis: The lung fields are clear without any focal consolidation.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: neutral -> 20426 (2935.34)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Lungs otherwise appear clear. #Hypothesis: Moderate hyperinflation is chronic.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: neutral -> 7936 (2173.86)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Peristent collapse of the left lower lobe. #Hypothesis: There is a patchy opacity obscuring the right heart border, probably reflecting slight opacification in the right middle lobe.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: neutral -> 7486 (2131.74)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: GASTROINTESTINAL:  [**Known patient lastname 635**] was started on Reglan and Zantac on [**10-16**] to maximize medical therapy that may be contributing to his apneic and bradycardic episodes. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m He has a history of obesity\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: neutral -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: She fell to her knees and her husband and neighbor came to help her clean up. #Hypothesis:  The patient has balance issues. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: neutral -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The heart demonstrates mild to moderate enlargement, not significantly changed. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient is hypertensive. \u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: neutral -> 562 (762.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The lungs are clear. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe heart is normal in size.\u001b[0m\n",
      "\u001b[1mSource: anli | Label: neutral -> 141850 (5012.51)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Authors Anonymous is a 2014 comedy American film directed and produced by Ellie Kanner. It stars Kaley Cuoco, Chris Klein, Tricia Helfer, Jonathan Banks, Jonathan Bennett, Teri Polo and Dennis Farina in his final film role. The film was released on March 18, 2014 through video on demand prior to its limited release on April 18, 2014 by Screen Media Films and Starz Digital. #Hypothesis: Authors Anonymous received many awards.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: neutral -> 274304 (5895.82)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The rare, almost sightless Gangetic dolphin may even come up for a blow alongside your canoe if you are extremely lucky. #Hypothesis: Gangetic dolphin's eyes have evolved to become accustomed to the dark depths of the ocean.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: snli | Label: neutral -> 378436 (6362.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Two white dogs walk through a huge bank of mountain snow. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mTwo dogs are playing with their owners.\u001b[0m\n",
      "----\n",
      "Number of RadNLI test samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The cardiac and mediastinal silhouettes are stable. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormal hilar or mediastinal contours.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Dense retrocardiac opacification persists, consistent with left lower lobe consolidation and small pleural effusion. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo pleural effusion or pneumothorax is seen.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The mediastinal silhouette is unremarkable. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere are mediastinal drains.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: moderate right pleural effusion is larger today than on ___. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mmoderate right pleural effusion is worse today than on ___.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: tiny right apical pneumothorax, slightly decreased. #Hypothesis: tiny right apical pneumothorax, slightly increased.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset...\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2metadata dataset\u001b[0m\n",
      "Loaded 19989 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\n",
      "Loaded 19948 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2M: right humeral head osteolysis\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"anatomical location\": \"right humeral head\", \"detailed observation\": \"osteolysis of the right humeral head\", \"short observation\": \"osteolysis\", \"category\": \"anatomical finding\", \"health status\": \"abnormal\", \"prev_study_comparison?\": \"no\", \"comparison status\": \"\"}\u001b[0m\n",
      "Number of train examples: 59921\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2comparison dataset\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Added 341589 paraphrased inputs (total 443344)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mestablished respiratory condition\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2C: There is a redistribution of blood in the upper lung fields\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "Counter:\n",
      "Counter({'no comparison': 276665, 'stable/unchanged': 46121, 'worsened': 27557, 'resolved': 26871, 'improved': 18519, 'progressed': 10316, 'new finding': 8625, 'larger': 5687, 'increase': 5642, 'decrease': 5399, 'smaller': 4471, 'unclear comparison': 3230, 'position changed': 2691, 'reappeared': 1522, 'other': 28})\n",
      "Output: no comparison, train size: 276665, weight: 326.80615211646756\n",
      "Output: worsened, train size: 27557, weight: 217.56637091187034\n",
      "Output: resolved, train size: 26871, weight: 216.4948037154779\n",
      "Output: larger, train size: 5687, weight: 155.5870070153868\n",
      "Output: improved, train size: 18519, weight: 200.97934960420258\n",
      "Output: smaller, train size: 4471, weight: 147.0491363001147\n",
      "Output: progressed, train size: 10316, weight: 177.7581176401765\n",
      "Output: increase, train size: 5642, weight: 155.30121845247666\n",
      "Output: decrease, train size: 5399, weight: 153.7222195569894\n",
      "Output: unclear comparison, train size: 3230, weight: 135.8930734349483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: new finding, train size: 8625, weight: 170.93754906650108\n",
      "Output: stable/unchanged, train size: 46121, weight: 240.03726849161214\n",
      "Output: reappeared, train size: 1522, weight: 111.7619539554242\n",
      "Output: other, train size: 28, weight: 23.11066134663147\n",
      "Output: position changed, train size: 2691, weight: 129.8215650898703\n",
      "Number of train examples: 443344\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_observations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:20<00:00, 27367.56it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMild reticulation in the right mid lung at the upper pole of the hilus is probably mild bronchiectasis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"bronchiectasis\", \"increased reticular markings/ild pattern\", \"lung opacity\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThis corresponds to an opacity overlying the heart on the lateral view.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mOpacities in the left perihilar region have improved.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\"]\u001b[0m\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\n",
      "Loaded 14859 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\n",
      "Loaded 19952 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\n",
      "Loaded 19959 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\n",
      "Loaded 9977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\n",
      "Loaded 9974 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\n",
      "Loaded 9987 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\n",
      "Added 103692 paraphrased inputs (total 749511)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced air volume in the left lower lobe\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"linear/patchy atelectasis\"]\u001b[0m\n",
      "100%|███████████████████████████████| 749511/749511 [00:03<00:00, 211262.33it/s]\n",
      "Number of total examples: 749511\n",
      "Number of examples without labels: 160864\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 4498\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  barium\u001b[0m\n",
      "\u001b[93m\u001b[1m  lung surface\u001b[0m\n",
      "\u001b[93m\u001b[1m  lung infection\u001b[0m\n",
      "\u001b[93m\u001b[1m  valvular heart disease\u001b[0m\n",
      "\u001b[93m\u001b[1m  pleural drain\u001b[0m\n",
      "\u001b[93m\u001b[1m  manufacturing process\u001b[0m\n",
      "\u001b[93m\u001b[1m  lower lobe lung pathology\u001b[0m\n",
      "\u001b[93m\u001b[1m  not practical for paging\u001b[0m\n",
      "\u001b[93m\u001b[1m  metastases\u001b[0m\n",
      "\u001b[93m\u001b[1m  left lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: lung opacity, train size: 306147, weight: 6052.314258424769\n",
      "Label: None, train size: 160864, weight: 5173.661459041586\n",
      "Label: atelectasis, train size: 94030, weight: 4509.162252814013\n",
      "Label: pleural effusion, train size: 93022, weight: 4496.442365546256\n",
      "Label: pneumonia, train size: 46693, weight: 3731.7501716078773\n",
      "Label: pulmonary edema/hazy opacity, train size: 43454, weight: 3657.3900232811834\n",
      "Label: enlarged cardiac silhouette, train size: 35280, weight: 3447.4421996052774\n",
      "Label: enteric tube, train size: 27597, weight: 3210.498555548174\n",
      "Label: vascular congestion, train size: 25952, weight: 3152.9573291446445\n",
      "Label: consolidation, train size: 25260, weight: 3127.8725669115815\n",
      "Label: lung lesion, train size: 23572, weight: 3064.285857407827\n",
      "Label: endotracheal tube, train size: 21905, weight: 2997.8020284788163\n",
      "Label: pleural/parenchymal scarring, train size: 18682, weight: 2856.8572521836454\n",
      "Label: low lung volumes, train size: 18588, weight: 2852.463878874611\n",
      "Label: pneumothorax, train size: 16796, weight: 2765.12204817844\n",
      "Label: mass/nodule (not otherwise specified), train size: 14471, weight: 2640.017501159535\n",
      "Label: picc, train size: 14283, weight: 2629.221328708754\n",
      "Label: cardiac pacer and wires, train size: 13389, weight: 2576.2889815362387\n",
      "Label: ij line, train size: 12405, weight: 2514.6981737246747\n",
      "Label: lobar/segmental collapse, train size: 12371, weight: 2512.5021455815854\n",
      "Label: aspiration, train size: 12238, weight: 2503.8658589102106\n",
      "Label: linear/patchy atelectasis, train size: 12101, weight: 2494.8922039241074\n",
      "Label: chest tube, train size: 11749, weight: 2471.46349448124\n",
      "Label: airspace opacity, train size: 11717, weight: 2469.306345996421\n",
      "Label: enlarged hilum, train size: 11714, weight: 2469.103875848375\n",
      "Label: rib fracture, train size: 9956, weight: 2342.7541486499636\n",
      "Label: hyperaeration, train size: 8180, weight: 2195.9279342345876\n",
      "Label: mediastinal widening, train size: 7463, weight: 2129.5314346971195\n",
      "Label: chest port, train size: 7009, weight: 2084.8845125773137\n",
      "Label: copd/emphysema, train size: 6983, weight: 2082.260542425887\n",
      "Label: costophrenic angle blunting, train size: 6952, weight: 2079.1220606210954\n",
      "Label: vascular calcification, train size: 6944, weight: 2078.310371948699\n",
      "Label: tortuous aorta, train size: 6438, weight: 2025.4354983318656\n",
      "Label: fluid overload/heart failure, train size: 6402, weight: 2021.5528120628337\n",
      "Label: elevated hemidiaphragm, train size: 6133, weight: 1991.9943635549255\n",
      "Label: infiltration, train size: 5736, weight: 1946.4900159268673\n",
      "Label: mediastinal displacement, train size: 5220, weight: 1883.5769616626842\n",
      "Label: subcutaneous air, train size: 5140, weight: 1873.4003520541203\n",
      "Label: increased reticular markings/ild pattern, train size: 5109, weight: 1869.4242864055232\n",
      "Label: multiple masses/nodules, train size: 4868, weight: 1837.866541377119\n",
      "Label: spinal fracture, train size: 4604, weight: 1801.8953640063596\n",
      "Label: subclavian line, train size: 4440, weight: 1778.7494591002173\n",
      "Label: interstitial lung disease, train size: 4264, weight: 1753.173688697151\n",
      "Label: pigtail catheter, train size: 4257, weight: 1752.1399805275955\n",
      "Label: spinal degenerative changes, train size: 4162, weight: 1737.9816177147438\n",
      "Label: superior mediastinal mass/enlargement, train size: 4157, weight: 1737.2296621177059\n",
      "Label: hernia, train size: 3762, weight: 1675.527028301087\n",
      "Label: vascular redistribution, train size: 3623, weight: 1652.6453368828752\n",
      "Label: calcified nodule, train size: 3469, weight: 1626.5059586547704\n",
      "Label: sub-diaphragmatic air, train size: 3423, weight: 1618.5283906143927\n",
      "Label: tracheostomy tube, train size: 3391, weight: 1612.9309556389703\n",
      "Label: bone lesion, train size: 3358, weight: 1607.116732365291\n",
      "Label: scoliosis, train size: 3061, weight: 1552.7507585367864\n",
      "Label: granulomatous disease, train size: 2943, weight: 1530.0471242872911\n",
      "Label: swan-ganz catheter, train size: 2894, weight: 1520.4184683791714\n",
      "Label: prosthetic valve, train size: 2572, weight: 1453.9094023139187\n",
      "Label: lung cancer, train size: 2555, weight: 1450.2289229189291\n",
      "Label: alveolar hemorrhage, train size: 2530, weight: 1444.7831652398547\n",
      "Label: rotated, train size: 2258, weight: 1382.7784896739577\n",
      "Label: shoulder osteoarthritis, train size: 2196, weight: 1367.8758349752577\n",
      "Label: bronchiectasis, train size: 2124, weight: 1350.1735399073136\n",
      "Label: cabg grafts, train size: 1959, weight: 1307.8676309222412\n",
      "Label: pericardial effusion, train size: 1814, weight: 1268.4657612483188\n",
      "Label: hydropneumothorax, train size: 1631, weight: 1215.294185273704\n",
      "Label: artifact, train size: 1623, weight: 1212.872238143207\n",
      "Label: breast/nipple shadows, train size: 1557, weight: 1192.5514618266216\n",
      "Label: clavicle fracture, train size: 1556, weight: 1192.2387865976527\n",
      "Label: cyst/bullae, train size: 1475, weight: 1166.411761557198\n",
      "Label: pneumomediastinum, train size: 1358, weight: 1127.223892522407\n",
      "Label: intra-aortic balloon pump, train size: 931, weight: 959.3546703073852\n",
      "Label: goiter, train size: 906, weight: 947.9406351817632\n",
      "Label: mediastinal drain, train size: 904, weight: 947.0179422691563\n",
      "Label: aortic graft/repair, train size: 775, weight: 884.1976644686547\n",
      "Label: diaphragmatic eventration (benign), train size: 406, weight: 650.6631467873593\n",
      "Label: skin fold, train size: 311, weight: 567.8220950583348\n",
      "--------\n",
      "Number of train examples: 1270062\n",
      "Number of train datasets: 75\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: There is again noted to be diffuse and mild interstitial prominence.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: The convex to the of the right upper mediastinum, reflecting the ascending thoracic aorta is more pronounced today than in ___ when a chest CT showed mild dilatation of the ascending aorta.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"mediastinal widening\", \"superior mediastinal mass/enlargement\", \"tortuous aorta\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: midline still in place\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Aortic knob is again calcified.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"vascular calcification\"]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_anatomical_locations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:12<00:00, 45589.10it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mSince ___ mild interstitial edema has developed moderate cardiomegaly moderate bilateral pleural effusion have increased.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"left costophrenic angle\", \"left hilar structures\", \"left lung\", \"right costophrenic angle\", \"right hilar structures\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere are minimal degenerative changes in the mid thoracic spine.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"spine\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe lung apices remain relatively spared of this process.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left apical zone\", \"left lung\", \"right apical zone\", \"right lung\"]\u001b[0m\n",
      "Loaded 24929 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\n",
      "Loaded 24951 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\n",
      "Loaded 24988 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\n",
      "Added 352702 paraphrased inputs (total 983681)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mPneumonitis resulting from the leakage of air and fluid from the bronchopleural communication\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "100%|███████████████████████████████| 983681/983681 [00:05<00:00, 193176.53it/s]\n",
      "Number of total examples: 983681\n",
      "Number of examples without labels: 96958\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 24482\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  chest CT examinations\u001b[0m\n",
      "\u001b[93m\u001b[1m  left anterior descending artery\u001b[0m\n",
      "\u001b[93m\u001b[1m  left ventricular wall\u001b[0m\n",
      "\u001b[93m\u001b[1m  right thyroid\u001b[0m\n",
      "\u001b[93m\u001b[1m  aspiration\u001b[0m\n",
      "\u001b[93m\u001b[1m  bronchiectasis\u001b[0m\n",
      "\u001b[93m\u001b[1m  bilateral pulmonary opacities\u001b[0m\n",
      "\u001b[93m\u001b[1m  distal SVC\u001b[0m\n",
      "\u001b[93m\u001b[1m  left spine\u001b[0m\n",
      "\u001b[93m\u001b[1m  left anterior fifth rib\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: right lung, train size: 349137, weight: 6243.158334871754\n",
      "Label: left lung, train size: 347284, weight: 6235.3525090961175\n",
      "Label: mediastinum, train size: 144031, weight: 5031.875282788735\n",
      "Label: left lower lung zone, train size: 139395, weight: 4990.409355647508\n",
      "Label: right lower lung zone, train size: 133961, weight: 4940.320692958888\n",
      "Label: right hilar structures, train size: 104637, weight: 4636.605540005738\n",
      "Label: left hilar structures, train size: 99507, weight: 4576.371609488107\n",
      "Label: cardiac silhouette, train size: 97597, weight: 4553.288558553895\n",
      "Label: None, train size: 96958, weight: 4545.482695622845\n",
      "Label: left costophrenic angle, train size: 87782, weight: 4428.425926894693\n",
      "Label: right costophrenic angle, train size: 85867, weight: 4402.731970876394\n",
      "Label: abdomen, train size: 69685, weight: 4164.391638360652\n",
      "Label: neck, train size: 68283, weight: 4141.664068017979\n",
      "Label: right mid lung zone, train size: 42058, weight: 3623.944216201201\n",
      "Label: right chest wall, train size: 39937, weight: 3571.3605151829925\n",
      "Label: left chest wall, train size: 39710, weight: 3565.5994137333387\n",
      "Label: left mid lung zone, train size: 39072, weight: 3549.263144482792\n",
      "Label: trachea, train size: 37858, weight: 3517.5713767379216\n",
      "Label: spine, train size: 36967, weight: 3493.7833910652507\n",
      "Label: right upper lung zone, train size: 31930, weight: 3349.8346375050337\n",
      "Label: upper mediastinum, train size: 31127, weight: 3325.2150888629853\n",
      "Label: left hemidiaphragm, train size: 26182, weight: 3161.175801242095\n",
      "Label: left upper lung zone, train size: 24852, weight: 3112.8230688218005\n",
      "Label: right hemidiaphragm, train size: 22859, weight: 3036.325975604692\n",
      "Label: svc, train size: 22448, weight: 3019.8900361258507\n",
      "Label: aortic arch, train size: 21171, weight: 2967.237668771257\n",
      "Label: right apical zone, train size: 20033, weight: 2918.1322541597774\n",
      "Label: left apical zone, train size: 17367, weight: 2793.7252100239602\n",
      "Label: carina, train size: 17254, weight: 2788.1246491411166\n",
      "Label: right atrium, train size: 15526, weight: 2698.6228163775186\n",
      "Label: right shoulder, train size: 13652, weight: 2592.1436672609016\n",
      "Label: right clavicle, train size: 12056, weight: 2491.9272027167312\n",
      "Label: left clavicle, train size: 11934, weight: 2483.844699572688\n",
      "Label: left shoulder, train size: 10455, weight: 2380.2892980476154\n",
      "Label: cavoatrial junction, train size: 7297, weight: 2113.457361673451\n",
      "Label: left arm, train size: 6546, weight: 2036.9838564325023\n",
      "Label: right arm, train size: 6398, weight: 2021.1203630444604\n",
      "Label: left breast, train size: 3000, weight: 1541.097763435584\n",
      "Label: right breast, train size: 2328, weight: 1399.2439717598722\n",
      "--------\n",
      "Number of train examples: 2384141\n",
      "Number of train datasets: 39\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: chest without pulmonary edema\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Right lung base confluent opacity has progressed in comparison to prior exams.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"right lower lung zone\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Appearance of bilateral ground-glass opacities in the basal regions of the lungs\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lower lung zone\", \"right lower lung zone\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: The tracheostomy tube's diameter is less than half of the original trachea's size\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"trachea\"]\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing mlm dataset\u001b[0m\n",
      "Number of general sentences: 1377099\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 1377099\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:06<00:00, 202603.82it/s]\n",
      "\tlen(valid_tokens): 21522\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:10<00:00, 137113.49it/s]\n",
      "Number of sentences: 1348333\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Czar Lazar was killed by [tok1] who wanted to take over his country .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] Turks\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: He has other offices other than in [tok1] .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] Plano\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Eligible aliens have a [tok1] time finding someone to give them legal [tok2] .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] hard [tok2] help\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: three [tok1] walk past the store\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] people\u001b[0m\n",
      "Number of medical sentences: 2914528\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 2914528\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 2914528/2914528 [00:11<00:00, 244291.58it/s]\n",
      "\tlen(valid_tokens): 8332\n",
      "100%|█████████████████████████████| 2914528/2914528 [00:26<00:00, 110243.43it/s]\n",
      "Number of sentences: 2885960\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: supraclavicular [tok1]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] grooves\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Pneumonia [tok1] be causing the [tok2] opacity in the lung base\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] might [tok2] observed\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: possibility of a small , free-moving pleural [tok1] at the base of the right [tok2]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] effusion [tok2] chest\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: mild haziness in the [tok1] parenchyma at the lower part of the right upper lobe is [tok2]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] lung [tok2] stable\u001b[0m\n",
      "----------------------------------------\n",
      "Number of train datasets: 7\n",
      "Number of val datasets: 1\n",
      "----------------------------------------\n",
      "Examples of val datasets:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: minimal increase in the right - sided loculated hydro pneumothorax. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mminimal decrease in the right - sided loculated hydro pneumothorax.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Pleural effusion is small if any. #Hypothesis: A small right pleural effusion is stable.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: there is atelectasis at the lung bases, unchanged. #Hypothesis: there is atelectasis at the lung bases, new.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: the widespread bilateral diffuse parenchymal opacities are unchanged. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mthe widespread bilateral diffuse parenchymal opacities are stable.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_trainer.name =  multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_180720_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_180720_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_3_s2s_loss=0.5731.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_162937_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/checkpoint_3_s2s_loss=0.5731.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_180720_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.35833, s2s_loss 0.51352, 60.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78295, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.5709.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.50486, s2s_loss 0.51800, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78340, 2.44 secs\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.28650, s2s_loss 0.51319, 59.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78288, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.5709.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40361, s2s_loss 0.50903, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78636, 2.48 secs\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.34708, s2s_loss 0.51320, 59.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78566, 2.45 secs\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.46740, s2s_loss 0.51915, 59.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78559, 2.48 secs\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.41033, s2s_loss 0.51554, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78371, 2.43 secs\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.48584, s2s_loss 0.50819, 58.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78354, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_s2s_loss=0.5709.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.54993, s2s_loss 0.50748, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78388, 2.47 secs\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.34837, s2s_loss 0.50824, 58.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78341, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_s2s_loss=0.5710.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.65417, s2s_loss 0.51142, 59.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78333, 2.43 secs\n",
      "\u001b[1m---- Epoch 12/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.44794, s2s_loss 0.51624, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78332, 2.42 secs\n",
      "\u001b[1m---- Epoch 13/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37735, s2s_loss 0.50766, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78700, 2.45 secs\n",
      "\u001b[1m---- Epoch 14/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.90812, s2s_loss 0.51644, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78403, 2.47 secs\n",
      "\u001b[1m---- Epoch 15/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36661, s2s_loss 0.50962, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78082, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_s2s_loss=0.5716.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.33510, s2s_loss 0.51397, 58.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78077, 2.47 secs\n",
      "\u001b[1m---- Epoch 17/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.84446, s2s_loss 0.51771, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78117, 2.44 secs\n",
      "\u001b[1m---- Epoch 18/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.34644, s2s_loss 0.51032, 53.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78147, 2.46 secs\n",
      "\u001b[1m---- Epoch 19/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.98165, s2s_loss 0.51223, 59.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78169, 2.48 secs\n",
      "\u001b[1m---- Epoch 20/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.36603, s2s_loss 0.52326, 58.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78175, 2.41 secs\n",
      "\u001b[1m---- Epoch 21/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.56266, s2s_loss 0.50693, 58.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77642, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_s2s_loss=0.5730.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43755, s2s_loss 0.51473, 57.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78382, 2.41 secs\n",
      "\u001b[1m---- Epoch 23/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.57372, s2s_loss 0.51231, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.72640, s2s_loss 0.50987, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78306, 2.42 secs\n",
      "\u001b[1m---- Epoch 26/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.28890, s2s_loss 0.52236, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78289, 2.47 secs\n",
      "\u001b[1m---- Epoch 27/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.45285, s2s_loss 0.51230, 58.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78282, 2.45 secs\n",
      "\u001b[1m---- Epoch 28/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37240, s2s_loss 0.50737, 58.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78274, 2.46 secs\n",
      "\u001b[1m---- Epoch 29/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.57159, s2s_loss 0.51672, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78133, 2.42 secs\n",
      "\u001b[1m---- Epoch 30/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.49251, s2s_loss 0.50860, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78002, 2.38 secs\n",
      "\u001b[1m---- Epoch 31/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.18025, s2s_loss 0.51191, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77885, 2.47 secs\n",
      "\u001b[1m---- Epoch 32/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.53349, s2s_loss 0.51187, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77803, 2.44 secs\n",
      "\u001b[1m---- Epoch 33/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.50667, s2s_loss 0.51343, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77742, 2.42 secs\n",
      "\u001b[1m---- Epoch 34/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.36932, s2s_loss 0.52613, 58.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77738, 2.46 secs\n",
      "\u001b[1m---- Epoch 35/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32743, s2s_loss 0.50744, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77740, 2.43 secs\n",
      "\u001b[1m---- Epoch 36/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.47275, s2s_loss 0.51324, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77736, 2.44 secs\n",
      "\u001b[1m---- Epoch 37/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.51260, s2s_loss 0.51125, 58.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78205, 2.42 secs\n",
      "\u001b[1m---- Epoch 38/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31007, s2s_loss 0.51008, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78185, 2.43 secs\n",
      "\u001b[1m---- Epoch 39/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.30059, s2s_loss 0.50074, 53.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77846, 2.42 secs\n",
      "\u001b[1m---- Epoch 40/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.23741, s2s_loss 0.50636, 58.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77870, 2.43 secs\n",
      "\u001b[1m---- Epoch 41/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.58154, s2s_loss 0.50733, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77847, 2.43 secs\n",
      "\u001b[1m---- Epoch 42/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.26598, s2s_loss 0.50884, 58.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77837, 2.38 secs\n",
      "\u001b[1m---- Epoch 43/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.38140, s2s_loss 0.51184, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77847, 2.43 secs\n",
      "\u001b[1m---- Epoch 44/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31785, s2s_loss 0.51470, 58.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77855, 2.42 secs\n",
      "\u001b[1m---- Epoch 45/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.49885, s2s_loss 0.51364, 58.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77889, 2.42 secs\n",
      "\u001b[1m---- Epoch 46/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38966, s2s_loss 0.52302, 58.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78022, 2.42 secs\n",
      "\u001b[1m---- Epoch 47/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41914, s2s_loss 0.50843, 58.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78132, 2.43 secs\n",
      "\u001b[1m---- Epoch 48/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.45091, s2s_loss 0.50781, 58.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78089, 2.46 secs\n",
      "\u001b[1m---- Epoch 49/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.49265, s2s_loss 0.51329, 58.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78029, 2.43 secs\n",
      "\u001b[1m---- Epoch 50/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.44624, s2s_loss 0.50699, 58.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77991, 2.43 secs\n",
      "\u001b[1m---- Epoch 51/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.69703, s2s_loss 0.51115, 58.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77998, 2.42 secs\n",
      "\u001b[1m---- Epoch 52/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37238, s2s_loss 0.51038, 58.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78000, 2.42 secs\n",
      "\u001b[1m---- Epoch 53/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.30145, s2s_loss 0.50952, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78321, 2.46 secs\n",
      "\u001b[1m---- Epoch 54/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.51892, s2s_loss 0.50983, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77914, 2.48 secs\n",
      "\u001b[1m---- Epoch 55/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.75274, s2s_loss 0.51839, 59.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77821, 2.43 secs\n",
      "\u001b[1m---- Epoch 56/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.25631, s2s_loss 0.51254, 59.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77963, 2.42 secs\n",
      "\u001b[1m---- Epoch 57/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.48198, s2s_loss 0.50579, 58.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77952, 2.43 secs\n",
      "\u001b[1m---- Epoch 58/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.56576, s2s_loss 0.51038, 58.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77923, 2.43 secs\n",
      "\u001b[1m---- Epoch 59/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.36163, s2s_loss 0.50607, 44.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77919, 2.42 secs\n",
      "\u001b[1m---- Epoch 60/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.34782, s2s_loss 0.51311, 58.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77904, 2.42 secs\n",
      "\u001b[1m---- Epoch 61/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.78032, s2s_loss 0.51030, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77258, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_61_s2s_loss=0.5739.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 62/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.51820, s2s_loss 0.51443, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77370, 2.43 secs\n",
      "\u001b[1m---- Epoch 63/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.23929, s2s_loss 0.51456, 58.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77371, 2.44 secs\n",
      "\u001b[1m---- Epoch 64/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.44415, s2s_loss 0.50429, 44.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77360, 2.42 secs\n",
      "\u001b[1m---- Epoch 65/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54399, s2s_loss 0.51110, 58.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77298, 2.41 secs\n",
      "\u001b[1m---- Epoch 66/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.64964, s2s_loss 0.50122, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77313, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_66_s2s_loss=0.5742.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 67/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62386, s2s_loss 0.52190, 58.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77326, 2.46 secs\n",
      "\u001b[1m---- Epoch 68/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.57217, s2s_loss 0.50989, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77331, 2.39 secs\n",
      "\u001b[1m---- Epoch 69/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32387, s2s_loss 0.50277, 58.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77409, 2.45 secs\n",
      "\u001b[1m---- Epoch 70/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.47094, s2s_loss 0.49911, 59.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77642, 2.42 secs\n",
      "\u001b[1m---- Epoch 71/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.57284, s2s_loss 0.51345, 59.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77464, 2.43 secs\n",
      "\u001b[1m---- Epoch 72/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.43013, s2s_loss 0.51168, 59.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77416, 2.46 secs\n",
      "\u001b[1m---- Epoch 73/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.67555, s2s_loss 0.50525, 59.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77372, 2.44 secs\n",
      "\u001b[1m---- Epoch 74/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.51197, s2s_loss 0.50656, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77371, 2.44 secs\n",
      "\u001b[1m---- Epoch 75/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.89565, s2s_loss 0.50560, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77366, 2.44 secs\n",
      "\u001b[1m---- Epoch 76/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.76627, s2s_loss 0.51326, 58.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77356, 2.43 secs\n",
      "\u001b[1m---- Epoch 77/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.82111, s2s_loss 0.50469, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77116, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_77_s2s_loss=0.5746.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 78/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.60915, s2s_loss 0.51349, 58.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77380, 2.43 secs\n",
      "\u001b[1m---- Epoch 79/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36039, s2s_loss 0.50229, 58.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77321, 2.44 secs\n",
      "\u001b[1m---- Epoch 80/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.37063, s2s_loss 0.51541, 52.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77295, 2.43 secs\n",
      "\u001b[1m---- Epoch 81/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.33389, s2s_loss 0.50894, 58.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77285, 2.43 secs\n",
      "\u001b[1m---- Epoch 82/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.48331, s2s_loss 0.50991, 58.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77254, 2.44 secs\n",
      "\u001b[1m---- Epoch 83/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.72820, s2s_loss 0.49988, 58.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77254, 2.47 secs\n",
      "\u001b[1m---- Epoch 84/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.28409, s2s_loss 0.50265, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77266, 2.45 secs\n",
      "\u001b[1m---- Epoch 85/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32814, s2s_loss 0.51923, 58.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76992, 2.45 secs\n",
      "\u001b[1m---- Epoch 86/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43342, s2s_loss 0.51057, 59.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77024, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_86_s2s_loss=0.5746.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 87/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40528, s2s_loss 0.51564, 58.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77190, 2.45 secs\n",
      "\u001b[1m---- Epoch 88/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.61929, s2s_loss 0.50269, 59.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77118, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_88_s2s_loss=0.5747.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 89/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.35443, s2s_loss 0.51407, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77076, 2.43 secs\n",
      "\u001b[1m---- Epoch 90/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.71210, s2s_loss 0.51326, 57.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77112, 2.43 secs\n",
      "\u001b[1m---- Epoch 91/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40431, s2s_loss 0.50639, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77131, 2.49 secs\n",
      "\u001b[1m---- Epoch 92/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.21930, s2s_loss 0.50952, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77130, 2.44 secs\n",
      "\u001b[1m---- Epoch 93/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.56559, s2s_loss 0.49578, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76869, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_93_s2s_loss=0.5757.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 94/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.45321, s2s_loss 0.51453, 58.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77122, 2.46 secs\n",
      "\u001b[1m---- Epoch 95/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.53860, s2s_loss 0.51082, 58.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77085, 2.42 secs\n",
      "\u001b[1m---- Epoch 96/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.67211, s2s_loss 0.50080, 57.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77094, 2.43 secs\n",
      "\u001b[1m---- Epoch 97/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.79504, s2s_loss 0.51956, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77068, 2.45 secs\n",
      "\u001b[1m---- Epoch 98/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.49483, s2s_loss 0.51241, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77078, 2.43 secs\n",
      "\u001b[1m---- Epoch 99/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28586, s2s_loss 0.50778, 58.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77094, 2.46 secs\n",
      "\u001b[1m---- Epoch 100/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55407, s2s_loss 0.50917, 58.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77087, 2.43 secs\n",
      "\u001b[1m---- Epoch 101/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.57485, s2s_loss 0.51809, 58.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77056, 2.44 secs\n",
      "\u001b[1m---- Epoch 102/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33501, s2s_loss 0.51176, 59.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77205, 2.44 secs\n",
      "\u001b[1m---- Epoch 103/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.49867, s2s_loss 0.51291, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77103, 2.43 secs\n",
      "\u001b[1m---- Epoch 104/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.57909, s2s_loss 0.50722, 58.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77063, 2.44 secs\n",
      "\u001b[1m---- Epoch 105/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.49252, s2s_loss 0.50871, 57.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77040, 2.46 secs\n",
      "\u001b[1m---- Epoch 106/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.59006, s2s_loss 0.50749, 59.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77020, 2.44 secs\n",
      "\u001b[1m---- Epoch 107/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.39955, s2s_loss 0.49859, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77021, 2.47 secs\n",
      "\u001b[1m---- Epoch 108/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.39446, s2s_loss 0.50663, 59.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77015, 2.46 secs\n",
      "\u001b[1m---- Epoch 109/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.55354, s2s_loss 0.50788, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77241, 2.43 secs\n",
      "\u001b[1m---- Epoch 110/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.72147, s2s_loss 0.50352, 59.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76960, 2.43 secs\n",
      "\u001b[1m---- Epoch 111/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.57199, s2s_loss 0.51027, 58.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77067, 2.43 secs\n",
      "\u001b[1m---- Epoch 112/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.75602, s2s_loss 0.52205, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77136, 2.43 secs\n",
      "\u001b[1m---- Epoch 113/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47696, s2s_loss 0.51022, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77140, 2.44 secs\n",
      "\u001b[1m---- Epoch 114/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45700, s2s_loss 0.49993, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77134, 2.46 secs\n",
      "\u001b[1m---- Epoch 115/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31720, s2s_loss 0.50983, 58.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77132, 2.44 secs\n",
      "\u001b[1m---- Epoch 116/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.62440, s2s_loss 0.50332, 58.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77125, 2.46 secs\n",
      "\u001b[1m---- Epoch 117/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33855, s2s_loss 0.51121, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76929, 2.42 secs\n",
      "\u001b[1m---- Epoch 118/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.49266, s2s_loss 0.50511, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77173, 2.43 secs\n",
      "\u001b[1m---- Epoch 119/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.57401, s2s_loss 0.50888, 59.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77122, 2.45 secs\n",
      "\u001b[1m---- Epoch 120/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.72499, s2s_loss 0.51464, 58.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77200, 2.42 secs\n",
      "\u001b[1m---- Epoch 121/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.49210, s2s_loss 0.51057, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77209, 2.48 secs\n",
      "\u001b[1m---- Epoch 122/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.40459, s2s_loss 0.50596, 58.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77218, 2.43 secs\n",
      "\u001b[1m---- Epoch 123/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.79545, s2s_loss 0.51208, 58.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77199, 2.47 secs\n",
      "\u001b[1m---- Epoch 124/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38633, s2s_loss 0.50075, 58.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77188, 2.41 secs\n",
      "\u001b[1m---- Epoch 125/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "   iteration 74550\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.48719, s2s_loss 0.49985, 58.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76575, 2.42 secs\n",
      "\u001b[1m---- Epoch 159/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.73356, s2s_loss 0.50375, 57.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76847, 2.44 secs\n",
      "\u001b[1m---- Epoch 160/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.37936, s2s_loss 0.50110, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76720, 2.41 secs\n",
      "\u001b[1m---- Epoch 161/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41455, s2s_loss 0.50792, 58.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76681, 2.42 secs\n",
      "\u001b[1m---- Epoch 162/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63864, s2s_loss 0.50114, 58.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76673, 2.41 secs\n",
      "\u001b[1m---- Epoch 163/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.70854, s2s_loss 0.50571, 58.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76645, 2.42 secs\n",
      "\u001b[1m---- Epoch 164/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.51162, s2s_loss 0.49726, 58.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76632, 2.42 secs\n",
      "\u001b[1m---- Epoch 165/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.39105, s2s_loss 0.50749, 58.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76341, 2.47 secs\n",
      "\u001b[1m---- Epoch 166/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.59374, s2s_loss 0.50506, 58.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76035, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_166_s2s_loss=0.5777.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 167/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41104, s2s_loss 0.50633, 53.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76120, 2.43 secs\n",
      "\u001b[1m---- Epoch 168/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.89963, s2s_loss 0.51158, 44.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76152, 2.41 secs\n",
      "\u001b[1m---- Epoch 169/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.40348, s2s_loss 0.51463, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76190, 2.42 secs\n",
      "\u001b[1m---- Epoch 170/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.77281, s2s_loss 0.50524, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76249, 2.43 secs\n",
      "\u001b[1m---- Epoch 171/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43745, s2s_loss 0.50490, 58.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76273, 2.40 secs\n",
      "\u001b[1m---- Epoch 172/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.86213, s2s_loss 0.51618, 58.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76274, 2.43 secs\n",
      "\u001b[1m---- Epoch 173/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.58241, s2s_loss 0.50713, 58.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76190, 2.42 secs\n",
      "\u001b[1m---- Epoch 174/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.18524, s2s_loss 0.49911, 58.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76099, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_174_s2s_loss=0.5778.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 175/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41671, s2s_loss 0.51431, 58.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76178, 2.42 secs\n",
      "\u001b[1m---- Epoch 176/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.57449, s2s_loss 0.50967, 59.37 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76293, 2.45 secs\n",
      "\u001b[1m---- Epoch 177/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.43742, s2s_loss 0.50345, 59.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76257, 2.44 secs\n",
      "\u001b[1m---- Epoch 178/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.18585, s2s_loss 0.50153, 59.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76240, 2.42 secs\n",
      "\u001b[1m---- Epoch 179/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.29716, s2s_loss 0.50416, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76246, 2.42 secs\n",
      "\u001b[1m---- Epoch 180/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.67749, s2s_loss 0.50753, 58.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76241, 2.40 secs\n",
      "\u001b[1m---- Epoch 181/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44655, s2s_loss 0.51568, 57.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76709, 2.45 secs\n",
      "\u001b[1m---- Epoch 182/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29869, s2s_loss 0.50801, 58.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76598, 2.45 secs\n",
      "\u001b[1m---- Epoch 183/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.27613, s2s_loss 0.49930, 58.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76543, 2.43 secs\n",
      "\u001b[1m---- Epoch 184/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.37630, s2s_loss 0.49633, 58.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76495, 2.42 secs\n",
      "\u001b[1m---- Epoch 185/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.44098, s2s_loss 0.50806, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76431, 2.44 secs\n",
      "\u001b[1m---- Epoch 186/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35957, s2s_loss 0.51448, 58.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76439, 2.43 secs\n",
      "\u001b[1m---- Epoch 187/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.53520, s2s_loss 0.50657, 58.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76442, 2.46 secs\n",
      "\u001b[1m---- Epoch 188/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.56175, s2s_loss 0.49440, 58.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76442, 2.44 secs\n",
      "\u001b[1m---- Epoch 189/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64868, s2s_loss 0.51008, 58.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76239, 2.43 secs\n",
      "\u001b[1m---- Epoch 190/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.35276, s2s_loss 0.50347, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.75823, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_190_s2s_loss=0.5784.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 191/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.37577, s2s_loss 0.52081, 58.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.75723, 2.47 secs\n",
      "\u001b[1m---- Epoch 192/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63381, s2s_loss 0.50362, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.75868, 2.46 secs\n",
      "\u001b[1m---- Epoch 193/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.50531, s2s_loss 0.50840, 58.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.75912, 2.37 secs\n",
      "\u001b[1m---- Epoch 194/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50760, s2s_loss 0.50209, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.75957, 2.40 secs\n",
      "\u001b[1m---- Epoch 195/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46005, s2s_loss 0.50840, 58.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.75972, 2.42 secs\n",
      "\u001b[1m---- Epoch 196/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.65800, s2s_loss 0.50072, 58.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.75977, 2.42 secs\n",
      "\u001b[1m---- Epoch 197/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.43144, s2s_loss 0.50739, 58.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76216, 2.43 secs\n",
      "\u001b[1m---- Epoch 198/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.59945, s2s_loss 0.50086, 58.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76512, 2.41 secs\n",
      "\u001b[1m---- Epoch 199/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.50423, s2s_loss 0.50486, 58.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76538, 2.41 secs\n",
      "\u001b[1m---- Epoch 200/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.21628, s2s_loss 0.50112, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76443, 2.43 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_162937_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\" \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 20 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--task_name \"multitask\" \\\n",
    "--experiment_name \"s2f+f2m+f2c+s2co+s2cal+nli+mlm\" \\\n",
    "--multitask_name_list \\\n",
    "\"nli\" \\\n",
    "\"sentence2facts\" \\\n",
    "\"fact2metadata\" \\\n",
    "\"fact2comparison\" \\\n",
    "\"sentence2chestimagenome_observations\" \\\n",
    "\"sentence2chestimagenome_anatomical_locations\" \\\n",
    "\"mlm\" \\\n",
    "--task2weight '{\"sentence2facts\": 1.0, \"fact2metadata\": 1.0, \"fact2comparison\": 0.3, \"sentence2chestimagenome_observations\": 1.0, \"sentence2chestimagenome_anatomical_locations\": 1.0, \"nli\": 8.0, \"mlm\": 5.0}' \\\n",
    "--sentence_to_facts_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\"\\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\" \\\n",
    "--fact_to_metadata_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--fact_to_comparison_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "--chest_imagenome_phrases2labels_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\" \\\n",
    "--chest_imagenome_obs_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--chest_imagenome_anatloc_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(157742,20319191).jsonl\" \\\n",
    "--use_sentence2facts_for_nli \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--paraphrased_inputs_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--only_validate_nli \\\n",
    "--seq2seq_model_name \"t5\" \\\n",
    "--t5_model_name \"t5-small\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 300\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_180720_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 20\n",
      "   override_lr: False\n",
      "   task_name: multitask\n",
      "   experiment_name: s2f+f2m+f2c+s2co+s2cal+nli+mlm\n",
      "   multitask_name_list: ['nli', 'sentence2facts', 'fact2metadata', 'fact2comparison', 'sentence2chestimagenome_observations', 'sentence2chestimagenome_anatomical_locations', 'mlm']\n",
      "   task2weight: {'sentence2facts': 1.0, 'fact2metadata': 1.0, 'fact2comparison': 0.3, 'sentence2chestimagenome_observations': 1.0, 'sentence2chestimagenome_anatomical_locations': 1.0, 'nli': 8.0, 'mlm': 5.0}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_phrases2labels_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl']\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl']\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl']\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(157742,20319191).jsonl\n",
      "   use_sentence2facts_for_nli: True\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   only_validate_nli: True\n",
      "   nli1_only_on_val: True\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 20, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9891 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mB\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mbilateral\u001b[0m\n",
      "\u001b[1m\u001b[35mboth sides\u001b[0m\n",
      "\u001b[1m\u001b[35mboth\u001b[0m\n",
      "\u001b[1m\u001b[35mon both sides\u001b[0m\n",
      "\u001b[1m\u001b[35msymmetrical\u001b[0m\n",
      "\u001b[1m\u001b[35mequally on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mpresent on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mseen on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35moccurring on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mappearing on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mfound on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mseen bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mobserved bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mdetected bilaterally\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9890 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilus\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the hilum on the left side\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 8511 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary vasculature\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral lung vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary blood vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mlung periphery vasculature\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature of the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels in the outer regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular network in the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary circulation\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels supplying the outer areas of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature in the peripheral regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular system in the outer regions of the lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 1864 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35ma\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mone\u001b[0m\n",
      "\u001b[1m\u001b[35msingle\u001b[0m\n",
      "\u001b[1m\u001b[35msole\u001b[0m\n",
      "\u001b[1m\u001b[35monly\u001b[0m\n",
      "\u001b[1m\u001b[35mindividual\u001b[0m\n",
      "\u001b[1m\u001b[35mlone\u001b[0m\n",
      "\u001b[1m\u001b[35msolitary\u001b[0m\n",
      "\u001b[1m\u001b[35munique\u001b[0m\n",
      "\u001b[1m\u001b[35mdistinct\u001b[0m\n",
      "\u001b[1m\u001b[35msingular\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 10000 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9999 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9997 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9994 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9996 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome not detected\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome is not observed\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant constriction\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of notable narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of substantial narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of notable constriction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19943 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of wheezing\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing detected\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing observed\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing present\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing found\u001b[0m\n",
      "--------\n",
      "\u001b[1mNumber of unique inputs: 190085\u001b[0m\n",
      "\u001b[1mNumber of total paraphrases: 2093173\u001b[0m\n",
      "Number of medical sentences from paraphrases: 2039914\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2facts dataset\u001b[0m\n",
      "Loaded 9999 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19971 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\n",
      "Loaded 14990 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\n",
      "Loaded 14991 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2F: Unstable moderate cardiomegaly and unchanged moderate pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"unstable moderate cardiomegaly\", \"unchanged moderate pulmonary edema\"]\u001b[0m\n",
      "Number of train examples: 84935\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing nli dataset\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(157742,20319191).jsonl...\n",
      "Number of samples: 157742\n",
      "Number of sources: 7\n",
      "----\n",
      "\u001b[1mLoading sentence2facts input/output pairs for NLI...\u001b[0m\n",
      "Number of entailment samples added: 368966\n",
      "----\n",
      "\u001b[1mLoading general domain datasets...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset...\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: contradiction -> 158860 (5157.45)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Moderate pulmonary edema and small bilateral pleural effusions are new since . #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere are large bilateral pleural effusions and severe pulmonary edema\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: contradiction -> 16332 (2741.30)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Tiny left apical pneumothorax is barely visible, continues to resolve. #Hypothesis: Left apical pneumothorax is again seen and not significantly changed since previous exam.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: contradiction -> 8340 (2210.12)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The heart size is seen normal. #Hypothesis: Cardiac silhouette may be slightly enlarged but accentuated by a relatively low lung volumes.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: contradiction -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: ROS pos for cough productive of bloody sputum (occasionally tinged but sometimes large amts). #Hypothesis:  ROS is negative for hemoptysis \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: contradiction -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Her hemodynamics showed PA pressures of 56/35, wedge of 31, cardiac output of 3.88 and index of 1.86. #Hypothesis:  The patient has low pulmonary arterial pressure. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: contradiction -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The patient reports that his abdominal pain began at 10 A.M. on the day prior to admission, was constant, and was unable to be reduced. #Hypothesis:  the patient improved with antacids\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: contradiction -> 212 (461.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No pneumothorax. #Hypothesis: There is a small left apical lateral pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: anli | Label: contradiction -> 88178 (4433.68)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: ISLAMABAD, Jul 26 (APP):Mutahida Majlis Amal Pakistan (MMAP) candidate Mehmood Ahmad Khan has won election from Khyber Pakhtunkhwa constituency PK-94 Tank by securing 27,651 votes. According to unofficial results announced by the Election Commission of Pakistan (ECP), Awami National Party (ANP) candidate stood second by securing 15,452 votes and Pakistan Tehreek Insaf (PTI) candidate Irfanullah Kundi grabbed third position by getting 14,512 votes. Voter turnout was recorded at 44.18%. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mISLAMABAD has a x\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: contradiction -> 274712 (5897.92)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Every story about the son applauds his democratic instincts, his ease with all kinds of people, his ebullience and physicality, his informality. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere are no stories written about the son.\u001b[0m\n",
      "\u001b[1mSource: snli | Label: contradiction -> 379404 (6365.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: A child in a lake. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe child is playing on the grass.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: entailment -> 26370 (3167.85)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: In comparison with the study of , the cardiac silhouette is less prominent and there has been a decrease in pulmonary vascular congestion. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe patient's heart size has decreased and there is less fluid in the lungs compared to the previous study.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: entailment -> 6380 (2019.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is superimposed mild pulmonary edema. #Hypothesis: There is at least mild pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: entailment -> 3540 (1638.66)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Otherwise, cardiomediastinal hilar contours are within normal limits. #Hypothesis: The cardiomediastinal silhouette and hilar contours are otherwise normal.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: entailment -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Aortic endocarditis: s/p porcine AVR in [**2625**] following MSSA endocarditis c/b both left and right heart failure, septic emboli to kidney and spleen. #Hypothesis:  History of inflammation to the endocardium\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: entailment -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: PAST MEDICAL HISTORY:  [**Doctor Last Name 2217**], status post ablation in [**2972**], colonic polyps per colonoscope on [**2982-3-25**]. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m Patient had polyps in the colon removed\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: entailment -> 946 (966.10)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: In the delivery room, the infant emerged with good cry, suctioned, dried, given blow-by oxygen. #Hypothesis:  The infant emerged breathing \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: entailment -> 186 (428.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: No evidence of pneumothorax. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is no pneumothorax.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSource: s2f | Label: entailment -> 368966 (6324.57)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Mild pulmonary vascular engorgement could be due to tachycardia or volume overload, but there is no pulmonary edema or appreciable pleural effusion. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno pulmonary edema\u001b[0m\n",
      "\u001b[1mSource: anli | Label: entailment -> 108502 (4680.39)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: How to spot hail damage<br>Search for damage on the metal. Check metal roof vents, flashing or metal valleys on the roof to see if there are any dents. Soft metal will show dents, and also indicate the size of the hail. #Hypothesis: Hail can damage the metal valleys of your roof.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: entailment -> 275682 (5902.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: On 10 September 1229, a Catalan army led by King Jaume I of Arag??n and Catalunya took the Mallorcan shore near the present-day resort of Santa Ponca. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe modern resort of Santa Ponca is close to the Mallorcan shore.\u001b[0m\n",
      "\u001b[1mSource: snli | Label: entailment -> 380226 (6369.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The handicapped biker uses his arms to move his yellow sports bike. #Hypothesis: The biker has learned how to use his arms to his advantage.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: neutral -> 38246 (3527.79)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A drain is seen in the right upper quadrant. #Hypothesis: The left lung field is clear.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: neutral -> 20426 (2935.34)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No pleural effusion or pneumothorax is otherwise noted. #Hypothesis: No pneumonia or vascular congestion or pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: neutral -> 7936 (2173.86)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Moderate cardiomegaly appears similar to prior examination though complete comparison is difficult given the presence of a moderate-to-large right subpulmonic pleural effusion as well as increased density in the right lung base, which may represent pneumonia. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mComplete opacification of the right hemithorax with right-sided chest tube in place.\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: neutral -> 7486 (2131.74)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: States that she was recovering from a URI; no abdominal pain, no dysuria. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m She has a cough\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: neutral -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: History was taken entirely from the patients husband and daughters. #Hypothesis:  the patient has altered mental status\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: neutral -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: [**5-26**] admission to ICU in [**State 53**] for \"like a truck sitting on my chest\" with clot reportedly found on end of portacath, was on \"a medication for this,\" not currently on anticoagulation. #Hypothesis:  History of cancer \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: neutral -> 562 (762.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Aortic and tricuspid valve prostheses are in unchanged location. #Hypothesis: The aortic knob is calcified.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: anli | Label: neutral -> 141850 (5012.51)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Kerala Janapaksham ('Kerala People's Party') is a political party in Kerala, India. The party is led by K. Raman Pillai. Pillai is the president of the party. Pillai had previously been a leader of the Bharatiya Janata Party, but formed Kerala Janapaksham in 2007. Baby Ambatt was appointed general secretary of the party in May 2010. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mKerala Janapaksham was formed in March of 2007.\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: neutral -> 274304 (5895.82)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: He had been bound by an honourable promise not to open any such letters if they did arrive, but to repair to the National Gallery, where his colleague would meet him at ten o'clock. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mHe didn't open the letters but brought them with him to meet his colleague.\u001b[0m\n",
      "\u001b[1mSource: snli | Label: neutral -> 378436 (6362.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: An elderly gentleman takes time away from his tasks to eat. #Hypothesis: a man enjoys a great meal\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "----\n",
      "Number of RadNLI test samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: small left pleural effusion stable. #Hypothesis: small left pleural effusion resolved.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is no pneumothorax. #Hypothesis: There is no pneumothorax, pulmonary edema, pneumonia, or pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Cardiomediastinal and hilar contours are within normal limits. #Hypothesis: The cardiomediastinal silhouette is within normal limits.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is slight interval increase in right pleural effusion. #Hypothesis: As compared to the previous radiograph, there is a slight decrease in extent of the known right pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: more severe consolidation in the left lower lobe has worsened as well. #Hypothesis: more severe consolidation in the left lower lobe has improved as well.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset...\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2metadata dataset\u001b[0m\n",
      "Loaded 19989 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\n",
      "Loaded 19948 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2M: additional leads overlying the expected location of the right atrium\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"anatomical location\": \"expected location of the right atrium\", \"detailed observation\": \"additional leads overlying the expected location of the right atrium\", \"short observation\": \"additional leads overlying right atrium\", \"category\": \"technical assessment\", \"health status\": \"unknown\", \"prev_study_comparison?\": \"no\", \"comparison status\": \"\"}\u001b[0m\n",
      "Number of train examples: 59921\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2comparison dataset\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Added 341589 paraphrased inputs (total 443344)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe lower areas of the lungs show friable and extensive atelectasis\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2C: marked collapse in the middle left lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mworsened\u001b[0m\n",
      "Counter:\n",
      "Counter({'no comparison': 276665, 'stable/unchanged': 46121, 'worsened': 27557, 'resolved': 26871, 'improved': 18519, 'progressed': 10316, 'new finding': 8625, 'larger': 5687, 'increase': 5642, 'decrease': 5399, 'smaller': 4471, 'unclear comparison': 3230, 'position changed': 2691, 'reappeared': 1522, 'other': 28})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: no comparison, train size: 276665, weight: 5907.929922971326\n",
      "Output: worsened, train size: 27557, weight: 3209.1325193060784\n",
      "Output: resolved, train size: 26871, weight: 3185.453100878379\n",
      "Output: larger, train size: 5687, weight: 1940.7070776699022\n",
      "Output: improved, train size: 18519, weight: 2849.227679075855\n",
      "Output: smaller, train size: 4471, weight: 1783.1739746266992\n",
      "Output: progressed, train size: 10316, weight: 2369.9771784894483\n",
      "Output: increase, train size: 5642, weight: 1935.3623789696771\n",
      "Output: decrease, train size: 5399, weight: 1905.921326927956\n",
      "Output: unclear comparison, train size: 3230, weight: 1584.1488321235274\n",
      "Output: new finding, train size: 8625, weight: 2234.8902920005276\n",
      "Output: stable/unchanged, train size: 46121, weight: 3718.930087463258\n",
      "Output: reappeared, train size: 1522, weight: 1181.5197321713774\n",
      "Output: other, train size: 28, weight: 111.1011515767352\n",
      "Output: position changed, train size: 2691, weight: 1479.1773935508618\n",
      "Number of train examples: 443344\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_observations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:20<00:00, 27755.06it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mFrontal and lateral radiographs of the chest demonstrate persistent moderate-sized right-sided pleural effusion with adjacent atelectasis, and tiny left-sided pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"lung opacity\", \"pleural effusion\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mModerate enlargement of cardiac silhouette has worsened and mediastinal veins are more distended.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"enlarged cardiac silhouette\", \"mediastinal widening\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m1 cm nodular opacity in the left mid lung.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung lesion\", \"lung opacity\", \"mass/nodule (not otherwise specified)\"]\u001b[0m\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\n",
      "Loaded 14859 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\n",
      "Loaded 19952 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\n",
      "Loaded 19959 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\n",
      "Loaded 9977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\n",
      "Loaded 9974 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\n",
      "Loaded 9987 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\n",
      "Added 103692 paraphrased inputs (total 749511)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35maugmented interstitial markings in the left lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"increased reticular markings/ild pattern\", \"lung opacity\"]\u001b[0m\n",
      "100%|███████████████████████████████| 749511/749511 [00:03<00:00, 212997.16it/s]\n",
      "Number of total examples: 749511\n",
      "Number of examples without labels: 160864\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 4498\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  left atrium\u001b[0m\n",
      "\u001b[93m\u001b[1m  limited ability to recruit collapsed alveoli\u001b[0m\n",
      "\u001b[93m\u001b[1m  PICC position\u001b[0m\n",
      "\u001b[93m\u001b[1m  subclavian artery\u001b[0m\n",
      "\u001b[93m\u001b[1m  chest wall abnormality\u001b[0m\n",
      "\u001b[93m\u001b[1m  posterior rods\u001b[0m\n",
      "\u001b[93m\u001b[1m  superior vena cava (SVC) line\u001b[0m\n",
      "\u001b[93m\u001b[1m  pleural pigtail catheter\u001b[0m\n",
      "\u001b[93m\u001b[1m  shoulder injury\u001b[0m\n",
      "\u001b[93m\u001b[1m  perforation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: lung opacity, train size: 306147, weight: 6052.314258424769\n",
      "Label: None, train size: 160864, weight: 5173.661459041586\n",
      "Label: atelectasis, train size: 94030, weight: 4509.162252814013\n",
      "Label: pleural effusion, train size: 93022, weight: 4496.442365546256\n",
      "Label: pneumonia, train size: 46693, weight: 3731.7501716078773\n",
      "Label: pulmonary edema/hazy opacity, train size: 43454, weight: 3657.3900232811834\n",
      "Label: enlarged cardiac silhouette, train size: 35280, weight: 3447.4421996052774\n",
      "Label: enteric tube, train size: 27597, weight: 3210.498555548174\n",
      "Label: vascular congestion, train size: 25952, weight: 3152.9573291446445\n",
      "Label: consolidation, train size: 25260, weight: 3127.8725669115815\n",
      "Label: lung lesion, train size: 23572, weight: 3064.285857407827\n",
      "Label: endotracheal tube, train size: 21905, weight: 2997.8020284788163\n",
      "Label: pleural/parenchymal scarring, train size: 18682, weight: 2856.8572521836454\n",
      "Label: low lung volumes, train size: 18588, weight: 2852.463878874611\n",
      "Label: pneumothorax, train size: 16796, weight: 2765.12204817844\n",
      "Label: mass/nodule (not otherwise specified), train size: 14471, weight: 2640.017501159535\n",
      "Label: picc, train size: 14283, weight: 2629.221328708754\n",
      "Label: cardiac pacer and wires, train size: 13389, weight: 2576.2889815362387\n",
      "Label: ij line, train size: 12405, weight: 2514.6981737246747\n",
      "Label: lobar/segmental collapse, train size: 12371, weight: 2512.5021455815854\n",
      "Label: aspiration, train size: 12238, weight: 2503.8658589102106\n",
      "Label: linear/patchy atelectasis, train size: 12101, weight: 2494.8922039241074\n",
      "Label: chest tube, train size: 11749, weight: 2471.46349448124\n",
      "Label: airspace opacity, train size: 11717, weight: 2469.306345996421\n",
      "Label: enlarged hilum, train size: 11714, weight: 2469.103875848375\n",
      "Label: rib fracture, train size: 9956, weight: 2342.7541486499636\n",
      "Label: hyperaeration, train size: 8180, weight: 2195.9279342345876\n",
      "Label: mediastinal widening, train size: 7463, weight: 2129.5314346971195\n",
      "Label: chest port, train size: 7009, weight: 2084.8845125773137\n",
      "Label: copd/emphysema, train size: 6983, weight: 2082.260542425887\n",
      "Label: costophrenic angle blunting, train size: 6952, weight: 2079.1220606210954\n",
      "Label: vascular calcification, train size: 6944, weight: 2078.310371948699\n",
      "Label: tortuous aorta, train size: 6438, weight: 2025.4354983318656\n",
      "Label: fluid overload/heart failure, train size: 6402, weight: 2021.5528120628337\n",
      "Label: elevated hemidiaphragm, train size: 6133, weight: 1991.9943635549255\n",
      "Label: infiltration, train size: 5736, weight: 1946.4900159268673\n",
      "Label: mediastinal displacement, train size: 5220, weight: 1883.5769616626842\n",
      "Label: subcutaneous air, train size: 5140, weight: 1873.4003520541203\n",
      "Label: increased reticular markings/ild pattern, train size: 5109, weight: 1869.4242864055232\n",
      "Label: multiple masses/nodules, train size: 4868, weight: 1837.866541377119\n",
      "Label: spinal fracture, train size: 4604, weight: 1801.8953640063596\n",
      "Label: subclavian line, train size: 4440, weight: 1778.7494591002173\n",
      "Label: interstitial lung disease, train size: 4264, weight: 1753.173688697151\n",
      "Label: pigtail catheter, train size: 4257, weight: 1752.1399805275955\n",
      "Label: spinal degenerative changes, train size: 4162, weight: 1737.9816177147438\n",
      "Label: superior mediastinal mass/enlargement, train size: 4157, weight: 1737.2296621177059\n",
      "Label: hernia, train size: 3762, weight: 1675.527028301087\n",
      "Label: vascular redistribution, train size: 3623, weight: 1652.6453368828752\n",
      "Label: calcified nodule, train size: 3469, weight: 1626.5059586547704\n",
      "Label: sub-diaphragmatic air, train size: 3423, weight: 1618.5283906143927\n",
      "Label: tracheostomy tube, train size: 3391, weight: 1612.9309556389703\n",
      "Label: bone lesion, train size: 3358, weight: 1607.116732365291\n",
      "Label: scoliosis, train size: 3061, weight: 1552.7507585367864\n",
      "Label: granulomatous disease, train size: 2943, weight: 1530.0471242872911\n",
      "Label: swan-ganz catheter, train size: 2894, weight: 1520.4184683791714\n",
      "Label: prosthetic valve, train size: 2572, weight: 1453.9094023139187\n",
      "Label: lung cancer, train size: 2555, weight: 1450.2289229189291\n",
      "Label: alveolar hemorrhage, train size: 2530, weight: 1444.7831652398547\n",
      "Label: rotated, train size: 2258, weight: 1382.7784896739577\n",
      "Label: shoulder osteoarthritis, train size: 2196, weight: 1367.8758349752577\n",
      "Label: bronchiectasis, train size: 2124, weight: 1350.1735399073136\n",
      "Label: cabg grafts, train size: 1959, weight: 1307.8676309222412\n",
      "Label: pericardial effusion, train size: 1814, weight: 1268.4657612483188\n",
      "Label: hydropneumothorax, train size: 1631, weight: 1215.294185273704\n",
      "Label: artifact, train size: 1623, weight: 1212.872238143207\n",
      "Label: breast/nipple shadows, train size: 1557, weight: 1192.5514618266216\n",
      "Label: clavicle fracture, train size: 1556, weight: 1192.2387865976527\n",
      "Label: cyst/bullae, train size: 1475, weight: 1166.411761557198\n",
      "Label: pneumomediastinum, train size: 1358, weight: 1127.223892522407\n",
      "Label: intra-aortic balloon pump, train size: 931, weight: 959.3546703073852\n",
      "Label: goiter, train size: 906, weight: 947.9406351817632\n",
      "Label: mediastinal drain, train size: 904, weight: 947.0179422691563\n",
      "Label: aortic graft/repair, train size: 775, weight: 884.1976644686547\n",
      "Label: diaphragmatic eventration (benign), train size: 406, weight: 650.6631467873593\n",
      "Label: skin fold, train size: 311, weight: 567.8220950583348\n",
      "--------\n",
      "Number of train examples: 1270062\n",
      "Number of train datasets: 75\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: consistent aortic imaging findings\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: RIGHT INTERNAL JUGULAR LINE HAS BEEN DISCONTINUED.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"ij line\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: The heart size appears overall stable and mildly enlarged.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"enlarged cardiac silhouette\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: FINDINGS: There has been removal of the left chest tube with a small left apical pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"pneumothorax\"]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_anatomical_locations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:12<00:00, 45703.89it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mProbable degenerative changes in both glenohumeral joints, not fully evaluated on this examination.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left shoulder\", \"right shoulder\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mRight apical pleural thickening and scarring with associated calcification is re- demonstrated, likely reflective of prior post treatment changes from prior radiation therapy.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"right apical zone\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mRight basilar consolidative opacity concerning for pneumonia with associated moderate pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"right costophrenic angle\", \"right lower lung zone\", \"right lung\"]\u001b[0m\n",
      "Loaded 24929 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\n",
      "Loaded 24951 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\n",
      "Loaded 24988 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\n",
      "Added 352702 paraphrased inputs (total 983681)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mDiscoid atelectasis in the lower lobe on the right side\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"right lower lung zone\"]\u001b[0m\n",
      "100%|███████████████████████████████| 983681/983681 [00:05<00:00, 195291.44it/s]\n",
      "Number of total examples: 983681\n",
      "Number of examples without labels: 96958\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 24482\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  right liver lobe\u001b[0m\n",
      "\u001b[93m\u001b[1m  lung\u001b[0m\n",
      "\u001b[93m\u001b[1m  duodenum\u001b[0m\n",
      "\u001b[93m\u001b[1m  lower mid abdomen\u001b[0m\n",
      "\u001b[93m\u001b[1m  lung bases\u001b[0m\n",
      "\u001b[93m\u001b[1m  bilateral pulmonary opacities\u001b[0m\n",
      "\u001b[93m\u001b[1m  pleural fluid\u001b[0m\n",
      "\u001b[93m\u001b[1m  lateral image\u001b[0m\n",
      "\u001b[93m\u001b[1m  persistent left-sided superior vena cava\u001b[0m\n",
      "\u001b[93m\u001b[1m  right carina\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: right lung, train size: 349137, weight: 6243.158334871754\n",
      "Label: left lung, train size: 347284, weight: 6235.3525090961175\n",
      "Label: mediastinum, train size: 144031, weight: 5031.875282788735\n",
      "Label: left lower lung zone, train size: 139395, weight: 4990.409355647508\n",
      "Label: right lower lung zone, train size: 133961, weight: 4940.320692958888\n",
      "Label: right hilar structures, train size: 104637, weight: 4636.605540005738\n",
      "Label: left hilar structures, train size: 99507, weight: 4576.371609488107\n",
      "Label: cardiac silhouette, train size: 97597, weight: 4553.288558553895\n",
      "Label: None, train size: 96958, weight: 4545.482695622845\n",
      "Label: left costophrenic angle, train size: 87782, weight: 4428.425926894693\n",
      "Label: right costophrenic angle, train size: 85867, weight: 4402.731970876394\n",
      "Label: abdomen, train size: 69685, weight: 4164.391638360652\n",
      "Label: neck, train size: 68283, weight: 4141.664068017979\n",
      "Label: right mid lung zone, train size: 42058, weight: 3623.944216201201\n",
      "Label: right chest wall, train size: 39937, weight: 3571.3605151829925\n",
      "Label: left chest wall, train size: 39710, weight: 3565.5994137333387\n",
      "Label: left mid lung zone, train size: 39072, weight: 3549.263144482792\n",
      "Label: trachea, train size: 37858, weight: 3517.5713767379216\n",
      "Label: spine, train size: 36967, weight: 3493.7833910652507\n",
      "Label: right upper lung zone, train size: 31930, weight: 3349.8346375050337\n",
      "Label: upper mediastinum, train size: 31127, weight: 3325.2150888629853\n",
      "Label: left hemidiaphragm, train size: 26182, weight: 3161.175801242095\n",
      "Label: left upper lung zone, train size: 24852, weight: 3112.8230688218005\n",
      "Label: right hemidiaphragm, train size: 22859, weight: 3036.325975604692\n",
      "Label: svc, train size: 22448, weight: 3019.8900361258507\n",
      "Label: aortic arch, train size: 21171, weight: 2967.237668771257\n",
      "Label: right apical zone, train size: 20033, weight: 2918.1322541597774\n",
      "Label: left apical zone, train size: 17367, weight: 2793.7252100239602\n",
      "Label: carina, train size: 17254, weight: 2788.1246491411166\n",
      "Label: right atrium, train size: 15526, weight: 2698.6228163775186\n",
      "Label: right shoulder, train size: 13652, weight: 2592.1436672609016\n",
      "Label: right clavicle, train size: 12056, weight: 2491.9272027167312\n",
      "Label: left clavicle, train size: 11934, weight: 2483.844699572688\n",
      "Label: left shoulder, train size: 10455, weight: 2380.2892980476154\n",
      "Label: cavoatrial junction, train size: 7297, weight: 2113.457361673451\n",
      "Label: left arm, train size: 6546, weight: 2036.9838564325023\n",
      "Label: right arm, train size: 6398, weight: 2021.1203630444604\n",
      "Label: left breast, train size: 3000, weight: 1541.097763435584\n",
      "Label: right breast, train size: 2328, weight: 1399.2439717598722\n",
      "--------\n",
      "Number of train examples: 2384141\n",
      "Number of train datasets: 39\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Right IJ catheter tip is in the proximal SVC.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"mediastinum\", \"neck\", \"svc\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: In comparison with study of ___, the cardiac silhouette remains within normal limits and there is no vascular congestion, pleural effusion, or acute focal pneumonia.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"left costophrenic angle\", \"left hilar structures\", \"left lung\", \"right costophrenic angle\", \"right hilar structures\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: FINDINGS: The heart continues to be mildly enlarged with bilateral alveolar infiltrates, upper lobe greater than lower lobe.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"left lower lung zone\", \"left lung\", \"left mid lung zone\", \"left upper lung zone\", \"right lower lung zone\", \"right lung\", \"right mid lung zone\", \"right upper lung zone\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: The size of the cardiac silhouette is now normal.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\"]\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing mlm dataset\u001b[0m\n",
      "Number of general sentences: 1377099\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 1377099\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:06<00:00, 210023.66it/s]\n",
      "\tlen(valid_tokens): 21522\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:12<00:00, 107027.40it/s]\n",
      "Number of sentences: 1348333\n",
      "Number of bins: 7\n",
      "Bin size: 958219, weight: 7845.006940358202\n",
      "Bin size: 219530, weight: 5586.745498624757\n",
      "Bin size: 110971, weight: 4707.694773844805\n",
      "Bin size: 46036, weight: 3717.013976819967\n",
      "Bin size: 8332, weight: 2209.4180153128023\n",
      "Bin size: 4974, weight: 1851.8906668241702\n",
      "Bin size: 271, weight: 527.9351334797618\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: There are [tok1] looking at eateries .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] people\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A man bowls , throwing a black ball down the [tok1] .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] alley\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A man is in front of a [tok1] .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] woman\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: My last [tok1] is Smith .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] name\u001b[0m\n",
      "Number of medical sentences: 2914528\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 2914528\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 2914528/2914528 [00:11<00:00, 251072.46it/s]\n",
      "\tlen(valid_tokens): 8332\n",
      "100%|██████████████████████████████| 2914528/2914528 [00:32<00:00, 90095.60it/s]\n",
      "Number of sentences: 2885960\n",
      "Number of bins: 10\n",
      "Bin size: 1342184, weight: 8435.037029157607\n",
      "Bin size: 595311, weight: 7059.417561043633\n",
      "Bin size: 538442, weight: 6900.705475481864\n",
      "Bin size: 301809, weight: 6031.824313316108\n",
      "Bin size: 88459, weight: 4437.398885480121\n",
      "Bin size: 15753, weight: 2710.81797873336\n",
      "Bin size: 3420, weight: 1618.0053116099564\n",
      "Bin size: 261, weight: 517.3766629445327\n",
      "Bin size: 176, weight: 415.06604923310664\n",
      "Bin size: 145, weight: 370.132172294277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Remnant patchy [tok1] observed in the [tok2] middle lobe\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] opacity [tok2] right\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: widespread consolidation in the right [tok1]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] lung\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A rounded [tok1] was seen on the chest CT scan performed [tok2]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] opacity [tok2] earlier\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A minimal atelectasis at the right lung [tok1] is unchanged .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] base\u001b[0m\n",
      "----------------------------------------\n",
      "Number of train datasets: 7\n",
      "Number of val datasets: 1\n",
      "----------------------------------------\n",
      "Examples of val datasets:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is no pneumothorax or pleural effusion. #Hypothesis: There is no pleural effusion, focal consolidation or pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is no pleural effusion or pneumothorax. #Hypothesis: There is evidence of a right pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: unchanged moderate right pleural effusion. #Hypothesis: stable moderate right pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Moderate cardiomegaly and mediastinal vascular engorgement is stable. #Hypothesis: Cardiomediastinal and hilar silhouettes are normal.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "seq2seq_trainer.name =  multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240106_002348_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240106_002348_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_190_s2s_loss=0.5784.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_180720_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/checkpoint_190_s2s_loss=0.5784.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240106_002348_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40503, s2s_loss 0.47850, 60.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05980, exact_match 0.86564, 1.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_exact_match+s2s_loss=0.8818.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.50344, s2s_loss 0.48204, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06009, exact_match 0.86326, 1.40 secs\n",
      "\u001b[1m---- Epoch 3/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.55973, s2s_loss 0.49562, 58.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05904, exact_match 0.86920, 1.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_exact_match+s2s_loss=0.8829.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.49164, s2s_loss 0.48630, 58.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06506, exact_match 0.85137, 1.39 secs\n",
      "\u001b[1m---- Epoch 5/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.55973, s2s_loss 0.48554, 57.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06976, exact_match 0.84423, 1.48 secs\n",
      "\u001b[1m---- Epoch 6/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.92967, s2s_loss 0.48548, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06959, exact_match 0.84423, 1.40 secs\n",
      "\u001b[1m---- Epoch 7/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.63799, s2s_loss 0.48513, 58.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06923, exact_match 0.84542, 1.40 secs\n",
      "\u001b[1m---- Epoch 8/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.47856, s2s_loss 0.48816, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06932, exact_match 0.84661, 1.42 secs\n",
      "\u001b[1m---- Epoch 9/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.59114, s2s_loss 0.48223, 58.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06898, exact_match 0.84780, 1.38 secs\n",
      "\u001b[1m---- Epoch 10/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.60844, s2s_loss 0.47841, 59.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06885, exact_match 0.84661, 1.39 secs\n",
      "\u001b[1m---- Epoch 11/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.58083, s2s_loss 0.49586, 58.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06846, exact_match 0.84542, 1.39 secs\n",
      "\u001b[1m---- Epoch 12/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.45466, s2s_loss 0.48533, 59.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06828, exact_match 0.84542, 1.39 secs\n",
      "\u001b[1m---- Epoch 13/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.62862, s2s_loss 0.48859, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06399, exact_match 0.85850, 1.42 secs\n",
      "\u001b[1m---- Epoch 14/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.64182, s2s_loss 0.48928, 59.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06472, exact_match 0.85256, 1.41 secs\n",
      "\u001b[1m---- Epoch 15/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.34744, s2s_loss 0.49283, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06521, exact_match 0.85018, 1.39 secs\n",
      "\u001b[1m---- Epoch 16/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29616, s2s_loss 0.49168, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06566, exact_match 0.84780, 1.40 secs\n",
      "\u001b[1m---- Epoch 17/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.52474, s2s_loss 0.49268, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06651, exact_match 0.84661, 1.39 secs\n",
      "\u001b[1m---- Epoch 18/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.33338, s2s_loss 0.50009, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06600, exact_match 0.84780, 1.39 secs\n",
      "\u001b[1m---- Epoch 19/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31580, s2s_loss 0.49054, 46.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06616, exact_match 0.84780, 1.40 secs\n",
      "\u001b[1m---- Epoch 20/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.34125, s2s_loss 0.48680, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06619, exact_match 0.84780, 1.39 secs\n",
      "\u001b[1m---- Epoch 21/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.47599, s2s_loss 0.48294, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06876, exact_match 0.83829, 1.38 secs\n",
      "\u001b[1m---- Epoch 22/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.32457, s2s_loss 0.48695, 58.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06897, exact_match 0.84661, 1.39 secs\n",
      "\u001b[1m---- Epoch 23/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36873, s2s_loss 0.48945, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06835, exact_match 0.85018, 1.44 secs\n",
      "\u001b[1m---- Epoch 24/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.30746, s2s_loss 0.48798, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06734, exact_match 0.85256, 1.39 secs\n",
      "\u001b[1m---- Epoch 25/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.46869, s2s_loss 0.48848, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06727, exact_match 0.84899, 1.38 secs\n",
      "\u001b[1m---- Epoch 26/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42013, s2s_loss 0.47754, 58.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06748, exact_match 0.84780, 1.39 secs\n",
      "\u001b[1m---- Epoch 27/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.24587, s2s_loss 0.48843, 58.71 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.06769, exact_match 0.85018, 1.38 secs\n",
      "\u001b[1m---- Epoch 28/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32205, s2s_loss 0.48521, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06769, exact_match 0.84780, 1.40 secs\n",
      "\u001b[1m---- Epoch 29/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.54253, s2s_loss 0.48165, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06261, exact_match 0.85969, 1.41 secs\n",
      "\u001b[1m---- Epoch 30/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30510, s2s_loss 0.48836, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06559, exact_match 0.85256, 1.40 secs\n",
      "\u001b[1m---- Epoch 31/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.32092, s2s_loss 0.49474, 58.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06540, exact_match 0.85612, 1.39 secs\n",
      "\u001b[1m---- Epoch 32/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.22997, s2s_loss 0.48935, 59.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06509, exact_match 0.85731, 1.38 secs\n",
      "\u001b[1m---- Epoch 33/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.35717, s2s_loss 0.49770, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06453, exact_match 0.85731, 1.39 secs\n",
      "\u001b[1m---- Epoch 34/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45727, s2s_loss 0.48432, 58.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06434, exact_match 0.85493, 1.39 secs\n",
      "\u001b[1m---- Epoch 35/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.51233, s2s_loss 0.48548, 58.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06413, exact_match 0.85612, 1.40 secs\n",
      "\u001b[1m---- Epoch 36/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.36493, s2s_loss 0.48626, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06417, exact_match 0.85731, 1.40 secs\n",
      "\u001b[1m---- Epoch 37/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.51350, s2s_loss 0.49068, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05958, exact_match 0.86564, 1.38 secs\n",
      "\u001b[1m---- Epoch 38/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30087, s2s_loss 0.48480, 58.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06170, exact_match 0.86683, 1.40 secs\n",
      "\u001b[1m---- Epoch 39/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.37486, s2s_loss 0.48999, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06248, exact_match 0.86683, 1.40 secs\n",
      "\u001b[1m---- Epoch 40/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.15286, s2s_loss 0.48280, 58.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06157, exact_match 0.86326, 1.40 secs\n",
      "\u001b[1m---- Epoch 41/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.48508, s2s_loss 0.47892, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06230, exact_match 0.86088, 1.39 secs\n",
      "\u001b[1m---- Epoch 42/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31848, s2s_loss 0.49608, 59.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06206, exact_match 0.85969, 1.39 secs\n",
      "\u001b[1m---- Epoch 43/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.60704, s2s_loss 0.48423, 58.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06206, exact_match 0.86088, 1.39 secs\n",
      "\u001b[1m---- Epoch 44/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.27463, s2s_loss 0.48727, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06204, exact_match 0.86088, 1.38 secs\n",
      "\u001b[1m---- Epoch 45/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38410, s2s_loss 0.48010, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05929, exact_match 0.87158, 1.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_45_exact_match+s2s_loss=0.8846.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 46/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.21951, s2s_loss 0.48673, 58.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06583, exact_match 0.85612, 1.39 secs\n",
      "\u001b[1m---- Epoch 47/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.26884, s2s_loss 0.48198, 58.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06425, exact_match 0.85375, 1.39 secs\n",
      "\u001b[1m---- Epoch 48/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.60563, s2s_loss 0.48939, 59.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06421, exact_match 0.85375, 1.38 secs\n",
      "\u001b[1m---- Epoch 49/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.30796, s2s_loss 0.48645, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06427, exact_match 0.85493, 1.39 secs\n",
      "\u001b[1m---- Epoch 50/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50899, s2s_loss 0.48046, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06400, exact_match 0.85612, 1.39 secs\n",
      "\u001b[1m---- Epoch 51/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.70609, s2s_loss 0.49083, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06400, exact_match 0.85493, 1.39 secs\n",
      "\u001b[1m---- Epoch 52/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.48659, s2s_loss 0.49043, 57.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06390, exact_match 0.85612, 1.38 secs\n",
      "\u001b[1m---- Epoch 53/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.39128, s2s_loss 0.48943, 58.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06174, exact_match 0.85850, 1.43 secs\n",
      "\u001b[1m---- Epoch 54/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.50230, s2s_loss 0.48552, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06336, exact_match 0.84423, 1.40 secs\n",
      "\u001b[1m---- Epoch 55/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.47630, s2s_loss 0.48108, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06056, exact_match 0.86088, 1.40 secs\n",
      "\u001b[1m---- Epoch 56/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.53160, s2s_loss 0.46935, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06116, exact_match 0.85612, 1.40 secs\n",
      "\u001b[1m---- Epoch 57/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.50019, s2s_loss 0.48358, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06206, exact_match 0.85375, 1.39 secs\n",
      "\u001b[1m---- Epoch 58/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.39208, s2s_loss 0.48022, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06179, exact_match 0.85731, 1.38 secs\n",
      "\u001b[1m---- Epoch 59/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.47721, s2s_loss 0.49299, 59.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06174, exact_match 0.85850, 1.41 secs\n",
      "\u001b[1m---- Epoch 60/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32476, s2s_loss 0.48364, 59.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06178, exact_match 0.85731, 1.40 secs\n",
      "\u001b[1m---- Epoch 61/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37631, s2s_loss 0.48980, 59.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05768, exact_match 0.87753, 1.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_61_exact_match+s2s_loss=0.8875.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 62/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.45355, s2s_loss 0.48928, 58.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06244, exact_match 0.85493, 1.43 secs\n",
      "\u001b[1m---- Epoch 63/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40779, s2s_loss 0.48539, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06400, exact_match 0.85612, 1.38 secs\n",
      "\u001b[1m---- Epoch 64/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48063, s2s_loss 0.48449, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06467, exact_match 0.85375, 1.40 secs\n",
      "\u001b[1m---- Epoch 65/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11091, s2s_loss 0.48987, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06401, exact_match 0.85612, 1.41 secs\n",
      "\u001b[1m---- Epoch 66/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41583, s2s_loss 0.48548, 58.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06370, exact_match 0.85731, 1.38 secs\n",
      "\u001b[1m---- Epoch 67/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.57514, s2s_loss 0.48141, 58.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06372, exact_match 0.85731, 1.39 secs\n",
      "\u001b[1m---- Epoch 68/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40628, s2s_loss 0.49051, 59.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06358, exact_match 0.85850, 1.39 secs\n",
      "\u001b[1m---- Epoch 69/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.16995, s2s_loss 0.48415, 59.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05943, exact_match 0.86564, 1.40 secs\n",
      "\u001b[1m---- Epoch 70/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.43973, s2s_loss 0.47627, 59.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06118, exact_match 0.86207, 1.41 secs\n",
      "\u001b[1m---- Epoch 71/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.88558, s2s_loss 0.48283, 58.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06043, exact_match 0.86326, 1.41 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 72/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.25101, s2s_loss 0.47699, 58.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06108, exact_match 0.85850, 1.39 secs\n",
      "\u001b[1m---- Epoch 73/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.50491, s2s_loss 0.48299, 58.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06044, exact_match 0.86326, 1.39 secs\n",
      "\u001b[1m---- Epoch 74/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.51640, s2s_loss 0.47999, 59.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06027, exact_match 0.86564, 1.40 secs\n",
      "\u001b[1m---- Epoch 75/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.29040, s2s_loss 0.47986, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06041, exact_match 0.86801, 1.39 secs\n",
      "\u001b[1m---- Epoch 76/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.71320, s2s_loss 0.49056, 59.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06039, exact_match 0.86801, 1.42 secs\n",
      "\u001b[1m---- Epoch 77/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41349, s2s_loss 0.48996, 58.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06145, exact_match 0.85256, 1.41 secs\n",
      "\u001b[1m---- Epoch 78/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31723, s2s_loss 0.49150, 59.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06422, exact_match 0.85018, 1.40 secs\n",
      "\u001b[1m---- Epoch 79/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.46315, s2s_loss 0.49699, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06220, exact_match 0.85850, 1.41 secs\n",
      "\u001b[1m---- Epoch 80/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48731, s2s_loss 0.49259, 58.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06303, exact_match 0.85612, 1.39 secs\n",
      "\u001b[1m---- Epoch 81/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.71273, s2s_loss 0.48126, 58.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06265, exact_match 0.85493, 1.38 secs\n",
      "\u001b[1m---- Epoch 82/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.27381, s2s_loss 0.48179, 58.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06282, exact_match 0.85493, 1.39 secs\n",
      "\u001b[1m---- Epoch 83/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.70036, s2s_loss 0.49749, 59.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06257, exact_match 0.85493, 1.40 secs\n",
      "\u001b[1m---- Epoch 84/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.26235, s2s_loss 0.48562, 58.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06260, exact_match 0.85612, 1.38 secs\n",
      "\u001b[1m---- Epoch 85/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33367, s2s_loss 0.48280, 41.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06187, exact_match 0.85731, 1.38 secs\n",
      "\u001b[1m---- Epoch 86/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.56633, s2s_loss 0.47929, 41.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06288, exact_match 0.86326, 1.39 secs\n",
      "\u001b[1m---- Epoch 87/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.39407, s2s_loss 0.46764, 41.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06432, exact_match 0.86207, 1.39 secs\n",
      "\u001b[1m---- Epoch 88/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.40680, s2s_loss 0.48787, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06288, exact_match 0.86445, 1.41 secs\n",
      "\u001b[1m---- Epoch 89/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.53698, s2s_loss 0.48521, 59.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06230, exact_match 0.86445, 1.39 secs\n",
      "\u001b[1m---- Epoch 90/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.48080, s2s_loss 0.49129, 58.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06187, exact_match 0.86445, 1.42 secs\n",
      "\u001b[1m---- Epoch 91/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41017, s2s_loss 0.49906, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06204, exact_match 0.86564, 1.39 secs\n",
      "\u001b[1m---- Epoch 92/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43361, s2s_loss 0.48175, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06210, exact_match 0.86683, 1.39 secs\n",
      "\u001b[1m---- Epoch 93/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40113, s2s_loss 0.49716, 57.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06142, exact_match 0.86326, 1.40 secs\n",
      "\u001b[1m---- Epoch 94/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37103, s2s_loss 0.48373, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06263, exact_match 0.86920, 1.38 secs\n",
      "\u001b[1m---- Epoch 95/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.39748, s2s_loss 0.48189, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06056, exact_match 0.86326, 1.39 secs\n",
      "\u001b[1m---- Epoch 96/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.42180, s2s_loss 0.48573, 58.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06030, exact_match 0.86564, 1.40 secs\n",
      "\u001b[1m---- Epoch 97/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.64190, s2s_loss 0.49099, 59.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05934, exact_match 0.87158, 1.41 secs\n",
      "\u001b[1m---- Epoch 98/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50866, s2s_loss 0.47687, 59.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05938, exact_match 0.87158, 1.40 secs\n",
      "\u001b[1m---- Epoch 99/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.49119, s2s_loss 0.47844, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05934, exact_match 0.87277, 1.41 secs\n",
      "\u001b[1m---- Epoch 100/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32935, s2s_loss 0.49447, 58.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05939, exact_match 0.87158, 1.39 secs\n",
      "\u001b[1m---- Epoch 101/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.49163, s2s_loss 0.48491, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05997, exact_match 0.87039, 1.39 secs\n",
      "\u001b[1m---- Epoch 102/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30986, s2s_loss 0.47931, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05808, exact_match 0.88704, 1.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_102_exact_match+s2s_loss=0.8921.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 103/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.84763, s2s_loss 0.49193, 59.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05859, exact_match 0.86920, 1.42 secs\n",
      "\u001b[1m---- Epoch 104/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.61822, s2s_loss 0.48439, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05880, exact_match 0.86564, 1.40 secs\n",
      "\u001b[1m---- Epoch 105/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.60471, s2s_loss 0.48729, 57.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05929, exact_match 0.86564, 1.43 secs\n",
      "\u001b[1m---- Epoch 106/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.59893, s2s_loss 0.47974, 59.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05903, exact_match 0.86564, 1.40 secs\n",
      "\u001b[1m---- Epoch 107/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.37542, s2s_loss 0.48223, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05897, exact_match 0.86445, 1.39 secs\n",
      "\u001b[1m---- Epoch 108/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.35342, s2s_loss 0.48856, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05899, exact_match 0.86445, 1.37 secs\n",
      "\u001b[1m---- Epoch 109/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.52923, s2s_loss 0.49854, 58.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06069, exact_match 0.86326, 1.40 secs\n",
      "\u001b[1m---- Epoch 110/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.55999, s2s_loss 0.48196, 58.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05976, exact_match 0.87277, 1.39 secs\n",
      "\u001b[1m---- Epoch 111/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.87809, s2s_loss 0.48709, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06158, exact_match 0.86445, 1.41 secs\n",
      "\u001b[1m---- Epoch 112/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.40632, s2s_loss 0.48649, 59.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06187, exact_match 0.86326, 1.38 secs\n",
      "\u001b[1m---- Epoch 113/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.79909, s2s_loss 0.47861, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06190, exact_match 0.86445, 1.38 secs\n",
      "\u001b[1m---- Epoch 114/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29155, s2s_loss 0.47704, 58.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06213, exact_match 0.86564, 1.39 secs\n",
      "\u001b[1m---- Epoch 115/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.33081, s2s_loss 0.47944, 58.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06190, exact_match 0.86445, 1.38 secs\n",
      "\u001b[1m---- Epoch 116/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.54180, s2s_loss 0.47960, 58.61 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.06181, exact_match 0.86445, 1.39 secs\n",
      "\u001b[1m---- Epoch 117/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.66055, s2s_loss 0.48050, 52.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06132, exact_match 0.85731, 1.41 secs\n",
      "\u001b[1m---- Epoch 118/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40893, s2s_loss 0.48927, 58.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06199, exact_match 0.86326, 1.40 secs\n",
      "\u001b[1m---- Epoch 119/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.16871, s2s_loss 0.48951, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06164, exact_match 0.86326, 1.40 secs\n",
      "\u001b[1m---- Epoch 120/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.61857, s2s_loss 0.48444, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06097, exact_match 0.86683, 1.39 secs\n",
      "\u001b[1m---- Epoch 121/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.55113, s2s_loss 0.47868, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06092, exact_match 0.86445, 1.38 secs\n",
      "\u001b[1m---- Epoch 122/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42366, s2s_loss 0.49362, 58.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06097, exact_match 0.86326, 1.39 secs\n",
      "\u001b[1m---- Epoch 123/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44874, s2s_loss 0.48515, 59.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06081, exact_match 0.86445, 1.39 secs\n",
      "\u001b[1m---- Epoch 124/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55141, s2s_loss 0.48015, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06080, exact_match 0.86445, 1.39 secs\n",
      "\u001b[1m---- Epoch 125/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40773, s2s_loss 0.48330, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05659, exact_match 0.87634, 1.39 secs\n",
      "\u001b[1m---- Epoch 126/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.58376, s2s_loss 0.48839, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05756, exact_match 0.88347, 1.38 secs\n",
      "\u001b[1m---- Epoch 127/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.29959, s2s_loss 0.48639, 58.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05887, exact_match 0.87039, 1.39 secs\n",
      "\u001b[1m---- Epoch 128/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.56412, s2s_loss 0.48322, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05823, exact_match 0.86920, 1.39 secs\n",
      "\u001b[1m---- Epoch 129/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.40624, s2s_loss 0.47899, 58.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05818, exact_match 0.86683, 1.39 secs\n",
      "\u001b[1m---- Epoch 130/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.18197, s2s_loss 0.48028, 57.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05787, exact_match 0.86920, 1.39 secs\n",
      "\u001b[1m---- Epoch 131/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.61596, s2s_loss 0.48294, 58.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05775, exact_match 0.87158, 1.39 secs\n",
      "\u001b[1m---- Epoch 132/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.53410, s2s_loss 0.48889, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05782, exact_match 0.87039, 1.38 secs\n",
      "\u001b[1m---- Epoch 133/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.60290, s2s_loss 0.48472, 58.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05622, exact_match 0.87990, 1.42 secs\n",
      "\u001b[1m---- Epoch 134/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38304, s2s_loss 0.47565, 58.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06034, exact_match 0.87396, 1.39 secs\n",
      "\u001b[1m---- Epoch 135/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.51498, s2s_loss 0.47234, 58.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05799, exact_match 0.87634, 1.44 secs\n",
      "\u001b[1m---- Epoch 136/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41334, s2s_loss 0.48673, 41.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05815, exact_match 0.87039, 1.39 secs\n",
      "\u001b[1m---- Epoch 137/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41680, s2s_loss 0.47810, 54.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05877, exact_match 0.87515, 1.40 secs\n",
      "\u001b[1m---- Epoch 138/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.38799, s2s_loss 0.48998, 59.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05906, exact_match 0.87753, 1.40 secs\n",
      "\u001b[1m---- Epoch 139/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.52145, s2s_loss 0.48835, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05899, exact_match 0.87872, 1.39 secs\n",
      "\u001b[1m---- Epoch 140/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.29393, s2s_loss 0.48852, 59.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05882, exact_match 0.87872, 1.40 secs\n",
      "\u001b[1m---- Epoch 141/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36435, s2s_loss 0.48439, 58.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05830, exact_match 0.88466, 1.38 secs\n",
      "\u001b[1m---- Epoch 142/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.44698, s2s_loss 0.48734, 59.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05760, exact_match 0.87396, 1.37 secs\n",
      "\u001b[1m---- Epoch 143/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.58314, s2s_loss 0.48401, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05657, exact_match 0.88109, 1.40 secs\n",
      "\u001b[1m---- Epoch 144/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.51913, s2s_loss 0.47728, 58.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05621, exact_match 0.88347, 1.38 secs\n",
      "\u001b[1m---- Epoch 145/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.57813, s2s_loss 0.48227, 58.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05641, exact_match 0.88228, 1.39 secs\n",
      "\u001b[1m---- Epoch 146/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.86609, s2s_loss 0.47771, 59.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05675, exact_match 0.88347, 1.43 secs\n",
      "\u001b[1m---- Epoch 147/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.67709, s2s_loss 0.48328, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05694, exact_match 0.88347, 1.40 secs\n",
      "\u001b[1m---- Epoch 148/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.36195, s2s_loss 0.48756, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05708, exact_match 0.88228, 1.39 secs\n",
      "\u001b[1m---- Epoch 149/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38903, s2s_loss 0.48887, 58.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06039, exact_match 0.87396, 1.38 secs\n",
      "\u001b[1m---- Epoch 150/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.68753, s2s_loss 0.47921, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05918, exact_match 0.87277, 1.39 secs\n",
      "\u001b[1m---- Epoch 151/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.47886, s2s_loss 0.47773, 59.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05790, exact_match 0.88228, 1.39 secs\n",
      "\u001b[1m---- Epoch 152/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.67991, s2s_loss 0.47876, 58.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05834, exact_match 0.87990, 1.40 secs\n",
      "\u001b[1m---- Epoch 153/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.42309, s2s_loss 0.48826, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05843, exact_match 0.87990, 1.38 secs\n",
      "\u001b[1m---- Epoch 154/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.28909, s2s_loss 0.47296, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05818, exact_match 0.87990, 1.43 secs\n",
      "\u001b[1m---- Epoch 155/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.39324, s2s_loss 0.48382, 55.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05840, exact_match 0.87990, 1.42 secs\n",
      "\u001b[1m---- Epoch 156/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.39857, s2s_loss 0.47563, 58.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05843, exact_match 0.87990, 1.38 secs\n",
      "\u001b[1m---- Epoch 157/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.47485, s2s_loss 0.48420, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06435, exact_match 0.86326, 1.39 secs\n",
      "\u001b[1m---- Epoch 158/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.57373, s2s_loss 0.48184, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05877, exact_match 0.87634, 1.38 secs\n",
      "\u001b[1m---- Epoch 159/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.47430, s2s_loss 0.48055, 58.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05749, exact_match 0.87990, 1.38 secs\n",
      "\u001b[1m---- Epoch 160/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.42285, s2s_loss 0.46749, 58.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05804, exact_match 0.87634, 1.40 secs\n",
      "\u001b[1m---- Epoch 161/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.36901, s2s_loss 0.47532, 58.73 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.05767, exact_match 0.87753, 1.42 secs\n",
      "\u001b[1m---- Epoch 162/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41885, s2s_loss 0.48651, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05796, exact_match 0.87634, 1.40 secs\n",
      "\u001b[1m---- Epoch 163/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.68088, s2s_loss 0.48002, 59.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05795, exact_match 0.87634, 1.41 secs\n",
      "\u001b[1m---- Epoch 164/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.19492, s2s_loss 0.48794, 58.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05802, exact_match 0.87634, 1.39 secs\n",
      "\u001b[1m---- Epoch 165/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38700, s2s_loss 0.47527, 59.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05579, exact_match 0.89180, 1.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_165_exact_match+s2s_loss=0.8953.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 166/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.50162, s2s_loss 0.47765, 58.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05400, exact_match 0.89417, 1.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_166_exact_match+s2s_loss=0.8970.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 167/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.74757, s2s_loss 0.48400, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05538, exact_match 0.88466, 1.41 secs\n",
      "\u001b[1m---- Epoch 168/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.26113, s2s_loss 0.46595, 58.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05749, exact_match 0.87872, 1.38 secs\n",
      "\u001b[1m---- Epoch 169/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.45059, s2s_loss 0.48547, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05803, exact_match 0.87990, 1.39 secs\n",
      "\u001b[1m---- Epoch 170/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.47369, s2s_loss 0.47224, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05796, exact_match 0.87990, 1.38 secs\n",
      "\u001b[1m---- Epoch 171/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46769, s2s_loss 0.47948, 58.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05779, exact_match 0.87990, 1.39 secs\n",
      "\u001b[1m---- Epoch 172/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.45786, s2s_loss 0.48345, 59.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05777, exact_match 0.87872, 1.40 secs\n",
      "\u001b[1m---- Epoch 173/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.53931, s2s_loss 0.48891, 59.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05603, exact_match 0.89180, 1.40 secs\n",
      "\u001b[1m---- Epoch 174/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.23047, s2s_loss 0.48263, 59.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05452, exact_match 0.89180, 1.40 secs\n",
      "\u001b[1m---- Epoch 175/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.60543, s2s_loss 0.48635, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05466, exact_match 0.89180, 1.39 secs\n",
      "\u001b[1m---- Epoch 176/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29800, s2s_loss 0.48905, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05527, exact_match 0.88942, 1.43 secs\n",
      "\u001b[1m---- Epoch 177/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "loss 0.53363, s2s_loss 0.48225, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05569, exact_match 0.88704, 1.40 secs\n",
      "\u001b[1m---- Epoch 178/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.43995, s2s_loss 0.48158, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05571, exact_match 0.88704, 1.40 secs\n",
      "\u001b[1m---- Epoch 179/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.67746, s2s_loss 0.48048, 58.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05566, exact_match 0.88823, 1.40 secs\n",
      "\u001b[1m---- Epoch 180/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.60693, s2s_loss 0.48621, 59.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05566, exact_match 0.88823, 1.41 secs\n",
      "\u001b[1m---- Epoch 181/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.57926, s2s_loss 0.48005, 58.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05789, exact_match 0.88466, 1.39 secs\n",
      "\u001b[1m---- Epoch 182/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.80029, s2s_loss 0.49547, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05721, exact_match 0.88585, 1.39 secs\n",
      "\u001b[1m---- Epoch 183/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.71398, s2s_loss 0.48972, 59.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05926, exact_match 0.87515, 1.39 secs\n",
      "\u001b[1m---- Epoch 184/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.47630, s2s_loss 0.47945, 58.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05666, exact_match 0.88466, 1.35 secs\n",
      "\u001b[1m---- Epoch 185/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.56194, s2s_loss 0.46876, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05736, exact_match 0.88109, 1.40 secs\n",
      "\u001b[1m---- Epoch 186/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.48698, s2s_loss 0.48618, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05765, exact_match 0.88109, 1.38 secs\n",
      "\u001b[1m---- Epoch 187/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.49021, s2s_loss 0.47672, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05785, exact_match 0.87990, 1.39 secs\n",
      "\u001b[1m---- Epoch 188/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31593, s2s_loss 0.47798, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05797, exact_match 0.87872, 1.39 secs\n",
      "\u001b[1m---- Epoch 189/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36773, s2s_loss 0.47971, 58.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05872, exact_match 0.87039, 1.39 secs\n",
      "\u001b[1m---- Epoch 190/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31358, s2s_loss 0.49140, 58.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05735, exact_match 0.88585, 1.39 secs\n",
      "\u001b[1m---- Epoch 191/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.47598, s2s_loss 0.49002, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05488, exact_match 0.89417, 1.38 secs\n",
      "\u001b[1m---- Epoch 192/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.52773, s2s_loss 0.47382, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05532, exact_match 0.89655, 1.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_192_exact_match+s2s_loss=0.8977.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 193/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.62628, s2s_loss 0.47659, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05530, exact_match 0.89536, 1.39 secs\n",
      "\u001b[1m---- Epoch 194/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.37975, s2s_loss 0.47433, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05534, exact_match 0.89417, 1.39 secs\n",
      "\u001b[1m---- Epoch 195/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.49989, s2s_loss 0.47936, 59.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05538, exact_match 0.89417, 1.41 secs\n",
      "\u001b[1m---- Epoch 196/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.53874, s2s_loss 0.46780, 58.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05546, exact_match 0.89417, 1.39 secs\n",
      "\u001b[1m---- Epoch 197/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.54016, s2s_loss 0.49140, 59.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06032, exact_match 0.87277, 1.39 secs\n",
      "\u001b[1m---- Epoch 198/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.46953, s2s_loss 0.48257, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06056, exact_match 0.87634, 1.39 secs\n",
      "\u001b[1m---- Epoch 199/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.58618, s2s_loss 0.47520, 58.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06001, exact_match 0.87396, 1.38 secs\n",
      "\u001b[1m---- Epoch 200/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.54999, s2s_loss 0.47690, 58.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05967, exact_match 0.87277, 1.38 secs\n",
      "\u001b[1m---- Epoch 201/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54250, s2s_loss 0.49247, 59.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05972, exact_match 0.87515, 1.40 secs\n",
      "\u001b[1m---- Epoch 202/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.71320, s2s_loss 0.48792, 58.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06007, exact_match 0.86920, 1.43 secs\n",
      "\u001b[1m---- Epoch 203/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.54449, s2s_loss 0.48278, 58.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05989, exact_match 0.87039, 1.39 secs\n",
      "\u001b[1m---- Epoch 204/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.40788, s2s_loss 0.48226, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05978, exact_match 0.87039, 1.41 secs\n",
      "\u001b[1m---- Epoch 205/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44705, s2s_loss 0.48986, 58.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05826, exact_match 0.87990, 1.40 secs\n",
      "\u001b[1m---- Epoch 206/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.48360, s2s_loss 0.48138, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06010, exact_match 0.87515, 1.38 secs\n",
      "\u001b[1m---- Epoch 207/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.39217, s2s_loss 0.48162, 58.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05750, exact_match 0.88347, 1.38 secs\n",
      "\u001b[1m---- Epoch 208/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.34763, s2s_loss 0.48213, 58.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05734, exact_match 0.88466, 1.42 secs\n",
      "\u001b[1m---- Epoch 209/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.43091, s2s_loss 0.49069, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05662, exact_match 0.88942, 1.41 secs\n",
      "\u001b[1m---- Epoch 210/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.57638, s2s_loss 0.48454, 58.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05660, exact_match 0.88942, 1.39 secs\n",
      "\u001b[1m---- Epoch 211/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.34798, s2s_loss 0.46838, 57.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05660, exact_match 0.88704, 1.39 secs\n",
      "\u001b[1m---- Epoch 212/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.44309, s2s_loss 0.49160, 58.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05652, exact_match 0.88942, 1.38 secs\n",
      "\u001b[1m---- Epoch 213/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.81508, s2s_loss 0.47830, 58.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05942, exact_match 0.87277, 1.39 secs\n",
      "\u001b[1m---- Epoch 214/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.16223, s2s_loss 0.48116, 58.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05890, exact_match 0.87753, 1.38 secs\n",
      "\u001b[1m---- Epoch 215/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40443, s2s_loss 0.48404, 58.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05916, exact_match 0.87515, 1.41 secs\n",
      "\u001b[1m---- Epoch 216/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35894, s2s_loss 0.47914, 58.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05883, exact_match 0.87872, 1.41 secs\n",
      "\u001b[1m---- Epoch 217/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.52593, s2s_loss 0.47745, 59.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05920, exact_match 0.87753, 1.39 secs\n",
      "\u001b[1m---- Epoch 218/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.19575, s2s_loss 0.48415, 58.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05900, exact_match 0.87634, 1.40 secs\n",
      "\u001b[1m---- Epoch 219/300\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "   iteration 130900\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240105_180720_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\" \\\n",
    "--epochs 300 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 20 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--task_name \"multitask\" \\\n",
    "--experiment_name \"s2f+f2m+f2c+s2co+s2cal+nli+mlm\" \\\n",
    "--multitask_name_list \\\n",
    "\"nli\" \\\n",
    "\"sentence2facts\" \\\n",
    "\"fact2metadata\" \\\n",
    "\"fact2comparison\" \\\n",
    "\"sentence2chestimagenome_observations\" \\\n",
    "\"sentence2chestimagenome_anatomical_locations\" \\\n",
    "\"mlm\" \\\n",
    "--task2weight '{\"sentence2facts\": 1.0, \"fact2metadata\": 1.0, \"fact2comparison\": 0.3, \"sentence2chestimagenome_observations\": 1.0, \"sentence2chestimagenome_anatomical_locations\": 1.0, \"nli\": 8.0, \"mlm\": 5.0}' \\\n",
    "--sentence_to_facts_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\"\\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\" \\\n",
    "--fact_to_metadata_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--fact_to_comparison_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "--chest_imagenome_phrases2labels_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\" \\\n",
    "--chest_imagenome_obs_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--chest_imagenome_anatloc_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(157742,20319191).jsonl\" \\\n",
    "--use_sentence2facts_for_nli \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--paraphrased_inputs_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--only_validate_nli \\\n",
    "--nli1_only_on_val \\\n",
    "--seq2seq_model_name \"t5\" \\\n",
    "--t5_model_name \"t5-small\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240106_002348_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 20\n",
      "   override_lr: False\n",
      "   task_name: multitask\n",
      "   experiment_name: s2f+f2m+f2c+s2co+s2cal+nli+mlm\n",
      "   multitask_name_list: ['nli', 'sentence2facts', 'fact2metadata', 'fact2comparison', 'sentence2chestimagenome_observations', 'sentence2chestimagenome_anatomical_locations', 'mlm']\n",
      "   task2weight: {'sentence2facts': 1.0, 'fact2metadata': 1.0, 'fact2comparison': 0.3, 'sentence2chestimagenome_observations': 1.0, 'sentence2chestimagenome_anatomical_locations': 1.0, 'nli': 8.0, 'mlm': 5.0}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_phrases2labels_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl']\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl']\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl']\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl\n",
      "   use_sentence2facts_for_nli: True\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   only_validate_nli: True\n",
      "   nli1_only_on_val: True\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 20, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9891 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mB\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mbilateral\u001b[0m\n",
      "\u001b[1m\u001b[35mboth sides\u001b[0m\n",
      "\u001b[1m\u001b[35mboth\u001b[0m\n",
      "\u001b[1m\u001b[35mon both sides\u001b[0m\n",
      "\u001b[1m\u001b[35msymmetrical\u001b[0m\n",
      "\u001b[1m\u001b[35mequally on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mpresent on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mseen on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35moccurring on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mappearing on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mfound on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mseen bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mobserved bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mdetected bilaterally\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9890 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilus\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the hilum on the left side\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 8511 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary vasculature\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral lung vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary blood vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mlung periphery vasculature\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature of the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels in the outer regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular network in the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary circulation\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels supplying the outer areas of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature in the peripheral regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular system in the outer regions of the lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 1864 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35ma\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mone\u001b[0m\n",
      "\u001b[1m\u001b[35msingle\u001b[0m\n",
      "\u001b[1m\u001b[35msole\u001b[0m\n",
      "\u001b[1m\u001b[35monly\u001b[0m\n",
      "\u001b[1m\u001b[35mindividual\u001b[0m\n",
      "\u001b[1m\u001b[35mlone\u001b[0m\n",
      "\u001b[1m\u001b[35msolitary\u001b[0m\n",
      "\u001b[1m\u001b[35munique\u001b[0m\n",
      "\u001b[1m\u001b[35mdistinct\u001b[0m\n",
      "\u001b[1m\u001b[35msingular\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 10000 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9999 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9997 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9994 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9996 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome not detected\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome is not observed\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant constriction\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of notable narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of substantial narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of notable constriction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19943 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of wheezing\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing detected\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing observed\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing present\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing found\u001b[0m\n",
      "--------\n",
      "\u001b[1mNumber of unique inputs: 190085\u001b[0m\n",
      "\u001b[1mNumber of total paraphrases: 2093173\u001b[0m\n",
      "Number of medical sentences from paraphrases: 2039914\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2facts dataset\u001b[0m\n",
      "Loaded 9999 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19971 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\n",
      "Loaded 14990 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\n",
      "Loaded 14991 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2F: Global infiltrative pulmonary abnormality has been improving radiographic since , but some of the change may be due to positive pressure ventilator support, rather than real improvement.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"global infiltrative pulmonary abnormality\", \"improvement of global infiltrative pulmonary abnormality\", \"change due to positive pressure ventilator support\", \"change not indicating real improvement\"]\u001b[0m\n",
      "Number of train examples: 84935\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing nli dataset\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl...\n",
      "Number of samples: 169025\n",
      "Number of sources: 7\n",
      "----\n",
      "\u001b[1mLoading sentence2facts input/output pairs for NLI...\u001b[0m\n",
      "Number of entailment samples added: 368966\n",
      "----\n",
      "\u001b[1mLoading general domain datasets...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset...\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: contradiction -> 158860 (5157.45)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The pulmonary edema has improved but the left lower lobe atelectasis has not. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mInterval improvement in the pulmonary edema and left lower lobe atelectasis.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: contradiction -> 21464 (2979.54)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Stable large left pleural effusion and slight increase in small right pleural effusion. #Hypothesis: No pneumothorax or pleural effusion is clearly evident.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: contradiction -> 8340 (2210.12)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: An endotracheal tube terminates 6.4 cm above the carina. #Hypothesis: There is a new endotracheal tube, which terminates approximately 1.2 cm above the carina.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: contradiction -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: 71yo female with known aortic stenosis. #Hypothesis:  The patient has normal cardiac function\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: contradiction -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Non-contrast head CT done before her seizure demonstrated possible low attenuation within the white matter of the left occipital lobe and within the right occipital lobe to a lesser extent. #Hypothesis:  Patient has normal imaging findings\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: contradiction -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Per family members, rash started at least for 1 wk. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m Patient has negative ROS\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: contradiction -> 212 (461.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Lungs are well-expanded and clear without focal consolidation concerning for pneumonia. #Hypothesis: Lungs are hyperinflated but clear.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: anli | Label: contradiction -> 88178 (4433.68)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Of course, and that again -- no, I think that that was ill-considered. I did support it at the time. It was in the previous administration, in the Bush-Quayle administration, and I think in retrospect the lessons there are ones that we should take very, very seriously. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mI supported it after the Bush-Quayle administration\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: contradiction -> 274712 (5897.92)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: In 1997, LSC adopted final regulations clarifying a504(a)(16). #Hypothesis: In 2004 the LSC adopted final regulations. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: snli | Label: contradiction -> 379404 (6365.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: A man (with a name tag on his shirt) is sitting in a printed cloth chair in what appears to be a hospital room. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mA man attends a funeral.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: entailment -> 26370 (3167.85)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The side port is just below the gastroesophageal junction. #Hypothesis: The side port is located near the gastroesophageal junction.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: entailment -> 10890 (2411.88)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The hilar mediastinal contour is within normal limits. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe hilar and mediastinal structures are unremarkable.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: entailment -> 3540 (1638.66)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Bibasal consolidations are extensive and unchanged. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mBibasal consolidations are extensive and unchanged.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSource: mednli_train | Label: entailment -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: He denies any history of throat swelling or respiratory involvement. #Hypothesis:  The patient denies throat swelling\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: entailment -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: On arrival to the MICU, patient is hemodynamically stable. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m Patient has normal vital signs\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: entailment -> 946 (966.10)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The history is per the pt and her granddaughter who was present for the events. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient did not give the history.\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: entailment -> 186 (428.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No pleural effusion or pneumothorax is seen. #Hypothesis: There is no pneumothorax or pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: s2f | Label: entailment -> 368966 (6324.57)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: I would recommend repeating the examination with external devices removes, so that the abnormality can be better evaluated. #Hypothesis: presence of external devices\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: anli | Label: entailment -> 108502 (4680.39)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The Seven Ages of Man is a series of paintings by Robert Smirke, derived from a monologue from William Shakespeare's \"As You Like It\", spoken as the melancholy Jaques in Act II Scene VII. The phrase begins as \"all the world's a stage\". The stages referred are: infant, schoolboy, lover, soldier, justice, pantaloon and old age. #Hypothesis: robert smirke has hands\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: entailment -> 275682 (5902.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: You are a clever woman, Rita; but you are also a fool!  #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mYou are smart Rita, but you are also stupid!\u001b[0m\n",
      "\u001b[1mSource: snli | Label: entailment -> 380226 (6369.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: A man in a brown jumpsuit riding his bicycle on a sidewalk. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mA man riding a bike outside.\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: neutral -> 38246 (3527.79)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: In comparison with the study of , the cardiac silhouette remains at the upper limits of normal in size and there is continued blunting of the costophrenic angles. #Hypothesis: There is no evidence of lung consolidation in the patient's chest X-ray.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: gpt-4-1106-preview_reasoning-prompt | Label: neutral -> 33350 (3392.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Remainder of the heart and lungs appears unchanged. #Hypothesis: The hilar and cardiomediastinal silhouettes are normal.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613_reasoning-prompt | Label: neutral -> 7936 (2173.86)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Midline drain and right thoracostomy tube still in place. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe cardiac mediastinal silhouette is unremarkable.\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: neutral -> 7486 (2131.74)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The patient reported two admissions to outside hospitals since her discharge on [**9-27**] and weakness as well as gastrointestinal bleed. #Hypothesis:  patient has lower gi bleed\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: neutral -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Over the past week PTA he has been more somnolent and difficult to arrouse. . #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m He is disorientated and complains of weakness \u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: neutral -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: WBC 21.7, Hct 42.4, Plate 493, N 88, band 1. #Hypothesis:  The patient is bacteremic. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: neutral -> 562 (762.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Minimal septal thickening seen within the peripheral aspect of the left lung base suggests mild pulmonary vascular engorgement. #Hypothesis: There is decreased retrosternal space implying mild right ventricular enlargement.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: anli | Label: neutral -> 141850 (5012.51)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Felix Anthony Silla (born Roccacasale, L'Aquila - Italy, January 11, 1937), also credited as Felix Cilla, is an Italian-born American film and television actor and stuntman, known for his role as the costumed character of \"Cousin Itt\" on television's \"The Addams Family\", with the voice usually provided by Anthony Magro (1923–2004). Silla also appeared in many other classic character roles. #Hypothesis: Felix Anthony Silla currently lives in the US.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: neutral -> 274304 (5895.82)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: These were bilateral trade accords, countertrade, export financing, and compliance with trade-related industrial policy. #Hypothesis: The export financing accounted for the bulk of it, however.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: snli | Label: neutral -> 378436 (6362.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: A couple with an inflatable tiger. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe couple are holding hands.\u001b[0m\n",
      "----\n",
      "Number of RadNLI test samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is no focal consolidation, pleural effusion or pneumothorax. #Hypothesis: Small left pleural effusion is difficult to exclude.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Hilar and mediastinal silhouettes are unremarkable. #Hypothesis: The mediastinal and hilar contours are normal.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: moderate left pleural effusion has increased since ___. #Hypothesis: moderate left pleural effusion has worsened since ___.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The large right pleural effusion has decreased somewhat, but a moderate pleural effusion still remains in spite of drainage catheter placement. #Hypothesis: There is improved aeration of the left lung base without blunting of the costophrenic angle to suggest pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Cardiac silhouette size is normal. #Hypothesis: The cardiac silhouette remains enlarged.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset...\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2metadata dataset\u001b[0m\n",
      "Loaded 19989 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\n",
      "Loaded 19948 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2M: orogastric tube reverses course in the distal esophagus\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"anatomical location\": \"distal esophagus\", \"detailed observation\": \"orogastric tube reverses course in the distal esophagus\", \"short observation\": \"orogastric tube reversal in distal esophagus\", \"category\": \"tubes and lines\", \"health status\": \"unknown\", \"prev_study_comparison?\": \"no\", \"comparison status\": \"\"}\u001b[0m\n",
      "Number of train examples: 59921\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2comparison dataset\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Added 341589 paraphrased inputs (total 443344)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mPersistent enlargement of the cardiac and mediastinal silhouette\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2C: aposterior spinal fusion of the thoracolumbar spine\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "Counter:\n",
      "Counter({'no comparison': 276665, 'stable/unchanged': 46121, 'worsened': 27557, 'resolved': 26871, 'improved': 18519, 'progressed': 10316, 'new finding': 8625, 'larger': 5687, 'increase': 5642, 'decrease': 5399, 'smaller': 4471, 'unclear comparison': 3230, 'position changed': 2691, 'reappeared': 1522, 'other': 28})\n",
      "Output: no comparison, train size: 276665, weight: 5907.929922971326\n",
      "Output: worsened, train size: 27557, weight: 3209.1325193060784\n",
      "Output: resolved, train size: 26871, weight: 3185.453100878379\n",
      "Output: larger, train size: 5687, weight: 1940.7070776699022\n",
      "Output: improved, train size: 18519, weight: 2849.227679075855\n",
      "Output: smaller, train size: 4471, weight: 1783.1739746266992\n",
      "Output: progressed, train size: 10316, weight: 2369.9771784894483\n",
      "Output: increase, train size: 5642, weight: 1935.3623789696771\n",
      "Output: decrease, train size: 5399, weight: 1905.921326927956\n",
      "Output: unclear comparison, train size: 3230, weight: 1584.1488321235274\n",
      "Output: new finding, train size: 8625, weight: 2234.8902920005276\n",
      "Output: stable/unchanged, train size: 46121, weight: 3718.930087463258\n",
      "Output: reappeared, train size: 1522, weight: 1181.5197321713774\n",
      "Output: other, train size: 28, weight: 111.1011515767352\n",
      "Output: position changed, train size: 2691, weight: 1479.1773935508618\n",
      "Number of train examples: 443344\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_observations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:19<00:00, 27833.46it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mBibasilar patchy opacities are not fully evaluated on this view.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe left-sided pleural effusion and left retrocardiac opacity are unchanged.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\", \"pleural effusion\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThese finding may represent persistent consolidation and/or atelectasis and lateral view or CT could be performed to further assess.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"consolidation\", \"lung opacity\"]\u001b[0m\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\n",
      "Loaded 14859 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\n",
      "Loaded 19952 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\n",
      "Loaded 19959 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\n",
      "Loaded 9977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\n",
      "Loaded 9987 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\n",
      "Added 103692 paraphrased inputs (total 749511)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mobtain another PA chest radiograph after symptoms have improved\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "100%|███████████████████████████████| 749511/749511 [00:03<00:00, 212588.66it/s]\n",
      "Number of total examples: 749511\n",
      "Number of examples without labels: 160864\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 4498\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  central venous line\u001b[0m\n",
      "\u001b[93m\u001b[1m  Ability to recognize and respond to signs of cardiac arrest\u001b[0m\n",
      "\u001b[93m\u001b[1m  chest wall abnormality\u001b[0m\n",
      "\u001b[93m\u001b[1m  surgical sponge\u001b[0m\n",
      "\u001b[93m\u001b[1m  communication between stomach and lung\u001b[0m\n",
      "\u001b[93m\u001b[1m  trachea abnormality\u001b[0m\n",
      "\u001b[93m\u001b[1m  the art of macro photography\u001b[0m\n",
      "\u001b[93m\u001b[1m  thoracic inlet\u001b[0m\n",
      "\u001b[93m\u001b[1m  malignancy\u001b[0m\n",
      "\u001b[93m\u001b[1m  PICC line\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: lung opacity, train size: 306147, weight: 6052.314258424769\n",
      "Label: None, train size: 160864, weight: 5173.661459041586\n",
      "Label: atelectasis, train size: 94030, weight: 4509.162252814013\n",
      "Label: pleural effusion, train size: 93022, weight: 4496.442365546256\n",
      "Label: pneumonia, train size: 46693, weight: 3731.7501716078773\n",
      "Label: pulmonary edema/hazy opacity, train size: 43454, weight: 3657.3900232811834\n",
      "Label: enlarged cardiac silhouette, train size: 35280, weight: 3447.4421996052774\n",
      "Label: enteric tube, train size: 27597, weight: 3210.498555548174\n",
      "Label: vascular congestion, train size: 25952, weight: 3152.9573291446445\n",
      "Label: consolidation, train size: 25260, weight: 3127.8725669115815\n",
      "Label: lung lesion, train size: 23572, weight: 3064.285857407827\n",
      "Label: endotracheal tube, train size: 21905, weight: 2997.8020284788163\n",
      "Label: pleural/parenchymal scarring, train size: 18682, weight: 2856.8572521836454\n",
      "Label: low lung volumes, train size: 18588, weight: 2852.463878874611\n",
      "Label: pneumothorax, train size: 16796, weight: 2765.12204817844\n",
      "Label: mass/nodule (not otherwise specified), train size: 14471, weight: 2640.017501159535\n",
      "Label: picc, train size: 14283, weight: 2629.221328708754\n",
      "Label: cardiac pacer and wires, train size: 13389, weight: 2576.2889815362387\n",
      "Label: ij line, train size: 12405, weight: 2514.6981737246747\n",
      "Label: lobar/segmental collapse, train size: 12371, weight: 2512.5021455815854\n",
      "Label: aspiration, train size: 12238, weight: 2503.8658589102106\n",
      "Label: linear/patchy atelectasis, train size: 12101, weight: 2494.8922039241074\n",
      "Label: chest tube, train size: 11749, weight: 2471.46349448124\n",
      "Label: airspace opacity, train size: 11717, weight: 2469.306345996421\n",
      "Label: enlarged hilum, train size: 11714, weight: 2469.103875848375\n",
      "Label: rib fracture, train size: 9956, weight: 2342.7541486499636\n",
      "Label: hyperaeration, train size: 8180, weight: 2195.9279342345876\n",
      "Label: mediastinal widening, train size: 7463, weight: 2129.5314346971195\n",
      "Label: chest port, train size: 7009, weight: 2084.8845125773137\n",
      "Label: copd/emphysema, train size: 6983, weight: 2082.260542425887\n",
      "Label: costophrenic angle blunting, train size: 6952, weight: 2079.1220606210954\n",
      "Label: vascular calcification, train size: 6944, weight: 2078.310371948699\n",
      "Label: tortuous aorta, train size: 6438, weight: 2025.4354983318656\n",
      "Label: fluid overload/heart failure, train size: 6402, weight: 2021.5528120628337\n",
      "Label: elevated hemidiaphragm, train size: 6133, weight: 1991.9943635549255\n",
      "Label: infiltration, train size: 5736, weight: 1946.4900159268673\n",
      "Label: mediastinal displacement, train size: 5220, weight: 1883.5769616626842\n",
      "Label: subcutaneous air, train size: 5140, weight: 1873.4003520541203\n",
      "Label: increased reticular markings/ild pattern, train size: 5109, weight: 1869.4242864055232\n",
      "Label: multiple masses/nodules, train size: 4868, weight: 1837.866541377119\n",
      "Label: spinal fracture, train size: 4604, weight: 1801.8953640063596\n",
      "Label: subclavian line, train size: 4440, weight: 1778.7494591002173\n",
      "Label: interstitial lung disease, train size: 4264, weight: 1753.173688697151\n",
      "Label: pigtail catheter, train size: 4257, weight: 1752.1399805275955\n",
      "Label: spinal degenerative changes, train size: 4162, weight: 1737.9816177147438\n",
      "Label: superior mediastinal mass/enlargement, train size: 4157, weight: 1737.2296621177059\n",
      "Label: hernia, train size: 3762, weight: 1675.527028301087\n",
      "Label: vascular redistribution, train size: 3623, weight: 1652.6453368828752\n",
      "Label: calcified nodule, train size: 3469, weight: 1626.5059586547704\n",
      "Label: sub-diaphragmatic air, train size: 3423, weight: 1618.5283906143927\n",
      "Label: tracheostomy tube, train size: 3391, weight: 1612.9309556389703\n",
      "Label: bone lesion, train size: 3358, weight: 1607.116732365291\n",
      "Label: scoliosis, train size: 3061, weight: 1552.7507585367864\n",
      "Label: granulomatous disease, train size: 2943, weight: 1530.0471242872911\n",
      "Label: swan-ganz catheter, train size: 2894, weight: 1520.4184683791714\n",
      "Label: prosthetic valve, train size: 2572, weight: 1453.9094023139187\n",
      "Label: lung cancer, train size: 2555, weight: 1450.2289229189291\n",
      "Label: alveolar hemorrhage, train size: 2530, weight: 1444.7831652398547\n",
      "Label: rotated, train size: 2258, weight: 1382.7784896739577\n",
      "Label: shoulder osteoarthritis, train size: 2196, weight: 1367.8758349752577\n",
      "Label: bronchiectasis, train size: 2124, weight: 1350.1735399073136\n",
      "Label: cabg grafts, train size: 1959, weight: 1307.8676309222412\n",
      "Label: pericardial effusion, train size: 1814, weight: 1268.4657612483188\n",
      "Label: hydropneumothorax, train size: 1631, weight: 1215.294185273704\n",
      "Label: artifact, train size: 1623, weight: 1212.872238143207\n",
      "Label: breast/nipple shadows, train size: 1557, weight: 1192.5514618266216\n",
      "Label: clavicle fracture, train size: 1556, weight: 1192.2387865976527\n",
      "Label: cyst/bullae, train size: 1475, weight: 1166.411761557198\n",
      "Label: pneumomediastinum, train size: 1358, weight: 1127.223892522407\n",
      "Label: intra-aortic balloon pump, train size: 931, weight: 959.3546703073852\n",
      "Label: goiter, train size: 906, weight: 947.9406351817632\n",
      "Label: mediastinal drain, train size: 904, weight: 947.0179422691563\n",
      "Label: aortic graft/repair, train size: 775, weight: 884.1976644686547\n",
      "Label: diaphragmatic eventration (benign), train size: 406, weight: 650.6631467873593\n",
      "Label: skin fold, train size: 311, weight: 567.8220950583348\n",
      "--------\n",
      "Number of train examples: 1270062\n",
      "Number of train datasets: 75\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Improvement is noted in the consolidation at the base of the right lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"consolidation\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: With the chin down, tip of the endotracheal tube is 6 cm from the carina.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"endotracheal tube\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: On the left, I cannot exclude concurrent pneumonia as well as a small pleural effusion.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\", \"pleural effusion\", \"pneumonia\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: There is severe degenerative change of the right shoulder.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"shoulder osteoarthritis\"]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_anatomical_locations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:12<00:00, 46127.49it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMultifocal opacities in the right lower lobe and a small area of increased density on the left.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lower lung zone\", \"left lung\", \"left mid lung zone\", \"right lower lung zone\", \"right lung\", \"right mid lung zone\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mFINDINGS: Tip of endotracheal tube terminates about 6 cm above the carina.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"carina\", \"neck\", \"trachea\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe bilateral mid to lower lung hazy opacities, right more than left, have improved.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left hilar structures\", \"left lower lung zone\", \"left lung\", \"left mid lung zone\", \"right hilar structures\", \"right lower lung zone\", \"right lung\", \"right mid lung zone\"]\u001b[0m\n",
      "Loaded 24929 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\n",
      "Loaded 24951 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\n",
      "Loaded 24988 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\n",
      "Added 352702 paraphrased inputs (total 983681)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe mediastinal region above the heart shows no signs of change\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"mediastinum\", \"right upper lung zone\"]\u001b[0m\n",
      "100%|███████████████████████████████| 983681/983681 [00:05<00:00, 193753.28it/s]\n",
      "Number of total examples: 983681\n",
      "Number of examples without labels: 96958\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 24482\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  right mid abdomen\u001b[0m\n",
      "\u001b[93m\u001b[1m  pulmonary\u001b[0m\n",
      "\u001b[93m\u001b[1m  left thorax\u001b[0m\n",
      "\u001b[93m\u001b[1m  left mediastinum\u001b[0m\n",
      "\u001b[93m\u001b[1m  positive pressure ventilation\u001b[0m\n",
      "\u001b[93m\u001b[1m  chest tube\u001b[0m\n",
      "\u001b[93m\u001b[1m  left pectoral\u001b[0m\n",
      "\u001b[93m\u001b[1m  sternum\u001b[0m\n",
      "\u001b[93m\u001b[1m  arm\u001b[0m\n",
      "\u001b[93m\u001b[1m  endotracheal tube\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: right lung, train size: 349137, weight: 6243.158334871754\n",
      "Label: left lung, train size: 347284, weight: 6235.3525090961175\n",
      "Label: mediastinum, train size: 144031, weight: 5031.875282788735\n",
      "Label: left lower lung zone, train size: 139395, weight: 4990.409355647508\n",
      "Label: right lower lung zone, train size: 133961, weight: 4940.320692958888\n",
      "Label: right hilar structures, train size: 104637, weight: 4636.605540005738\n",
      "Label: left hilar structures, train size: 99507, weight: 4576.371609488107\n",
      "Label: cardiac silhouette, train size: 97597, weight: 4553.288558553895\n",
      "Label: None, train size: 96958, weight: 4545.482695622845\n",
      "Label: left costophrenic angle, train size: 87782, weight: 4428.425926894693\n",
      "Label: right costophrenic angle, train size: 85867, weight: 4402.731970876394\n",
      "Label: abdomen, train size: 69685, weight: 4164.391638360652\n",
      "Label: neck, train size: 68283, weight: 4141.664068017979\n",
      "Label: right mid lung zone, train size: 42058, weight: 3623.944216201201\n",
      "Label: right chest wall, train size: 39937, weight: 3571.3605151829925\n",
      "Label: left chest wall, train size: 39710, weight: 3565.5994137333387\n",
      "Label: left mid lung zone, train size: 39072, weight: 3549.263144482792\n",
      "Label: trachea, train size: 37858, weight: 3517.5713767379216\n",
      "Label: spine, train size: 36967, weight: 3493.7833910652507\n",
      "Label: right upper lung zone, train size: 31930, weight: 3349.8346375050337\n",
      "Label: upper mediastinum, train size: 31127, weight: 3325.2150888629853\n",
      "Label: left hemidiaphragm, train size: 26182, weight: 3161.175801242095\n",
      "Label: left upper lung zone, train size: 24852, weight: 3112.8230688218005\n",
      "Label: right hemidiaphragm, train size: 22859, weight: 3036.325975604692\n",
      "Label: svc, train size: 22448, weight: 3019.8900361258507\n",
      "Label: aortic arch, train size: 21171, weight: 2967.237668771257\n",
      "Label: right apical zone, train size: 20033, weight: 2918.1322541597774\n",
      "Label: left apical zone, train size: 17367, weight: 2793.7252100239602\n",
      "Label: carina, train size: 17254, weight: 2788.1246491411166\n",
      "Label: right atrium, train size: 15526, weight: 2698.6228163775186\n",
      "Label: right shoulder, train size: 13652, weight: 2592.1436672609016\n",
      "Label: right clavicle, train size: 12056, weight: 2491.9272027167312\n",
      "Label: left clavicle, train size: 11934, weight: 2483.844699572688\n",
      "Label: left shoulder, train size: 10455, weight: 2380.2892980476154\n",
      "Label: cavoatrial junction, train size: 7297, weight: 2113.457361673451\n",
      "Label: left arm, train size: 6546, weight: 2036.9838564325023\n",
      "Label: right arm, train size: 6398, weight: 2021.1203630444604\n",
      "Label: left breast, train size: 3000, weight: 1541.097763435584\n",
      "Label: right breast, train size: 2328, weight: 1399.2439717598722\n",
      "--------\n",
      "Number of train examples: 2384141\n",
      "Number of train datasets: 39\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Cardiac silhouette within the normal range\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Right PICC line tip is in the right atrium and should be pulled back at least 4 cm to secure it position at the cavoatrial junction or above.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"cavoatrial junction\", \"mediastinum\", \"right atrium\", \"right shoulder\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: WET READ: ___ ___ ___ 5:29 PM 1. Cardiomegaly with new mild-moderate pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac silhouette\", \"left hilar structures\", \"left lung\", \"right hilar structures\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: IMPRESSION: Left PICC terminates in the mid-to-proximal forearm, not in appropriate position.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left shoulder\", \"mediastinum\"]\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing mlm dataset\u001b[0m\n",
      "Number of general sentences: 1377099\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 1377099\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:06<00:00, 209347.25it/s]\n",
      "\tlen(valid_tokens): 21522\n",
      "100%|█████████████████████████████| 1377099/1377099 [00:12<00:00, 106317.94it/s]\n",
      "Number of sentences: 1348333\n",
      "Number of bins: 7\n",
      "Bin size: 958219, weight: 7845.006940358202\n",
      "Bin size: 219530, weight: 5586.745498624757\n",
      "Bin size: 110971, weight: 4707.694773844805\n",
      "Bin size: 46036, weight: 3717.013976819967\n",
      "Bin size: 8332, weight: 2209.4180153128023\n",
      "Bin size: 4974, weight: 1851.8906668241702\n",
      "Bin size: 271, weight: 527.9351334797618\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Jon [tok1] to climb quietly to the outcrop so as not to wake the [tok2] of the group .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] tried [tok2] rest\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A [tok1] is putting on a show for people .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] man\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: Tom is [tok1] the race at home .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] watching\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: A boy is [tok1] blue .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] wearing\u001b[0m\n",
      "Number of medical sentences: 2919318\n",
      "Preparing masked LM dataset with masking\n",
      "\tmin_token_count: 20\n",
      "\tmasking_fraction: 0.15\n",
      "\tlen(sentences): 2919318\n",
      "Tokenizing sentences...\n",
      "Lowercasing tokens...\n",
      "100%|█████████████████████████████| 2919318/2919318 [00:11<00:00, 251629.60it/s]\n",
      "\tlen(valid_tokens): 8332\n",
      "100%|█████████████████████████████| 2919318/2919318 [00:25<00:00, 113640.28it/s]\n",
      "Number of sentences: 2890662\n",
      "Number of bins: 10\n",
      "Bin size: 1344291, weight: 8437.850531101125\n",
      "Bin size: 596533, weight: 7062.684120902851\n",
      "Bin size: 539383, weight: 6903.445074875901\n",
      "Bin size: 302165, weight: 6033.515132988266\n",
      "Bin size: 88538, weight: 4438.442259502969\n",
      "Bin size: 15748, weight: 2710.5508683545645\n",
      "Bin size: 3422, weight: 1618.3540693788286\n",
      "Bin size: 261, weight: 517.3766629445327\n",
      "Bin size: 176, weight: 415.06604923310664\n",
      "Bin size: 145, weight: 370.132172294277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: right lateral pulmonary [tok1]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] lobe\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: No change in the positioning of the [tok1] PICC\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] left\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: congestion in the [tok1] lung base\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] right\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[35mMLM: lower lung [tok1]\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[35m[tok1] limit\u001b[0m\n",
      "----------------------------------------\n",
      "Number of train datasets: 7\n",
      "Number of val datasets: 1\n",
      "----------------------------------------\n",
      "Examples of val datasets:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: interval improvement in mild pulmonary edema. #Hypothesis: interval increase in mild pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The lungs are clear. #Hypothesis: The lungs are well expanded and clear.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There is mild pulmonary vascular congestion. #Hypothesis: There is extensive scoliosis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: small left effusion has minimally increased. #Hypothesis: small left effusion has minimally worsened.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "seq2seq_trainer.name =  multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240116_111115_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240116_111115_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_192_exact_match+s2s_loss=0.8977.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240106_002348_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/checkpoint_192_exact_match+s2s_loss=0.8977.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240116_111115_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.34723, s2s_loss 0.49062, 59.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05541, exact_match 0.89655, 1.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_exact_match+s2s_loss=0.8969.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.76995, s2s_loss 0.48013, 58.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05574, exact_match 0.89417, 1.37 secs\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.53919, s2s_loss 0.48430, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05569, exact_match 0.89298, 1.37 secs\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.57712, s2s_loss 0.47783, 58.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05383, exact_match 0.88942, 1.37 secs\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.74316, s2s_loss 0.48377, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05456, exact_match 0.88823, 1.38 secs\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.48281, s2s_loss 0.47745, 57.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05589, exact_match 0.89061, 1.36 secs\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.40156, s2s_loss 0.48643, 57.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05563, exact_match 0.88942, 1.36 secs\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.33111, s2s_loss 0.48747, 58.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05580, exact_match 0.89536, 1.35 secs\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.38792, s2s_loss 0.48586, 57.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05658, exact_match 0.88942, 1.37 secs\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31661, s2s_loss 0.48648, 57.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05661, exact_match 0.88823, 1.37 secs\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.58746, s2s_loss 0.48423, 55.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05650, exact_match 0.88823, 1.37 secs\n",
      "\u001b[1m---- Epoch 12/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46419, s2s_loss 0.48084, 58.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05641, exact_match 0.88823, 1.36 secs\n",
      "\u001b[1m---- Epoch 13/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32486, s2s_loss 0.49027, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05661, exact_match 0.88823, 1.37 secs\n",
      "\u001b[1m---- Epoch 14/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.56944, s2s_loss 0.47629, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05682, exact_match 0.88585, 1.36 secs\n",
      "\u001b[1m---- Epoch 15/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.44858, s2s_loss 0.48354, 54.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06057, exact_match 0.87158, 1.37 secs\n",
      "\u001b[1m---- Epoch 16/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.39317, s2s_loss 0.48394, 56.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06104, exact_match 0.86920, 1.39 secs\n",
      "\u001b[1m---- Epoch 17/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41784, s2s_loss 0.49170, 58.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06019, exact_match 0.87158, 1.36 secs\n",
      "\u001b[1m---- Epoch 18/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.76162, s2s_loss 0.47173, 58.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06028, exact_match 0.87158, 1.37 secs\n",
      "\u001b[1m---- Epoch 19/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.42620, s2s_loss 0.48570, 58.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06026, exact_match 0.87277, 1.37 secs\n",
      "\u001b[1m---- Epoch 20/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63226, s2s_loss 0.47828, 56.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06020, exact_match 0.87277, 1.37 secs\n",
      "\u001b[1m---- Epoch 21/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.73554, s2s_loss 0.48215, 58.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06064, exact_match 0.87515, 1.36 secs\n",
      "\u001b[1m---- Epoch 22/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.76840, s2s_loss 0.48696, 58.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06040, exact_match 0.87396, 1.36 secs\n",
      "\u001b[1m---- Epoch 23/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.30423, s2s_loss 0.47947, 58.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06021, exact_match 0.87396, 1.33 secs\n",
      "\u001b[1m---- Epoch 24/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.70270, s2s_loss 0.48088, 56.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05926, exact_match 0.87872, 1.37 secs\n",
      "\u001b[1m---- Epoch 25/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47284, s2s_loss 0.47648, 58.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05912, exact_match 0.87753, 1.36 secs\n",
      "\u001b[1m---- Epoch 26/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.40937, s2s_loss 0.47937, 58.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05922, exact_match 0.87634, 1.37 secs\n",
      "\u001b[1m---- Epoch 27/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40144, s2s_loss 0.48036, 57.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05911, exact_match 0.87753, 1.40 secs\n",
      "\u001b[1m---- Epoch 28/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55635, s2s_loss 0.47194, 54.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05919, exact_match 0.87515, 1.38 secs\n",
      "\u001b[1m---- Epoch 29/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.46079, s2s_loss 0.48508, 47.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06020, exact_match 0.87753, 1.37 secs\n",
      "\u001b[1m---- Epoch 30/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.39602, s2s_loss 0.47930, 57.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06215, exact_match 0.87039, 1.37 secs\n",
      "\u001b[1m---- Epoch 31/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.42124, s2s_loss 0.47519, 58.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05985, exact_match 0.87753, 1.36 secs\n",
      "\u001b[1m---- Epoch 32/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48924, s2s_loss 0.47888, 58.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06115, exact_match 0.87396, 1.39 secs\n",
      "\u001b[1m---- Epoch 33/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39270, s2s_loss 0.47431, 56.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06124, exact_match 0.87158, 1.37 secs\n",
      "\u001b[1m---- Epoch 34/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62824, s2s_loss 0.48364, 57.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06116, exact_match 0.87039, 1.41 secs\n",
      "\u001b[1m---- Epoch 35/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.38935, s2s_loss 0.48963, 57.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06107, exact_match 0.87158, 1.38 secs\n",
      "\u001b[1m---- Epoch 36/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31179, s2s_loss 0.47545, 58.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06104, exact_match 0.87158, 1.37 secs\n",
      "\u001b[1m---- Epoch 37/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35887, s2s_loss 0.48303, 57.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06284, exact_match 0.87039, 1.37 secs\n",
      "\u001b[1m---- Epoch 38/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.27320, s2s_loss 0.48635, 51.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05749, exact_match 0.88347, 1.38 secs\n",
      "\u001b[1m---- Epoch 39/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.78593, s2s_loss 0.48181, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05752, exact_match 0.88347, 1.36 secs\n",
      "\u001b[1m---- Epoch 40/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.46177, s2s_loss 0.48231, 58.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05873, exact_match 0.88466, 1.37 secs\n",
      "\u001b[1m---- Epoch 41/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.53177, s2s_loss 0.47254, 57.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05880, exact_match 0.88466, 1.38 secs\n",
      "\u001b[1m---- Epoch 42/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.66012, s2s_loss 0.48121, 52.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05886, exact_match 0.88347, 1.39 secs\n",
      "\u001b[1m---- Epoch 43/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41087, s2s_loss 0.48196, 58.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05896, exact_match 0.88347, 1.36 secs\n",
      "\u001b[1m---- Epoch 44/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.29365, s2s_loss 0.48678, 58.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05901, exact_match 0.88228, 1.48 secs\n",
      "\u001b[1m---- Epoch 45/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36354, s2s_loss 0.47856, 58.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05936, exact_match 0.87515, 1.45 secs\n",
      "\u001b[1m---- Epoch 46/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.76387, s2s_loss 0.48416, 59.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05887, exact_match 0.87753, 1.52 secs\n",
      "\u001b[1m---- Epoch 47/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.51930, s2s_loss 0.48647, 55.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05908, exact_match 0.87515, 1.40 secs\n",
      "\u001b[1m---- Epoch 48/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.58551, s2s_loss 0.48773, 55.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05999, exact_match 0.87634, 1.42 secs\n",
      "\u001b[1m---- Epoch 49/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.50051, s2s_loss 0.48081, 55.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05948, exact_match 0.87872, 1.41 secs\n",
      "\u001b[1m---- Epoch 50/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.57863, s2s_loss 0.48533, 55.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05942, exact_match 0.87753, 1.43 secs\n",
      "\u001b[1m---- Epoch 51/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.58601, s2s_loss 0.48423, 54.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05944, exact_match 0.87753, 1.47 secs\n",
      "\u001b[1m---- Epoch 52/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55221, s2s_loss 0.47964, 55.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05955, exact_match 0.87872, 1.56 secs\n",
      "\u001b[1m---- Epoch 53/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.54137, s2s_loss 0.48894, 55.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05959, exact_match 0.87158, 1.42 secs\n",
      "\u001b[1m---- Epoch 54/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.67218, s2s_loss 0.47592, 54.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05734, exact_match 0.88466, 1.42 secs\n",
      "\u001b[1m---- Epoch 55/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36326, s2s_loss 0.48299, 55.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05602, exact_match 0.89180, 1.38 secs\n",
      "\u001b[1m---- Epoch 56/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.44452, s2s_loss 0.47637, 55.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05707, exact_match 0.88942, 1.43 secs\n",
      "\u001b[1m---- Epoch 57/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.55420, s2s_loss 0.48240, 56.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05659, exact_match 0.89061, 1.40 secs\n",
      "\u001b[1m---- Epoch 58/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.55810, s2s_loss 0.48890, 55.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05666, exact_match 0.89061, 1.40 secs\n",
      "\u001b[1m---- Epoch 59/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.56145, s2s_loss 0.48931, 55.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05667, exact_match 0.89061, 1.37 secs\n",
      "\u001b[1m---- Epoch 60/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40922, s2s_loss 0.48061, 56.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05676, exact_match 0.89061, 1.37 secs\n",
      "\u001b[1m---- Epoch 61/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33235, s2s_loss 0.48605, 55.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05778, exact_match 0.87277, 1.44 secs\n",
      "\u001b[1m---- Epoch 62/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "loss 0.56032, s2s_loss 0.47890, 55.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05820, exact_match 0.88228, 1.37 secs\n",
      "\u001b[1m---- Epoch 63/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.67417, s2s_loss 0.47783, 55.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05837, exact_match 0.88347, 1.38 secs\n",
      "\u001b[1m---- Epoch 64/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.45627, s2s_loss 0.48109, 55.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05851, exact_match 0.87990, 1.37 secs\n",
      "\u001b[1m---- Epoch 65/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.73405, s2s_loss 0.47187, 54.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05804, exact_match 0.88109, 1.39 secs\n",
      "\u001b[1m---- Epoch 66/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45633, s2s_loss 0.48309, 55.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05787, exact_match 0.88228, 1.46 secs\n",
      "\u001b[1m---- Epoch 67/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.48886, s2s_loss 0.48392, 55.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05789, exact_match 0.88585, 1.38 secs\n",
      "\u001b[1m---- Epoch 68/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38368, s2s_loss 0.47065, 55.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05797, exact_match 0.88466, 1.38 secs\n",
      "\u001b[1m---- Epoch 69/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.34020, s2s_loss 0.47204, 54.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06170, exact_match 0.86326, 1.47 secs\n",
      "\u001b[1m---- Epoch 70/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.47954, s2s_loss 0.47869, 55.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05928, exact_match 0.87753, 1.43 secs\n",
      "\u001b[1m---- Epoch 71/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.03823, s2s_loss 0.47300, 56.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06089, exact_match 0.87515, 1.37 secs\n",
      "\u001b[1m---- Epoch 72/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.45899, s2s_loss 0.47611, 55.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06169, exact_match 0.87396, 1.37 secs\n",
      "\u001b[1m---- Epoch 73/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.54122, s2s_loss 0.48386, 56.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06103, exact_match 0.87515, 1.39 secs\n",
      "\u001b[1m---- Epoch 74/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31502, s2s_loss 0.48379, 55.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06075, exact_match 0.87634, 1.40 secs\n",
      "\u001b[1m---- Epoch 75/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40526, s2s_loss 0.48561, 55.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06056, exact_match 0.87634, 1.38 secs\n",
      "\u001b[1m---- Epoch 76/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37107, s2s_loss 0.47931, 54.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06046, exact_match 0.87634, 1.37 secs\n",
      "\u001b[1m---- Epoch 77/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44665, s2s_loss 0.48314, 56.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06103, exact_match 0.87634, 1.39 secs\n",
      "\u001b[1m---- Epoch 78/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.46689, s2s_loss 0.47656, 55.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05758, exact_match 0.88109, 1.36 secs\n",
      "\u001b[1m---- Epoch 79/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.50802, s2s_loss 0.47668, 55.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05791, exact_match 0.88109, 1.40 secs\n",
      "\u001b[1m---- Epoch 80/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55367, s2s_loss 0.47742, 54.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05772, exact_match 0.88347, 1.42 secs\n",
      "\u001b[1m---- Epoch 81/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.71030, s2s_loss 0.48353, 55.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05773, exact_match 0.88466, 1.43 secs\n",
      "\u001b[1m---- Epoch 82/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.44678, s2s_loss 0.47832, 55.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05760, exact_match 0.88466, 1.42 secs\n",
      "\u001b[1m---- Epoch 83/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62502, s2s_loss 0.47890, 55.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05773, exact_match 0.88466, 1.40 secs\n",
      "\u001b[1m---- Epoch 84/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.51537, s2s_loss 0.47664, 55.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05782, exact_match 0.88466, 1.43 secs\n",
      "\u001b[1m---- Epoch 85/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.29963, s2s_loss 0.47765, 54.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05953, exact_match 0.86920, 1.39 secs\n",
      "\u001b[1m---- Epoch 86/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38436, s2s_loss 0.47621, 55.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06031, exact_match 0.86445, 1.36 secs\n",
      "\u001b[1m---- Epoch 87/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.66184, s2s_loss 0.48938, 56.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05982, exact_match 0.87039, 1.38 secs\n",
      "\u001b[1m---- Epoch 88/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.33980, s2s_loss 0.46210, 55.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05972, exact_match 0.87515, 1.38 secs\n",
      "\u001b[1m---- Epoch 89/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.56205, s2s_loss 0.48353, 54.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05903, exact_match 0.87396, 1.38 secs\n",
      "\u001b[1m---- Epoch 90/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.54953, s2s_loss 0.47560, 56.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05899, exact_match 0.87515, 1.39 secs\n",
      "\u001b[1m---- Epoch 91/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28355, s2s_loss 0.48440, 56.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05891, exact_match 0.87277, 1.37 secs\n",
      "\u001b[1m---- Epoch 92/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41080, s2s_loss 0.48533, 55.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05889, exact_match 0.87396, 1.37 secs\n",
      "\u001b[1m---- Epoch 93/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.58272, s2s_loss 0.48011, 56.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06159, exact_match 0.87158, 1.56 secs\n",
      "\u001b[1m---- Epoch 94/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.56703, s2s_loss 0.48636, 54.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05688, exact_match 0.88585, 1.38 secs\n",
      "\u001b[1m---- Epoch 95/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.45598, s2s_loss 0.47170, 54.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05633, exact_match 0.88704, 1.38 secs\n",
      "\u001b[1m---- Epoch 96/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.36888, s2s_loss 0.48245, 56.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05625, exact_match 0.88823, 1.39 secs\n",
      "\u001b[1m---- Epoch 97/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.49235, s2s_loss 0.47410, 55.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05692, exact_match 0.88585, 1.39 secs\n",
      "\u001b[1m---- Epoch 98/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45994, s2s_loss 0.48422, 53.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05689, exact_match 0.88585, 1.41 secs\n",
      "\u001b[1m---- Epoch 99/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.51815, s2s_loss 0.47123, 55.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05697, exact_match 0.88585, 1.37 secs\n",
      "\u001b[1m---- Epoch 100/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.49806, s2s_loss 0.48430, 55.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05704, exact_match 0.88585, 1.37 secs\n",
      "\u001b[1m---- Epoch 101/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.58591, s2s_loss 0.49001, 56.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05536, exact_match 0.89893, 1.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_101_exact_match+s2s_loss=0.8980.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 102/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40445, s2s_loss 0.48754, 54.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05657, exact_match 0.89298, 1.43 secs\n",
      "\u001b[1m---- Epoch 103/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.52664, s2s_loss 0.48764, 53.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05875, exact_match 0.88347, 1.39 secs\n",
      "\u001b[1m---- Epoch 104/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.46980, s2s_loss 0.47407, 55.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05838, exact_match 0.88466, 1.36 secs\n",
      "\u001b[1m---- Epoch 105/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.48338, s2s_loss 0.48989, 55.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05828, exact_match 0.88466, 1.37 secs\n",
      "\u001b[1m---- Epoch 106/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.67678, s2s_loss 0.48656, 55.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05846, exact_match 0.88347, 1.39 secs\n",
      "\u001b[1m---- Epoch 107/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.59480, s2s_loss 0.47549, 55.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05850, exact_match 0.88466, 1.38 secs\n",
      "\u001b[1m---- Epoch 108/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.52555, s2s_loss 0.48020, 53.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05842, exact_match 0.88466, 1.39 secs\n",
      "\u001b[1m---- Epoch 109/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40909, s2s_loss 0.48109, 53.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05804, exact_match 0.87753, 1.35 secs\n",
      "\u001b[1m---- Epoch 110/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.81646, s2s_loss 0.48539, 56.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06683, exact_match 0.86445, 1.36 secs\n",
      "\u001b[1m---- Epoch 111/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.31514, s2s_loss 0.49310, 53.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06246, exact_match 0.87277, 1.41 secs\n",
      "\u001b[1m---- Epoch 112/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.52068, s2s_loss 0.47084, 55.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06036, exact_match 0.87396, 1.41 secs\n",
      "\u001b[1m---- Epoch 113/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.68719, s2s_loss 0.48034, 56.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06051, exact_match 0.87396, 1.32 secs\n",
      "\u001b[1m---- Epoch 114/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.22404, s2s_loss 0.48185, 56.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06022, exact_match 0.87634, 1.44 secs\n",
      "\u001b[1m---- Epoch 115/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41036, s2s_loss 0.48197, 55.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06018, exact_match 0.87634, 1.38 secs\n",
      "\u001b[1m---- Epoch 116/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.58763, s2s_loss 0.48708, 55.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06026, exact_match 0.87515, 1.38 secs\n",
      "\u001b[1m---- Epoch 117/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38422, s2s_loss 0.48409, 55.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05556, exact_match 0.89298, 1.40 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 118/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.23862, s2s_loss 0.47576, 56.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05599, exact_match 0.88585, 1.36 secs\n",
      "\u001b[1m---- Epoch 119/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.72553, s2s_loss 0.47830, 55.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05649, exact_match 0.88466, 1.40 secs\n",
      "\u001b[1m---- Epoch 120/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35526, s2s_loss 0.47622, 56.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05659, exact_match 0.88228, 1.37 secs\n",
      "\u001b[1m---- Epoch 121/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.26853, s2s_loss 0.47234, 54.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05658, exact_match 0.88228, 1.37 secs\n",
      "\u001b[1m---- Epoch 122/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.30925, s2s_loss 0.47769, 55.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05697, exact_match 0.88347, 1.37 secs\n",
      "\u001b[1m---- Epoch 123/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.60640, s2s_loss 0.47982, 55.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05692, exact_match 0.88109, 1.36 secs\n",
      "\u001b[1m---- Epoch 124/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.50121, s2s_loss 0.47612, 55.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05701, exact_match 0.88109, 1.41 secs\n",
      "\u001b[1m---- Epoch 125/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.49927, s2s_loss 0.48029, 54.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05646, exact_match 0.87872, 1.36 secs\n",
      "\u001b[1m---- Epoch 126/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.80624, s2s_loss 0.48279, 55.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05622, exact_match 0.87277, 1.45 secs\n",
      "\u001b[1m---- Epoch 127/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.68634, s2s_loss 0.48391, 56.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05786, exact_match 0.87039, 1.34 secs\n",
      "\u001b[1m---- Epoch 128/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.60161, s2s_loss 0.48022, 55.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05776, exact_match 0.87158, 1.37 secs\n",
      "\u001b[1m---- Epoch 129/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.48489, s2s_loss 0.48530, 55.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05719, exact_match 0.87277, 1.42 secs\n",
      "\u001b[1m---- Epoch 130/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.64412, s2s_loss 0.47824, 56.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05691, exact_match 0.87277, 1.41 secs\n",
      "\u001b[1m---- Epoch 131/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32784, s2s_loss 0.48500, 54.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05681, exact_match 0.87277, 1.37 secs\n",
      "\u001b[1m---- Epoch 132/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.87567, s2s_loss 0.47909, 55.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05672, exact_match 0.87277, 1.40 secs\n",
      "\u001b[1m---- Epoch 133/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.52501, s2s_loss 0.47137, 53.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05523, exact_match 0.88466, 1.40 secs\n",
      "\u001b[1m---- Epoch 134/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37653, s2s_loss 0.48590, 56.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05240, exact_match 0.89417, 1.41 secs\n",
      "\u001b[1m---- Epoch 135/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.61838, s2s_loss 0.48931, 55.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05319, exact_match 0.89061, 1.42 secs\n",
      "\u001b[1m---- Epoch 136/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.68864, s2s_loss 0.47524, 55.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05477, exact_match 0.88823, 1.36 secs\n",
      "\u001b[1m---- Epoch 137/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41055, s2s_loss 0.47724, 55.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05518, exact_match 0.88704, 1.42 secs\n",
      "\u001b[1m---- Epoch 138/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.49813, s2s_loss 0.47163, 55.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05555, exact_match 0.88466, 1.36 secs\n",
      "\u001b[1m---- Epoch 139/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.61190, s2s_loss 0.48872, 56.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05539, exact_match 0.88585, 1.38 secs\n",
      "\u001b[1m---- Epoch 140/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55491, s2s_loss 0.47893, 55.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05534, exact_match 0.88585, 1.63 secs\n",
      "\u001b[1m---- Epoch 141/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.55104, s2s_loss 0.47708, 55.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05324, exact_match 0.89655, 1.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_141_exact_match+s2s_loss=0.8984.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 142/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37806, s2s_loss 0.47896, 55.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05768, exact_match 0.87634, 1.38 secs\n",
      "\u001b[1m---- Epoch 143/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.38837, s2s_loss 0.47575, 55.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05708, exact_match 0.87753, 1.36 secs\n",
      "\u001b[1m---- Epoch 144/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.38298, s2s_loss 0.47930, 55.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05771, exact_match 0.87396, 1.43 secs\n",
      "\u001b[1m---- Epoch 145/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38344, s2s_loss 0.47857, 55.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05821, exact_match 0.87277, 1.38 secs\n",
      "\u001b[1m---- Epoch 146/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.40116, s2s_loss 0.47769, 55.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05810, exact_match 0.87396, 1.36 secs\n",
      "\u001b[1m---- Epoch 147/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.69947, s2s_loss 0.48581, 55.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05825, exact_match 0.87277, 1.47 secs\n",
      "\u001b[1m---- Epoch 148/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.28069, s2s_loss 0.48682, 56.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05823, exact_match 0.87277, 1.49 secs\n",
      "\u001b[1m---- Epoch 149/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.71534, s2s_loss 0.48208, 56.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05849, exact_match 0.88347, 1.37 secs\n",
      "\u001b[1m---- Epoch 150/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33052, s2s_loss 0.47440, 55.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05729, exact_match 0.88585, 1.42 secs\n",
      "\u001b[1m---- Epoch 151/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.73089, s2s_loss 0.47837, 56.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05596, exact_match 0.88585, 1.37 secs\n",
      "\u001b[1m---- Epoch 152/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35064, s2s_loss 0.47248, 55.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05565, exact_match 0.89061, 1.39 secs\n",
      "\u001b[1m---- Epoch 153/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.42056, s2s_loss 0.49541, 55.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05497, exact_match 0.89536, 1.37 secs\n",
      "\u001b[1m---- Epoch 154/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35995, s2s_loss 0.47445, 56.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05542, exact_match 0.89417, 1.38 secs\n",
      "\u001b[1m---- Epoch 155/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43926, s2s_loss 0.48671, 55.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05558, exact_match 0.89180, 1.37 secs\n",
      "\u001b[1m---- Epoch 156/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.22973, s2s_loss 0.48420, 54.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05557, exact_match 0.89061, 1.37 secs\n",
      "\u001b[1m---- Epoch 157/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44009, s2s_loss 0.48044, 55.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05549, exact_match 0.88704, 1.39 secs\n",
      "\u001b[1m---- Epoch 158/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37095, s2s_loss 0.48608, 54.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05490, exact_match 0.88823, 1.45 secs\n",
      "\u001b[1m---- Epoch 159/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.89537, s2s_loss 0.48701, 55.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05595, exact_match 0.88466, 1.40 secs\n",
      "\u001b[1m---- Epoch 160/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.43280, s2s_loss 0.48721, 55.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05766, exact_match 0.87990, 1.37 secs\n",
      "\u001b[1m---- Epoch 161/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.23505, s2s_loss 0.47754, 55.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05761, exact_match 0.88347, 1.39 secs\n",
      "\u001b[1m---- Epoch 162/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.39847, s2s_loss 0.47013, 56.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05747, exact_match 0.88347, 1.42 secs\n",
      "\u001b[1m---- Epoch 163/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43977, s2s_loss 0.48603, 54.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05731, exact_match 0.88466, 1.36 secs\n",
      "\u001b[1m---- Epoch 164/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.26764, s2s_loss 0.47822, 54.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05722, exact_match 0.88466, 1.32 secs\n",
      "\u001b[1m---- Epoch 165/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41645, s2s_loss 0.47724, 55.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05854, exact_match 0.88585, 1.40 secs\n",
      "\u001b[1m---- Epoch 166/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.44932, s2s_loss 0.48529, 55.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05869, exact_match 0.87753, 1.39 secs\n",
      "\u001b[1m---- Epoch 167/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.44416, s2s_loss 0.48120, 55.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05813, exact_match 0.88228, 1.38 secs\n",
      "\u001b[1m---- Epoch 168/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.40117, s2s_loss 0.48210, 56.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05849, exact_match 0.87990, 1.49 secs\n",
      "\u001b[1m---- Epoch 169/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63535, s2s_loss 0.47625, 55.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05818, exact_match 0.87872, 1.41 secs\n",
      "\u001b[1m---- Epoch 170/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.33750, s2s_loss 0.48519, 55.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05850, exact_match 0.87990, 1.38 secs\n",
      "\u001b[1m---- Epoch 171/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.45833, s2s_loss 0.46949, 56.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05873, exact_match 0.87990, 1.36 secs\n",
      "\u001b[1m---- Epoch 172/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41467, s2s_loss 0.47591, 55.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05871, exact_match 0.87872, 1.40 secs\n",
      "\u001b[1m---- Epoch 173/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.34130, s2s_loss 0.47710, 56.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05775, exact_match 0.88109, 1.39 secs\n",
      "\u001b[1m---- Epoch 174/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.41096, s2s_loss 0.49051, 55.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05832, exact_match 0.87872, 1.39 secs\n",
      "\u001b[1m---- Epoch 175/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.35552, s2s_loss 0.47187, 56.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05702, exact_match 0.88585, 1.37 secs\n",
      "\u001b[1m---- Epoch 176/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.52329, s2s_loss 0.47095, 55.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05679, exact_match 0.88704, 1.39 secs\n",
      "\u001b[1m---- Epoch 177/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.45291, s2s_loss 0.46929, 55.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05735, exact_match 0.88347, 1.40 secs\n",
      "\u001b[1m---- Epoch 178/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42951, s2s_loss 0.47765, 53.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05748, exact_match 0.88347, 1.37 secs\n",
      "\u001b[1m---- Epoch 179/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.48099, s2s_loss 0.48588, 54.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05748, exact_match 0.88347, 1.38 secs\n",
      "\u001b[1m---- Epoch 180/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32351, s2s_loss 0.47719, 55.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05749, exact_match 0.88347, 1.36 secs\n",
      "\u001b[1m---- Epoch 181/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33996, s2s_loss 0.47609, 55.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05586, exact_match 0.88942, 1.36 secs\n",
      "\u001b[1m---- Epoch 182/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.70170, s2s_loss 0.48412, 55.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05506, exact_match 0.88942, 1.44 secs\n",
      "\u001b[1m---- Epoch 183/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.55749, s2s_loss 0.47803, 55.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05429, exact_match 0.89417, 1.35 secs\n",
      "\u001b[1m---- Epoch 184/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.27170, s2s_loss 0.47448, 53.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05465, exact_match 0.89180, 1.41 secs\n",
      "\u001b[1m---- Epoch 185/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.57891, s2s_loss 0.47407, 55.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05481, exact_match 0.89180, 1.43 secs\n",
      "\u001b[1m---- Epoch 186/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.60573, s2s_loss 0.47562, 55.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05470, exact_match 0.89180, 1.40 secs\n",
      "\u001b[1m---- Epoch 187/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.36451, s2s_loss 0.47904, 56.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05452, exact_match 0.89180, 1.35 secs\n",
      "\u001b[1m---- Epoch 188/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37244, s2s_loss 0.47028, 55.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05448, exact_match 0.89180, 1.38 secs\n",
      "\u001b[1m---- Epoch 189/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.72064, s2s_loss 0.47900, 56.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05507, exact_match 0.88823, 1.39 secs\n",
      "\u001b[1m---- Epoch 190/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.35969, s2s_loss 0.48230, 55.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05427, exact_match 0.88823, 1.38 secs\n",
      "\u001b[1m---- Epoch 191/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36109, s2s_loss 0.48193, 56.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05498, exact_match 0.88585, 1.42 secs\n",
      "\u001b[1m---- Epoch 192/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55707, s2s_loss 0.47736, 56.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05514, exact_match 0.88466, 1.37 secs\n",
      "\u001b[1m---- Epoch 193/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.48465, s2s_loss 0.47294, 54.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05470, exact_match 0.88585, 1.38 secs\n",
      "\u001b[1m---- Epoch 194/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.38080, s2s_loss 0.46730, 55.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05438, exact_match 0.88823, 1.40 secs\n",
      "\u001b[1m---- Epoch 195/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.54134, s2s_loss 0.47479, 56.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05437, exact_match 0.88823, 1.40 secs\n",
      "\u001b[1m---- Epoch 196/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41361, s2s_loss 0.48105, 55.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05441, exact_match 0.88823, 1.37 secs\n",
      "\u001b[1m---- Epoch 197/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37922, s2s_loss 0.48528, 55.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05408, exact_match 0.88942, 1.37 secs\n",
      "\u001b[1m---- Epoch 198/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.59959, s2s_loss 0.48112, 53.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05615, exact_match 0.89180, 1.38 secs\n",
      "\u001b[1m---- Epoch 199/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36833, s2s_loss 0.47994, 54.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05438, exact_match 0.89893, 1.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_199_exact_match+s2s_loss=0.8989.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 200/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.38577, s2s_loss 0.48154, 55.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05356, exact_match 0.89893, 1.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_200_exact_match+s2s_loss=0.8991.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20240106_002348_multitask(s2f+f2m+f2c+s2co+s2cal+nli+mlm)_Seq2Seq(t5-small)\" \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 20 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--task_name \"multitask\" \\\n",
    "--experiment_name \"s2f+f2m+f2c+s2co+s2cal+nli+mlm\" \\\n",
    "--multitask_name_list \\\n",
    "\"nli\" \\\n",
    "\"sentence2facts\" \\\n",
    "\"fact2metadata\" \\\n",
    "\"fact2comparison\" \\\n",
    "\"sentence2chestimagenome_observations\" \\\n",
    "\"sentence2chestimagenome_anatomical_locations\" \\\n",
    "\"mlm\" \\\n",
    "--task2weight '{\"sentence2facts\": 1.0, \"fact2metadata\": 1.0, \"fact2comparison\": 0.3, \"sentence2chestimagenome_observations\": 1.0, \"sentence2chestimagenome_anatomical_locations\": 1.0, \"nli\": 8.0, \"mlm\": 5.0}' \\\n",
    "--sentence_to_facts_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\"\\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\" \\\n",
    "--fact_to_metadata_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--fact_to_comparison_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "--chest_imagenome_phrases2labels_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\" \\\n",
    "--chest_imagenome_obs_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--chest_imagenome_anatloc_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl\" \\\n",
    "--use_sentence2facts_for_nli \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--paraphrased_inputs_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--only_validate_nli \\\n",
    "--nli1_only_on_val \\\n",
    "--seq2seq_model_name \"t5\" \\\n",
    "--t5_model_name \"t5-small\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
