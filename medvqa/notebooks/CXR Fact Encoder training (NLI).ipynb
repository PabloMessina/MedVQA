{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916831ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 800\n",
      "   batch_size: 21\n",
      "   val_batch_size: 100\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: 128\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_folder_paths: ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240207_184445_MIMIC-CXR(triplets+classif+entcont+nli+radgraph+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)']\n",
      "   freeze_huggingface_model: False\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   fact_decoder_embed_size: 256\n",
      "   fact_decoder_hidden_size: 256\n",
      "   fact_decoder_nhead: 1\n",
      "   fact_decoder_dim_feedforward: 256\n",
      "   fact_decoder_num_layers: 1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   sentence_autoencoder_loss_weight: 1.0\n",
      "   triplets_filepath: None\n",
      "   triplet_rule_weights: None\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl\n",
      "   integrated_sentence_facts_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\n",
      "   sentences_and_cluster_ids_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\n",
      "   dataset_name: MIMIC-CXR(nli)\n",
      "   triplets_weight: 0\n",
      "   metadata_classification_weight: 0\n",
      "   chest_imagenome_observations_classification_weight: 0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0\n",
      "   nli_weight: 1.0\n",
      "   entcon_weight: 0\n",
      "   radgraph_ner_re_weight: 0\n",
      "   sentence_autoencoder_weight: 0\n",
      "   use_nli_val_in_train: False\n",
      "   use_anli: False\n",
      "   use_multinli: False\n",
      "   use_snli: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "----\n",
      "Number of RadNLI samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 841\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset and dataloader...\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl...\n",
      "Number of samples: 169025\n",
      "Loading integrated sentence facts from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl...\n",
      "Number of integrated sentence facts: 677694\n",
      "Number of entailment samples added: 1323679\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Source: 0 | Label: 0 -> 13185 (2563.82)\n",
      "Source: 1 | Label: 0 -> 5445 (1911.57)\n",
      "Source: 2 | Label: 0 -> 1770 (1256.05)\n",
      "Source: 3 | Label: 0 -> 3744 (1672.60)\n",
      "Source: 4 | Label: 0 -> 465 (695.76)\n",
      "Source: 5 | Label: 0 -> 473 (701.58)\n",
      "Source: 6 | Label: 0 -> 93 (279.62)\n",
      "Source: 7 | Label: 0 -> 1323679 (8410.16)\n",
      "Source: 0 | Label: 1 -> 19123 (2877.24)\n",
      "Source: 1 | Label: 1 -> 16675 (2758.96)\n",
      "Source: 2 | Label: 1 -> 3968 (1708.29)\n",
      "Source: 3 | Label: 1 -> 3743 (1672.44)\n",
      "Source: 4 | Label: 1 -> 465 (695.76)\n",
      "Source: 5 | Label: 1 -> 474 (702.30)\n",
      "Source: 6 | Label: 1 -> 281 (538.25)\n",
      "Source: 0 | Label: 2 -> 79430 (4312.76)\n",
      "Source: 1 | Label: 2 -> 10732 (2400.52)\n",
      "Source: 2 | Label: 2 -> 4170 (1739.18)\n",
      "Source: 3 | Label: 2 -> 3744 (1672.60)\n",
      "Source: 4 | Label: 2 -> 465 (695.76)\n",
      "Source: 5 | Label: 2 -> 474 (702.30)\n",
      "Source: 6 | Label: 2 -> 106 (304.54)\n",
      "NLI dataset examples:\n",
      "\u001b[1mExample 0:\u001b[0m\n",
      "Premise: Minimal linear opacity at right costodiaphragmatic angle is compatible with atelectasis.\n",
      "Hypothesis: atelectasis in the right costodiaphragmatic angle\n",
      "Label: 0\n",
      "\u001b[1mExample 1:\u001b[0m\n",
      "Premise: Bilateral peribronchovascular infiltration has improved, but is persistent.\n",
      "Hypothesis: There is no evidence of pneumothorax.\n",
      "Label: 1\n",
      "\u001b[1mExample 2:\u001b[0m\n",
      "Premise: The patient's bones show no signs of sclerosis or renal osteodystrophy.\n",
      "Hypothesis: The bones appear sclerotic compatible with known renal osteodystrophy.\n",
      "Label: 2\n",
      "\u001b[1mExample 3:\u001b[0m\n",
      "Premise: Overall, there has been no appreciable change.\n",
      "Hypothesis: The patient's chest condition remains stable.\n",
      "Label: 0\n",
      "\u001b[1mExample 4:\u001b[0m\n",
      "Premise: A small right pleural effusion is also probably present.\n",
      "Hypothesis: Presumed right pleural effusion is probably small and unchanged.\n",
      "Label: 1\n",
      "len(_train_dataloaders) = 1\n",
      "len(_train_weights) = 1\n",
      "_train_weights = [1.0]\n",
      "len(_val_dataloaders) = 1\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(nli)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: True\n",
      "  nli_hidden_layer_size: 128\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "  use_fact_decoder: False\n",
      "  fact_decoder_embed_size: 256\n",
      "  fact_decoder_hidden_size: 256\n",
      "  fact_decoder_nhead: 1\n",
      "  fact_decoder_dim_feedforward: 256\n",
      "  fact_decoder_num_layers: 1\n",
      "  fact_decoder_start_idx: None\n",
      "  fact_decoder_vocab_size: None\n",
      "  fact_decoder_dropout_prob: 0\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': None, 'pretrained_checkpoint_folder_paths': ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240207_184445_MIMIC-CXR(triplets+classif+entcont+nli+radgraph+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)']}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240208_083520_MIMIC-CXR(nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240208_083520_MIMIC-CXR(nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_91_cacc+chf1+chf1+cscc+encc+hscc+nlcc+sass+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9337.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240207_184445_MIMIC-CXR(triplets+classif+entcont+nli+radgraph+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_91_cacc+chf1+chf1+cscc+encc+hscc+nlcc+sass+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9337.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mWarning: model state dict has 215 keys, loaded state dict has 258 keys, intersection has 215 keys, union has 258 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  spert_rel_classifier.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.linear1.weight\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder_input_layer.weight\u001b[0m\n",
      "\u001b[93m  health_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.norm2.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.multihead_attn.in_proj_bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.W_vocab.bias\u001b[0m\n",
      "\u001b[93m  spert_size_embeddings.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240208_083520_MIMIC-CXR(nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.21739, nli_loss 0.21739, nli_acc 0.91673, 102.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87990, 0.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.8836.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.22564, nli_loss 0.22564, nli_acc 0.91429, 99.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87396, 1.02 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.22044, nli_loss 0.22044, nli_acc 0.91500, 102.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87753, 1.07 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.23267, nli_loss 0.23267, nli_acc 0.90804, 101.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88466, 1.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_nli_acc=0.8870.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.22872, nli_loss 0.22872, nli_acc 0.91095, 101.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88228, 1.12 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.22089, nli_loss 0.22089, nli_acc 0.91101, 102.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87753, 1.14 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.21179, nli_loss 0.21179, nli_acc 0.91607, 103.88 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88347, 1.05 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.20266, nli_loss 0.20266, nli_acc 0.92083, 99.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87872, 1.23 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.20259, nli_loss 0.20259, nli_acc 0.92256, 104.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87872, 1.21 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.19731, nli_loss 0.19731, nli_acc 0.92470, 104.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87872, 1.40 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.20481, nli_loss 0.20481, nli_acc 0.92113, 104.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87396, 1.26 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.19665, nli_loss 0.19665, nli_acc 0.92214, 103.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87515, 1.10 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.20748, nli_loss 0.20748, nli_acc 0.91988, 105.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88585, 1.18 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc=0.8893.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.21210, nli_loss 0.21210, nli_acc 0.91845, 106.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88466, 1.11 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.20525, nli_loss 0.20525, nli_acc 0.92089, 102.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88109, 1.16 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.19915, nli_loss 0.19915, nli_acc 0.92268, 106.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88585, 1.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_nli_acc=0.8895.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.19305, nli_loss 0.19305, nli_acc 0.92482, 104.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88704, 1.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_nli_acc=0.8908.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.17974, nli_loss 0.17974, nli_acc 0.93018, 104.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88228, 1.03 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.18240, nli_loss 0.18240, nli_acc 0.92958, 103.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88347, 1.12 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.19059, nli_loss 0.19059, nli_acc 0.92661, 102.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88466, 1.09 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.20824, nli_loss 0.20824, nli_acc 0.91970, 103.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87872, 1.15 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.19399, nli_loss 0.19399, nli_acc 0.92631, 104.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90131, 1.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_nli_acc=0.9038.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.19530, nli_loss 0.19530, nli_acc 0.92530, 105.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89061, 1.12 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.19305, nli_loss 0.19305, nli_acc 0.92542, 104.09 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89298, 1.27 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.18281, nli_loss 0.18281, nli_acc 0.93089, 102.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89061, 1.11 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.17688, nli_loss 0.17688, nli_acc 0.93512, 103.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89298, 1.16 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.17094, nli_loss 0.17094, nli_acc 0.93577, 103.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89061, 1.11 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.17646, nli_loss 0.17646, nli_acc 0.93536, 106.58 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89417, 1.22 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.19965, nli_loss 0.19965, nli_acc 0.92244, 104.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88109, 1.12 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.18671, nli_loss 0.18671, nli_acc 0.92851, 105.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89774, 1.09 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.17846, nli_loss 0.17846, nli_acc 0.93173, 104.96 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90606, 1.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_nli_acc=0.9086.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.16958, nli_loss 0.16958, nli_acc 0.93554, 103.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90131, 1.22 secs\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.17632, nli_loss 0.17632, nli_acc 0.93208, 101.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90131, 1.16 secs\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.16857, nli_loss 0.16857, nli_acc 0.93780, 104.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90012, 1.08 secs\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.16978, nli_loss 0.16978, nli_acc 0.93542, 103.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90131, 1.18 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.16578, nli_loss 0.16578, nli_acc 0.93720, 104.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.24 secs\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.19597, nli_loss 0.19597, nli_acc 0.92214, 105.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87753, 1.25 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.18711, nli_loss 0.18711, nli_acc 0.92887, 103.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88585, 1.16 secs\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.17064, nli_loss 0.17064, nli_acc 0.93571, 104.95 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89536, 1.06 secs\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.17067, nli_loss 0.17067, nli_acc 0.93607, 102.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89298, 1.17 secs\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.15932, nli_loss 0.15932, nli_acc 0.94095, 104.74 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89180, 1.18 secs\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.15557, nli_loss 0.15557, nli_acc 0.94107, 104.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89061, 1.15 secs\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.15935, nli_loss 0.15935, nli_acc 0.93958, 105.39 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88942, 1.37 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.15775, nli_loss 0.15775, nli_acc 0.94179, 102.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88942, 1.21 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.18504, nli_loss 0.18504, nli_acc 0.92726, 106.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88942, 1.10 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.17548, nli_loss 0.17548, nli_acc 0.93381, 103.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88823, 1.11 secs\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.16865, nli_loss 0.16865, nli_acc 0.93762, 103.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88228, 1.16 secs\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.15759, nli_loss 0.15759, nli_acc 0.94280, 104.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.25 secs\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.14915, nli_loss 0.14915, nli_acc 0.94488, 104.53 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89298, 1.22 secs\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.15065, nli_loss 0.15065, nli_acc 0.94333, 107.20 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89298, 1.31 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.15137, nli_loss 0.15137, nli_acc 0.94488, 101.24 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89417, 1.37 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.15170, nli_loss 0.15170, nli_acc 0.94321, 104.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89417, 1.30 secs\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.17227, nli_loss 0.17227, nli_acc 0.93506, 106.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89061, 1.24 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.17226, nli_loss 0.17226, nli_acc 0.93327, 105.39 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90369, 1.10 secs\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.16027, nli_loss 0.16027, nli_acc 0.93851, 105.36 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90369, 1.20 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.15376, nli_loss 0.15376, nli_acc 0.94268, 105.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.36 secs\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.14169, nli_loss 0.14169, nli_acc 0.94821, 105.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.13 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.14747, nli_loss 0.14747, nli_acc 0.94506, 103.39 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90012, 1.23 secs\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.15059, nli_loss 0.15059, nli_acc 0.94429, 104.58 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.26 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.13865, nli_loss 0.13865, nli_acc 0.95000, 105.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90250, 1.23 secs\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.16568, nli_loss 0.16568, nli_acc 0.93696, 104.74 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88466, 1.18 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.16398, nli_loss 0.16398, nli_acc 0.93798, 104.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89536, 1.17 secs\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.15441, nli_loss 0.15441, nli_acc 0.94423, 105.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.32 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.14874, nli_loss 0.14874, nli_acc 0.94464, 103.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90488, 1.18 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_nli_acc=0.9089.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.14325, nli_loss 0.14325, nli_acc 0.94613, 105.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.16 secs\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.13512, nli_loss 0.13512, nli_acc 0.95119, 104.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90369, 1.26 secs\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.14165, nli_loss 0.14165, nli_acc 0.94839, 106.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89774, 1.19 secs\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.13455, nli_loss 0.13455, nli_acc 0.95065, 103.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.06 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.15339, nli_loss 0.15339, nli_acc 0.94387, 102.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88942, 1.27 secs\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.16014, nli_loss 0.16014, nli_acc 0.93851, 106.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.24 secs\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.14415, nli_loss 0.14415, nli_acc 0.94696, 104.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.16 secs\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.14019, nli_loss 0.14019, nli_acc 0.94887, 106.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89298, 1.13 secs\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.13225, nli_loss 0.13225, nli_acc 0.95214, 105.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.24 secs\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.13457, nli_loss 0.13457, nli_acc 0.95042, 106.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.05 secs\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.13311, nli_loss 0.13311, nli_acc 0.95351, 105.73 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89774, 1.77 secs\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.12803, nli_loss 0.12803, nli_acc 0.95500, 106.96 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90012, 1.69 secs\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.14865, nli_loss 0.14865, nli_acc 0.94310, 104.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.90131, 1.68 secs\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.14896, nli_loss 0.14896, nli_acc 0.94476, 108.05 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88466, 1.26 secs\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.14278, nli_loss 0.14278, nli_acc 0.94851, 106.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.30 secs\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.13614, nli_loss 0.13614, nli_acc 0.95244, 106.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.19 secs\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.12476, nli_loss 0.12476, nli_acc 0.95625, 106.09 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89180, 1.34 secs\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.13005, nli_loss 0.13005, nli_acc 0.95357, 103.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89298, 1.20 secs\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.13035, nli_loss 0.13035, nli_acc 0.95149, 105.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89180, 1.21 secs\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.11927, nli_loss 0.11927, nli_acc 0.95685, 107.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89180, 1.29 secs\n",
      "\u001b[1m---- Epoch 85/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.14328, nli_loss 0.14328, nli_acc 0.94595, 107.70 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.87396, 1.16 secs\n",
      "\u001b[1m---- Epoch 86/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.14424, nli_loss 0.14424, nli_acc 0.94619, 105.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.88347, 1.35 secs\n",
      "\u001b[1m---- Epoch 87/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.13966, nli_loss 0.13966, nli_acc 0.95131, 104.36 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.18 secs\n",
      "\u001b[1m---- Epoch 88/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.13152, nli_loss 0.13152, nli_acc 0.95190, 102.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.23 secs\n",
      "\u001b[1m---- Epoch 89/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.12678, nli_loss 0.12678, nli_acc 0.95446, 106.52 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89774, 1.28 secs\n",
      "\u001b[1m---- Epoch 90/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.12130, nli_loss 0.12130, nli_acc 0.95625, 106.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.21 secs\n",
      "\u001b[1m---- Epoch 91/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.12513, nli_loss 0.12513, nli_acc 0.95655, 105.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.34 secs\n",
      "\u001b[1m---- Epoch 92/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.11861, nli_loss 0.11861, nli_acc 0.95750, 105.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89893, 1.13 secs\n",
      "\u001b[1m---- Epoch 93/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.14652, nli_loss 0.14652, nli_acc 0.94405, 106.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89180, 1.14 secs\n",
      "\u001b[1m---- Epoch 94/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.13861, nli_loss 0.13861, nli_acc 0.94756, 107.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89417, 1.17 secs\n",
      "\u001b[1m---- Epoch 95/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.12852, nli_loss 0.12852, nli_acc 0.95351, 105.70 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89536, 1.08 secs\n",
      "\u001b[1m---- Epoch 96/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.12372, nli_loss 0.12372, nli_acc 0.95548, 103.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89655, 1.28 secs\n",
      "\u001b[1m---- Epoch 97/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.12328, nli_loss 0.12328, nli_acc 0.95679, 108.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89417, 1.29 secs\n",
      "\u001b[1m---- Epoch 98/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.12269, nli_loss 0.12269, nli_acc 0.95696, 105.74 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89061, 1.60 secs\n",
      "\u001b[1m---- Epoch 99/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11490, nli_loss 0.11490, nli_acc 0.95923, 109.39 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89417, 1.26 secs\n",
      "\u001b[1m---- Epoch 100/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.10980, nli_loss 0.10980, nli_acc 0.96179, 107.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.89180, 1.15 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_paths \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240207_184445_MIMIC-CXR(triplets+classif+entcont+nli+radgraph+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 800 \\\n",
    "--batch_size 21 \\\n",
    "--val_batch_size 100 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl\" \\\n",
    "--integrated_sentence_facts_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\" \\\n",
    "--sentences_and_cluster_ids_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\" \\\n",
    "--nli_weight 1.0 \\\n",
    "--dataset_name \"MIMIC-CXR(nli)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--nli_hidden_layer_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
