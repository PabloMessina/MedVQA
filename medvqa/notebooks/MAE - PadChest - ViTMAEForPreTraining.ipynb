{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.train_mae import debug_main\n",
    "import shlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 1\n",
      "   batches_per_epoch: 20\n",
      "   checkpoint_folder: None\n",
      "   pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "   optimizer_name: adamw\n",
      "   num_accumulation_steps: 1\n",
      "   lr: 1e-06\n",
      "   override_lr: False\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,4,4e-4,76,5e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   num_workers: 3\n",
      "   batch_size: 80\n",
      "   device: GPU\n",
      "   padchest_weight: 1.0\n",
      "   use_amp: True\n",
      "   use_padchest: True\n",
      "   padchest_training_data_mode: all\n",
      "   padchest_use_validation: True\n",
      "   padchest_train_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\n",
      "   padchest_val_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\n",
      "   padchest_test_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\n",
      "   save: False\n",
      "\u001b[34m------------------- Training ViTMAE from scratch -------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mDevice: cuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of ViTMAEForPreTraining ...\u001b[0m\n",
      "pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mCreating optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mCreating learning rate scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,4,4e-4,76,5e-6\n",
      "1e-06 4 0.0004 76 5e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mCreating instance of ViTFeatureExtractor ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mCreating image transform ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating PadChest MAE trainer ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pamessina/medvqa/medvqa/train_mae.py:387: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug=debug,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset and dataloader ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:40<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Train dataset size: 1000000000000000000\n",
      "Creating val dataset and dataloader ...\n",
      "Done!\n",
      "Val dataset size: 5669\n"
     ]
    }
   ],
   "source": [
    "output = debug_main(args=shlex.split(\n",
    "    ' '.join([        \n",
    "        '--epochs 1',\n",
    "        '--batches-per-epoch 20',\n",
    "        '--batch-size 80',\n",
    "        '--num-accumulation-steps 1',\n",
    "        '--num-workers 3',\n",
    "        '--optimizer-name \"adamw\"',\n",
    "        '--scheduler \"warmup+decay\"',\n",
    "        '--lr 1e-6',\n",
    "        '--warmup-and-decay-args \"1e-6,4,4e-4,76,5e-6\"',\n",
    "        '--use-padchest',\n",
    "        '--padchest-use-validation',\n",
    "        '--padchest-weight 1',\n",
    "        '--padchest-train-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\"',\n",
    "        '--padchest-val-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\"',\n",
    "        '--padchest-test-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\"',\n",
    "        '--use-amp',\n",
    "        '--no-save',\n",
    "    ])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['padchest_mae_trainer'].train_dataset[0]['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'i': tensor([[[ 0.8104,  0.8447,  0.8961,  ..., -1.0219, -1.0733, -1.1075],\n",
       "          [ 0.8789,  0.8961,  0.9132,  ..., -1.0390, -1.0904, -1.1247],\n",
       "          [ 0.9474,  0.9646,  0.9303,  ..., -1.0562, -1.0904, -1.1247],\n",
       "          ...,\n",
       "          [ 0.5364,  0.5536,  0.5878,  ...,  1.2899,  1.2043,  1.1358],\n",
       "          [ 0.5022,  0.5364,  0.5878,  ...,  1.4954,  1.4440,  1.4098],\n",
       "          [ 0.5022,  0.5193,  0.5707,  ...,  1.7523,  1.7009,  1.6667]],\n",
       " \n",
       "         [[ 0.9580,  0.9930,  1.0455,  ..., -0.9153, -0.9678, -1.0028],\n",
       "          [ 1.0280,  1.0455,  1.0630,  ..., -0.9328, -0.9853, -1.0203],\n",
       "          [ 1.0980,  1.1155,  1.0805,  ..., -0.9503, -0.9853, -1.0203],\n",
       "          ...,\n",
       "          [ 0.6779,  0.6954,  0.7304,  ...,  1.4482,  1.3606,  1.2906],\n",
       "          [ 0.6429,  0.6779,  0.7304,  ...,  1.6583,  1.6057,  1.5707],\n",
       "          [ 0.6429,  0.6604,  0.7129,  ...,  1.9209,  1.8683,  1.8333]],\n",
       " \n",
       "         [[ 1.1759,  1.2108,  1.2631,  ..., -0.6890, -0.7413, -0.7761],\n",
       "          [ 1.2457,  1.2631,  1.2805,  ..., -0.7064, -0.7587, -0.7936],\n",
       "          [ 1.3154,  1.3328,  1.2980,  ..., -0.7238, -0.7587, -0.7936],\n",
       "          ...,\n",
       "          [ 0.8971,  0.9145,  0.9494,  ...,  1.6640,  1.5768,  1.5071],\n",
       "          [ 0.8622,  0.8971,  0.9494,  ...,  1.8731,  1.8208,  1.7860],\n",
       "          [ 0.8622,  0.8797,  0.9319,  ...,  2.1346,  2.0823,  2.0474]]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['padchest_mae_trainer'].val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 60\n",
      "   batches_per_epoch: 320\n",
      "   checkpoint_folder: None\n",
      "   pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "   optimizer_name: adamw\n",
      "   num_accumulation_steps: 4\n",
      "   lr: 1e-06\n",
      "   override_lr: False\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,1e-4,55,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   num_workers: 4\n",
      "   batch_size: 70\n",
      "   device: GPU\n",
      "   padchest_weight: 1.0\n",
      "   use_amp: True\n",
      "   use_padchest: True\n",
      "   padchest_training_data_mode: all\n",
      "   padchest_use_validation: True\n",
      "   padchest_train_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\n",
      "   padchest_val_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\n",
      "   padchest_test_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\n",
      "   save: True\n",
      "\u001b[34m------------------- Training ViTMAE from scratch -------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mDevice: cuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of ViTMAEForPreTraining ...\u001b[0m\n",
      "pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mCreating optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mCreating learning rate scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,1e-4,55,1e-6\n",
      "1e-06 5 0.0001 55 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mCreating instance of ViTFeatureExtractor ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mCreating image transform ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating PadChest MAE trainer ...\u001b[0m\n",
      "../train_mae.py:387: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug=debug,\n",
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n",
      "Creating train dataset and dataloader ...\n",
      "100%|█████████████████████████████████████████| 193/193 [00:40<00:00,  4.76it/s]\n",
      "Done!\n",
      "Train dataset size: 1000000000000000000\n",
      "Creating val dataset and dataloader ...\n",
      "Done!\n",
      "Val dataset size: 5669\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = padchest\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mCreating learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mCreating log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.03407, 56.99 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03940, 9.36 secs\n",
      "Adjusting learning rate of group 0 to 2.5119e-06.\n",
      "\u001b[1m---- Epoch 2/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.04414, 56.64 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03499, 9.26 secs\n",
      "Adjusting learning rate of group 0 to 6.3096e-06.\n",
      "\u001b[1m---- Epoch 3/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.04464, 56.79 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03324, 9.68 secs\n",
      "Adjusting learning rate of group 0 to 1.5849e-05.\n",
      "\u001b[1m---- Epoch 4/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.04147, 56.89 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03087, 9.69 secs\n",
      "Adjusting learning rate of group 0 to 3.9811e-05.\n",
      "\u001b[1m---- Epoch 5/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 0.03507, 56.88 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02844, 9.54 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[1m---- Epoch 6/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.03782, 57.00 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03100, 9.96 secs\n",
      "Adjusting learning rate of group 0 to 9.1968e-05.\n",
      "\u001b[1m---- Epoch 7/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000092) ...\n",
      "loss 0.03327, 56.71 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02800, 10.04 secs\n",
      "Adjusting learning rate of group 0 to 8.4581e-05.\n",
      "\u001b[1m---- Epoch 8/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000085) ...\n",
      "loss 0.02784, 56.91 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03069, 10.32 secs\n",
      "Adjusting learning rate of group 0 to 7.7787e-05.\n",
      "\u001b[1m---- Epoch 9/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000078) ...\n",
      "loss 0.03028, 56.83 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02485, 9.87 secs\n",
      "Adjusting learning rate of group 0 to 7.1539e-05.\n",
      "\u001b[1m---- Epoch 10/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000072) ...\n",
      "loss 0.03275, 57.04 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02634, 10.19 secs\n",
      "Adjusting learning rate of group 0 to 6.5793e-05.\n",
      "\u001b[1m---- Epoch 11/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000066) ...\n",
      "loss 0.03029, 56.81 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02565, 10.02 secs\n",
      "Adjusting learning rate of group 0 to 6.0509e-05.\n",
      "\u001b[1m---- Epoch 12/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000061) ...\n",
      "loss 0.03414, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02688, 10.31 secs\n",
      "Adjusting learning rate of group 0 to 5.5649e-05.\n",
      "\u001b[1m---- Epoch 13/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.02692, 56.67 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02397, 9.97 secs\n",
      "Adjusting learning rate of group 0 to 5.1179e-05.\n",
      "\u001b[1m---- Epoch 14/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000051) ...\n",
      "loss 0.03256, 57.03 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02464, 10.08 secs\n",
      "Adjusting learning rate of group 0 to 4.7068e-05.\n",
      "\u001b[1m---- Epoch 15/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.02936, 56.91 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02606, 10.34 secs\n",
      "Adjusting learning rate of group 0 to 4.3288e-05.\n",
      "\u001b[1m---- Epoch 16/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.02917, 56.95 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02354, 10.25 secs\n",
      "Adjusting learning rate of group 0 to 3.9811e-05.\n",
      "\u001b[1m---- Epoch 17/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 0.02714, 56.86 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02606, 10.08 secs\n",
      "Adjusting learning rate of group 0 to 3.6613e-05.\n",
      "\u001b[1m---- Epoch 18/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000037) ...\n",
      "loss 0.02824, 56.90 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02490, 10.19 secs\n",
      "Adjusting learning rate of group 0 to 3.3672e-05.\n",
      "\u001b[1m---- Epoch 19/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 0.02850, 56.78 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02314, 10.09 secs\n",
      "Adjusting learning rate of group 0 to 3.0968e-05.\n",
      "\u001b[1m---- Epoch 20/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000031) ...\n",
      "loss 0.02800, 56.91 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02527, 10.75 secs\n",
      "Adjusting learning rate of group 0 to 2.8480e-05.\n",
      "\u001b[1m---- Epoch 21/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.02821, 56.87 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.02762, 10.10 secs\n",
      "Adjusting learning rate of group 0 to 2.6193e-05.\n",
      "\u001b[1m---- Epoch 22/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000026) ...\n",
      "loss 0.02398, 56.82 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02597, 10.34 secs\n",
      "Adjusting learning rate of group 0 to 2.4089e-05.\n",
      "\u001b[1m---- Epoch 23/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n",
      "loss 0.02881, 56.79 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02383, 10.38 secs\n",
      "Adjusting learning rate of group 0 to 2.2154e-05.\n",
      "\u001b[1m---- Epoch 24/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.03339, 57.04 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02495, 10.16 secs\n",
      "Adjusting learning rate of group 0 to 2.0375e-05.\n",
      "\u001b[1m---- Epoch 25/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 0.02908, 56.73 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02405, 10.23 secs\n",
      "Adjusting learning rate of group 0 to 1.8738e-05.\n",
      "\u001b[1m---- Epoch 26/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.02633, 56.66 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02448, 10.48 secs\n",
      "Adjusting learning rate of group 0 to 1.7233e-05.\n",
      "\u001b[1m---- Epoch 27/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.02674, 56.99 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02442, 10.20 secs\n",
      "Adjusting learning rate of group 0 to 1.5849e-05.\n",
      "\u001b[1m---- Epoch 28/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.02517, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02248, 10.32 secs\n",
      "Adjusting learning rate of group 0 to 1.4576e-05.\n",
      "\u001b[1m---- Epoch 29/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.02652, 57.05 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02429, 10.37 secs\n",
      "Adjusting learning rate of group 0 to 1.3405e-05.\n",
      "\u001b[1m---- Epoch 30/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.02973, 56.79 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02439, 10.02 secs\n",
      "Adjusting learning rate of group 0 to 1.2328e-05.\n",
      "\u001b[1m---- Epoch 31/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.02881, 56.87 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02307, 10.11 secs\n",
      "Adjusting learning rate of group 0 to 1.1338e-05.\n",
      "\u001b[1m---- Epoch 32/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.02647, 57.00 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02290, 10.18 secs\n",
      "Adjusting learning rate of group 0 to 1.0428e-05.\n",
      "\u001b[1m---- Epoch 33/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.02686, 56.94 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02400, 10.38 secs\n",
      "Adjusting learning rate of group 0 to 9.5900e-06.\n",
      "\u001b[1m---- Epoch 34/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.02807, 56.85 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02956, 10.64 secs\n",
      "Adjusting learning rate of group 0 to 8.8197e-06.\n",
      "\u001b[1m---- Epoch 35/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.02791, 56.84 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02443, 10.20 secs\n",
      "Adjusting learning rate of group 0 to 8.1113e-06.\n",
      "\u001b[1m---- Epoch 36/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.02583, 56.85 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02149, 10.23 secs\n",
      "Adjusting learning rate of group 0 to 7.4598e-06.\n",
      "\u001b[1m---- Epoch 37/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.02585, 57.15 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02197, 10.38 secs\n",
      "Adjusting learning rate of group 0 to 6.8606e-06.\n",
      "\u001b[1m---- Epoch 38/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.02963, 56.70 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02199, 10.16 secs\n",
      "Adjusting learning rate of group 0 to 6.3096e-06.\n",
      "\u001b[1m---- Epoch 39/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.02856, 56.85 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02339, 10.25 secs\n",
      "Adjusting learning rate of group 0 to 5.8028e-06.\n",
      "\u001b[1m---- Epoch 40/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.02522, 56.91 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02382, 9.96 secs\n",
      "Adjusting learning rate of group 0 to 5.3367e-06.\n",
      "\u001b[1m---- Epoch 41/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.02995, 56.86 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02261, 10.32 secs\n",
      "Adjusting learning rate of group 0 to 4.9081e-06.\n",
      "\u001b[1m---- Epoch 42/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.02681, 56.86 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02389, 10.46 secs\n",
      "Adjusting learning rate of group 0 to 4.5138e-06.\n",
      "\u001b[1m---- Epoch 43/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.02691, 57.10 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02484, 10.27 secs\n",
      "Adjusting learning rate of group 0 to 4.1513e-06.\n",
      "\u001b[1m---- Epoch 44/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.02582, 56.82 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02372, 10.24 secs\n",
      "Adjusting learning rate of group 0 to 3.8178e-06.\n",
      "\u001b[1m---- Epoch 45/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.03012, 56.98 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02371, 10.18 secs\n",
      "Adjusting learning rate of group 0 to 3.5112e-06.\n",
      "\u001b[1m---- Epoch 46/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.02574, 56.97 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02368, 10.27 secs\n",
      "Adjusting learning rate of group 0 to 3.2292e-06.\n",
      "\u001b[1m---- Epoch 47/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.02557, 56.91 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02252, 10.37 secs\n",
      "Adjusting learning rate of group 0 to 2.9698e-06.\n",
      "\u001b[1m---- Epoch 48/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.02545, 57.04 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02510, 9.55 secs\n",
      "Adjusting learning rate of group 0 to 2.7313e-06.\n",
      "\u001b[1m---- Epoch 49/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.02556, 56.83 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02339, 10.21 secs\n",
      "Adjusting learning rate of group 0 to 2.5119e-06.\n",
      "\u001b[1m---- Epoch 50/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.02746, 57.00 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02624, 10.51 secs\n",
      "Adjusting learning rate of group 0 to 2.3101e-06.\n",
      "\u001b[1m---- Epoch 51/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02777, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02230, 10.19 secs\n",
      "Adjusting learning rate of group 0 to 2.1246e-06.\n",
      "\u001b[1m---- Epoch 52/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.03027, 56.84 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02135, 10.33 secs\n",
      "Adjusting learning rate of group 0 to 1.9539e-06.\n",
      "\u001b[1m---- Epoch 53/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02496, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02147, 10.55 secs\n",
      "Adjusting learning rate of group 0 to 1.7970e-06.\n",
      "\u001b[1m---- Epoch 54/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.03224, 56.92 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02342, 10.35 secs\n",
      "Adjusting learning rate of group 0 to 1.6527e-06.\n",
      "\u001b[1m---- Epoch 55/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02969, 56.86 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02202, 10.34 secs\n",
      "Adjusting learning rate of group 0 to 1.5199e-06.\n",
      "\u001b[1m---- Epoch 56/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02922, 57.02 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02239, 10.04 secs\n",
      "Adjusting learning rate of group 0 to 1.3978e-06.\n",
      "\u001b[1m---- Epoch 57/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.02603, 56.83 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02312, 10.11 secs\n",
      "Adjusting learning rate of group 0 to 1.2856e-06.\n",
      "\u001b[1m---- Epoch 58/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.02609, 57.04 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02514, 10.12 secs\n",
      "Adjusting learning rate of group 0 to 1.1823e-06.\n",
      "\u001b[1m---- Epoch 59/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.03054, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02309, 10.06 secs\n",
      "Adjusting learning rate of group 0 to 1.0873e-06.\n",
      "\u001b[1m---- Epoch 60/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.02875, 56.79 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.02055, 10.16 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_mae.py \\\n",
    "        --epochs 60 \\\n",
    "        --batches-per-epoch 320 \\\n",
    "        --batch-size 70 \\\n",
    "        --num-accumulation-steps 4 \\\n",
    "        --num-workers 4 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,1e-4,55,1e-6\" \\\n",
    "        --use-padchest \\\n",
    "        --padchest-use-validation \\\n",
    "        --padchest-weight 1 \\\n",
    "        --padchest-train-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-val-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-test-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\" \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
