{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b6006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "    --integrated_fact_metadata_filepath \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "    --num_words_per_sentence 1 \\\n",
    "    --offset 0 \\\n",
    "    --num_sentences 10000 \\\n",
    "    --max_requests_per_minute 3000 \\\n",
    "    --max_tokens_per_minute 85000 \\\n",
    "    --max_tokens_per_request 300 \\\n",
    "    --logging_level \"INFO\" \\\n",
    "    --api_key_name \"OPENAI_API_KEY_1\" \\\n",
    "    --openai_model_name \"gpt-3.5-turbo-0613\" \\\n",
    "    --alias \"__single-words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3fb7ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = load_jsonl('/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59e376bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1864"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "22b79581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'observation': 'VNS'},\n",
       " 'parsed_response': ['vagus nerve stimulation',\n",
       "  'VNS therapy',\n",
       "  'stimulation of the vagus nerve',\n",
       "  'electrical stimulation of the vagus nerve',\n",
       "  'nerve stimulation therapy',\n",
       "  'vagal nerve stimulation',\n",
       "  'vagus nerve stimulator',\n",
       "  'vagus nerve activation',\n",
       "  'activation of the vagus nerve',\n",
       "  'vagal stimulation']}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c1cf237f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:35:33,573 - \u001b[1;32mINFO\u001b[1;0m - Loaded 20 already paraphrased sentences from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\n",
      "2023-07-12 17:35:33,573 - \u001b[1;32mINFO\u001b[1;0m - Loading facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "100%|███████████████████████████████| 578733/578733 [00:03<00:00, 186128.10it/s]\n",
      "2023-07-12 17:35:41,871 - \u001b[1;32mINFO\u001b[1;0m - Found 771124 unique observations\n",
      "2023-07-12 17:35:44,205 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to those with at least 2 words\n",
      "2023-07-12 17:35:44,918 - \u001b[1;32mINFO\u001b[1;0m - Found 769122 observations with at least 2 words\n",
      "2023-07-12 17:35:44,918 - \u001b[1;32mINFO\u001b[1;0m - Filtering observations to the 0-th of every 4 sentences\n",
      "2023-07-12 17:35:45,143 - \u001b[1;32mINFO\u001b[1;0m - Found 192281 observations that are the 0-th of every 4 sentences\n",
      "2023-07-12 17:35:45,143 - \u001b[1;32mINFO\u001b[1;0m - Sampling 15000 equally spaced sentences\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - Total number of sentences to paraphrase: 14997\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - Example sentences to paraphrase:\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 1. no PIC\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 1667. likely represents effusion\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 3333. chronic abnormalities in lingula\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 4999. patient received a tracheostomy tube\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 6665. resolved bibasilar opacities on the left\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 8332. small pleural effusions in most of the lungs\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 9998. increased opacity projecting over the right hilum\n",
      "2023-07-12 17:35:45,152 - \u001b[1;32mINFO\u001b[1;0m - 11664. similar appearance of bilateral lower lobe atelectasis\n",
      "2023-07-12 17:35:45,153 - \u001b[1;32mINFO\u001b[1;0m - 13330. considerable displacement of multiple right-sided rib fractures\n",
      "2023-07-12 17:35:45,153 - \u001b[1;32mINFO\u001b[1;0m - 14997. severely limited cross-table lateral chest radiograph by the patient's markedly abnormal body habitus secondary to osteogenesis imperfecta\n",
      "2023-07-12 17:35:45,202 - \u001b[1;32mINFO\u001b[1;0m - Saving API requests to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_requests_20230712_173545.jsonl\n",
      "2023-07-12 17:35:45,202 - \u001b[1;32mINFO\u001b[1;0m - Saving API responses to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_173545.jsonl\n",
      "2023-07-12 17:35:46,188 - \u001b[1;32mINFO\u001b[1;0m - Starting request #0\n",
      "2023-07-12 17:35:46,344 - \u001b[1;32mINFO\u001b[1;0m - Starting request #50\n",
      "2023-07-12 17:35:46,508 - \u001b[1;32mINFO\u001b[1;0m - Starting request #100\n",
      "2023-07-12 17:35:48,344 - \u001b[1;32mINFO\u001b[1;0m - Starting request #150\n",
      "2023-07-12 17:36:08,933 - \u001b[1;32mINFO\u001b[1;0m - Starting request #200\n",
      "2023-07-12 17:36:18,532 - \u001b[1;33mWARNING\u001b[1;0m - Request 5 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ead19eb73c21d708f74667dd082bd175 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:36:18,722 - \u001b[1;33mWARNING\u001b[1;0m - Request 128 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a8998865fb5fa8baf688f219fd6126f2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:36:29,526 - \u001b[1;32mINFO\u001b[1;0m - Starting request #248\n",
      "2023-07-12 17:36:50,131 - \u001b[1;32mINFO\u001b[1;0m - Starting request #298\n",
      "2023-07-12 17:37:10,741 - \u001b[1;32mINFO\u001b[1;0m - Starting request #348\n",
      "2023-07-12 17:37:31,345 - \u001b[1;32mINFO\u001b[1;0m - Starting request #398\n",
      "2023-07-12 17:37:51,956 - \u001b[1;32mINFO\u001b[1;0m - Starting request #448\n",
      "2023-07-12 17:38:12,583 - \u001b[1;32mINFO\u001b[1;0m - Starting request #498\n",
      "2023-07-12 17:38:27,653 - \u001b[1;33mWARNING\u001b[1;0m - Request 461 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2f2655f383007c6654d59e4a574a08f7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:38:33,192 - \u001b[1;32mINFO\u001b[1;0m - Starting request #547\n",
      "2023-07-12 17:38:53,819 - \u001b[1;32mINFO\u001b[1;0m - Starting request #597\n",
      "2023-07-12 17:39:14,445 - \u001b[1;32mINFO\u001b[1;0m - Starting request #647\n",
      "2023-07-12 17:39:35,082 - \u001b[1;32mINFO\u001b[1;0m - Starting request #697\n",
      "2023-07-12 17:39:55,725 - \u001b[1;32mINFO\u001b[1;0m - Starting request #747\n",
      "2023-07-12 17:40:09,813 - \u001b[1;33mWARNING\u001b[1;0m - Request 524 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 48a5aea8ebe76c00634630a7f3f63782 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:40:16,345 - \u001b[1;32mINFO\u001b[1;0m - Starting request #796\n",
      "2023-07-12 17:40:36,981 - \u001b[1;32mINFO\u001b[1;0m - Starting request #846\n",
      "2023-07-12 17:40:44,123 - \u001b[1;33mWARNING\u001b[1;0m - Request 790 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 154bf0d78fd72d20faae1f7c6d81b56d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:40:57,614 - \u001b[1;32mINFO\u001b[1;0m - Starting request #895\n",
      "2023-07-12 17:41:18,249 - \u001b[1;32mINFO\u001b[1;0m - Starting request #945\n",
      "2023-07-12 17:41:38,900 - \u001b[1;32mINFO\u001b[1;0m - Starting request #995\n",
      "2023-07-12 17:41:41,965 - \u001b[1;33mWARNING\u001b[1;0m - Request 929 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7670cb75efd17b6d3b37fe9b82e8e6c9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:41:51,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 301 failed with Exception \n",
      "2023-07-12 17:41:59,542 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1043\n",
      "2023-07-12 17:42:17,423 - \u001b[1;33mWARNING\u001b[1;0m - Request 1014 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 98638df8d5a91bace9c88e2c523c8e35 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:42:20,195 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1092\n",
      "2023-07-12 17:42:40,842 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1142\n",
      "2023-07-12 17:43:01,504 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1192\n",
      "2023-07-12 17:43:22,153 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1242\n",
      "2023-07-12 17:43:42,803 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1292\n",
      "2023-07-12 17:43:51,999 - \u001b[1;33mWARNING\u001b[1;0m - Request 1241 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d7b55cf41236e8cd6f9a6e22461782b6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:44:03,457 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1341\n",
      "2023-07-12 17:44:24,127 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1391\n",
      "2023-07-12 17:44:44,790 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1441\n",
      "2023-07-12 17:45:05,451 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1491\n",
      "2023-07-12 17:45:26,103 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1541\n",
      "2023-07-12 17:45:46,769 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1591\n",
      "2023-07-12 17:46:07,436 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1641\n",
      "2023-07-12 17:46:20,362 - \u001b[1;33mWARNING\u001b[1;0m - Request 1599 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d94be0762e98dd45e003430043356f38 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:46:28,105 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1690\n",
      "2023-07-12 17:46:48,766 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1740\n",
      "2023-07-12 17:47:09,439 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1790\n",
      "2023-07-12 17:47:30,117 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1840\n",
      "2023-07-12 17:47:50,787 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1890\n",
      "2023-07-12 17:48:11,457 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1940\n",
      "2023-07-12 17:48:32,130 - \u001b[1;32mINFO\u001b[1;0m - Starting request #1990\n",
      "2023-07-12 17:48:34,418 - \u001b[1;33mWARNING\u001b[1;0m - Request 1922 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 94ef5f2de4be827cddb29eed130dac9f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:48:52,793 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2039\n",
      "2023-07-12 17:49:13,482 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2089\n",
      "2023-07-12 17:49:15,677 - \u001b[1;33mWARNING\u001b[1;0m - Request 2021 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d6fcbb64ce7ea130786c2a6ce4aad4f3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:49:34,167 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2138\n",
      "2023-07-12 17:49:40,003 - \u001b[1;33mWARNING\u001b[1;0m - Request 2080 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b00e27d454ad41de5a9f1e68c8f40c80 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:49:48,774 - \u001b[1;33mWARNING\u001b[1;0m - Request 2100 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 265686667a7be45c6c2ef18f31f0e830 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:49:53,242 - \u001b[1;33mWARNING\u001b[1;0m - Request 2111 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 79001d28dce35efba13912f706e1efda in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:49:54,838 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2185\n",
      "2023-07-12 17:50:15,521 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2235\n",
      "2023-07-12 17:50:32,527 - \u001b[1;33mWARNING\u001b[1;0m - Request 2203 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e6fedb5415b06d819bdf249d4858566a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:50:36,189 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2284\n",
      "2023-07-12 17:50:56,877 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2334\n",
      "2023-07-12 17:51:17,585 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2384\n",
      "2023-07-12 17:51:38,277 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2434\n",
      "2023-07-12 17:51:58,971 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2484\n",
      "2023-07-12 17:52:19,651 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2534\n",
      "2023-07-12 17:52:40,329 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2584\n",
      "2023-07-12 17:53:01,021 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2634\n",
      "2023-07-12 17:53:21,747 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2684\n",
      "2023-07-12 17:53:42,450 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2734\n",
      "2023-07-12 17:53:56,975 - \u001b[1;33mWARNING\u001b[1;0m - Request 2696 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f8ca513b99a76b81392a9e2562688d18 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:54:03,134 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2783\n",
      "2023-07-12 17:54:20,560 - \u001b[1;33mWARNING\u001b[1;0m - Request 2753 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ddd8b95d5cf7e6db3fdbbdae6487a653 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:54:23,823 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2832\n",
      "2023-07-12 17:54:44,513 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2882\n",
      "2023-07-12 17:55:05,212 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2932\n",
      "2023-07-12 17:55:06,967 - \u001b[1;33mWARNING\u001b[1;0m - Request 2863 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID caa8ef7b9b7d3599abfe6e35a76783c5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:55:21,471 - \u001b[1;33mWARNING\u001b[1;0m - Request 2898 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8e68e6d55cb26b4e99c9ddee6435c19b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:55:25,945 - \u001b[1;32mINFO\u001b[1;0m - Starting request #2980\n",
      "2023-07-12 17:55:46,643 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3030\n",
      "2023-07-12 17:56:07,346 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3080\n",
      "2023-07-12 17:56:28,056 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3130\n",
      "2023-07-12 17:56:48,745 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3180\n",
      "2023-07-12 17:57:04,986 - \u001b[1;33mWARNING\u001b[1;0m - Request 3146 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 719359d081ecdd2e919ff1eb91d433e3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:57:09,429 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3229\n",
      "2023-07-12 17:57:18,985 - \u001b[1;33mWARNING\u001b[1;0m - Request 3180 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c21b7f93d419b90eb304129f462fccbb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:57:30,128 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3278\n",
      "2023-07-12 17:57:50,862 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3328\n",
      "2023-07-12 17:58:11,564 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3378\n",
      "2023-07-12 17:58:32,267 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3428\n",
      "2023-07-12 17:58:52,982 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3478\n",
      "2023-07-12 17:59:13,679 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3528\n",
      "2023-07-12 17:59:26,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 2837 failed with Exception \n",
      "2023-07-12 17:59:29,053 - \u001b[1;33mWARNING\u001b[1;0m - Request 3492 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 083d6c10c4c118df10932cbcb94cae77 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 17:59:34,377 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3576\n",
      "2023-07-12 17:59:55,085 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3626\n",
      "2023-07-12 18:00:15,817 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3676\n",
      "2023-07-12 18:00:29,495 - \u001b[1;33mWARNING\u001b[1;0m - Request 2988 failed with Exception \n",
      "2023-07-12 18:00:36,525 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3725\n",
      "2023-07-12 18:00:37,792 - \u001b[1;33mWARNING\u001b[1;0m - Request 3656 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 15cb803a0aa57d54720b558038fa758f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:00:57,240 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3774\n",
      "2023-07-12 18:01:17,951 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3824\n",
      "2023-07-12 18:01:38,659 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3874\n",
      "2023-07-12 18:01:59,372 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3924\n",
      "2023-07-12 18:02:20,088 - \u001b[1;32mINFO\u001b[1;0m - Starting request #3974\n",
      "2023-07-12 18:02:20,938 - \u001b[1;33mWARNING\u001b[1;0m - Request 3903 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6931698e800beca63e84d3a3a0304e30 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:02:40,815 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4023\n",
      "2023-07-12 18:03:01,536 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4073\n",
      "2023-07-12 18:03:22,252 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4123\n",
      "2023-07-12 18:03:42,979 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4173\n",
      "2023-07-12 18:04:03,694 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4223\n",
      "2023-07-12 18:04:24,413 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4273\n",
      "2023-07-12 18:04:45,132 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4323\n",
      "2023-07-12 18:05:05,883 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4373\n",
      "2023-07-12 18:05:26,602 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4423\n",
      "2023-07-12 18:05:47,330 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4473\n",
      "2023-07-12 18:06:08,050 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4523\n",
      "2023-07-12 18:06:28,788 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4573\n",
      "2023-07-12 18:06:49,508 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4623\n",
      "2023-07-12 18:07:02,018 - \u001b[1;33mWARNING\u001b[1;0m - Request 4580 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d611152a006038901794ebea7cf56324 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:07:10,225 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4672\n",
      "2023-07-12 18:07:30,952 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4722\n",
      "2023-07-12 18:07:51,694 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4772\n",
      "2023-07-12 18:07:52,902 - \u001b[1;33mWARNING\u001b[1;0m - Request 4702 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b93e3a4c7df9aff6464320c454e73142 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:08:12,433 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4821\n",
      "2023-07-12 18:08:33,172 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4871\n",
      "2023-07-12 18:08:53,903 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4921\n",
      "2023-07-12 18:09:05,069 - \u001b[1;33mWARNING\u001b[1;0m - Request 4875 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0761c88d4cd404915b8e30160a985be6 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:09:14,633 - \u001b[1;32mINFO\u001b[1;0m - Starting request #4970\n",
      "2023-07-12 18:09:14,696 - \u001b[1;33mWARNING\u001b[1;0m - Request 4898 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 74518ed5d3c7d9c05e63a0fd5e9ced81 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:09:35,352 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5019\n",
      "2023-07-12 18:09:56,086 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5069\n",
      "2023-07-12 18:09:58,565 - \u001b[1;33mWARNING\u001b[1;0m - Request 5002 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a23b0f221579a209396641c87fe0e289 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:10:16,820 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5118\n",
      "2023-07-12 18:10:37,578 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5168\n",
      "2023-07-12 18:10:40,437 - \u001b[1;33mWARNING\u001b[1;0m - Request 5102 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 45dffec25a35f8223d6fd10081e97e38 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:10:50,872 - \u001b[1;33mWARNING\u001b[1;0m - Request 5127 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 97bed72dccbb58c5eba3ade037aaa1fb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:10:58,320 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5216\n",
      "2023-07-12 18:11:19,075 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5266\n",
      "2023-07-12 18:11:39,815 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5316\n",
      "2023-07-12 18:12:00,556 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5366\n",
      "2023-07-12 18:12:21,281 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5416\n",
      "2023-07-12 18:12:42,012 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5466\n",
      "2023-07-12 18:13:02,757 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5516\n",
      "2023-07-12 18:13:23,524 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5566\n",
      "2023-07-12 18:13:44,271 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5616\n",
      "2023-07-12 18:14:05,031 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5666\n",
      "2023-07-12 18:14:25,790 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5716\n",
      "2023-07-12 18:14:46,539 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5766\n",
      "2023-07-12 18:15:07,275 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5816\n",
      "2023-07-12 18:15:28,015 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5866\n",
      "2023-07-12 18:15:48,763 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5916\n",
      "2023-07-12 18:16:09,533 - \u001b[1;32mINFO\u001b[1;0m - Starting request #5966\n",
      "2023-07-12 18:16:30,294 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6016\n",
      "2023-07-12 18:16:51,041 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6066\n",
      "2023-07-12 18:17:11,790 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6116\n",
      "2023-07-12 18:17:12,134 - \u001b[1;33mWARNING\u001b[1;0m - Request 6044 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5343841c74ac415bb1e6afbe7e463685 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:17:32,552 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6165\n",
      "2023-07-12 18:17:38,714 - \u001b[1;33mWARNING\u001b[1;0m - Request 6108 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ec0a63abcddab3d04f69236935faf9a9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:17:53,315 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6214\n",
      "2023-07-12 18:18:14,071 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6264\n",
      "2023-07-12 18:18:16,175 - \u001b[1;33mWARNING\u001b[1;0m - Request 6196 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 86e1bba7555a0fa9e73783fb5d11ec9c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:18:34,825 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6313\n",
      "2023-07-12 18:18:55,595 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6363\n",
      "2023-07-12 18:19:16,360 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6413\n",
      "2023-07-12 18:19:37,134 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6463\n",
      "2023-07-12 18:19:57,894 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:20:18,668 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6563\n",
      "2023-07-12 18:20:39,414 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6613\n",
      "2023-07-12 18:21:00,162 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6663\n",
      "2023-07-12 18:21:20,921 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6713\n",
      "2023-07-12 18:21:35,491 - \u001b[1;33mWARNING\u001b[1;0m - Request 6675 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 78248637bb3376d12552ca850e2f82f1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:21:41,692 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6762\n",
      "2023-07-12 18:21:49,179 - \u001b[1;33mWARNING\u001b[1;0m - Request 6708 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 92bab0f3282e521aa9b4f20565c7eb50 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:22:02,479 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6811\n",
      "2023-07-12 18:22:23,263 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6861\n",
      "2023-07-12 18:22:44,037 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6911\n",
      "2023-07-12 18:22:57,247 - \u001b[1;33mWARNING\u001b[1;0m - Request 6870 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 55100af42961bd46b065aa00d0e94544 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:23:04,814 - \u001b[1;32mINFO\u001b[1;0m - Starting request #6960\n",
      "2023-07-12 18:23:25,585 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7010\n",
      "2023-07-12 18:23:46,345 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7060\n",
      "2023-07-12 18:24:07,117 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7110\n",
      "2023-07-12 18:24:19,917 - \u001b[1;33mWARNING\u001b[1;0m - Request 7068 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cc695cdb80ee682710dc1a33ee3903b0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:24:27,893 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7159\n",
      "2023-07-12 18:24:41,100 - \u001b[1;33mWARNING\u001b[1;0m - Request 7119 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d1c8c0def2099759a0531d751ddbbc70 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:24:48,680 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7208\n",
      "2023-07-12 18:25:09,464 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7258\n",
      "2023-07-12 18:25:24,761 - \u001b[1;33mWARNING\u001b[1;0m - Request 7222 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ebf3a0395e0da435050328fc5ddcc58b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:25:30,238 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7307\n",
      "2023-07-12 18:25:51,015 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7357\n",
      "2023-07-12 18:26:02,162 - \u001b[1;33mWARNING\u001b[1;0m - Request 7311 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8aa696c4085f2c7233d969cebd56c38f in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:26:06,793 - \u001b[1;33mWARNING\u001b[1;0m - Request 7322 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 79d3911b90f6b7255bd54bdd5a403fda in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:26:11,794 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7405\n",
      "2023-07-12 18:26:32,563 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7455\n",
      "2023-07-12 18:26:32,906 - \u001b[1;33mWARNING\u001b[1;0m - Request 7311 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a0556020131ab1c952ca3adf0eeefc6e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:26:53,330 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7504\n",
      "2023-07-12 18:27:14,103 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7554\n",
      "2023-07-12 18:27:34,910 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7604\n",
      "2023-07-12 18:27:55,696 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7654\n",
      "2023-07-12 18:28:15,145 - \u001b[1;33mWARNING\u001b[1;0m - Request 7628 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a6bd3356075c3a92fd0585a322de0fdb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:28:16,480 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7703\n",
      "2023-07-12 18:28:34,276 - \u001b[1;33mWARNING\u001b[1;0m - Request 7674 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3b8545b63eb56ad5b911eda395f7617d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:28:37,251 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7752\n",
      "2023-07-12 18:28:58,031 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7802\n",
      "2023-07-12 18:29:18,795 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7852\n",
      "2023-07-12 18:29:38,326 - \u001b[1;33mWARNING\u001b[1;0m - Request 7826 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 914afea38b3c867c68421c688b4311f5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:29:39,565 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7901\n",
      "2023-07-12 18:29:43,251 - \u001b[1;33mWARNING\u001b[1;0m - Request 7838 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d26f62c5ac4176a3b2b3144033752e12 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:30:00,344 - \u001b[1;32mINFO\u001b[1;0m - Starting request #7950\n",
      "2023-07-12 18:30:21,027 - \u001b[1;33mWARNING\u001b[1;0m - Request 7927 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e5dbe764ef0da2a4842fbafd42efe306 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:30:21,143 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8000\n",
      "2023-07-12 18:30:31,082 - \u001b[1;33mWARNING\u001b[1;0m - Request 7951 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2585e801fdd5c165500a630d4380067d in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:30:41,938 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8048\n",
      "2023-07-12 18:31:02,740 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8098\n",
      "2023-07-12 18:31:23,532 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8148\n",
      "2023-07-12 18:31:44,321 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:32:05,090 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8248\n",
      "2023-07-12 18:32:25,875 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8298\n",
      "2023-07-12 18:32:46,668 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8348\n",
      "2023-07-12 18:33:07,481 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8398\n",
      "2023-07-12 18:33:28,284 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8448\n",
      "2023-07-12 18:33:49,091 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8498\n",
      "2023-07-12 18:34:09,877 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8548\n",
      "2023-07-12 18:34:30,695 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8598\n",
      "2023-07-12 18:34:45,958 - \u001b[1;33mWARNING\u001b[1;0m - Request 8562 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3339068f5632d69a6ceb063e57ce8b76 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:34:51,478 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8647\n",
      "2023-07-12 18:35:12,262 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8697\n",
      "2023-07-12 18:35:33,059 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8747\n",
      "2023-07-12 18:35:53,874 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8797\n",
      "2023-07-12 18:36:14,672 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8847\n",
      "2023-07-12 18:36:35,487 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8897\n",
      "2023-07-12 18:36:54,500 - \u001b[1;33mWARNING\u001b[1;0m - Request 8870 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d91e843f11787f93f33430190505277c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:36:56,275 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8946\n",
      "2023-07-12 18:37:17,077 - \u001b[1;32mINFO\u001b[1;0m - Starting request #8996\n",
      "2023-07-12 18:37:37,863 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9046\n",
      "2023-07-12 18:37:58,665 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9096\n",
      "2023-07-12 18:38:19,473 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9146\n",
      "2023-07-12 18:38:40,301 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9196\n",
      "2023-07-12 18:39:01,123 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9246\n",
      "2023-07-12 18:39:21,931 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9296\n",
      "2023-07-12 18:39:24,286 - \u001b[1;33mWARNING\u001b[1;0m - Request 9229 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 38f98187b72ccc4e8c8032aa2cdbb2ee in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:39:42,743 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9345\n",
      "2023-07-12 18:40:03,545 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9395\n",
      "2023-07-12 18:40:24,359 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9445\n",
      "2023-07-12 18:40:42,613 - \u001b[1;33mWARNING\u001b[1;0m - Request 9416 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 90ac4724a86d7ea094c2b5511702cabf in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:40:45,167 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9494\n",
      "2023-07-12 18:41:06,007 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9544\n",
      "2023-07-12 18:41:26,796 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9594\n",
      "2023-07-12 18:41:47,615 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9644\n",
      "2023-07-12 18:42:08,434 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9694\n",
      "2023-07-12 18:42:29,234 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9744\n",
      "2023-07-12 18:42:50,033 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9794\n",
      "2023-07-12 18:43:10,846 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9844\n",
      "2023-07-12 18:43:31,669 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9894\n",
      "2023-07-12 18:43:52,486 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9944\n",
      "2023-07-12 18:44:00,739 - \u001b[1;33mWARNING\u001b[1;0m - Request 9891 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2a6ad12c4c1516a48796f2a68f248ffe in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:44:13,330 - \u001b[1;32mINFO\u001b[1;0m - Starting request #9993\n",
      "2023-07-12 18:44:34,150 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10043\n",
      "2023-07-12 18:44:42,439 - \u001b[1;33mWARNING\u001b[1;0m - Request 9990 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 294979995444ab94a5ba2558d5a5ed69 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:44:54,968 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10092\n",
      "2023-07-12 18:45:15,779 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10142\n",
      "2023-07-12 18:45:36,587 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10192\n",
      "2023-07-12 18:45:57,426 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10242\n",
      "2023-07-12 18:46:18,249 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10292\n",
      "2023-07-12 18:46:39,095 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10342\n",
      "2023-07-12 18:46:59,934 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10392\n",
      "2023-07-12 18:47:20,755 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10442\n",
      "2023-07-12 18:47:41,580 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10492\n",
      "2023-07-12 18:48:02,412 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10542\n",
      "2023-07-12 18:48:23,257 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10592\n",
      "2023-07-12 18:48:44,107 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10642\n",
      "2023-07-12 18:49:04,928 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10692\n",
      "2023-07-12 18:49:25,757 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10742\n",
      "2023-07-12 18:49:46,591 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10792\n",
      "2023-07-12 18:49:59,326 - \u001b[1;33mWARNING\u001b[1;0m - Request 10750 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8e8be363b6d3b6feff6c2b4ff2f2a3e3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:50:07,413 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10841\n",
      "2023-07-12 18:50:28,281 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10891\n",
      "2023-07-12 18:50:30,565 - \u001b[1;33mWARNING\u001b[1;0m - Request 10824 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7ca50227d19c3ff7e64fdef338c6201c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:50:35,594 - \u001b[1;33mWARNING\u001b[1;0m - Request 10836 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 30c8f6ea96e9b45f2a8d802ab913ee18 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:50:49,140 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10939\n",
      "2023-07-12 18:50:53,513 - \u001b[1;33mWARNING\u001b[1;0m - Request 10879 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 35138ccc741e494083fd251f1b4bb373 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:51:09,987 - \u001b[1;32mINFO\u001b[1;0m - Starting request #10988\n",
      "2023-07-12 18:51:30,820 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11038\n",
      "2023-07-12 18:51:51,642 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11088\n",
      "2023-07-12 18:52:12,486 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11138\n",
      "2023-07-12 18:52:18,614 - \u001b[1;33mWARNING\u001b[1;0m - Request 11080 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 52236aa7e35093b428872a9b19a98468 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 18:52:33,351 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11187\n",
      "2023-07-12 18:52:54,213 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11237\n",
      "2023-07-12 18:53:15,046 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11287\n",
      "2023-07-12 18:53:26,177 - \u001b[1;33mWARNING\u001b[1;0m - Request 11241 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2c8ff78204d8938754ee600ae4fe9ab9 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:53:35,895 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11336\n",
      "2023-07-12 18:53:52,476 - \u001b[1;33mWARNING\u001b[1;0m - Request 11304 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 06e76daceb09bd6d2f10f7c4ec1b9555 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:53:56,728 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11385\n",
      "2023-07-12 18:54:17,583 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11435\n",
      "2023-07-12 18:54:38,445 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11485\n",
      "2023-07-12 18:54:39,883 - \u001b[1;33mWARNING\u001b[1;0m - Request 11416 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 259a06060b8c682970bfdba242e78cfe in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:54:52,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 10806 failed with Exception \n",
      "2023-07-12 18:54:57,134 - \u001b[1;33mWARNING\u001b[1;0m - Request 11457 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9177dfe3c852bb351bd01e2b2ee41ec5 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:54:59,316 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11532\n",
      "2023-07-12 18:55:01,249 - \u001b[1;33mWARNING\u001b[1;0m - Request 11467 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ec5c0acd532f20730d4907b3d0ebe553 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:55:20,162 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11581\n",
      "2023-07-12 18:55:41,003 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11631\n",
      "2023-07-12 18:56:01,866 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11681\n",
      "2023-07-12 18:56:22,732 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11731\n",
      "2023-07-12 18:56:43,597 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11781\n",
      "2023-07-12 18:57:04,459 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11831\n",
      "2023-07-12 18:57:09,221 - \u001b[1;33mWARNING\u001b[1;0m - Request 11770 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c822a82019db9e3b59f04c1f101b1755 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:57:25,315 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11880\n",
      "2023-07-12 18:57:46,166 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11930\n",
      "2023-07-12 18:57:58,037 - \u001b[1;33mWARNING\u001b[1;0m - Request 11886 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e1553984ab68d9e9759ae01c84b42704 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:58:04,275 - \u001b[1;33mWARNING\u001b[1;0m - Request 11901 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 994fb74da33673b850d5cb4a214ba823 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:58:07,050 - \u001b[1;32mINFO\u001b[1;0m - Starting request #11978\n",
      "2023-07-12 18:58:27,916 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12028\n",
      "2023-07-12 18:58:48,790 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12078\n",
      "2023-07-12 18:59:09,645 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12128\n",
      "2023-07-12 18:59:30,515 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12178\n",
      "2023-07-12 18:59:31,093 - \u001b[1;33mWARNING\u001b[1;0m - Request 12107 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 37696f583d6c19a857923fa4298197ba in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 18:59:51,406 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12227\n",
      "2023-07-12 19:00:12,294 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12277\n",
      "2023-07-12 19:00:26,284 - \u001b[1;33mWARNING\u001b[1;0m - Request 12238 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 553e56ea4cf4a76c3f1d8d19f9f36709 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:00:33,161 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12326\n",
      "2023-07-12 19:00:54,023 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12376\n",
      "2023-07-12 19:01:06,282 - \u001b[1;33mWARNING\u001b[1;0m - Request 12333 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6e0768e82be0bd7267daa62aeca4aa99 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:01:06,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11691 failed with Exception \n",
      "2023-07-12 19:01:14,921 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12424\n",
      "2023-07-12 19:01:35,803 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12474\n",
      "2023-07-12 19:01:49,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 11795 failed with Exception \n",
      "2023-07-12 19:01:51,494 - \u001b[1;33mWARNING\u001b[1;0m - Request 12439 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 26cd0f95edcfbe8b368209114afcd330 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:01:56,676 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12522\n",
      "2023-07-12 19:02:17,527 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12572\n",
      "2023-07-12 19:02:38,426 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12622\n",
      "2023-07-12 19:02:46,157 - \u001b[1;33mWARNING\u001b[1;0m - Request 12568 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 047e8fef19da47d558fa559be7adecb0 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:02:59,336 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12671\n",
      "2023-07-12 19:03:20,233 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12721\n",
      "2023-07-12 19:03:41,116 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12771\n",
      "2023-07-12 19:04:02,031 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12821\n",
      "2023-07-12 19:04:12,256 - \u001b[1;33mWARNING\u001b[1;0m - Request 12773 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 040f07a6f3f49def7df3de7b5e50a0fb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:04:17,600 - \u001b[1;33mWARNING\u001b[1;0m - Request 12786 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 344ecd00cb71385f84f884e68728cb50 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:04:22,944 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12869\n",
      "2023-07-12 19:04:23,533 - \u001b[1;33mWARNING\u001b[1;0m - Request 12800 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a12ab7ba4ab4039968c45404332d528c in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:04:43,834 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12918\n",
      "2023-07-12 19:05:04,712 - \u001b[1;32mINFO\u001b[1;0m - Starting request #12968\n",
      "2023-07-12 19:05:25,626 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13018\n",
      "2023-07-12 19:05:27,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12312 failed with Exception \n",
      "2023-07-12 19:05:31,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12321 failed with Exception \n",
      "2023-07-12 19:05:46,533 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13066\n",
      "2023-07-12 19:06:07,423 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13116\n",
      "2023-07-12 19:06:28,334 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13166\n",
      "2023-07-12 19:06:49,249 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13216\n",
      "2023-07-12 19:07:10,158 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13266\n",
      "2023-07-12 19:07:31,086 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13316\n",
      "2023-07-12 19:07:52,000 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13366\n",
      "2023-07-12 19:08:10,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 12696 failed with Exception \n",
      "2023-07-12 19:08:12,908 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13415\n",
      "2023-07-12 19:08:33,837 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13465\n",
      "2023-07-12 19:08:43,194 - \u001b[1;33mWARNING\u001b[1;0m - Request 13415 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fd25e2191f9923eb09eed48799154818 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:08:54,761 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13514\n",
      "2023-07-12 19:09:15,684 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13564\n",
      "2023-07-12 19:09:17,868 - \u001b[1;33mWARNING\u001b[1;0m - Request 13497 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ef411fcb72df02ae3715c040c4451daf in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:09:36,622 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13613\n",
      "2023-07-12 19:09:41,707 - \u001b[1;33mWARNING\u001b[1;0m - Request 13553 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1737763d39c18f6db74a02cb1adf7ba7 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:09:57,549 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13662\n",
      "2023-07-12 19:10:09,005 - \u001b[1;33mWARNING\u001b[1;0m - Request 13618 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 166fb4602fd3a57461cc382c01b648eb in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:10:18,483 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13711\n",
      "2023-07-12 19:10:39,433 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13761\n",
      "2023-07-12 19:11:00,370 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13811\n",
      "2023-07-12 19:11:21,329 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13861\n",
      "2023-07-12 19:11:24,785 - \u001b[1;33mWARNING\u001b[1;0m - Request 13797 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dac6611bc38e61f135d095301cdfed9e in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:11:26,392 - \u001b[1;33mWARNING\u001b[1;0m - Request 13801 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 16db059dd47bfe0966bb7440ae710a97 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:11:42,250 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13909\n",
      "2023-07-12 19:12:03,197 - \u001b[1;32mINFO\u001b[1;0m - Starting request #13959\n",
      "2023-07-12 19:12:23,088 - \u001b[1;33mWARNING\u001b[1;0m - Request 13934 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9d50ac40584b6d27be2517459289ce1a in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:12:24,142 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14008\n",
      "2023-07-12 19:12:45,100 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14058\n",
      "2023-07-12 19:13:06,054 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14108\n",
      "2023-07-12 19:13:06,939 - \u001b[1;33mWARNING\u001b[1;0m - Request 14038 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f36623e0a0c94cd61f4726deb645935b in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:13:10,361 - \u001b[1;33mWARNING\u001b[1;0m - Request 14046 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0c372c529d74171aae0a19a25507e112 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:13:27,021 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14156\n",
      "2023-07-12 19:13:47,995 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14206\n",
      "2023-07-12 19:13:50,180 - \u001b[1;33mWARNING\u001b[1;0m - Request 14139 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 42c8e699204a89b6cee273ca2e1f8840 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:14:08,965 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14255\n",
      "2023-07-12 19:14:29,937 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14305\n",
      "2023-07-12 19:14:30,355 - \u001b[1;33mWARNING\u001b[1;0m - Request 14234 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3edbbf8e283b250cb45f78f56eef3e98 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:14:50,922 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14354\n",
      "2023-07-12 19:15:11,908 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14404\n",
      "2023-07-12 19:15:32,908 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14454\n",
      "2023-07-12 19:15:53,916 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14504\n",
      "2023-07-12 19:16:14,928 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14554\n",
      "2023-07-12 19:16:26,999 - \u001b[1;33mWARNING\u001b[1;0m - Request 14581 failed with Exception 0, message='Attempt to decode JSON with unexpected mimetype: text/html', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "2023-07-12 19:16:35,932 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14603\n",
      "2023-07-12 19:16:39,350 - \u001b[1;33mWARNING\u001b[1;0m - Request 14540 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 17440aa6812655fb45c3bd31d18744bc in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:16:56,958 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:17:17,974 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14702\n",
      "2023-07-12 19:17:19,639 - \u001b[1;33mWARNING\u001b[1;0m - Request 14634 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5ea8fb20fc0cc088a082c15276e05314 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:17:39,025 - \u001b[1;33mWARNING\u001b[1;0m - Request 14680 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 272a00bc500a8052bd3e0d879604c9f1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:17:39,027 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14751\n",
      "2023-07-12 19:17:51,493 - \u001b[1;33mWARNING\u001b[1;0m - Request 14073 failed with Exception \n",
      "2023-07-12 19:18:00,082 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14799\n",
      "2023-07-12 19:18:21,151 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14849\n",
      "2023-07-12 19:18:42,245 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14899\n",
      "2023-07-12 19:19:03,401 - \u001b[1;32mINFO\u001b[1;0m - Starting request #14949\n",
      "2023-07-12 19:19:32,414 - \u001b[1;33mWARNING\u001b[1;0m - Request 14946 failed with error {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b3c524623ba0f013c49157bc318f75c3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}\n",
      "2023-07-12 19:21:12,962 - \u001b[1;32mINFO\u001b[1;0m - Parallel processing complete. Results saved to /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_173545.jsonl\n",
      "2023-07-12 19:21:12,967 - \u001b[1;32mINFO\u001b[1;0m - Loading API responses from /home/pamessina/medvqa-workspace/tmp/mimiccxr/openai/api_responses_20230712_173545.jsonl\n",
      "2023-07-12 19:21:13,352 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"better new\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,352 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS... request?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 28, \"total_tokens\": 309}}, {\"observation\": \"better new\"}] for sentence \"better new\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"better new\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,352 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: Sure! Please provide the sentence describing the fact from the chest X-ray report for each project, and I will generate the JSON array of paraphrases for you.\n",
      "2023-07-12 19:21:13,353 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS... for you.\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 32, \"total_tokens\": 313}}, {\"observation\": \"2 projects\"}] for sentence \"2 projects\": Could not parse output: Sure! Please provide the sentence describing the fact from the chest X-ray report for each project, and I will generate the JSON array of paraphrases for you.\n",
      "2023-07-12 19:21:13,356 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"\\n]\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 272, \"total_tokens\": 553}}, {\"observation\": \"more revealing\"}] for sentence \"more revealing\": Could not parse output: pulmonary infiltrates\n",
      "[\n",
      "\"abnormal lung opacities\",\n",
      "\"lung infiltrations\",\n",
      "\"pulmonary infiltrations\",\n",
      "\"abnormal lung infiltrates\",\n",
      "\"abnormal lung markings\",\n",
      "\"abnormal lung shadows\",\n",
      "\"abnormal lung densities\",\n",
      "\"abnormal lung patterns\",\n",
      "\"abnormal lung findings\",\n",
      "\"abnormal lung opacities on imaging\"\n",
      "]\n",
      "\n",
      "pleural effusion\n",
      "[\n",
      "\"fluid in the lungs\",\n",
      "\"accumulation of fluid in the pleural space\",\n",
      "\"excess fluid around the lungs\",\n",
      "\"fluid collection in the pleural cavity\",\n",
      "\"pleural fluid buildup\",\n",
      "\"fluid-filled lungs\",\n",
      "\"pleural fluid accumulation\",\n",
      "\"fluid in the pleural cavity\",\n",
      "\"pleural effusion present\",\n",
      "\"presence of pleural fluid\"\n",
      "]\n",
      "\n",
      "atelectasis\n",
      "[\n",
      "\"collapsed lung\",\n",
      "\"incomplete expansion of the lung\",\n",
      "\"incomplete lung inflation\",\n",
      "\"incomplete lung aeration\",\n",
      "\"incomplete lung ventilation\",\n",
      "\"incomplete lung expansion\",\n",
      "\"incomplete lung filling\",\n",
      "\"incomplete lung inflation\"\n",
      "]\n",
      "\n",
      "pneumothorax\n",
      "[\n",
      "\"collapsed lung due to air in the pleural space\",\n",
      "\"air in the chest cavity\",\n",
      "\"air in the pleural cavity\",\n",
      "\"collapsed lung caused by air accumulation\",\n",
      "\"lung collapse due to air buildup\",\n",
      "\"air-filled pleural space\",\n",
      "\"presence of air in the pleural space\"\n",
      "]\n",
      "2023-07-12 19:21:13,359 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information to generate the paraphrases. Could you please provide a sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 19:21:13,359 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...t?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 31, \"total_tokens\": 314}}, {\"observation\": \"patient in the ER\"}] for sentence \"patient in the ER\": Could not parse output: I'm sorry, but I need more information to generate the paraphrases. Could you please provide a sentence describing a fact from a chest X-ray report?\n",
      "2023-07-12 19:21:13,360 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: Please provide a specific fact from a chest X-ray report for me to paraphrase.\n",
      "2023-07-12 19:21:13,360 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...e.\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 17, \"total_tokens\": 298}}, {\"observation\": \"similar situation\"}] for sentence \"similar situation\": Could not parse output: Please provide a specific fact from a chest X-ray report for me to paraphrase.\n",
      "2023-07-12 19:21:13,361 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need more information. Could you please provide a specific clinical condition or fact from a chest X-ray report?\n",
      "2023-07-12 19:21:13,362 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...?\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 281, \"completion_tokens\": 27, \"total_tokens\": 308}}, {\"observation\": \"clinical condition\"}] for sentence \"clinical condition\": Could not parse output: I'm sorry, but I need more information. Could you please provide a specific clinical condition or fact from a chest X-ray report?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 00:27\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 00:27\"}] for sentence \"comparison to 00:27\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 00:27\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison to 06:26\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 32, \"total_tokens\": 317}}, {\"observation\": \"comparison to 06:26\"}] for sentence \"comparison to 06:26\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison to 06:26\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I apologize, but I'm not sure what you mean by \"comparison to 11:18\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 31, \"total_tokens\": 316}}, {\"observation\": \"comparison to 11:18\"}] for sentence \"comparison to 11:18\": Could not parse output: I apologize, but I'm not sure what you mean by \"comparison to 11:18\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I don't understand what you mean by \"comparison with :30\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,364 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 30, \"total_tokens\": 313}}, {\"observation\": \"comparison with :30\"}] for sentence \"comparison with :30\": Could not parse output: I'm sorry, but I don't understand what you mean by \"comparison with :30\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,365 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I apologize, but I'm not sure what you mean by \"most recently 17:19\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,365 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 31, \"total_tokens\": 316}}, {\"observation\": \"most recently 17:19\"}] for sentence \"most recently 17:19\": Could not parse output: I apologize, but I'm not sure what you mean by \"most recently 17:19\". Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,366 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"recommend pull back.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 19:21:13,366 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 29, \"total_tokens\": 311}}, {\"observation\": \"recommend pull back\"}] for sentence \"recommend pull back\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"recommend pull back.\" Could you please provide more information or clarify your request?\n",
      "2023-07-12 19:21:13,366 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 223, \"total_tokens\": 505}}, {\"observation\": \"prior lines removed\"}] for sentence \"prior lines removed\": Could not parse output: \"benign calcification\"\n",
      "[\n",
      "\"non-cancerous calcification\",\n",
      "\"harmless calcification\",\n",
      "\"innocuous calcification\",\n",
      "\"benign calcified lesion\",\n",
      "\"non-malignant calcification\",\n",
      "\"non-threatening calcification\",\n",
      "\"not indicative of cancer calcification\",\n",
      "\"safe calcification\",\n",
      "\"non-dangerous calcification\",\n",
      "\"non-metastatic calcification\"\n",
      "]\n",
      "\n",
      "\"osteoporosis\"\n",
      "[\n",
      "\"decreased bone density\",\n",
      "\"brittle bones\",\n",
      "\"low bone mass\",\n",
      "\"thinning of the bones\",\n",
      "\"weakening of the bones\",\n",
      "\"porous bones\",\n",
      "\"fragile bones\",\n",
      "\"reduced bone strength\",\n",
      "\"loss of bone density\",\n",
      "\"degenerative bone disease\"\n",
      "]\n",
      "\n",
      "\"no osteoporosis\"\n",
      "[\n",
      "\"normal bone density\",\n",
      "\"healthy bones\",\n",
      "\"adequate bone mass\",\n",
      "\"strong bones\",\n",
      "\"normal bone strength\",\n",
      "\"no signs of osteoporosis\",\n",
      "\"absence of osteoporosis\",\n",
      "\"no evidence of bone thinning\",\n",
      "\"no indication of bone weakening\",\n",
      "\"no osteoporotic changes\",\n",
      "\"no degenerative bone disease\"\n",
      "]\n",
      "2023-07-12 19:21:13,366 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I need the specific sentence describing a fact from a chest X-ray report in order to provide the paraphrases. Could you please provide the sentence?\n",
      "2023-07-12 19:21:13,366 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 34, \"total_tokens\": 317}}, {\"observation\": \"updated findings #1\"}] for sentence \"updated findings #1\": Could not parse output: I'm sorry, but I need the specific sentence describing a fact from a chest X-ray report in order to provide the paraphrases. Could you please provide the sentence?\n",
      "2023-07-12 19:21:13,368 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 83, \"total_tokens\": 366}}, {\"observation\": \"nominary in position\"}] for sentence \"nominary in position\": Could not parse output: {\n",
      "  \"fact\": \"nominary in position\",\n",
      "  \"paraphrases\": [\n",
      "    \"not in the correct position\",\n",
      "    \"misplaced\",\n",
      "    \"out of place\",\n",
      "    \"dislocated\",\n",
      "    \"abnormal positioning\",\n",
      "    \"improperly positioned\",\n",
      "    \"malpositioned\",\n",
      "    \"misaligned\",\n",
      "    \"not in its expected location\",\n",
      "    \"not properly situated\"\n",
      "  ]\n",
      "}\n",
      "2023-07-12 19:21:13,383 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 287, \"completion_tokens\": 14, \"total_tokens\": 301}}, {\"observation\": \"findings posted at 15:55\"}] for sentence \"findings posted at 15:55\": Could not parse output: I apologize, but I'm not able to assist with that request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:21:13,387 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...sh_reason\": \"length\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 300, \"total_tokens\": 582}}, {\"observation\": \"better evaluation desired\"}] for sentence \"better evaluation desired\": Could not parse output: I apologize for the previous response. Here is an improved version:\n",
      "\n",
      "For a given sentence describing a fact from a chest X-ray report, the following JSON array provides a wide range of paraphrases using diverse terminology, synonyms, and abbreviations commonly used by radiologists to express the same idea:\n",
      "\n",
      "{\n",
      "  \"benign calcification\": [\n",
      "    \"non-cancerous calcification\",\n",
      "    \"harmless calcification\",\n",
      "    \"innocuous calcification\",\n",
      "    \"benign calcified lesion\",\n",
      "    \"non-malignant calcification\",\n",
      "    \"non-threatening calcification\",\n",
      "    \"not indicative of cancer calcification\",\n",
      "    \"safe calcification\",\n",
      "    \"non-dangerous calcification\",\n",
      "    \"non-metastatic calcification\"\n",
      "  ],\n",
      "  \"osteoporosis\": [\n",
      "    \"decreased bone density\",\n",
      "    \"brittle bones\",\n",
      "    \"low bone mass\",\n",
      "    \"thinning of the bones\",\n",
      "    \"weakening of the bones\",\n",
      "    \"porous bones\",\n",
      "    \"fragile bones\",\n",
      "    \"reduced bone strength\",\n",
      "    \"loss of bone density\",\n",
      "    \"degenerative bone disease\"\n",
      "  ],\n",
      "  \"no osteoporosis\": [\n",
      "    \"normal bone density\",\n",
      "    \"healthy bones\",\n",
      "    \"adequate bone mass\",\n",
      "    \"strong bones\",\n",
      "    \"normal bone strength\",\n",
      "    \"no signs of osteoporosis\",\n",
      "    \"absence of osteoporosis\",\n",
      "    \"no evidence of bone thinning\",\n",
      "    \"no indication of bone\n",
      "2023-07-12 19:21:13,412 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...eason\": \"length\"}], \"usage\": {\"prompt_tokens\": 283, \"completion_tokens\": 300, \"total_tokens\": 583}}, {\"observation\": \"additional detail of interest\"}] for sentence \"additional detail of interest\": Could not parse output: pulmonary nodule\n",
      "[\n",
      "\"lung nodule\",\n",
      "\"pulmonary mass\",\n",
      "\"spot on the lung\",\n",
      "\"abnormality in the lung\",\n",
      "\"lesion in the lung\",\n",
      "\"small growth in the lung\",\n",
      "\"abnormal spot in the lung\",\n",
      "\"nodular opacity in the lung\",\n",
      "\"pulmonary lesion\",\n",
      "\"abnormal pulmonary density\"\n",
      "]\n",
      "\n",
      "pleural effusion\n",
      "[\n",
      "\"fluid in the lungs\",\n",
      "\"accumulation of fluid in the pleural space\",\n",
      "\"excess fluid around the lungs\",\n",
      "\"fluid buildup in the pleural cavity\",\n",
      "\"pleural fluid collection\",\n",
      "\"fluid-filled lungs\",\n",
      "\"pleural fluid effusion\",\n",
      "\"fluid in the pleural cavity\",\n",
      "\"pleural fluid accumulation\"\n",
      "]\n",
      "\n",
      "atelectasis\n",
      "[\n",
      "\"collapsed lung\",\n",
      "\"incomplete expansion of the lung\",\n",
      "\"lung collapse\",\n",
      "\"incomplete lung inflation\",\n",
      "\"incomplete lung expansion\",\n",
      "\"partial lung collapse\",\n",
      "\"incomplete lung aeration\",\n",
      "\"incomplete lung ventilation\"\n",
      "]\n",
      "\n",
      "pneumothorax\n",
      "[\n",
      "\"collapsed lung due to air leakage\",\n",
      "\"air trapped between the chest wall and lungs\",\n",
      "\"air in the pleural space\",\n",
      "\"collapsed lung caused by air accumulation\",\n",
      "\"lung collapse due to air escape\",\n",
      "\"air-filled pleural cavity\",\n",
      "\"air pocket in the chest cavity\"\n",
      "]\n",
      "\n",
      "pulmonary embolism\n",
      "[\n",
      "\"blood clot in the lung\",\n",
      "\"blockage of a pulmonary artery by a blood clot\",\n",
      "\"clot in the pulmonary vasculature\",\n",
      "\"ob\n",
      "2023-07-12 19:21:13,419 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I'm sorry, but I'm not sure what you mean by \"AP chest compared to 9:16 p.m.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,419 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS..._reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 290, \"completion_tokens\": 36, \"total_tokens\": 326}}, {\"observation\": \"AP chest compared to 9:16 p.m.\"}] for sentence \"AP chest compared to 9:16 p.m.\": Could not parse output: I'm sorry, but I'm not sure what you mean by \"AP chest compared to 9:16 p.m.\" Could you please provide more context or clarify your request?\n",
      "2023-07-12 19:21:13,431 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS..._reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 285, \"completion_tokens\": 9, \"total_tokens\": 294}}, {\"observation\": \"most recent chest image is from\"}] for sentence \"most recent chest image is from\": Could not parse output: \"the most recent chest image is from\"\n",
      "2023-07-12 19:21:13,433 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...eason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 282, \"completion_tokens\": 264, \"total_tokens\": 546}}, {\"observation\": \"possible positional differences\"}] for sentence \"possible positional differences\": Could not parse output: I apologize for any confusion. The position of the terms within the JSON array may vary, as long as all the paraphrases are included. Here are the paraphrases for the examples provided:\n",
      "\n",
      "For \"benign calcification\":\n",
      "[\n",
      "\"non-cancerous calcification\",\n",
      "\"harmless calcification\",\n",
      "\"innocuous calcification\",\n",
      "\"benign calcified lesion\",\n",
      "\"non-malignant calcification\",\n",
      "\"non-threatening calcification\",\n",
      "\"not indicative of cancer calcification\",\n",
      "\"safe calcification\",\n",
      "\"non-dangerous calcification\",\n",
      "\"non-metastatic calcification\"\n",
      "]\n",
      "\n",
      "For \"osteoporosis\":\n",
      "[\n",
      "\"decreased bone density\",\n",
      "\"brittle bones\",\n",
      "\"low bone mass\",\n",
      "\"thinning of the bones\",\n",
      "\"weakening of the bones\",\n",
      "\"porous bones\",\n",
      "\"fragile bones\",\n",
      "\"reduced bone strength\",\n",
      "\"loss of bone density\",\n",
      "\"degenerative bone disease\"\n",
      "]\n",
      "\n",
      "For \"no osteoporosis\":\n",
      "[\n",
      "\"normal bone density\",\n",
      "\"healthy bones\",\n",
      "\"adequate bone mass\",\n",
      "\"strong bones\",\n",
      "\"normal bone strength\",\n",
      "\"no signs of osteoporosis\",\n",
      "\"absence of osteoporosis\",\n",
      "\"no evidence of bone thinning\",\n",
      "\"no indication of bone weakening\",\n",
      "\"no osteoporotic changes\",\n",
      "\"no degenerative bone disease\"\n",
      "]\n",
      "2023-07-12 19:21:13,474 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...n\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 148, \"total_tokens\": 432}}, {\"observation\": \"possible advancement of the project\"}] for sentence \"possible advancement of the project\": Could not parse output: - Implement a machine learning model to generate paraphrases for the given sentence.\n",
      "- Expand the range of terminology and synonyms used in the paraphrases to cover a broader spectrum of radiology terminology.\n",
      "- Incorporate a feedback loop where users can rate the quality and relevance of the generated paraphrases, allowing the system to improve over time.\n",
      "- Develop a user interface where radiologists can input sentences and receive paraphrases in real-time.\n",
      "- Integrate the system with existing radiology software or platforms to provide seamless access to the paraphrasing functionality.\n",
      "- Explore the possibility of generating paraphrases in multiple languages to cater to a global audience of radiologists.\n",
      "- Conduct user studies and gather feedback from radiologists to further refine and enhance the paraphrasing system.\n",
      "2023-07-12 19:21:13,923 - \u001b[1;33mWARNING\u001b[1;0m - GPT is protesting: I apologize for the confusion. Could you please provide me with the specific fact from the chest X-ray report that you would like a more complete assessment of?\n",
      "2023-07-12 19:21:13,923 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...n\": \"stop\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 31, \"total_tokens\": 315}}, {\"observation\": \"more complete assessment of findings\"}] for sentence \"more complete assessment of findings\": Could not parse output: I apologize for the confusion. Could you please provide me with the specific fact from the chest X-ray report that you would like a more complete assessment of?\n",
      "2023-07-12 19:21:13,950 - \u001b[1;31mERROR\u001b[1;0m - Error parsing response [{\"model\": \"gpt-3.5-turbo-0613\", \"messages\": [{\"role\": \"system\", \"content\": \"Given a sentence describing a fact from a chest X-ray report, output a JS...length\"}], \"usage\": {\"prompt_tokens\": 284, \"completion_tokens\": 300, \"total_tokens\": 584}}, {\"observation\": \"recommendation for document resolution\"}] for sentence \"recommendation for document resolution\": Could not parse output: To improve the resolution of the document, you can try the following recommendations:\n",
      "\n",
      "1. Use a higher quality scanner or camera: If you are scanning or capturing the document digitally, using a higher resolution scanner or camera can result in clearer and more detailed images.\n",
      "\n",
      "2. Adjust the settings: Check the settings on your scanning device or camera and ensure that it is set to capture images at the highest resolution possible. This will help to capture more details in the document.\n",
      "\n",
      "3. Clean the document: Before scanning or capturing the document, make sure it is clean and free from any smudges or dirt. This will help to avoid any blurriness or distortion in the final image.\n",
      "\n",
      "4. Use proper lighting: Ensure that the document is well-lit when scanning or capturing it. Good lighting can help to enhance the clarity and sharpness of the document.\n",
      "\n",
      "5. Use image editing software: After scanning or capturing the document, you can use image editing software to enhance its resolution. This can include adjusting brightness and contrast, sharpening the image, or using filters to improve clarity.\n",
      "\n",
      "6. Print at a higher resolution: If you plan to print the document, make sure to select a higher resolution option in your printer settings. This will result in a clearer and more detailed printout.\n",
      "\n",
      "7. Consider professional services: If you require high-quality resolution for important documents, you may consider using professional scanning or imaging services. These services often have specialized equipment and expertise to ensure optimal resolution.\n",
      "\n",
      "Remember to save a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:21:14,264 - \u001b[1;32mINFO\u001b[1;0m - Deleting API requests and responses\r\n",
      "2023-07-12 19:21:14,272 - \u001b[1;32mINFO\u001b[1;0m - Succesfully processed 14973 of 14997 API responses.\r\n",
      "                    24 of 14997 API responses could not be processed.\r\n",
      "                    Saving paraphrased observations to /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/mimiccxr/paraphrase_observations_with_openai.py \\\n",
    "    --integrated_fact_metadata_filepath \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "    --min_num_words_per_sentence 2 \\\n",
    "    --offset 0 \\\n",
    "    --num_sentences 15000 \\\n",
    "    --sample_equally_spaced_sentences \\\n",
    "    --process_kth_of_every_n_sentences 0 4 \\\n",
    "    --max_requests_per_minute 3000 \\\n",
    "    --max_tokens_per_minute 85000 \\\n",
    "    --max_tokens_per_request 300 \\\n",
    "    --logging_level \"INFO\" \\\n",
    "    --api_key_name \"OPENAI_API_KEY_1\" \\\n",
    "    --openai_model_name \"gpt-3.5-turbo-0613\" \\\n",
    "    --alias \"__two-or-more-words__part1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c5a0a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = load_jsonl('/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1c07c2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14993"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8cbaede9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'observation': 'suggestion for repeat radiography with standard PA and lateral technique with deaccessing of the port in the left upper hemithorax'},\n",
       " 'parsed_response': ['suggest repeat X-ray with standard posterior-anterior and lateral technique',\n",
       "  'recommend another X-ray with standard PA and lateral views',\n",
       "  'advise for a repeat radiograph using the standard PA and lateral technique',\n",
       "  'propose a repeat X-ray with deaccessing of the port in the left upper hemithorax',\n",
       "  'urge for a follow-up X-ray with deaccessing of the port in the left upper hemithorax',\n",
       "  'request for a repeat radiograph with deaccessing of the port in the left upper hemithorax',\n",
       "  'call for another X-ray with deaccessing of the port in the left upper hemithorax',\n",
       "  'suggest performing a repeat radiograph with deaccessing of the port in the left upper hemithorax',\n",
       "  'recommend obtaining another X-ray with deaccessing of the port in the left upper hemithorax',\n",
       "  'advise for a follow-up X-ray using the standard PA and lateral technique with deaccessing of the port in the left upper hemithorax']}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[-8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
