{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8288f907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 150\n",
      "   batches_per_epoch: 600\n",
      "   max_images_per_batch: 35\n",
      "   max_phrases_per_batch: 600\n",
      "   max_phrases_per_image: 30\n",
      "   val_batch_size_factor: 3.0\n",
      "   checkpoint_folder: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_folder_paths: None\n",
      "   freeze_image_encoder: False\n",
      "   raw_image_encoding: densenet-121\n",
      "   num_regions: 169\n",
      "   image_local_feat_size: 1024\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   yolov8_model_name_or_path: None\n",
      "   yolov8_model_alias: None\n",
      "   phrase_embedding_size: 128\n",
      "   regions_width: 13\n",
      "   regions_height: 13\n",
      "   qkv_size: 256\n",
      "   phrase_classifier_hidden_size: 256\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   attention_supervision_loss_weight: 1.0\n",
      "   phrase_classifier_loss_weight: 1.0\n",
      "   foreground_loss_weight: 1.0\n",
      "   background_loss_weight: 1.0\n",
      "   num_train_workers: 2\n",
      "   num_val_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   image_size: [416, 416]\n",
      "   dicom_id_to_pos_neg_facts_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_all_pos_neg_facts(hash=1013,1477415609053716381).pkl\n",
      "   mscxr_phrase2embedding_filepath: None\n",
      "   chest_imagenome_bbox_phrase_embeddings_filepath: None\n",
      "   vinbig_bbox_phrase_embeddings_filepath: None\n",
      "   chexlocalize_class_phrase_embeddings_filepath: None\n",
      "   exclude_noisy_images: True\n",
      "   mimiccxr_facts_weight: 1.0\n",
      "   chest_imagenome_anatlocs_weight: 1.0\n",
      "   mscxr_weight: 1.0\n",
      "   vinbig_weight: 1.0\n",
      "   chexlocalize_weight: 1.0\n",
      "   img_aug_mode: None\n",
      "   pos_area_prior: 0.4\n",
      "   neg_area_prior: 0.0\n",
      "   use_mimiccxr_facts_for_train: True\n",
      "   use_mimiccxr_facts_for_test: True\n",
      "   use_mscxr_for_train: False\n",
      "   use_mscxr_for_test: False\n",
      "   use_chest_imagenome_for_train: False\n",
      "   use_chest_imagenome_gold_for_test: False\n",
      "   use_vinbig_for_train: False\n",
      "   use_vinbig_for_test: False\n",
      "   use_chexlocalize_for_train: False\n",
      "   use_chexlocalize_for_test: False\n",
      "   vinbig_training_data_mode: train\n",
      "   mask_exponent: 1.0\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "source_image_size_mode: medium_512\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of PhraseGrounder ...\u001b[0m\n",
      "\u001b[93mWARNING: Unused kwargs: {'pretrained_checkpoint_folder_path': None, 'pretrained_checkpoint_folder_paths': None}\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: densenet-121\n",
      "create_densenet121_feature_extractor()\n",
      "   drop_rate: 0.0\n",
      "DenseNet121's pretrained weights loaded from ImageNet\n",
      "  self.global_feat_size = 2048\n",
      "  self.local_feat_size = 1024\n",
      "  Initializing auxiliary tasks\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating MIMIC-CXR Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "Using standard transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [416, 416], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "get_image_transform()\n",
      "Using standard transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [416, 416], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "mask_exponent = 1.0\n",
      "len(forbidden_train_dicom_ids) = 1030\n",
      "\u001b[1mPreparing MIMIC-CXR-Facts datasets and dataloaders for training/testing...\u001b[0m\n",
      "fact_embeddings.shape = (596394, 128)\n",
      "Using image size mode: medium_512\n",
      "227835it [00:01, 177946.48it/s]\n",
      "Total number of images: 376105\n",
      "len(train_indices) = 367955\n",
      "len(test_indices) = 8150\n",
      "\u001b[1mBuilding train fact dataloaders...\u001b[0m\n",
      "Number of facts: 30, # images: 361407\n",
      "Number of facts: 27, # images: 751\n",
      "Number of facts: 19, # images: 270\n",
      "Number of facts: 23, # images: 379\n",
      "Number of facts: 24, # images: 423\n",
      "Number of facts: 28, # images: 778\n",
      "Number of facts: 25, # images: 586\n",
      "Number of facts: 29, # images: 724\n",
      "Number of facts: 17, # images: 172\n",
      "Number of facts: 12, # images: 70\n",
      "Number of facts: 22, # images: 334\n",
      "Number of facts: 26, # images: 753\n",
      "Number of facts: 21, # images: 296\n",
      "Number of facts: 20, # images: 282\n",
      "Number of facts: 7, # images: 7\n",
      "Number of facts: 11, # images: 47\n",
      "Number of facts: 16, # images: 119\n",
      "Number of facts: 18, # images: 231\n",
      "Number of facts: 15, # images: 104\n",
      "Number of facts: 10, # images: 26\n",
      "Number of facts: 14, # images: 100\n",
      "Number of facts: 13, # images: 71\n",
      "Number of facts: 9, # images: 11\n",
      "Number of facts: 5, # images: 1\n",
      "Number of facts: 8, # images: 4\n",
      "Number of facts: 6, # images: 5\n",
      "Number of facts: 4, # images: 4\n",
      "len(self.train_fact_dataloader) = 18350\n",
      "\u001b[1mBuilding test fact dataloaders...\u001b[0m\n",
      "Number of facts: 30, # images: 8011\n",
      "Number of facts: 27, # images: 15\n",
      "Number of facts: 28, # images: 13\n",
      "Number of facts: 15, # images: 5\n",
      "Number of facts: 23, # images: 13\n",
      "Number of facts: 29, # images: 26\n",
      "Number of facts: 26, # images: 16\n",
      "Number of facts: 25, # images: 17\n",
      "Number of facts: 18, # images: 3\n",
      "Number of facts: 22, # images: 4\n",
      "Number of facts: 17, # images: 7\n",
      "Number of facts: 14, # images: 2\n",
      "Number of facts: 21, # images: 4\n",
      "Number of facts: 20, # images: 8\n",
      "Number of facts: 24, # images: 2\n",
      "Number of facts: 16, # images: 4\n",
      "len(self.test_fact_dataloader) = 149\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(mimiccxr_trainer.train_fact_dataloader) = 18350\n",
      "len(mimiccxr_trainer.test_fact_dataloader) = 149\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim-facts\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 8.32917, fg_cpg_loss 6.35840, fg_att_reg_loss 0.58452, fg_phrcls_loss 1.38624, 201.12 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 7.15680, fg_att_reg_loss 0.58024, fg_phrcls_loss 1.38550, 47.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.3919.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 7.52266, fg_cpg_loss 5.57951, fg_att_reg_loss 0.56112, fg_phrcls_loss 1.38203, 205.68 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 6.49888, fg_att_reg_loss 0.54122, fg_phrcls_loss 1.38032, 47.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4011.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 6.63944, fg_cpg_loss 4.82059, fg_att_reg_loss 0.45215, fg_phrcls_loss 1.36671, 207.43 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 6.10042, fg_att_reg_loss 0.39618, fg_phrcls_loss 1.36329, 47.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4268.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 6.09191, fg_cpg_loss 4.52276, fg_att_reg_loss 0.33561, fg_phrcls_loss 1.23355, 204.83 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.91452, fg_att_reg_loss 0.30155, fg_phrcls_loss 1.13555, 46.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4603.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 5.53019, fg_cpg_loss 4.28314, fg_att_reg_loss 0.26648, fg_phrcls_loss 0.98057, 199.71 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.74511, fg_att_reg_loss 0.28741, fg_phrcls_loss 1.07141, 46.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4718.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 5.36638, fg_cpg_loss 4.18029, fg_att_reg_loss 0.25935, fg_phrcls_loss 0.92673, 202.62 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.69337, fg_att_reg_loss 0.28448, fg_phrcls_loss 1.05303, 47.12 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4747.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 5.31612, fg_cpg_loss 4.14610, fg_att_reg_loss 0.25664, fg_phrcls_loss 0.91338, 204.06 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.66027, fg_att_reg_loss 0.28163, fg_phrcls_loss 1.04249, 47.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4764.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 5.25454, fg_cpg_loss 4.09712, fg_att_reg_loss 0.25477, fg_phrcls_loss 0.90265, 205.88 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.64748, fg_att_reg_loss 0.28148, fg_phrcls_loss 1.04077, 47.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4769.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 5.25120, fg_cpg_loss 4.09521, fg_att_reg_loss 0.25406, fg_phrcls_loss 0.90193, 207.27 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.61800, fg_att_reg_loss 0.28097, fg_phrcls_loss 1.03785, 47.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4774.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 5.20213, fg_cpg_loss 4.05428, fg_att_reg_loss 0.25283, fg_phrcls_loss 0.89502, 208.25 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.60496, fg_att_reg_loss 0.28052, fg_phrcls_loss 1.03680, 47.18 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4778.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 5.20398, fg_cpg_loss 4.05794, fg_att_reg_loss 0.25233, fg_phrcls_loss 0.89370, 200.20 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.60382, fg_att_reg_loss 0.28035, fg_phrcls_loss 1.03882, 47.06 secs\n",
      "\u001b[1m---- Epoch 12/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 5.21423, fg_cpg_loss 4.06680, fg_att_reg_loss 0.25230, fg_phrcls_loss 0.89514, 203.07 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.60067, fg_att_reg_loss 0.27997, fg_phrcls_loss 1.03502, 47.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4780.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 5.18219, fg_cpg_loss 4.04795, fg_att_reg_loss 0.24979, fg_phrcls_loss 0.88446, 205.27 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.54759, fg_att_reg_loss 0.27614, fg_phrcls_loss 1.01190, 47.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4810.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 4.98926, fg_cpg_loss 3.88877, fg_att_reg_loss 0.24514, fg_phrcls_loss 0.85536, 206.08 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.50139, fg_att_reg_loss 0.27584, fg_phrcls_loss 0.99838, 47.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4830.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 4.91680, fg_cpg_loss 3.82078, fg_att_reg_loss 0.24745, fg_phrcls_loss 0.84857, 201.04 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.37975, fg_att_reg_loss 0.27797, fg_phrcls_loss 0.98795, 46.78 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4843.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 4.87943, fg_cpg_loss 3.78507, fg_att_reg_loss 0.24865, fg_phrcls_loss 0.84572, 201.10 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.36007, fg_att_reg_loss 0.27886, fg_phrcls_loss 0.98285, 47.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4848.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 4.82714, fg_cpg_loss 3.73885, fg_att_reg_loss 0.24892, fg_phrcls_loss 0.83937, 203.96 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.32345, fg_att_reg_loss 0.27975, fg_phrcls_loss 0.97744, 47.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4854.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 4.78592, fg_cpg_loss 3.70648, fg_att_reg_loss 0.24804, fg_phrcls_loss 0.83140, 206.02 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.32414, fg_att_reg_loss 0.28026, fg_phrcls_loss 0.98085, 47.48 secs\n",
      "\u001b[1m---- Epoch 19/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 4.79218, fg_cpg_loss 3.70839, fg_att_reg_loss 0.24950, fg_phrcls_loss 0.83429, 204.64 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.30760, fg_att_reg_loss 0.28033, fg_phrcls_loss 0.97720, 47.00 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4855.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 4.82960, fg_cpg_loss 3.74041, fg_att_reg_loss 0.25026, fg_phrcls_loss 0.83894, 200.98 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.30983, fg_att_reg_loss 0.28046, fg_phrcls_loss 0.97841, 47.03 secs\n",
      "\u001b[1m---- Epoch 21/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 4.83760, fg_cpg_loss 3.75887, fg_att_reg_loss 0.25150, fg_phrcls_loss 0.82723, 203.53 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.33291, fg_att_reg_loss 0.28360, fg_phrcls_loss 0.96257, 47.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4858.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 4.72653, fg_cpg_loss 3.66464, fg_att_reg_loss 0.25246, fg_phrcls_loss 0.80943, 205.86 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.26002, fg_att_reg_loss 0.28372, fg_phrcls_loss 0.94205, 47.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4883.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 4.67531, fg_cpg_loss 3.62768, fg_att_reg_loss 0.25214, fg_phrcls_loss 0.79548, 206.27 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.20105, fg_att_reg_loss 0.28454, fg_phrcls_loss 0.93457, 47.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4894.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 4.63105, fg_cpg_loss 3.58959, fg_att_reg_loss 0.25200, fg_phrcls_loss 0.78946, 201.51 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.18188, fg_att_reg_loss 0.28470, fg_phrcls_loss 0.92858, 46.81 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4901.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 4.63688, fg_cpg_loss 3.59535, fg_att_reg_loss 0.25248, fg_phrcls_loss 0.78905, 201.90 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg_cpg_loss 5.19986, fg_att_reg_loss 0.28480, fg_phrcls_loss 0.92931, 47.21 secs\n",
      "\u001b[1m---- Epoch 26/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 4.60875, fg_cpg_loss 3.57643, fg_att_reg_loss 0.25146, fg_phrcls_loss 0.78086, 205.05 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.18973, fg_att_reg_loss 0.28477, fg_phrcls_loss 0.93021, 47.39 secs\n",
      "\u001b[1m---- Epoch 27/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 4.57924, fg_cpg_loss 3.54534, fg_att_reg_loss 0.25175, fg_phrcls_loss 0.78215, 204.34 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.17427, fg_att_reg_loss 0.28452, fg_phrcls_loss 0.92532, 46.99 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_27_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4906.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 28/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 4.57673, fg_cpg_loss 3.54650, fg_att_reg_loss 0.25143, fg_phrcls_loss 0.77880, 201.32 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.19554, fg_att_reg_loss 0.28556, fg_phrcls_loss 0.92903, 46.99 secs\n",
      "\u001b[1m---- Epoch 29/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 4.65946, fg_cpg_loss 3.62596, fg_att_reg_loss 0.25293, fg_phrcls_loss 0.78056, 203.87 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.23041, fg_att_reg_loss 0.28796, fg_phrcls_loss 0.91074, 47.30 secs\n",
      "\u001b[1m---- Epoch 30/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 4.62997, fg_cpg_loss 3.60584, fg_att_reg_loss 0.25467, fg_phrcls_loss 0.76946, 205.67 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 5.17187, fg_att_reg_loss 0.28533, fg_phrcls_loss 0.89643, 47.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4928.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "Current run is terminating due to exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 9.13 GiB already allocated; 8.56 MiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 9.13 GiB already allocated; 8.56 MiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 976, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 850, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 541, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 134, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1087, in _run_once_on_dataset_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/phrase_grounding.py\", line 472, in step_fn\n",
      "    output = step_fn__fact_grounding(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/phrase_grounding.py\", line 84, in step_fn__fact_grounding\n",
      "    model_output = model(**model_kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/phrase_grounding/phrase_grounder.py\", line 98, in forward\n",
      "    output = super().forward(\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/vision/visual_modules.py\", line 698, in forward\n",
      "    local_feat_NxCxHxW = self.raw_image_encoder(raw_images)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torchvision/models/densenet.py\", line 123, in forward\n",
      "    new_features = layer(features)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torchvision/models/densenet.py\", line 89, in forward\n",
      "    bottleneck_output = self.bn_function(prev_features)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torchvision/models/densenet.py\", line 49, in bn_function\n",
      "    concated_features = torch.cat(inputs, 1)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 9.13 GiB already allocated; 8.56 MiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python ../train_phrase_grounding.py \\\n",
    "--epochs 150 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--max_images_per_batch 35 \\\n",
    "--max_phrases_per_batch 600 \\\n",
    "--max_phrases_per_image 30 \\\n",
    "--val_batch_size_factor 3.0 \\\n",
    "--raw_image_encoding \"densenet-121\" \\\n",
    "--image_size 416 416 \\\n",
    "--image_local_feat_size 1024 \\\n",
    "--num_regions 169 \\\n",
    "--regions_width 13 \\\n",
    "--regions_height 13 \\\n",
    "--phrase_embedding_size 128 \\\n",
    "--qkv_size 256 \\\n",
    "--phrase_classifier_hidden_size 256 \\\n",
    "--num_train_workers 2 \\\n",
    "--num_val_workers 2 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--use_mimiccxr_facts_for_train \\\n",
    "--use_mimiccxr_facts_for_test \\\n",
    "--dicom_id_to_pos_neg_facts_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_all_pos_neg_facts(hash=1013,1477415609053716381).pkl\" \\\n",
    "--exclude_noisy_images \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a26021ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 600\n",
      "   max_images_per_batch: 20\n",
      "   max_phrases_per_batch: 250\n",
      "   max_phrases_per_image: 20\n",
      "   val_batch_size_factor: 2.0\n",
      "   checkpoint_folder: models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_folder_paths: None\n",
      "   freeze_image_encoder: False\n",
      "   raw_image_encoding: densenet-121\n",
      "   num_regions: 169\n",
      "   image_local_feat_size: 1024\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   yolov8_model_name_or_path: None\n",
      "   yolov8_model_alias: None\n",
      "   phrase_embedding_size: 128\n",
      "   regions_width: 13\n",
      "   regions_height: 13\n",
      "   qkv_size: 256\n",
      "   phrase_classifier_hidden_size: 256\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   attention_supervision_loss_weight: 1.0\n",
      "   phrase_classifier_loss_weight: 1.0\n",
      "   foreground_loss_weight: 1.0\n",
      "   background_loss_weight: 1.0\n",
      "   num_train_workers: 2\n",
      "   num_val_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   image_size: [416, 416]\n",
      "   dicom_id_to_pos_neg_facts_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_all_pos_neg_facts(hash=1013,1477415609053716381).pkl\n",
      "   mscxr_phrase2embedding_filepath: None\n",
      "   chest_imagenome_bbox_phrase_embeddings_filepath: None\n",
      "   vinbig_bbox_phrase_embeddings_filepath: None\n",
      "   chexlocalize_class_phrase_embeddings_filepath: None\n",
      "   exclude_noisy_images: True\n",
      "   mimiccxr_facts_weight: 1.0\n",
      "   chest_imagenome_anatlocs_weight: 1.0\n",
      "   mscxr_weight: 1.0\n",
      "   vinbig_weight: 1.0\n",
      "   chexlocalize_weight: 1.0\n",
      "   img_aug_mode: None\n",
      "   pos_area_prior: 0.4\n",
      "   neg_area_prior: 0.0\n",
      "   use_mimiccxr_facts_for_train: True\n",
      "   use_mimiccxr_facts_for_test: True\n",
      "   use_mscxr_for_train: False\n",
      "   use_mscxr_for_test: False\n",
      "   use_chest_imagenome_for_train: False\n",
      "   use_chest_imagenome_gold_for_test: False\n",
      "   use_vinbig_for_train: False\n",
      "   use_vinbig_for_test: False\n",
      "   use_chexlocalize_for_train: False\n",
      "   use_chexlocalize_for_test: False\n",
      "   vinbig_training_data_mode: train\n",
      "   mask_exponent: 1.0\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Resuming training ------\u001b[0m\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of PhraseGrounder ...\u001b[0m\n",
      "\u001b[93mWARNING: Unused kwargs: {'pretrained_checkpoint_folder_path': None, 'pretrained_checkpoint_folder_paths': None}\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: densenet-121\n",
      "create_densenet121_feature_extractor()\n",
      "   drop_rate: 0.0\n",
      "DenseNet121's pretrained weights loaded from ImageNet\n",
      "  self.global_feat_size = 2048\n",
      "  self.local_feat_size = 1024\n",
      "  Initializing auxiliary tasks\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating MIMIC-CXR Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "Using standard transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [416, 416], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "get_image_transform()\n",
      "Using standard transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [416, 416], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "mask_exponent = 1.0\n",
      "len(forbidden_train_dicom_ids) = 1030\n",
      "\u001b[1mPreparing MIMIC-CXR-Facts datasets and dataloaders for training/testing...\u001b[0m\n",
      "fact_embeddings.shape = (596394, 128)\n",
      "Using image size mode: medium_512\n",
      "227835it [00:01, 177832.70it/s]\n",
      "Total number of images: 376105\n",
      "len(train_indices) = 367955\n",
      "len(test_indices) = 8150\n",
      "\u001b[1mBuilding train fact dataloaders...\u001b[0m\n",
      "Number of facts: 20, # images: 366713\n",
      "Number of facts: 19, # images: 270\n",
      "Number of facts: 17, # images: 172\n",
      "Number of facts: 12, # images: 70\n",
      "Number of facts: 7, # images: 7\n",
      "Number of facts: 11, # images: 47\n",
      "Number of facts: 16, # images: 119\n",
      "Number of facts: 18, # images: 231\n",
      "Number of facts: 15, # images: 104\n",
      "Number of facts: 10, # images: 26\n",
      "Number of facts: 14, # images: 100\n",
      "Number of facts: 13, # images: 71\n",
      "Number of facts: 9, # images: 11\n",
      "Number of facts: 5, # images: 1\n",
      "Number of facts: 8, # images: 4\n",
      "Number of facts: 6, # images: 5\n",
      "Number of facts: 4, # images: 4\n",
      "len(self.train_fact_dataloader) = 30652\n",
      "\u001b[1mBuilding test fact dataloaders...\u001b[0m\n",
      "Number of facts: 20, # images: 8129\n",
      "Number of facts: 15, # images: 5\n",
      "Number of facts: 18, # images: 3\n",
      "Number of facts: 17, # images: 7\n",
      "Number of facts: 14, # images: 2\n",
      "Number of facts: 16, # images: 4\n",
      "len(self.test_fact_dataloader) = 344\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(mimiccxr_trainer.train_fact_dataloader) = 30652\n",
      "len(mimiccxr_trainer.test_fact_dataloader) = 344\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim-facts\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_163_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5368.pt', 'checkpoint_194_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5431.pt', 'checkpoint_234_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5470.pt', 'checkpoint_129_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5307.pt', 'checkpoint_30_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.4928.pt', 'checkpoint_91_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5153.pt', 'checkpoint_60_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5050.pt', 'checkpoint_275_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5471.pt']\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)/checkpoint_275_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5471.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 276/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 3.00428, fg_cpg_loss 2.25364, fg_att_reg_loss 0.22858, fg_phrcls_loss 0.52206, 118.50 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46266, fg_att_reg_loss 0.26314, fg_phrcls_loss 0.62610, 44.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_276_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5485.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 277/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.16641, fg_cpg_loss 2.39684, fg_att_reg_loss 0.23076, fg_phrcls_loss 0.53881, 122.00 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.54235, fg_att_reg_loss 0.26445, fg_phrcls_loss 0.64522, 43.95 secs\n",
      "\u001b[1m---- Epoch 278/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.14346, fg_cpg_loss 2.37117, fg_att_reg_loss 0.23155, fg_phrcls_loss 0.54073, 121.01 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.58739, fg_att_reg_loss 0.26377, fg_phrcls_loss 0.65002, 43.98 secs\n",
      "\u001b[1m---- Epoch 279/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.09015, fg_cpg_loss 2.32956, fg_att_reg_loss 0.22922, fg_phrcls_loss 0.53136, 120.97 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46809, fg_att_reg_loss 0.26495, fg_phrcls_loss 0.63007, 44.30 secs\n",
      "\u001b[1m---- Epoch 280/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 2.97796, fg_cpg_loss 2.23515, fg_att_reg_loss 0.22776, fg_phrcls_loss 0.51505, 120.04 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46345, fg_att_reg_loss 0.26506, fg_phrcls_loss 0.62813, 44.59 secs\n",
      "\u001b[1m---- Epoch 281/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 3.01802, fg_cpg_loss 2.26601, fg_att_reg_loss 0.22983, fg_phrcls_loss 0.52218, 121.62 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.48452, fg_att_reg_loss 0.26465, fg_phrcls_loss 0.62859, 44.29 secs\n",
      "\u001b[1m---- Epoch 282/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 3.03913, fg_cpg_loss 2.28816, fg_att_reg_loss 0.22989, fg_phrcls_loss 0.52108, 120.96 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.45901, fg_att_reg_loss 0.26412, fg_phrcls_loss 0.62467, 44.54 secs\n",
      "\u001b[1m---- Epoch 283/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 3.04021, fg_cpg_loss 2.28898, fg_att_reg_loss 0.22989, fg_phrcls_loss 0.52135, 121.56 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.45034, fg_att_reg_loss 0.26332, fg_phrcls_loss 0.62276, 44.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_283_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5489.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 284/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.99811, fg_cpg_loss 2.25162, fg_att_reg_loss 0.22868, fg_phrcls_loss 0.51781, 120.13 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.49678, fg_att_reg_loss 0.26443, fg_phrcls_loss 0.62713, 43.87 secs\n",
      "\u001b[1m---- Epoch 285/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.12645, fg_cpg_loss 2.35847, fg_att_reg_loss 0.23090, fg_phrcls_loss 0.53709, 122.68 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.56244, fg_att_reg_loss 0.26443, fg_phrcls_loss 0.64575, 43.60 secs\n",
      "\u001b[1m---- Epoch 286/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.10909, fg_cpg_loss 2.34635, fg_att_reg_loss 0.22991, fg_phrcls_loss 0.53283, 120.89 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.49082, fg_att_reg_loss 0.26292, fg_phrcls_loss 0.63170, 44.08 secs\n",
      "\u001b[1m---- Epoch 287/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.10194, fg_cpg_loss 2.34361, fg_att_reg_loss 0.22983, fg_phrcls_loss 0.52849, 121.07 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47121, fg_att_reg_loss 0.26485, fg_phrcls_loss 0.62921, 44.33 secs\n",
      "\u001b[1m---- Epoch 288/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 3.04221, fg_cpg_loss 2.29361, fg_att_reg_loss 0.22805, fg_phrcls_loss 0.52055, 121.62 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46102, fg_att_reg_loss 0.26490, fg_phrcls_loss 0.62315, 44.54 secs\n",
      "\u001b[1m---- Epoch 289/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 3.02364, fg_cpg_loss 2.27340, fg_att_reg_loss 0.22900, fg_phrcls_loss 0.52123, 121.73 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44766, fg_att_reg_loss 0.26477, fg_phrcls_loss 0.62384, 44.62 secs\n",
      "\u001b[1m---- Epoch 290/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 3.00313, fg_cpg_loss 2.25786, fg_att_reg_loss 0.23004, fg_phrcls_loss 0.51522, 121.68 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46687, fg_att_reg_loss 0.26456, fg_phrcls_loss 0.62401, 44.32 secs\n",
      "\u001b[1m---- Epoch 291/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 3.00561, fg_cpg_loss 2.25959, fg_att_reg_loss 0.22849, fg_phrcls_loss 0.51752, 120.27 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44156, fg_att_reg_loss 0.26281, fg_phrcls_loss 0.62309, 43.84 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_291_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5493.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 292/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 3.01610, fg_cpg_loss 2.26924, fg_att_reg_loss 0.22882, fg_phrcls_loss 0.51804, 121.72 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.45404, fg_att_reg_loss 0.26318, fg_phrcls_loss 0.62196, 44.22 secs\n",
      "\u001b[1m---- Epoch 293/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.11991, fg_cpg_loss 2.34903, fg_att_reg_loss 0.23068, fg_phrcls_loss 0.54020, 121.19 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.60627, fg_att_reg_loss 0.26415, fg_phrcls_loss 0.64822, 44.28 secs\n",
      "\u001b[1m---- Epoch 294/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.09905, fg_cpg_loss 2.33935, fg_att_reg_loss 0.22933, fg_phrcls_loss 0.53037, 120.86 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.51949, fg_att_reg_loss 0.26248, fg_phrcls_loss 0.63434, 44.34 secs\n",
      "\u001b[1m---- Epoch 295/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.05696, fg_cpg_loss 2.30581, fg_att_reg_loss 0.22780, fg_phrcls_loss 0.52335, 120.64 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46410, fg_att_reg_loss 0.26314, fg_phrcls_loss 0.62755, 44.43 secs\n",
      "\u001b[1m---- Epoch 296/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 3.07711, fg_cpg_loss 2.31861, fg_att_reg_loss 0.22972, fg_phrcls_loss 0.52878, 122.28 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47617, fg_att_reg_loss 0.26283, fg_phrcls_loss 0.62780, 44.55 secs\n",
      "\u001b[1m---- Epoch 297/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.99817, fg_cpg_loss 2.25242, fg_att_reg_loss 0.22652, fg_phrcls_loss 0.51923, 122.12 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47092, fg_att_reg_loss 0.26232, fg_phrcls_loss 0.62312, 44.75 secs\n",
      "\u001b[1m---- Epoch 298/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.95854, fg_cpg_loss 2.22147, fg_att_reg_loss 0.22506, fg_phrcls_loss 0.51201, 121.71 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47764, fg_att_reg_loss 0.26242, fg_phrcls_loss 0.62873, 43.90 secs\n",
      "\u001b[1m---- Epoch 299/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 3.01200, fg_cpg_loss 2.26472, fg_att_reg_loss 0.22768, fg_phrcls_loss 0.51960, 121.55 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43434, fg_att_reg_loss 0.26217, fg_phrcls_loss 0.62058, 43.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_299_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5498.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 300/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.95022, fg_cpg_loss 2.21191, fg_att_reg_loss 0.22600, fg_phrcls_loss 0.51232, 121.63 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.49399, fg_att_reg_loss 0.26269, fg_phrcls_loss 0.62545, 43.80 secs\n",
      "\u001b[1m---- Epoch 301/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.08841, fg_cpg_loss 2.33074, fg_att_reg_loss 0.22699, fg_phrcls_loss 0.53068, 121.00 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.57680, fg_att_reg_loss 0.26610, fg_phrcls_loss 0.64227, 44.19 secs\n",
      "\u001b[1m---- Epoch 302/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.04072, fg_cpg_loss 2.28767, fg_att_reg_loss 0.22887, fg_phrcls_loss 0.52419, 120.48 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.52472, fg_att_reg_loss 0.26380, fg_phrcls_loss 0.62948, 44.19 secs\n",
      "\u001b[1m---- Epoch 303/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.00470, fg_cpg_loss 2.25717, fg_att_reg_loss 0.22687, fg_phrcls_loss 0.52067, 121.85 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46782, fg_att_reg_loss 0.26440, fg_phrcls_loss 0.62366, 44.70 secs\n",
      "\u001b[1m---- Epoch 304/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 2.98536, fg_cpg_loss 2.24037, fg_att_reg_loss 0.22721, fg_phrcls_loss 0.51778, 122.48 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.48232, fg_att_reg_loss 0.26352, fg_phrcls_loss 0.62306, 44.72 secs\n",
      "\u001b[1m---- Epoch 305/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.98679, fg_cpg_loss 2.24524, fg_att_reg_loss 0.22772, fg_phrcls_loss 0.51382, 121.14 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43832, fg_att_reg_loss 0.26261, fg_phrcls_loss 0.62162, 44.55 secs\n",
      "\u001b[1m---- Epoch 306/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 3.00364, fg_cpg_loss 2.25984, fg_att_reg_loss 0.22756, fg_phrcls_loss 0.51625, 120.92 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg_cpg_loss 3.45442, fg_att_reg_loss 0.26273, fg_phrcls_loss 0.62055, 44.69 secs\n",
      "\u001b[1m---- Epoch 307/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.97923, fg_cpg_loss 2.24243, fg_att_reg_loss 0.22614, fg_phrcls_loss 0.51065, 122.03 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44909, fg_att_reg_loss 0.26374, fg_phrcls_loss 0.62250, 44.63 secs\n",
      "\u001b[1m---- Epoch 308/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 3.01048, fg_cpg_loss 2.26620, fg_att_reg_loss 0.22782, fg_phrcls_loss 0.51646, 122.29 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.42272, fg_att_reg_loss 0.26355, fg_phrcls_loss 0.62036, 44.97 secs\n",
      "\u001b[1m---- Epoch 309/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.06968, fg_cpg_loss 2.31883, fg_att_reg_loss 0.22683, fg_phrcls_loss 0.52401, 120.87 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.55534, fg_att_reg_loss 0.26430, fg_phrcls_loss 0.64245, 43.76 secs\n",
      "\u001b[1m---- Epoch 310/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.08673, fg_cpg_loss 2.32913, fg_att_reg_loss 0.22923, fg_phrcls_loss 0.52837, 122.59 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.50350, fg_att_reg_loss 0.26349, fg_phrcls_loss 0.63240, 44.00 secs\n",
      "\u001b[1m---- Epoch 311/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.07028, fg_cpg_loss 2.31551, fg_att_reg_loss 0.22941, fg_phrcls_loss 0.52536, 122.04 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.46281, fg_att_reg_loss 0.26284, fg_phrcls_loss 0.62474, 43.67 secs\n",
      "\u001b[1m---- Epoch 312/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 3.03250, fg_cpg_loss 2.28665, fg_att_reg_loss 0.22643, fg_phrcls_loss 0.51941, 121.40 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47359, fg_att_reg_loss 0.26315, fg_phrcls_loss 0.62182, 43.67 secs\n",
      "\u001b[1m---- Epoch 313/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.98677, fg_cpg_loss 2.24402, fg_att_reg_loss 0.22691, fg_phrcls_loss 0.51584, 120.78 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44166, fg_att_reg_loss 0.26259, fg_phrcls_loss 0.61668, 43.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_313_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5501.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 314/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 3.01159, fg_cpg_loss 2.27068, fg_att_reg_loss 0.22624, fg_phrcls_loss 0.51466, 120.13 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43245, fg_att_reg_loss 0.26294, fg_phrcls_loss 0.61627, 43.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_314_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5502.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 315/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 3.02380, fg_cpg_loss 2.27771, fg_att_reg_loss 0.22894, fg_phrcls_loss 0.51715, 120.44 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43552, fg_att_reg_loss 0.26326, fg_phrcls_loss 0.61840, 44.44 secs\n",
      "\u001b[1m---- Epoch 316/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 3.00630, fg_cpg_loss 2.26438, fg_att_reg_loss 0.22746, fg_phrcls_loss 0.51446, 121.85 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.45380, fg_att_reg_loss 0.26353, fg_phrcls_loss 0.61922, 44.66 secs\n",
      "\u001b[1m---- Epoch 317/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.13456, fg_cpg_loss 2.37056, fg_att_reg_loss 0.22989, fg_phrcls_loss 0.53411, 121.84 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.51308, fg_att_reg_loss 0.26396, fg_phrcls_loss 0.63598, 44.75 secs\n",
      "\u001b[1m---- Epoch 318/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.08594, fg_cpg_loss 2.32818, fg_att_reg_loss 0.22916, fg_phrcls_loss 0.52860, 121.10 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47128, fg_att_reg_loss 0.26245, fg_phrcls_loss 0.62891, 44.33 secs\n",
      "\u001b[1m---- Epoch 319/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.05878, fg_cpg_loss 2.30855, fg_att_reg_loss 0.22716, fg_phrcls_loss 0.52306, 120.33 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47278, fg_att_reg_loss 0.26140, fg_phrcls_loss 0.62326, 43.88 secs\n",
      "\u001b[1m---- Epoch 320/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 2.99321, fg_cpg_loss 2.25414, fg_att_reg_loss 0.22698, fg_phrcls_loss 0.51209, 122.02 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.42863, fg_att_reg_loss 0.26118, fg_phrcls_loss 0.62199, 43.99 secs\n",
      "\u001b[1m---- Epoch 321/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.96238, fg_cpg_loss 2.22689, fg_att_reg_loss 0.22568, fg_phrcls_loss 0.50981, 120.31 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43529, fg_att_reg_loss 0.26066, fg_phrcls_loss 0.62049, 44.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_321_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5503.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 322/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.97594, fg_cpg_loss 2.24058, fg_att_reg_loss 0.22650, fg_phrcls_loss 0.50886, 120.72 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44710, fg_att_reg_loss 0.26109, fg_phrcls_loss 0.61909, 44.64 secs\n",
      "\u001b[1m---- Epoch 323/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.95621, fg_cpg_loss 2.22307, fg_att_reg_loss 0.22597, fg_phrcls_loss 0.50717, 120.78 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.42851, fg_att_reg_loss 0.26187, fg_phrcls_loss 0.61718, 44.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_323_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5506.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 324/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 3.01313, fg_cpg_loss 2.26728, fg_att_reg_loss 0.22777, fg_phrcls_loss 0.51808, 121.74 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.42667, fg_att_reg_loss 0.26107, fg_phrcls_loss 0.61332, 44.53 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_324_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5509.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 325/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.16836, fg_cpg_loss 2.40307, fg_att_reg_loss 0.22820, fg_phrcls_loss 0.53709, 120.66 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.54804, fg_att_reg_loss 0.26385, fg_phrcls_loss 0.63817, 44.68 secs\n",
      "\u001b[1m---- Epoch 326/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.25233, fg_cpg_loss 2.47038, fg_att_reg_loss 0.23390, fg_phrcls_loss 0.54805, 124.13 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.67180, fg_att_reg_loss 0.26787, fg_phrcls_loss 0.68212, 43.75 secs\n",
      "\u001b[1m---- Epoch 327/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.35051, fg_cpg_loss 2.53052, fg_att_reg_loss 0.23780, fg_phrcls_loss 0.58219, 126.88 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.47775, fg_att_reg_loss 0.26308, fg_phrcls_loss 0.62325, 43.78 secs\n",
      "\u001b[1m---- Epoch 328/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 3.01748, fg_cpg_loss 2.27681, fg_att_reg_loss 0.22622, fg_phrcls_loss 0.51445, 122.07 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44093, fg_att_reg_loss 0.26244, fg_phrcls_loss 0.62157, 43.67 secs\n",
      "\u001b[1m---- Epoch 329/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.97907, fg_cpg_loss 2.23655, fg_att_reg_loss 0.22968, fg_phrcls_loss 0.51284, 121.98 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44102, fg_att_reg_loss 0.26367, fg_phrcls_loss 0.62053, 43.87 secs\n",
      "\u001b[1m---- Epoch 330/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.96288, fg_cpg_loss 2.22918, fg_att_reg_loss 0.22520, fg_phrcls_loss 0.50850, 121.30 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43058, fg_att_reg_loss 0.26213, fg_phrcls_loss 0.61759, 43.87 secs\n",
      "\u001b[1m---- Epoch 331/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.96411, fg_cpg_loss 2.22623, fg_att_reg_loss 0.22713, fg_phrcls_loss 0.51075, 120.48 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44890, fg_att_reg_loss 0.26293, fg_phrcls_loss 0.61628, 44.34 secs\n",
      "\u001b[1m---- Epoch 332/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.96038, fg_cpg_loss 2.22089, fg_att_reg_loss 0.22812, fg_phrcls_loss 0.51137, 122.20 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44970, fg_att_reg_loss 0.26191, fg_phrcls_loss 0.61789, 44.56 secs\n",
      "\u001b[1m---- Epoch 333/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.11360, fg_cpg_loss 2.35314, fg_att_reg_loss 0.22899, fg_phrcls_loss 0.53147, 121.52 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.61082, fg_att_reg_loss 0.26264, fg_phrcls_loss 0.65564, 44.06 secs\n",
      "\u001b[1m---- Epoch 334/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.05758, fg_cpg_loss 2.30687, fg_att_reg_loss 0.22812, fg_phrcls_loss 0.52258, 120.44 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.50036, fg_att_reg_loss 0.26350, fg_phrcls_loss 0.62571, 43.69 secs\n",
      "\u001b[1m---- Epoch 335/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.00898, fg_cpg_loss 2.26225, fg_att_reg_loss 0.22935, fg_phrcls_loss 0.51738, 121.27 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg_cpg_loss 3.44183, fg_att_reg_loss 0.26327, fg_phrcls_loss 0.62158, 43.90 secs\n",
      "\u001b[1m---- Epoch 336/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 2.99346, fg_cpg_loss 2.25214, fg_att_reg_loss 0.22717, fg_phrcls_loss 0.51415, 120.36 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.45328, fg_att_reg_loss 0.26306, fg_phrcls_loss 0.61911, 43.95 secs\n",
      "\u001b[1m---- Epoch 337/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.93370, fg_cpg_loss 2.20436, fg_att_reg_loss 0.22566, fg_phrcls_loss 0.50368, 120.95 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43855, fg_att_reg_loss 0.26322, fg_phrcls_loss 0.61802, 44.18 secs\n",
      "\u001b[1m---- Epoch 338/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 3.02008, fg_cpg_loss 2.27504, fg_att_reg_loss 0.22903, fg_phrcls_loss 0.51601, 121.11 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.39818, fg_att_reg_loss 0.26231, fg_phrcls_loss 0.61527, 44.42 secs\n",
      "\u001b[1m---- Epoch 339/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.98930, fg_cpg_loss 2.24885, fg_att_reg_loss 0.22743, fg_phrcls_loss 0.51302, 121.81 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.42561, fg_att_reg_loss 0.26228, fg_phrcls_loss 0.61588, 44.58 secs\n",
      "\u001b[1m---- Epoch 340/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 3.01835, fg_cpg_loss 2.27253, fg_att_reg_loss 0.22908, fg_phrcls_loss 0.51674, 120.56 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44062, fg_att_reg_loss 0.26204, fg_phrcls_loss 0.61520, 43.75 secs\n",
      "\u001b[1m---- Epoch 341/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.07707, fg_cpg_loss 2.32445, fg_att_reg_loss 0.22875, fg_phrcls_loss 0.52387, 120.71 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.50565, fg_att_reg_loss 0.26222, fg_phrcls_loss 0.63350, 43.60 secs\n",
      "\u001b[1m---- Epoch 342/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.04428, fg_cpg_loss 2.29493, fg_att_reg_loss 0.22812, fg_phrcls_loss 0.52123, 119.34 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.49247, fg_att_reg_loss 0.26249, fg_phrcls_loss 0.63139, 43.90 secs\n",
      "\u001b[1m---- Epoch 343/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.00580, fg_cpg_loss 2.26154, fg_att_reg_loss 0.22752, fg_phrcls_loss 0.51674, 120.80 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.49263, fg_att_reg_loss 0.26316, fg_phrcls_loss 0.62674, 44.27 secs\n",
      "\u001b[1m---- Epoch 344/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 3.01236, fg_cpg_loss 2.27469, fg_att_reg_loss 0.22668, fg_phrcls_loss 0.51098, 121.41 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43110, fg_att_reg_loss 0.26207, fg_phrcls_loss 0.61733, 44.37 secs\n",
      "\u001b[1m---- Epoch 345/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 3.00567, fg_cpg_loss 2.26403, fg_att_reg_loss 0.22750, fg_phrcls_loss 0.51414, 121.95 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.41161, fg_att_reg_loss 0.26163, fg_phrcls_loss 0.61469, 44.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_345_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5510.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 346/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.98606, fg_cpg_loss 2.24809, fg_att_reg_loss 0.22738, fg_phrcls_loss 0.51059, 122.36 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43241, fg_att_reg_loss 0.26217, fg_phrcls_loss 0.61852, 44.39 secs\n",
      "\u001b[1m---- Epoch 347/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.94430, fg_cpg_loss 2.21329, fg_att_reg_loss 0.22533, fg_phrcls_loss 0.50568, 120.86 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.42627, fg_att_reg_loss 0.26177, fg_phrcls_loss 0.61420, 43.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_347_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5511.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 348/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.99060, fg_cpg_loss 2.25096, fg_att_reg_loss 0.22785, fg_phrcls_loss 0.51179, 121.18 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.39982, fg_att_reg_loss 0.26166, fg_phrcls_loss 0.61267, 44.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_348_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5514.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 349/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.07134, fg_cpg_loss 2.31316, fg_att_reg_loss 0.22891, fg_phrcls_loss 0.52927, 121.32 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.51859, fg_att_reg_loss 0.26431, fg_phrcls_loss 0.63150, 44.12 secs\n",
      "\u001b[1m---- Epoch 350/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.02250, fg_cpg_loss 2.28021, fg_att_reg_loss 0.22622, fg_phrcls_loss 0.51607, 121.56 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.50358, fg_att_reg_loss 0.26237, fg_phrcls_loss 0.62871, 44.58 secs\n",
      "\u001b[1m---- Epoch 351/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.00420, fg_cpg_loss 2.26379, fg_att_reg_loss 0.22644, fg_phrcls_loss 0.51398, 122.15 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44423, fg_att_reg_loss 0.26020, fg_phrcls_loss 0.62128, 44.42 secs\n",
      "\u001b[1m---- Epoch 352/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 2.94312, fg_cpg_loss 2.21291, fg_att_reg_loss 0.22503, fg_phrcls_loss 0.50518, 121.22 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44521, fg_att_reg_loss 0.26093, fg_phrcls_loss 0.61799, 43.77 secs\n",
      "\u001b[1m---- Epoch 353/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.93838, fg_cpg_loss 2.20707, fg_att_reg_loss 0.22493, fg_phrcls_loss 0.50638, 122.15 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44281, fg_att_reg_loss 0.26029, fg_phrcls_loss 0.61662, 43.51 secs\n",
      "\u001b[1m---- Epoch 354/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.91826, fg_cpg_loss 2.19009, fg_att_reg_loss 0.22456, fg_phrcls_loss 0.50361, 120.72 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43139, fg_att_reg_loss 0.26011, fg_phrcls_loss 0.60948, 43.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_354_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5520.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 355/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.91220, fg_cpg_loss 2.18414, fg_att_reg_loss 0.22459, fg_phrcls_loss 0.50347, 121.07 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43982, fg_att_reg_loss 0.26083, fg_phrcls_loss 0.61331, 44.14 secs\n",
      "\u001b[1m---- Epoch 356/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.93305, fg_cpg_loss 2.20404, fg_att_reg_loss 0.22572, fg_phrcls_loss 0.50329, 122.22 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.41680, fg_att_reg_loss 0.26115, fg_phrcls_loss 0.61367, 43.98 secs\n",
      "\u001b[1m---- Epoch 357/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.06862, fg_cpg_loss 2.32018, fg_att_reg_loss 0.22482, fg_phrcls_loss 0.52361, 121.25 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.52203, fg_att_reg_loss 0.26127, fg_phrcls_loss 0.63384, 44.37 secs\n",
      "\u001b[1m---- Epoch 358/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.02277, fg_cpg_loss 2.28248, fg_att_reg_loss 0.22527, fg_phrcls_loss 0.51501, 121.51 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.51319, fg_att_reg_loss 0.26219, fg_phrcls_loss 0.62777, 44.06 secs\n",
      "\u001b[1m---- Epoch 359/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.00239, fg_cpg_loss 2.26372, fg_att_reg_loss 0.22768, fg_phrcls_loss 0.51100, 122.08 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.45827, fg_att_reg_loss 0.26130, fg_phrcls_loss 0.61831, 44.31 secs\n",
      "\u001b[1m---- Epoch 360/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 3.00191, fg_cpg_loss 2.26703, fg_att_reg_loss 0.22610, fg_phrcls_loss 0.50878, 120.76 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.42760, fg_att_reg_loss 0.26169, fg_phrcls_loss 0.61568, 44.39 secs\n",
      "\u001b[1m---- Epoch 361/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.94829, fg_cpg_loss 2.21612, fg_att_reg_loss 0.22608, fg_phrcls_loss 0.50608, 121.69 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44023, fg_att_reg_loss 0.26081, fg_phrcls_loss 0.61255, 43.67 secs\n",
      "\u001b[1m---- Epoch 362/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.96967, fg_cpg_loss 2.23135, fg_att_reg_loss 0.22819, fg_phrcls_loss 0.51013, 121.40 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43900, fg_att_reg_loss 0.26075, fg_phrcls_loss 0.61246, 43.50 secs\n",
      "\u001b[1m---- Epoch 363/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.92947, fg_cpg_loss 2.20472, fg_att_reg_loss 0.22306, fg_phrcls_loss 0.50169, 122.56 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.38192, fg_att_reg_loss 0.26037, fg_phrcls_loss 0.61161, 43.85 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_363_fg_att_reg_loss+fg_cpg_loss+fg_phrcls_loss=0.5525.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 364/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.97778, fg_cpg_loss 2.24048, fg_att_reg_loss 0.22814, fg_phrcls_loss 0.50916, 122.43 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg_cpg_loss 3.43824, fg_att_reg_loss 0.26149, fg_phrcls_loss 0.61213, 43.67 secs\n",
      "\u001b[1m---- Epoch 365/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.07789, fg_cpg_loss 2.32407, fg_att_reg_loss 0.22731, fg_phrcls_loss 0.52650, 122.54 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.51330, fg_att_reg_loss 0.26200, fg_phrcls_loss 0.62899, 44.01 secs\n",
      "\u001b[1m---- Epoch 366/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.02950, fg_cpg_loss 2.28343, fg_att_reg_loss 0.22829, fg_phrcls_loss 0.51778, 122.17 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44015, fg_att_reg_loss 0.26255, fg_phrcls_loss 0.61994, 44.04 secs\n",
      "\u001b[1m---- Epoch 367/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 3.04603, fg_cpg_loss 2.30115, fg_att_reg_loss 0.22814, fg_phrcls_loss 0.51673, 121.57 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43695, fg_att_reg_loss 0.26128, fg_phrcls_loss 0.61355, 44.23 secs\n",
      "\u001b[1m---- Epoch 368/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 2.98558, fg_cpg_loss 2.25012, fg_att_reg_loss 0.22690, fg_phrcls_loss 0.50857, 122.59 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.41951, fg_att_reg_loss 0.26175, fg_phrcls_loss 0.61555, 44.25 secs\n",
      "\u001b[1m---- Epoch 369/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.96454, fg_cpg_loss 2.23699, fg_att_reg_loss 0.22500, fg_phrcls_loss 0.50254, 121.95 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.40276, fg_att_reg_loss 0.26081, fg_phrcls_loss 0.61044, 44.09 secs\n",
      "\u001b[1m---- Epoch 370/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.99581, fg_cpg_loss 2.25913, fg_att_reg_loss 0.22569, fg_phrcls_loss 0.51100, 121.36 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.44262, fg_att_reg_loss 0.26117, fg_phrcls_loss 0.61308, 44.40 secs\n",
      "\u001b[1m---- Epoch 371/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 3.00436, fg_cpg_loss 2.26546, fg_att_reg_loss 0.22697, fg_phrcls_loss 0.51193, 121.31 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.43215, fg_att_reg_loss 0.26161, fg_phrcls_loss 0.61219, 44.11 secs\n",
      "\u001b[1m---- Epoch 372/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.94431, fg_cpg_loss 2.21512, fg_att_reg_loss 0.22594, fg_phrcls_loss 0.50325, 121.61 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.40977, fg_att_reg_loss 0.26080, fg_phrcls_loss 0.61019, 43.97 secs\n",
      "\u001b[1m---- Epoch 373/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 3.05690, fg_cpg_loss 2.30944, fg_att_reg_loss 0.22814, fg_phrcls_loss 0.51932, 121.90 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.50926, fg_att_reg_loss 0.26287, fg_phrcls_loss 0.63033, 44.00 secs\n",
      "\u001b[1m---- Epoch 374/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 3.05226, fg_cpg_loss 2.30494, fg_att_reg_loss 0.22723, fg_phrcls_loss 0.52009, 120.83 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.48767, fg_att_reg_loss 0.26161, fg_phrcls_loss 0.62356, 44.03 secs\n",
      "\u001b[1m---- Epoch 375/375\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 2.98960, fg_cpg_loss 2.25184, fg_att_reg_loss 0.22791, fg_phrcls_loss 0.50985, 121.24 secs\n",
      "(2) Validation stage ...\n",
      "fg_cpg_loss 3.39858, fg_att_reg_loss 0.26135, fg_phrcls_loss 0.61371, 44.42 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_phrase_grounding.py \\\n",
    "--checkpoint_folder \\\n",
    "\"models/phrase_grounding/20240426_072308_mim-facts_PhraseGrounder(dn121,128,256)\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--max_images_per_batch 20 \\\n",
    "--max_phrases_per_batch 250 \\\n",
    "--max_phrases_per_image 20 \\\n",
    "--val_batch_size_factor 2.0 \\\n",
    "--raw_image_encoding \"densenet-121\" \\\n",
    "--image_size 416 416 \\\n",
    "--image_local_feat_size 1024 \\\n",
    "--num_regions 169 \\\n",
    "--regions_width 13 \\\n",
    "--regions_height 13 \\\n",
    "--phrase_embedding_size 128 \\\n",
    "--qkv_size 256 \\\n",
    "--phrase_classifier_hidden_size 256 \\\n",
    "--num_train_workers 2 \\\n",
    "--num_val_workers 2 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--use_mimiccxr_facts_for_train \\\n",
    "--use_mimiccxr_facts_for_test \\\n",
    "--dicom_id_to_pos_neg_facts_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_all_pos_neg_facts(hash=1013,1477415609053716381).pkl\" \\\n",
    "--exclude_noisy_images \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
