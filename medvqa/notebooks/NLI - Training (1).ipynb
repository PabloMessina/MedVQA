{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25c052a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batch_size: 40\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 512\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: True\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  freeze_huggingface_model: True\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Freezing huggingface model\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 14049\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Here she was noted to follow simple comands but was combative prompting sedation and intubation.\n",
      "Hypothesis:  She was alert and oriented x 3. \n",
      "Label: contradiction\n",
      "Number of val samples: 480\n",
      "\u001b[1mExample val text:\u001b[0m\n",
      "Premise: No acute focal pneumonia or pleural effusion.\n",
      "Hypothesis: Again noted are diffuse infiltrative parenchymal opacities, right worse than left; this is largely due to pulmonary edema and the right-sided pleural effusion, but underlying pneumonia cannot be excluded.\n",
      "Label: contradiction\n",
      "\u001b[1mIncluding half of the RadNLI dev samples in the train set...\u001b[0m\n",
      "Number of train samples: 14288\n",
      "Number of val samples: 241\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: Improved aeration at the lung bases with residual subsegmental atelectasis at the left lung base.\n",
      "Hypothesis: There are likely patchy opacities in the lung bases reflective of atelectasis.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding val triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test triplet ranking dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_81_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9542.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_81_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9542.pt\n",
      "\u001b[93mWarning: model state dict has 215 keys, loaded state dict has 223 keys, intersection has 211 keys, union has 227 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in model but not in loaded state dict:\u001b[0m\n",
      "\u001b[93m  nli_classifier.weight\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  nli_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.09662, nli_loss 1.09662, nli_acc 0.39040, 20.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41494, 0.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.4125.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.09193, nli_loss 1.09193, nli_acc 0.46046, 21.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.40664, 0.57 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 1.07166, nli_loss 1.07166, nli_acc 0.51190, 20.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34025, 0.61 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.99213, nli_loss 0.99213, nli_acc 0.54129, 20.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.31535, 0.56 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.93346, nli_loss 0.93346, nli_acc 0.55571, 20.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.31535, 0.56 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.92253, nli_loss 0.92253, nli_acc 0.56019, 20.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.32780, 0.56 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.91924, nli_loss 0.91924, nli_acc 0.56033, 20.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34855, 0.55 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.91784, nli_loss 0.91784, nli_acc 0.56033, 20.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34855, 0.52 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.91559, nli_loss 0.91559, nli_acc 0.56096, 20.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.61 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.91767, nli_loss 0.91767, nli_acc 0.55725, 19.69 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.57 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.91472, nli_loss 0.91472, nli_acc 0.55802, 20.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.53 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.91299, nli_loss 0.91299, nli_acc 0.56215, 20.22 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.58 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.90900, nli_loss 0.90900, nli_acc 0.56110, 20.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.39834, 0.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc=0.4146.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.90211, nli_loss 0.90211, nli_acc 0.56698, 20.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.40249, 0.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc=0.4189.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.89535, nli_loss 0.89535, nli_acc 0.57335, 20.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.42324, 0.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_nli_acc=0.4382.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.89399, nli_loss 0.89399, nli_acc 0.57370, 19.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.57 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.89253, nli_loss 0.89253, nli_acc 0.57328, 20.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.61 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.89141, nli_loss 0.89141, nli_acc 0.57524, 20.63 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.61 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.89427, nli_loss 0.89427, nli_acc 0.57517, 20.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.57 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.89197, nli_loss 0.89197, nli_acc 0.57440, 20.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.59 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.88483, nli_loss 0.88483, nli_acc 0.57727, 20.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.61 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.87790, nli_loss 0.87790, nli_acc 0.57993, 20.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.59 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.87681, nli_loss 0.87681, nli_acc 0.57874, 20.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.59 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.87343, nli_loss 0.87343, nli_acc 0.58182, 20.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.62 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.87219, nli_loss 0.87219, nli_acc 0.58455, 20.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.56 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.87034, nli_loss 0.87034, nli_acc 0.58490, 20.24 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.54 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.87080, nli_loss 0.87080, nli_acc 0.58532, 20.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.57 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.87304, nli_loss 0.87304, nli_acc 0.58042, 20.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.64 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "^C iteration 10025\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 343, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 263, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 155, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 60, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 39, in step_fn\n",
      "    logits = model(premises_encoding, hypotheses_encoding)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 46, in forward\n",
      "    h_vectors = self._text_to_embedding_func(hypotheses_encoding)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 30, in <lambda>\n",
      "    self._text_to_embedding_func = lambda x: self.model.get_projected_text_embeddings(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1358, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1020, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 610, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 495, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 425, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 323, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 100 \\\n",
    "--batch_size 40 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--freeze_huggingface_model \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbc9c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 50\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 512\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 14049\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Following her cardiac catheterization, the patient continued to be hypoxic and hypotensive requiring Dopamine and five liters nasal cannula oxygen.\n",
      "Hypothesis:  The patient had a heart attack\n",
      "Label: neutral\n",
      "Number of val samples: 480\n",
      "\u001b[1mExample val text:\u001b[0m\n",
      "Premise: Small area of parenchymal sparing in the left upper lobe is unchanged.\n",
      "Hypothesis: No newly appeared parenchymal opacities.\n",
      "Label: neutral\n",
      "\u001b[1mIncluding half of the RadNLI dev samples in the train set...\u001b[0m\n",
      "Number of train samples: 14431\n",
      "Number of val samples: 98\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: Low lung volumes with little overall change in the appearance of the heart and lungs.\n",
      "Hypothesis: Low lung volumes accentuate the pulmonary vascular structures.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding val triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test triplet ranking dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_13_nli_acc=0.6636.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/checkpoint_13_nli_acc=0.6636.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46760, nli_loss 0.46760, nli_acc 0.80757, 93.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68367, 0.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.6961.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.45626, nli_loss 0.45626, nli_acc 0.81256, 94.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_nli_acc=0.7057.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.43799, nli_loss 0.43799, nli_acc 0.82046, 93.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71429, 0.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nli_acc=0.7249.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.46279, nli_loss 0.46279, nli_acc 0.80556, 92.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.51 secs\n",
      "\u001b[1m---- Epoch 5/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.39927, nli_loss 0.39927, nli_acc 0.83549, 91.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72449, 0.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nli_acc=0.7356.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.34970, nli_loss 0.34970, nli_acc 0.85961, 93.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73469, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_nli_acc=0.7472.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.31957, nli_loss 0.31957, nli_acc 0.87208, 93.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71429, 0.44 secs\n",
      "\u001b[1m---- Epoch 8/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.30354, nli_loss 0.30354, nli_acc 0.87603, 90.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72449, 0.43 secs\n",
      "\u001b[1m---- Epoch 9/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.29936, nli_loss 0.29936, nli_acc 0.88053, 89.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.44 secs\n",
      "\u001b[1m---- Epoch 10/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29454, nli_loss 0.29454, nli_acc 0.88137, 91.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.45 secs\n",
      "\u001b[1m---- Epoch 11/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28951, nli_loss 0.28951, nli_acc 0.88719, 93.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.43 secs\n",
      "\u001b[1m---- Epoch 12/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.28311, nli_loss 0.28311, nli_acc 0.88837, 91.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.45 secs\n",
      "\u001b[1m---- Epoch 13/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.35399, nli_loss 0.35399, nli_acc 0.85226, 95.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.46 secs\n",
      "\u001b[1m---- Epoch 14/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.30348, nli_loss 0.30348, nli_acc 0.87659, 94.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74490, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc=0.7581.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.25795, nli_loss 0.25795, nli_acc 0.89848, 95.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68367, 0.41 secs\n",
      "\u001b[1m---- Epoch 16/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.23823, nli_loss 0.23823, nli_acc 0.90708, 93.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.42 secs\n",
      "\u001b[1m---- Epoch 17/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.22737, nli_loss 0.22737, nli_acc 0.91165, 94.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.48 secs\n",
      "\u001b[1m---- Epoch 18/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.22223, nli_loss 0.22223, nli_acc 0.91352, 95.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.41 secs\n",
      "\u001b[1m---- Epoch 19/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.21676, nli_loss 0.21676, nli_acc 0.91546, 92.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.60 secs\n",
      "\u001b[1m---- Epoch 20/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.21354, nli_loss 0.21354, nli_acc 0.91775, 90.48 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.44 secs\n",
      "\u001b[1m---- Epoch 21/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.29496, nli_loss 0.29496, nli_acc 0.87887, 95.02 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.47 secs\n",
      "\u001b[1m---- Epoch 22/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.24369, nli_loss 0.24369, nli_acc 0.90229, 94.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.45 secs\n",
      "\u001b[1m---- Epoch 23/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.20478, nli_loss 0.20478, nli_acc 0.91920, 93.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.41 secs\n",
      "\u001b[1m---- Epoch 24/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.18193, nli_loss 0.18193, nli_acc 0.93112, 93.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.46 secs\n",
      "\u001b[1m---- Epoch 25/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.17825, nli_loss 0.17825, nli_acc 0.93140, 95.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.44 secs\n",
      "\u001b[1m---- Epoch 26/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.16974, nli_loss 0.16974, nli_acc 0.93743, 93.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.47 secs\n",
      "\u001b[1m---- Epoch 27/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "^C iteration 19100\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 343, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 263, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 155, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 60, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 47, in step_fn\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/\" \\\n",
    "--epochs 50 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ef52f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 800\n",
      "   batch_size: 25\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   merged_input: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(96629,13236055).jsonl\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  merged_input: False\n",
      "  hidden_size: 128\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 96629\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Right Port-A-Cath in situ.\n",
      "Hypothesis: The patient has a Port-A-Cath inserted on the right side.\n",
      "Label: entailment\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: The heart size remains normal.\n",
      "Hypothesis: The lungs are clear.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test NLI dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI+GPT-4)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_184632_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_184632_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_94_cacc+chf1+chf1+cscc+hscc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9266.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_94_cacc+chf1+chf1+cscc+hscc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9266.pt\n",
      "\u001b[93mWarning: model state dict has 215 keys, loaded state dict has 227 keys, intersection has 215 keys, union has 227 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_184632_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.19581, nli_loss 0.19581, nli_acc 0.92595, 91.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72708, 0.91 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.7470.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.17517, nli_loss 0.17517, nli_acc 0.93545, 90.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.85 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_nli_acc=0.7610.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.16228, nli_loss 0.16228, nli_acc 0.94085, 89.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72500, 0.87 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.22536, nli_loss 0.22536, nli_acc 0.91315, 91.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72083, 0.85 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.21544, nli_loss 0.21544, nli_acc 0.91625, 91.39 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nli_acc=0.7704.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.19736, nli_loss 0.19736, nli_acc 0.92165, 90.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.87 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.17872, nli_loss 0.17872, nli_acc 0.93215, 93.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.86 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.15586, nli_loss 0.15586, nli_acc 0.94275, 91.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73750, 0.87 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.13338, nli_loss 0.13338, nli_acc 0.95460, 90.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.86 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.13292, nli_loss 0.13292, nli_acc 0.95635, 92.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.87 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.14028, nli_loss 0.14028, nli_acc 0.95055, 89.56 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nli_acc 0.73125, 0.86 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.14773, nli_loss 0.14773, nli_acc 0.94580, 89.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73125, 0.85 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.20204, nli_loss 0.20204, nli_acc 0.92265, 92.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc=0.7916.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.19569, nli_loss 0.19569, nli_acc 0.92500, 93.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.86 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.18392, nli_loss 0.18392, nli_acc 0.92945, 90.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73333, 0.87 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.18099, nli_loss 0.18099, nli_acc 0.93095, 81.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.85 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.16704, nli_loss 0.16704, nli_acc 0.93835, 93.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.86 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.11278, nli_loss 0.11278, nli_acc 0.96500, 90.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.88 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.12054, nli_loss 0.12054, nli_acc 0.95980, 91.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.88 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.13595, nli_loss 0.13595, nli_acc 0.95185, 94.92 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.88 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.22298, nli_loss 0.22298, nli_acc 0.91205, 93.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.86 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.20876, nli_loss 0.20876, nli_acc 0.91915, 94.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.88 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.14658, nli_loss 0.14658, nli_acc 0.94880, 93.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.87 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.15836, nli_loss 0.15836, nli_acc 0.94085, 93.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.88 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.15912, nli_loss 0.15912, nli_acc 0.94070, 94.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.90 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.12487, nli_loss 0.12487, nli_acc 0.95625, 94.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.89 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11302, nli_loss 0.11302, nli_acc 0.96410, 93.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.89 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.11015, nli_loss 0.11015, nli_acc 0.96450, 94.29 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.89 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.20652, nli_loss 0.20652, nli_acc 0.92065, 95.53 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.87 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.20671, nli_loss 0.20671, nli_acc 0.92200, 94.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.88 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.17349, nli_loss 0.17349, nli_acc 0.93600, 95.52 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.86 secs\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.14891, nli_loss 0.14891, nli_acc 0.94760, 96.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.88 secs\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.13286, nli_loss 0.13286, nli_acc 0.95385, 91.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.89 secs\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.11164, nli_loss 0.11164, nli_acc 0.96255, 91.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.90 secs\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11370, nli_loss 0.11370, nli_acc 0.96330, 93.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.87 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.12424, nli_loss 0.12424, nli_acc 0.95785, 95.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.88 secs\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.17269, nli_loss 0.17269, nli_acc 0.93475, 94.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.89 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.17121, nli_loss 0.17121, nli_acc 0.93630, 95.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.88 secs\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.15057, nli_loss 0.15057, nli_acc 0.94345, 95.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.87 secs\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.14465, nli_loss 0.14465, nli_acc 0.94875, 95.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.88 secs\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.14240, nli_loss 0.14240, nli_acc 0.94620, 95.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.09192, nli_loss 0.09192, nli_acc 0.97110, 94.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.88 secs\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09943, nli_loss 0.09943, nli_acc 0.96760, 94.77 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.87 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.10781, nli_loss 0.10781, nli_acc 0.96425, 77.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.87 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.17521, nli_loss 0.17521, nli_acc 0.93375, 73.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.18369, nli_loss 0.18369, nli_acc 0.93165, 73.89 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.86 secs\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.12877, nli_loss 0.12877, nli_acc 0.95355, 72.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.86 secs\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.13278, nli_loss 0.13278, nli_acc 0.95170, 74.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 0.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_nli_acc=0.8002.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.13314, nli_loss 0.13314, nli_acc 0.95125, 73.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.86 secs\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.09541, nli_loss 0.09541, nli_acc 0.96840, 81.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.89 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09704, nli_loss 0.09704, nli_acc 0.96875, 90.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.78 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09454, nli_loss 0.09454, nli_acc 0.97005, 92.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.87 secs\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.15485, nli_loss 0.15485, nli_acc 0.94240, 88.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.88 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.16807, nli_loss 0.16807, nli_acc 0.93545, 93.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.87 secs\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.13301, nli_loss 0.13301, nli_acc 0.95040, 85.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.89 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.12189, nli_loss 0.12189, nli_acc 0.95595, 91.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.88 secs\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11070, nli_loss 0.11070, nli_acc 0.96200, 93.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.88 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.08477, nli_loss 0.08477, nli_acc 0.97420, 92.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.89 secs\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09084, nli_loss 0.09084, nli_acc 0.97140, 93.88 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.97 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09703, nli_loss 0.09703, nli_acc 0.96675, 94.15 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.89 secs\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.13972, nli_loss 0.13972, nli_acc 0.94830, 91.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.87 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.14902, nli_loss 0.14902, nli_acc 0.94360, 94.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 0.87 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_62_nli_acc=0.8087.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.11912, nli_loss 0.11912, nli_acc 0.95670, 92.70 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.88 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.11618, nli_loss 0.11618, nli_acc 0.95850, 94.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.89 secs\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11504, nli_loss 0.11504, nli_acc 0.95905, 94.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.89 secs\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.07234, nli_loss 0.07234, nli_acc 0.97860, 84.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.88 secs\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07787, nli_loss 0.07787, nli_acc 0.97665, 93.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.87 secs\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.08538, nli_loss 0.08538, nli_acc 0.97290, 94.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.87 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.14739, nli_loss 0.14739, nli_acc 0.94570, 94.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.15743, nli_loss 0.15743, nli_acc 0.94000, 94.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73750, 0.87 secs\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.11004, nli_loss 0.11004, nli_acc 0.96115, 93.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.86 secs\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.10730, nli_loss 0.10730, nli_acc 0.96220, 95.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.76 secs\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11201, nli_loss 0.11201, nli_acc 0.96005, 95.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.87 secs\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.07409, nli_loss 0.07409, nli_acc 0.97625, 95.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.86 secs\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07910, nli_loss 0.07910, nli_acc 0.97415, 89.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.86 secs\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07584, nli_loss 0.07584, nli_acc 0.97565, 93.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.90 secs\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.12201, nli_loss 0.12201, nli_acc 0.95300, 94.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.89 secs\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.14389, nli_loss 0.14389, nli_acc 0.94540, 90.43 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.88 secs\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.11107, nli_loss 0.11107, nli_acc 0.96125, 83.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.88 secs\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.09821, nli_loss 0.09821, nli_acc 0.96610, 89.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.89 secs\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09422, nli_loss 0.09422, nli_acc 0.96760, 95.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.87 secs\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06749, nli_loss 0.06749, nli_acc 0.97905, 84.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.87 secs\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06993, nli_loss 0.06993, nli_acc 0.97865, 92.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.87 secs\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07692, nli_loss 0.07692, nli_acc 0.97385, 96.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.86 secs\n",
      "\u001b[1m---- Epoch 85/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.12386, nli_loss 0.12386, nli_acc 0.95485, 87.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73125, 0.89 secs\n",
      "\u001b[1m---- Epoch 86/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.13300, nli_loss 0.13300, nli_acc 0.95070, 75.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.88 secs\n",
      "\u001b[1m---- Epoch 87/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.10072, nli_loss 0.10072, nli_acc 0.96480, 95.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.87 secs\n",
      "\u001b[1m---- Epoch 88/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.10164, nli_loss 0.10164, nli_acc 0.96430, 94.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.86 secs\n",
      "\u001b[1m---- Epoch 89/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09718, nli_loss 0.09718, nli_acc 0.96555, 93.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.86 secs\n",
      "\u001b[1m---- Epoch 90/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06114, nli_loss 0.06114, nli_acc 0.98130, 92.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.88 secs\n",
      "\u001b[1m---- Epoch 91/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06411, nli_loss 0.06411, nli_acc 0.98020, 93.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.86 secs\n",
      "\u001b[1m---- Epoch 92/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06833, nli_loss 0.06833, nli_acc 0.97970, 91.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.87 secs\n",
      "\u001b[1m---- Epoch 93/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.12518, nli_loss 0.12518, nli_acc 0.95510, 93.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.86 secs\n",
      "\u001b[1m---- Epoch 94/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.13544, nli_loss 0.13544, nli_acc 0.95085, 91.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.87 secs\n",
      "\u001b[1m---- Epoch 95/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.09121, nli_loss 0.09121, nli_acc 0.96810, 93.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 96/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.08907, nli_loss 0.08907, nli_acc 0.96860, 93.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.89 secs\n",
      "\u001b[1m---- Epoch 97/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09071, nli_loss 0.09071, nli_acc 0.96815, 84.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.88 secs\n",
      "\u001b[1m---- Epoch 98/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.05720, nli_loss 0.05720, nli_acc 0.98290, 95.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.89 secs\n",
      "\u001b[1m---- Epoch 99/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06513, nli_loss 0.06513, nli_acc 0.97910, 94.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.87 secs\n",
      "\u001b[1m---- Epoch 100/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05945, nli_loss 0.05945, nli_acc 0.98240, 82.77 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.87 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(96629,13236055).jsonl\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 800 \\\n",
    "--batch_size 25 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--nli_hidden_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da32fbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 800\n",
      "   batch_size: 40\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/\n",
      "   freeze_huggingface_model: False\n",
      "   merged_input: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of BertBasedNLI ...\u001b[0m\n",
      "BertBasedNLI\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  merged_input: False\n",
      "  hidden_size: 128\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 162036\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Moderate right pleural effusion has slightly decreased since .\n",
      "Hypothesis: Moderate right pleural effusion may be residual from , or new.\n",
      "Label: contradiction\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: The chest is hyperinflated.\n",
      "Hypothesis: The lungs are clear.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Number of train entailment samples: 26442\n",
      "Number of train neutral samples: 39817\n",
      "Number of train contradiction samples: 95777\n",
      "\u001b[1mBuilding test NLI dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI+GPT-4)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_230411_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_230411_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_77_nli_acc=0.7951.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/checkpoint_77_nli_acc=0.7951.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_230411_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.15181, nli_loss 0.15181, nli_acc 0.94337, 93.73 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.7918.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.12822, nli_loss 0.12822, nli_acc 0.95300, 87.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.60 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.11074, nli_loss 0.11074, nli_acc 0.96097, 93.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nli_acc=0.7955.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.13242, nli_loss 0.13242, nli_acc 0.95091, 95.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.60 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.12146, nli_loss 0.12146, nli_acc 0.95572, 95.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.59 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.10420, nli_loss 0.10420, nli_acc 0.96441, 94.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.60 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.08950, nli_loss 0.08950, nli_acc 0.96894, 93.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.60 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07763, nli_loss 0.07763, nli_acc 0.97513, 88.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.59 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.07173, nli_loss 0.07173, nli_acc 0.97775, 94.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.60 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.07684, nli_loss 0.07684, nli_acc 0.97469, 97.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.59 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07124, nli_loss 0.07124, nli_acc 0.97859, 93.96 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.59 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06716, nli_loss 0.06716, nli_acc 0.97906, 93.48 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.59 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.10983, nli_loss 0.10983, nli_acc 0.96062, 92.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.59 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.11232, nli_loss 0.11232, nli_acc 0.95950, 93.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.58 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.09592, nli_loss 0.09592, nli_acc 0.96741, 95.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.60 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.08310, nli_loss 0.08310, nli_acc 0.97203, 90.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.59 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.06907, nli_loss 0.06907, nli_acc 0.97844, 91.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.61 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.07054, nli_loss 0.07054, nli_acc 0.97778, 97.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.59 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06585, nli_loss 0.06585, nli_acc 0.98016, 88.95 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.60 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06691, nli_loss 0.06691, nli_acc 0.97913, 94.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.60 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.10877, nli_loss 0.10877, nli_acc 0.96044, 92.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.59 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.10551, nli_loss 0.10551, nli_acc 0.96169, 91.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.61 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.08642, nli_loss 0.08642, nli_acc 0.97091, 88.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.59 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.06921, nli_loss 0.06921, nli_acc 0.97750, 82.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.59 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.06565, nli_loss 0.06565, nli_acc 0.97875, 87.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.60 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06338, nli_loss 0.06338, nli_acc 0.97922, 142.15 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 1.30 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07445, nli_loss 0.07445, nli_acc 0.97550, 109.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.61 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05855, nli_loss 0.05855, nli_acc 0.98228, 92.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.62 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.10991, nli_loss 0.10991, nli_acc 0.96113, 95.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.58 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.10358, nli_loss 0.10358, nli_acc 0.96234, 92.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.60 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "^C iteration 24175\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 356, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 275, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 159, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 67, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 54, in step_fn\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 800 \\\n",
    "--batch_size 40 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--nli_hidden_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d98e133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 400\n",
      "   batch_size: 25\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_104023_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/\n",
      "   freeze_huggingface_model: False\n",
      "   merged_input: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,9e-5,10,2e-6,9e-5,10,2e-6\n",
      "   iters_to_accumulate: 8\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   integrated_sentence_facts_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of BertBasedNLI ...\u001b[0m\n",
      "BertBasedNLI\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  merged_input: False\n",
      "  hidden_size: 128\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_104023_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,9e-5,10,2e-6,9e-5,10,2e-6\n",
      "1e-06 3 9e-05 10 2e-06 9e-05 10 2e-06\n",
      "self.steps_to_restart = 10\n",
      "self.steps = -1\n",
      "self.initial_lr = 9e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 8, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 162036\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: No pleural effusion is present.\n",
      "Hypothesis: There is a small pleural effusion in the right lung.\n",
      "Label: contradiction\n",
      "Source: 0\n",
      "Loading integrated sentence facts from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl...\n",
      "Number of integrated sentence facts: 677694\n",
      "Number of train entailment samples added: 1323679\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: No focal consolidations are noted.\n",
      "Hypothesis: There is no significant change.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Label: entailment | Source: gpt-4-0613 -> 21667 (2987.98)\n",
      "Label: entailment | Source: mednli_train -> 3744 (1672.60)\n",
      "Label: entailment | Source: mednli_dev -> 465 (695.76)\n",
      "Label: entailment | Source: mednli_test -> 473 (701.58)\n",
      "Label: entailment | Source: radnli_dev -> 93 (279.62)\n",
      "Label: entailment | Source: integrated_sentence_facts -> 1323679 (8410.16)\n",
      "Label: neutral | Source: gpt-4-0613 -> 34854 (3435.46)\n",
      "Label: neutral | Source: mednli_train -> 3743 (1672.44)\n",
      "Label: neutral | Source: mednli_dev -> 465 (695.76)\n",
      "Label: neutral | Source: mednli_test -> 474 (702.30)\n",
      "Label: neutral | Source: radnli_dev -> 281 (538.25)\n",
      "Label: contradiction | Source: gpt-4-0613 -> 90988 (4470.43)\n",
      "Label: contradiction | Source: mednli_train -> 3744 (1672.60)\n",
      "Label: contradiction | Source: mednli_dev -> 465 (695.76)\n",
      "Label: contradiction | Source: mednli_test -> 474 (702.30)\n",
      "Label: contradiction | Source: radnli_dev -> 106 (304.54)\n",
      "Number of label datasets: 3\n",
      "Label weights: [1, 2, 1]\n",
      "\u001b[1mBuilding test NLI dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI+GPT-4)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_105607_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_105607_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_11_nli_acc=0.7900.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_104023_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/checkpoint_11_nli_acc=0.7900.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_105607_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.16054, nli_loss 0.16054, nli_acc 0.94050, 50.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.7747.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.14747, nli_loss 0.14747, nli_acc 0.94710, 44.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.93 secs\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 0.14910, nli_loss 0.14910, nli_acc 0.94670, 46.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nli_acc=0.7828.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.16370, nli_loss 0.16370, nli_acc 0.94080, 48.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.98 secs\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000062) ...\n",
      "loss 0.17352, nli_loss 0.17352, nli_acc 0.93590, 44.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.98 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nli_acc=0.7855.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000042) ...\n",
      "loss 0.16065, nli_loss 0.16065, nli_acc 0.93980, 47.24 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.97 secs\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000029) ...\n",
      "loss 0.15494, nli_loss 0.15494, nli_acc 0.94160, 48.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 1.03 secs\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 0.14911, nli_loss 0.14911, nli_acc 0.94590, 47.89 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.95 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_nli_acc=0.7958.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.14571, nli_loss 0.14571, nli_acc 0.94710, 46.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.93 secs\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.13564, nli_loss 0.13564, nli_acc 0.94990, 45.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.97 secs\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.14000, nli_loss 0.14000, nli_acc 0.94820, 47.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.94 secs\n",
      "\u001b[1m---- Epoch 12/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.13524, nli_loss 0.13524, nli_acc 0.95090, 47.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.96 secs\n",
      "\u001b[1m---- Epoch 13/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.13823, nli_loss 0.13823, nli_acc 0.95020, 47.02 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.87 secs\n",
      "\u001b[1m---- Epoch 14/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.13184, nli_loss 0.13184, nli_acc 0.95260, 49.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.99 secs\n",
      "\u001b[1m---- Epoch 15/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.15629, nli_loss 0.15629, nli_acc 0.94080, 48.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.96 secs\n",
      "\u001b[1m---- Epoch 16/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.15260, nli_loss 0.15260, nli_acc 0.94350, 47.82 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 0.99 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_nli_acc=0.7994.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.14847, nli_loss 0.14847, nli_acc 0.94710, 49.02 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 1.06 secs\n",
      "\u001b[1m---- Epoch 18/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.13061, nli_loss 0.13061, nli_acc 0.95220, 48.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 1.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_nli_acc=0.8040.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.12821, nli_loss 0.12821, nli_acc 0.95360, 49.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 1.01 secs\n",
      "\u001b[1m---- Epoch 20/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.12404, nli_loss 0.12404, nli_acc 0.95460, 49.85 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 1.08 secs\n",
      "\u001b[1m---- Epoch 21/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.13736, nli_loss 0.13736, nli_acc 0.95070, 48.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.98 secs\n",
      "\u001b[1m---- Epoch 22/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.12032, nli_loss 0.12032, nli_acc 0.95540, 45.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.07 secs\n",
      "\u001b[1m---- Epoch 23/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.13039, nli_loss 0.13039, nli_acc 0.95190, 50.89 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.98 secs\n",
      "\u001b[1m---- Epoch 24/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.12019, nli_loss 0.12019, nli_acc 0.95590, 47.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.98 secs\n",
      "\u001b[1m---- Epoch 25/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.15139, nli_loss 0.15139, nli_acc 0.94390, 47.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.99 secs\n",
      "\u001b[1m---- Epoch 26/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.15406, nli_loss 0.15406, nli_acc 0.94270, 49.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 0.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_nli_acc=0.8068.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.15604, nli_loss 0.15604, nli_acc 0.94070, 49.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 1.03 secs\n",
      "\u001b[1m---- Epoch 28/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.14025, nli_loss 0.14025, nli_acc 0.94790, 47.69 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 0.96 secs\n",
      "\u001b[1m---- Epoch 29/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.12462, nli_loss 0.12462, nli_acc 0.95300, 50.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 1.00 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_29_nli_acc=0.8078.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 30/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.11440, nli_loss 0.11440, nli_acc 0.96120, 46.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78542, 0.94 secs\n",
      "\u001b[1m---- Epoch 31/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.12055, nli_loss 0.12055, nli_acc 0.95510, 48.92 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79792, 1.12 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_nli_acc=0.8136.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.11766, nli_loss 0.11766, nli_acc 0.95700, 48.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 1.01 secs\n",
      "\u001b[1m---- Epoch 33/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.12021, nli_loss 0.12021, nli_acc 0.95520, 49.06 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 1.00 secs\n",
      "\u001b[1m---- Epoch 34/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11610, nli_loss 0.11610, nli_acc 0.95700, 49.22 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 1.06 secs\n",
      "\u001b[1m---- Epoch 35/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.13453, nli_loss 0.13453, nli_acc 0.94640, 48.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79583, 0.97 secs\n",
      "\u001b[1m---- Epoch 36/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.15965, nli_loss 0.15965, nli_acc 0.93990, 47.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 1.02 secs\n",
      "\u001b[1m---- Epoch 37/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.14146, nli_loss 0.14146, nli_acc 0.94690, 50.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 1.05 secs\n",
      "\u001b[1m---- Epoch 38/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.12789, nli_loss 0.12789, nli_acc 0.95330, 49.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 0.99 secs\n",
      "\u001b[1m---- Epoch 39/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.12565, nli_loss 0.12565, nli_acc 0.95290, 49.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 1.00 secs\n",
      "\u001b[1m---- Epoch 40/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.12014, nli_loss 0.12014, nli_acc 0.95670, 45.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79792, 0.99 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_nli_acc=0.8138.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11774, nli_loss 0.11774, nli_acc 0.95730, 50.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 1.05 secs\n",
      "\u001b[1m---- Epoch 42/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.11874, nli_loss 0.11874, nli_acc 0.95690, 45.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 0.94 secs\n",
      "\u001b[1m---- Epoch 43/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.11300, nli_loss 0.11300, nli_acc 0.95850, 48.49 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 0.90 secs\n",
      "\u001b[1m---- Epoch 44/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11338, nli_loss 0.11338, nli_acc 0.95680, 48.21 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 0.97 secs\n",
      "\u001b[1m---- Epoch 45/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.12985, nli_loss 0.12985, nli_acc 0.95130, 47.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 1.05 secs\n",
      "\u001b[1m---- Epoch 46/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.13341, nli_loss 0.13341, nli_acc 0.95170, 50.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 1.01 secs\n",
      "\u001b[1m---- Epoch 47/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.12405, nli_loss 0.12405, nli_acc 0.95410, 49.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 1.00 secs\n",
      "\u001b[1m---- Epoch 48/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.11475, nli_loss 0.11475, nli_acc 0.95880, 51.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 0.98 secs\n",
      "\u001b[1m---- Epoch 49/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.11474, nli_loss 0.11474, nli_acc 0.95760, 48.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 1.05 secs\n",
      "\u001b[1m---- Epoch 50/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.11017, nli_loss 0.11017, nli_acc 0.96180, 49.05 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 1.00 secs\n",
      "\u001b[1m---- Epoch 51/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11746, nli_loss 0.11746, nli_acc 0.95850, 48.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 1.00 secs\n",
      "\u001b[1m---- Epoch 52/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.11000, nli_loss 0.11000, nli_acc 0.96240, 49.16 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nli_acc 0.79167, 1.04 secs\n",
      "\u001b[1m---- Epoch 53/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.11417, nli_loss 0.11417, nli_acc 0.95990, 50.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 1.00 secs\n",
      "\u001b[1m---- Epoch 54/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11146, nli_loss 0.11146, nli_acc 0.95910, 48.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 0.99 secs\n",
      "\u001b[1m---- Epoch 55/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.12649, nli_loss 0.12649, nli_acc 0.95250, 50.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.93 secs\n",
      "\u001b[1m---- Epoch 56/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.13382, nli_loss 0.13382, nli_acc 0.95100, 49.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 0.98 secs\n",
      "\u001b[1m---- Epoch 57/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.12486, nli_loss 0.12486, nli_acc 0.95230, 48.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 1.03 secs\n",
      "\u001b[1m---- Epoch 58/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.11635, nli_loss 0.11635, nli_acc 0.95810, 47.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78542, 1.02 secs\n",
      "\u001b[1m---- Epoch 59/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.09868, nli_loss 0.09868, nli_acc 0.96500, 48.85 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79792, 0.97 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_59_nli_acc=0.8146.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 60/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.10461, nli_loss 0.10461, nli_acc 0.96530, 47.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 0.99 secs\n",
      "\u001b[1m---- Epoch 61/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.10525, nli_loss 0.10525, nli_acc 0.96260, 49.09 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79583, 0.96 secs\n",
      "\u001b[1m---- Epoch 62/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09872, nli_loss 0.09872, nli_acc 0.96380, 48.08 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 1.03 secs\n",
      "\u001b[1m---- Epoch 63/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.10211, nli_loss 0.10211, nli_acc 0.96240, 49.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79583, 1.00 secs\n",
      "\u001b[1m---- Epoch 64/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.10129, nli_loss 0.10129, nli_acc 0.96450, 48.70 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79167, 0.98 secs\n",
      "\u001b[1m---- Epoch 65/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.12854, nli_loss 0.12854, nli_acc 0.95340, 49.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.91 secs\n",
      "\u001b[1m---- Epoch 66/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.13140, nli_loss 0.13140, nli_acc 0.95300, 50.74 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 1.02 secs\n",
      "\u001b[1m---- Epoch 67/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.11873, nli_loss 0.11873, nli_acc 0.95650, 47.88 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 1.06 secs\n",
      "\u001b[1m---- Epoch 68/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.10714, nli_loss 0.10714, nli_acc 0.96240, 50.60 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.01 secs\n",
      "\u001b[1m---- Epoch 69/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.09701, nli_loss 0.09701, nli_acc 0.96570, 50.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 0.96 secs\n",
      "\u001b[1m---- Epoch 70/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.09979, nli_loss 0.09979, nli_acc 0.96630, 49.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 0.92 secs\n",
      "\u001b[1m---- Epoch 71/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.10249, nli_loss 0.10249, nli_acc 0.96390, 48.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 0.99 secs\n",
      "\u001b[1m---- Epoch 72/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09838, nli_loss 0.09838, nli_acc 0.96520, 49.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79792, 1.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_72_nli_acc=0.8146.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 73/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.10050, nli_loss 0.10050, nli_acc 0.96300, 51.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79583, 0.91 secs\n",
      "\u001b[1m---- Epoch 74/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09327, nli_loss 0.09327, nli_acc 0.96910, 49.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 1.12 secs\n",
      "\u001b[1m---- Epoch 75/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.11485, nli_loss 0.11485, nli_acc 0.95900, 49.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 0.97 secs\n",
      "\u001b[1m---- Epoch 76/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.12694, nli_loss 0.12694, nli_acc 0.95280, 48.02 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 1.01 secs\n",
      "\u001b[1m---- Epoch 77/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.12479, nli_loss 0.12479, nli_acc 0.95290, 50.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 0.95 secs\n",
      "\u001b[1m---- Epoch 78/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.09897, nli_loss 0.09897, nli_acc 0.96350, 50.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.93 secs\n",
      "\u001b[1m---- Epoch 79/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.09456, nli_loss 0.09456, nli_acc 0.96550, 48.21 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.96 secs\n",
      "\u001b[1m---- Epoch 80/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.09612, nli_loss 0.09612, nli_acc 0.96470, 50.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.97 secs\n",
      "\u001b[1m---- Epoch 81/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09854, nli_loss 0.09854, nli_acc 0.96560, 48.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 1.06 secs\n",
      "\u001b[1m---- Epoch 82/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09863, nli_loss 0.09863, nli_acc 0.96310, 49.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.04 secs\n",
      "\u001b[1m---- Epoch 83/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.09525, nli_loss 0.09525, nli_acc 0.96480, 49.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.01 secs\n",
      "\u001b[1m---- Epoch 84/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09846, nli_loss 0.09846, nli_acc 0.96320, 49.43 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 1.00 secs\n",
      "\u001b[1m---- Epoch 85/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.11987, nli_loss 0.11987, nli_acc 0.95580, 48.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 1.12 secs\n",
      "\u001b[1m---- Epoch 86/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.12047, nli_loss 0.12047, nli_acc 0.95490, 50.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.99 secs\n",
      "\u001b[1m---- Epoch 87/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.10865, nli_loss 0.10865, nli_acc 0.96090, 46.63 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.93 secs\n",
      "\u001b[1m---- Epoch 88/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.09703, nli_loss 0.09703, nli_acc 0.96490, 48.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 1.02 secs\n",
      "\u001b[1m---- Epoch 89/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.08693, nli_loss 0.08693, nli_acc 0.96950, 50.52 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.93 secs\n",
      "\u001b[1m---- Epoch 90/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.08695, nli_loss 0.08695, nli_acc 0.97050, 48.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.95 secs\n",
      "\u001b[1m---- Epoch 91/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09627, nli_loss 0.09627, nli_acc 0.96600, 49.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 1.01 secs\n",
      "\u001b[1m---- Epoch 92/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09227, nli_loss 0.09227, nli_acc 0.96800, 48.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 1.12 secs\n",
      "\u001b[1m---- Epoch 93/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.09785, nli_loss 0.09785, nli_acc 0.96450, 50.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 1.00 secs\n",
      "\u001b[1m---- Epoch 94/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09823, nli_loss 0.09823, nli_acc 0.96460, 50.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 1.03 secs\n",
      "\u001b[1m---- Epoch 95/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.11150, nli_loss 0.11150, nli_acc 0.95970, 47.89 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.14 secs\n",
      "\u001b[1m---- Epoch 96/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.12639, nli_loss 0.12639, nli_acc 0.95580, 50.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 1.08 secs\n",
      "\u001b[1m---- Epoch 97/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.12318, nli_loss 0.12318, nli_acc 0.95460, 49.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 0.94 secs\n",
      "\u001b[1m---- Epoch 98/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.09727, nli_loss 0.09727, nli_acc 0.96430, 50.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.96 secs\n",
      "\u001b[1m---- Epoch 99/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.09531, nli_loss 0.09531, nli_acc 0.96450, 49.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.95 secs\n",
      "\u001b[1m---- Epoch 100/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.09705, nli_loss 0.09705, nli_acc 0.96650, 50.88 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78542, 0.95 secs\n",
      "\u001b[1m---- Epoch 101/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09370, nli_loss 0.09370, nli_acc 0.96710, 48.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.96 secs\n",
      "\u001b[1m---- Epoch 102/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09133, nli_loss 0.09133, nli_acc 0.96710, 49.06 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.98 secs\n",
      "\u001b[1m---- Epoch 103/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.09188, nli_loss 0.09188, nli_acc 0.96500, 49.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.96 secs\n",
      "\u001b[1m---- Epoch 104/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09473, nli_loss 0.09473, nli_acc 0.96660, 48.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 1.14 secs\n",
      "\u001b[1m---- Epoch 105/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.10939, nli_loss 0.10939, nli_acc 0.95900, 50.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 1.06 secs\n",
      "\u001b[1m---- Epoch 106/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.13217, nli_loss 0.13217, nli_acc 0.95240, 49.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.96 secs\n",
      "\u001b[1m---- Epoch 107/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.11948, nli_loss 0.11948, nli_acc 0.95690, 49.60 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 1.02 secs\n",
      "\u001b[1m---- Epoch 108/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.09888, nli_loss 0.09888, nli_acc 0.96600, 50.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.98 secs\n",
      "\u001b[1m---- Epoch 109/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.08208, nli_loss 0.08208, nli_acc 0.97180, 49.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.98 secs\n",
      "\u001b[1m---- Epoch 110/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.09342, nli_loss 0.09342, nli_acc 0.96450, 49.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 1.03 secs\n",
      "\u001b[1m---- Epoch 111/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.08706, nli_loss 0.08706, nli_acc 0.96880, 49.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 1.06 secs\n",
      "\u001b[1m---- Epoch 112/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09061, nli_loss 0.09061, nli_acc 0.96850, 47.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 1.00 secs\n",
      "\u001b[1m---- Epoch 113/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.09461, nli_loss 0.09461, nli_acc 0.96390, 50.36 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.98 secs\n",
      "\u001b[1m---- Epoch 114/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.08812, nli_loss 0.08812, nli_acc 0.96930, 48.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 1.01 secs\n",
      "\u001b[1m---- Epoch 115/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.11917, nli_loss 0.11917, nli_acc 0.95530, 49.15 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 1.03 secs\n",
      "\u001b[1m---- Epoch 116/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.11941, nli_loss 0.11941, nli_acc 0.95610, 50.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78958, 0.97 secs\n",
      "\u001b[1m---- Epoch 117/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.10727, nli_loss 0.10727, nli_acc 0.95980, 49.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.98 secs\n",
      "\u001b[1m---- Epoch 118/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.09038, nli_loss 0.09038, nli_acc 0.96660, 50.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 1.00 secs\n",
      "\u001b[1m---- Epoch 119/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.08467, nli_loss 0.08467, nli_acc 0.96870, 49.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.99 secs\n",
      "\u001b[1m---- Epoch 120/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.08642, nli_loss 0.08642, nli_acc 0.96960, 48.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.95 secs\n",
      "\u001b[1m---- Epoch 121/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.08647, nli_loss 0.08647, nli_acc 0.96860, 48.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 1.03 secs\n",
      "\u001b[1m---- Epoch 122/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.08528, nli_loss 0.08528, nli_acc 0.97080, 50.60 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 1.00 secs\n",
      "\u001b[1m---- Epoch 123/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.08717, nli_loss 0.08717, nli_acc 0.96950, 47.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 1.00 secs\n",
      "\u001b[1m---- Epoch 124/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.08577, nli_loss 0.08577, nli_acc 0.96810, 51.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.98 secs\n",
      "\u001b[1m---- Epoch 125/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.11054, nli_loss 0.11054, nli_acc 0.96030, 51.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 1.03 secs\n",
      "\u001b[1m---- Epoch 126/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.11910, nli_loss 0.11910, nli_acc 0.95560, 49.29 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 1.00 secs\n",
      "\u001b[1m---- Epoch 127/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.11411, nli_loss 0.11411, nli_acc 0.95610, 49.82 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 1.00 secs\n",
      "\u001b[1m---- Epoch 128/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.09305, nli_loss 0.09305, nli_acc 0.96510, 43.83 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.96 secs\n",
      "\u001b[1m---- Epoch 129/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.08579, nli_loss 0.08579, nli_acc 0.96900, 49.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.96 secs\n",
      "\u001b[1m---- Epoch 130/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.07918, nli_loss 0.07918, nli_acc 0.97540, 47.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.97 secs\n",
      "\u001b[1m---- Epoch 131/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.07406, nli_loss 0.07406, nli_acc 0.97450, 47.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.04 secs\n",
      "\u001b[1m---- Epoch 132/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.07414, nli_loss 0.07414, nli_acc 0.97590, 50.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.93 secs\n",
      "\u001b[1m---- Epoch 133/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.07660, nli_loss 0.07660, nli_acc 0.97400, 47.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.06 secs\n",
      "\u001b[1m---- Epoch 134/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07927, nli_loss 0.07927, nli_acc 0.97150, 49.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 1.01 secs\n",
      "\u001b[1m---- Epoch 135/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.10099, nli_loss 0.10099, nli_acc 0.96140, 48.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 1.00 secs\n",
      "\u001b[1m---- Epoch 136/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.11395, nli_loss 0.11395, nli_acc 0.95740, 50.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 1.01 secs\n",
      "\u001b[1m---- Epoch 137/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.10256, nli_loss 0.10256, nli_acc 0.96170, 48.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79583, 1.01 secs\n",
      "\u001b[1m---- Epoch 138/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.08789, nli_loss 0.08789, nli_acc 0.96760, 51.06 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 0.95 secs\n",
      "\u001b[1m---- Epoch 139/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.08324, nli_loss 0.08324, nli_acc 0.96850, 49.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78542, 0.99 secs\n",
      "\u001b[1m---- Epoch 140/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.08503, nli_loss 0.08503, nli_acc 0.96970, 49.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78750, 0.99 secs\n",
      "\u001b[1m---- Epoch 141/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.07822, nli_loss 0.07822, nli_acc 0.97210, 48.21 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.97 secs\n",
      "\u001b[1m---- Epoch 142/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.08091, nli_loss 0.08091, nli_acc 0.97130, 48.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.02 secs\n",
      "\u001b[1m---- Epoch 143/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.07991, nli_loss 0.07991, nli_acc 0.96990, 50.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.00 secs\n",
      "\u001b[1m---- Epoch 144/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.08214, nli_loss 0.08214, nli_acc 0.97120, 50.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.08 secs\n",
      "\u001b[1m---- Epoch 145/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000090) ...\n",
      "loss 0.09151, nli_loss 0.09151, nli_acc 0.96770, 49.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.94 secs\n",
      "\u001b[1m---- Epoch 146/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000059) ...\n",
      "loss 0.11075, nli_loss 0.11075, nli_acc 0.95980, 49.15 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 1.08 secs\n",
      "\u001b[1m---- Epoch 147/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000039) ...\n",
      "loss 0.10164, nli_loss 0.10164, nli_acc 0.96390, 50.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 1.05 secs\n",
      "\u001b[1m---- Epoch 148/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.08480, nli_loss 0.08480, nli_acc 0.96930, 48.73 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.98 secs\n",
      "\u001b[1m---- Epoch 149/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.07564, nli_loss 0.07564, nli_acc 0.97410, 44.96 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.97 secs\n",
      "\u001b[1m---- Epoch 150/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.07556, nli_loss 0.07556, nli_acc 0.97470, 46.20 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.99 secs\n",
      "\u001b[1m---- Epoch 151/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "^C iteration 60075\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 359, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 278, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 160, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 129, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 67, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 47, in step_fn\n",
      "    logits = model(tokenized_premises, tokenized_hypotheses)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 70, in forward\n",
      "    return self._forward_without_merged_input(*args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 60, in _forward_without_merged_input\n",
      "    h_vectors = self._text_to_embedding_func(tokenized_hypotheses)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 38, in <lambda>\n",
      "    self._text_to_embedding_func = lambda x: self.model.get_projected_text_embeddings(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1358, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 993, in forward\n",
      "    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_104023_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\" \\\n",
    "--integrated_sentence_facts_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\" \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 400 \\\n",
    "--batch_size 25 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 8 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,9e-5,10,2e-6,9e-5,10,2e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--nli_hidden_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
