{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25c052a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batch_size: 40\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 512\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: True\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  freeze_huggingface_model: True\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Freezing huggingface model\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 14049\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Here she was noted to follow simple comands but was combative prompting sedation and intubation.\n",
      "Hypothesis:  She was alert and oriented x 3. \n",
      "Label: contradiction\n",
      "Number of val samples: 480\n",
      "\u001b[1mExample val text:\u001b[0m\n",
      "Premise: No acute focal pneumonia or pleural effusion.\n",
      "Hypothesis: Again noted are diffuse infiltrative parenchymal opacities, right worse than left; this is largely due to pulmonary edema and the right-sided pleural effusion, but underlying pneumonia cannot be excluded.\n",
      "Label: contradiction\n",
      "\u001b[1mIncluding half of the RadNLI dev samples in the train set...\u001b[0m\n",
      "Number of train samples: 14288\n",
      "Number of val samples: 241\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: Improved aeration at the lung bases with residual subsegmental atelectasis at the left lung base.\n",
      "Hypothesis: There are likely patchy opacities in the lung bases reflective of atelectasis.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding val triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test triplet ranking dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_81_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9542.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_81_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9542.pt\n",
      "\u001b[93mWarning: model state dict has 215 keys, loaded state dict has 223 keys, intersection has 211 keys, union has 227 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in model but not in loaded state dict:\u001b[0m\n",
      "\u001b[93m  nli_classifier.weight\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  nli_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.09662, nli_loss 1.09662, nli_acc 0.39040, 20.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41494, 0.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.4125.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.09193, nli_loss 1.09193, nli_acc 0.46046, 21.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.40664, 0.57 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 1.07166, nli_loss 1.07166, nli_acc 0.51190, 20.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34025, 0.61 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.99213, nli_loss 0.99213, nli_acc 0.54129, 20.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.31535, 0.56 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.93346, nli_loss 0.93346, nli_acc 0.55571, 20.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.31535, 0.56 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.92253, nli_loss 0.92253, nli_acc 0.56019, 20.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.32780, 0.56 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.91924, nli_loss 0.91924, nli_acc 0.56033, 20.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34855, 0.55 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.91784, nli_loss 0.91784, nli_acc 0.56033, 20.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34855, 0.52 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.91559, nli_loss 0.91559, nli_acc 0.56096, 20.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.61 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.91767, nli_loss 0.91767, nli_acc 0.55725, 19.69 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.57 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.91472, nli_loss 0.91472, nli_acc 0.55802, 20.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.53 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.91299, nli_loss 0.91299, nli_acc 0.56215, 20.22 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.58 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.90900, nli_loss 0.90900, nli_acc 0.56110, 20.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.39834, 0.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc=0.4146.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.90211, nli_loss 0.90211, nli_acc 0.56698, 20.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.40249, 0.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc=0.4189.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.89535, nli_loss 0.89535, nli_acc 0.57335, 20.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.42324, 0.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_nli_acc=0.4382.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.89399, nli_loss 0.89399, nli_acc 0.57370, 19.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.57 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.89253, nli_loss 0.89253, nli_acc 0.57328, 20.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.61 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.89141, nli_loss 0.89141, nli_acc 0.57524, 20.63 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.61 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.89427, nli_loss 0.89427, nli_acc 0.57517, 20.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.57 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.89197, nli_loss 0.89197, nli_acc 0.57440, 20.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.59 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.88483, nli_loss 0.88483, nli_acc 0.57727, 20.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.61 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.87790, nli_loss 0.87790, nli_acc 0.57993, 20.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.59 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.87681, nli_loss 0.87681, nli_acc 0.57874, 20.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.59 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.87343, nli_loss 0.87343, nli_acc 0.58182, 20.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.62 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.87219, nli_loss 0.87219, nli_acc 0.58455, 20.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.56 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.87034, nli_loss 0.87034, nli_acc 0.58490, 20.24 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.54 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.87080, nli_loss 0.87080, nli_acc 0.58532, 20.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.57 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.87304, nli_loss 0.87304, nli_acc 0.58042, 20.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.64 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "^C iteration 10025\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 343, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 263, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 155, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 60, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 39, in step_fn\n",
      "    logits = model(premises_encoding, hypotheses_encoding)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 46, in forward\n",
      "    h_vectors = self._text_to_embedding_func(hypotheses_encoding)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 30, in <lambda>\n",
      "    self._text_to_embedding_func = lambda x: self.model.get_projected_text_embeddings(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1358, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1020, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 610, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 495, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 425, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 323, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 100 \\\n",
    "--batch_size 40 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--freeze_huggingface_model \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbc9c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 50\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 512\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 14049\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Following her cardiac catheterization, the patient continued to be hypoxic and hypotensive requiring Dopamine and five liters nasal cannula oxygen.\n",
      "Hypothesis:  The patient had a heart attack\n",
      "Label: neutral\n",
      "Number of val samples: 480\n",
      "\u001b[1mExample val text:\u001b[0m\n",
      "Premise: Small area of parenchymal sparing in the left upper lobe is unchanged.\n",
      "Hypothesis: No newly appeared parenchymal opacities.\n",
      "Label: neutral\n",
      "\u001b[1mIncluding half of the RadNLI dev samples in the train set...\u001b[0m\n",
      "Number of train samples: 14431\n",
      "Number of val samples: 98\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: Low lung volumes with little overall change in the appearance of the heart and lungs.\n",
      "Hypothesis: Low lung volumes accentuate the pulmonary vascular structures.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding val triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test triplet ranking dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_13_nli_acc=0.6636.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/checkpoint_13_nli_acc=0.6636.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46760, nli_loss 0.46760, nli_acc 0.80757, 93.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68367, 0.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.6961.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.45626, nli_loss 0.45626, nli_acc 0.81256, 94.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_nli_acc=0.7057.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.43799, nli_loss 0.43799, nli_acc 0.82046, 93.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71429, 0.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nli_acc=0.7249.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.46279, nli_loss 0.46279, nli_acc 0.80556, 92.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.51 secs\n",
      "\u001b[1m---- Epoch 5/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.39927, nli_loss 0.39927, nli_acc 0.83549, 91.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72449, 0.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nli_acc=0.7356.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.34970, nli_loss 0.34970, nli_acc 0.85961, 93.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73469, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_nli_acc=0.7472.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.31957, nli_loss 0.31957, nli_acc 0.87208, 93.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71429, 0.44 secs\n",
      "\u001b[1m---- Epoch 8/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.30354, nli_loss 0.30354, nli_acc 0.87603, 90.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72449, 0.43 secs\n",
      "\u001b[1m---- Epoch 9/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.29936, nli_loss 0.29936, nli_acc 0.88053, 89.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.44 secs\n",
      "\u001b[1m---- Epoch 10/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29454, nli_loss 0.29454, nli_acc 0.88137, 91.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.45 secs\n",
      "\u001b[1m---- Epoch 11/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28951, nli_loss 0.28951, nli_acc 0.88719, 93.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.43 secs\n",
      "\u001b[1m---- Epoch 12/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.28311, nli_loss 0.28311, nli_acc 0.88837, 91.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.45 secs\n",
      "\u001b[1m---- Epoch 13/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.35399, nli_loss 0.35399, nli_acc 0.85226, 95.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.46 secs\n",
      "\u001b[1m---- Epoch 14/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.30348, nli_loss 0.30348, nli_acc 0.87659, 94.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74490, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc=0.7581.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.25795, nli_loss 0.25795, nli_acc 0.89848, 95.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68367, 0.41 secs\n",
      "\u001b[1m---- Epoch 16/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.23823, nli_loss 0.23823, nli_acc 0.90708, 93.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.42 secs\n",
      "\u001b[1m---- Epoch 17/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.22737, nli_loss 0.22737, nli_acc 0.91165, 94.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.48 secs\n",
      "\u001b[1m---- Epoch 18/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.22223, nli_loss 0.22223, nli_acc 0.91352, 95.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.41 secs\n",
      "\u001b[1m---- Epoch 19/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.21676, nli_loss 0.21676, nli_acc 0.91546, 92.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.60 secs\n",
      "\u001b[1m---- Epoch 20/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.21354, nli_loss 0.21354, nli_acc 0.91775, 90.48 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.44 secs\n",
      "\u001b[1m---- Epoch 21/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.29496, nli_loss 0.29496, nli_acc 0.87887, 95.02 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.47 secs\n",
      "\u001b[1m---- Epoch 22/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.24369, nli_loss 0.24369, nli_acc 0.90229, 94.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.45 secs\n",
      "\u001b[1m---- Epoch 23/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.20478, nli_loss 0.20478, nli_acc 0.91920, 93.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.41 secs\n",
      "\u001b[1m---- Epoch 24/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.18193, nli_loss 0.18193, nli_acc 0.93112, 93.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.46 secs\n",
      "\u001b[1m---- Epoch 25/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.17825, nli_loss 0.17825, nli_acc 0.93140, 95.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.44 secs\n",
      "\u001b[1m---- Epoch 26/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.16974, nli_loss 0.16974, nli_acc 0.93743, 93.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.47 secs\n",
      "\u001b[1m---- Epoch 27/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "^C iteration 19100\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 343, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 263, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 155, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 60, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 47, in step_fn\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/\" \\\n",
    "--epochs 50 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ef52f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 800\n",
      "   batch_size: 25\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   merged_input: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(96629,13236055).jsonl\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  merged_input: False\n",
      "  hidden_size: 128\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 96629\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Right Port-A-Cath in situ.\n",
      "Hypothesis: The patient has a Port-A-Cath inserted on the right side.\n",
      "Label: entailment\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: The heart size remains normal.\n",
      "Hypothesis: The lungs are clear.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test NLI dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI+GPT-4)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_184632_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_184632_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_94_cacc+chf1+chf1+cscc+hscc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9266.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_94_cacc+chf1+chf1+cscc+hscc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9266.pt\n",
      "\u001b[93mWarning: model state dict has 215 keys, loaded state dict has 227 keys, intersection has 215 keys, union has 227 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_184632_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.19581, nli_loss 0.19581, nli_acc 0.92595, 91.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72708, 0.91 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.7470.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.17517, nli_loss 0.17517, nli_acc 0.93545, 90.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.85 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_nli_acc=0.7610.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.16228, nli_loss 0.16228, nli_acc 0.94085, 89.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72500, 0.87 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.22536, nli_loss 0.22536, nli_acc 0.91315, 91.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72083, 0.85 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.21544, nli_loss 0.21544, nli_acc 0.91625, 91.39 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nli_acc=0.7704.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.19736, nli_loss 0.19736, nli_acc 0.92165, 90.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.87 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.17872, nli_loss 0.17872, nli_acc 0.93215, 93.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.86 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.15586, nli_loss 0.15586, nli_acc 0.94275, 91.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73750, 0.87 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.13338, nli_loss 0.13338, nli_acc 0.95460, 90.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.86 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.13292, nli_loss 0.13292, nli_acc 0.95635, 92.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.87 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.14028, nli_loss 0.14028, nli_acc 0.95055, 89.56 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nli_acc 0.73125, 0.86 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.14773, nli_loss 0.14773, nli_acc 0.94580, 89.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73125, 0.85 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.20204, nli_loss 0.20204, nli_acc 0.92265, 92.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc=0.7916.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.19569, nli_loss 0.19569, nli_acc 0.92500, 93.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.86 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.18392, nli_loss 0.18392, nli_acc 0.92945, 90.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73333, 0.87 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.18099, nli_loss 0.18099, nli_acc 0.93095, 81.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.85 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.16704, nli_loss 0.16704, nli_acc 0.93835, 93.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.86 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.11278, nli_loss 0.11278, nli_acc 0.96500, 90.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.88 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.12054, nli_loss 0.12054, nli_acc 0.95980, 91.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.88 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.13595, nli_loss 0.13595, nli_acc 0.95185, 94.92 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.88 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.22298, nli_loss 0.22298, nli_acc 0.91205, 93.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.86 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.20876, nli_loss 0.20876, nli_acc 0.91915, 94.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.88 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.14658, nli_loss 0.14658, nli_acc 0.94880, 93.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.87 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.15836, nli_loss 0.15836, nli_acc 0.94085, 93.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.88 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.15912, nli_loss 0.15912, nli_acc 0.94070, 94.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.90 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.12487, nli_loss 0.12487, nli_acc 0.95625, 94.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.89 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11302, nli_loss 0.11302, nli_acc 0.96410, 93.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.89 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.11015, nli_loss 0.11015, nli_acc 0.96450, 94.29 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.89 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.20652, nli_loss 0.20652, nli_acc 0.92065, 95.53 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.87 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.20671, nli_loss 0.20671, nli_acc 0.92200, 94.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.88 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.17349, nli_loss 0.17349, nli_acc 0.93600, 95.52 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.86 secs\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.14891, nli_loss 0.14891, nli_acc 0.94760, 96.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.88 secs\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.13286, nli_loss 0.13286, nli_acc 0.95385, 91.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.89 secs\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.11164, nli_loss 0.11164, nli_acc 0.96255, 91.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.90 secs\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11370, nli_loss 0.11370, nli_acc 0.96330, 93.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.87 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.12424, nli_loss 0.12424, nli_acc 0.95785, 95.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.88 secs\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.17269, nli_loss 0.17269, nli_acc 0.93475, 94.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.89 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.17121, nli_loss 0.17121, nli_acc 0.93630, 95.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.88 secs\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.15057, nli_loss 0.15057, nli_acc 0.94345, 95.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.87 secs\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.14465, nli_loss 0.14465, nli_acc 0.94875, 95.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.88 secs\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.14240, nli_loss 0.14240, nli_acc 0.94620, 95.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.09192, nli_loss 0.09192, nli_acc 0.97110, 94.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.88 secs\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09943, nli_loss 0.09943, nli_acc 0.96760, 94.77 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.87 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.10781, nli_loss 0.10781, nli_acc 0.96425, 77.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.87 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.17521, nli_loss 0.17521, nli_acc 0.93375, 73.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.18369, nli_loss 0.18369, nli_acc 0.93165, 73.89 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.86 secs\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.12877, nli_loss 0.12877, nli_acc 0.95355, 72.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.86 secs\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.13278, nli_loss 0.13278, nli_acc 0.95170, 74.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 0.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_nli_acc=0.8002.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.13314, nli_loss 0.13314, nli_acc 0.95125, 73.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.86 secs\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.09541, nli_loss 0.09541, nli_acc 0.96840, 81.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.89 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09704, nli_loss 0.09704, nli_acc 0.96875, 90.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.78 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09454, nli_loss 0.09454, nli_acc 0.97005, 92.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.87 secs\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.15485, nli_loss 0.15485, nli_acc 0.94240, 88.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.88 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.16807, nli_loss 0.16807, nli_acc 0.93545, 93.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.87 secs\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.13301, nli_loss 0.13301, nli_acc 0.95040, 85.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.89 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.12189, nli_loss 0.12189, nli_acc 0.95595, 91.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.88 secs\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.11070, nli_loss 0.11070, nli_acc 0.96200, 93.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.88 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.08477, nli_loss 0.08477, nli_acc 0.97420, 92.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.89 secs\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09084, nli_loss 0.09084, nli_acc 0.97140, 93.88 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.97 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09703, nli_loss 0.09703, nli_acc 0.96675, 94.15 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.89 secs\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.13972, nli_loss 0.13972, nli_acc 0.94830, 91.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.87 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.14902, nli_loss 0.14902, nli_acc 0.94360, 94.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.79375, 0.87 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_62_nli_acc=0.8087.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.11912, nli_loss 0.11912, nli_acc 0.95670, 92.70 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.88 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.11618, nli_loss 0.11618, nli_acc 0.95850, 94.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.89 secs\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11504, nli_loss 0.11504, nli_acc 0.95905, 94.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.89 secs\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.07234, nli_loss 0.07234, nli_acc 0.97860, 84.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.88 secs\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07787, nli_loss 0.07787, nli_acc 0.97665, 93.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.87 secs\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.08538, nli_loss 0.08538, nli_acc 0.97290, 94.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.87 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.14739, nli_loss 0.14739, nli_acc 0.94570, 94.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.15743, nli_loss 0.15743, nli_acc 0.94000, 94.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73750, 0.87 secs\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.11004, nli_loss 0.11004, nli_acc 0.96115, 93.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.86 secs\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.10730, nli_loss 0.10730, nli_acc 0.96220, 95.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.76 secs\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11201, nli_loss 0.11201, nli_acc 0.96005, 95.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.87 secs\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.07409, nli_loss 0.07409, nli_acc 0.97625, 95.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.86 secs\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07910, nli_loss 0.07910, nli_acc 0.97415, 89.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.86 secs\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07584, nli_loss 0.07584, nli_acc 0.97565, 93.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.90 secs\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.12201, nli_loss 0.12201, nli_acc 0.95300, 94.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.89 secs\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.14389, nli_loss 0.14389, nli_acc 0.94540, 90.43 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.88 secs\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.11107, nli_loss 0.11107, nli_acc 0.96125, 83.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.88 secs\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.09821, nli_loss 0.09821, nli_acc 0.96610, 89.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.89 secs\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09422, nli_loss 0.09422, nli_acc 0.96760, 95.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.87 secs\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06749, nli_loss 0.06749, nli_acc 0.97905, 84.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.87 secs\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06993, nli_loss 0.06993, nli_acc 0.97865, 92.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.87 secs\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07692, nli_loss 0.07692, nli_acc 0.97385, 96.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.86 secs\n",
      "\u001b[1m---- Epoch 85/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.12386, nli_loss 0.12386, nli_acc 0.95485, 87.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73125, 0.89 secs\n",
      "\u001b[1m---- Epoch 86/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.13300, nli_loss 0.13300, nli_acc 0.95070, 75.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.88 secs\n",
      "\u001b[1m---- Epoch 87/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.10072, nli_loss 0.10072, nli_acc 0.96480, 95.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.87 secs\n",
      "\u001b[1m---- Epoch 88/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.10164, nli_loss 0.10164, nli_acc 0.96430, 94.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.86 secs\n",
      "\u001b[1m---- Epoch 89/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09718, nli_loss 0.09718, nli_acc 0.96555, 93.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.86 secs\n",
      "\u001b[1m---- Epoch 90/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06114, nli_loss 0.06114, nli_acc 0.98130, 92.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.88 secs\n",
      "\u001b[1m---- Epoch 91/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06411, nli_loss 0.06411, nli_acc 0.98020, 93.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.86 secs\n",
      "\u001b[1m---- Epoch 92/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06833, nli_loss 0.06833, nli_acc 0.97970, 91.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.87 secs\n",
      "\u001b[1m---- Epoch 93/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.12518, nli_loss 0.12518, nli_acc 0.95510, 93.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.86 secs\n",
      "\u001b[1m---- Epoch 94/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.13544, nli_loss 0.13544, nli_acc 0.95085, 91.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.87 secs\n",
      "\u001b[1m---- Epoch 95/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.09121, nli_loss 0.09121, nli_acc 0.96810, 93.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.88 secs\n",
      "\u001b[1m---- Epoch 96/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.08907, nli_loss 0.08907, nli_acc 0.96860, 93.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.89 secs\n",
      "\u001b[1m---- Epoch 97/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.09071, nli_loss 0.09071, nli_acc 0.96815, 84.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.88 secs\n",
      "\u001b[1m---- Epoch 98/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.05720, nli_loss 0.05720, nli_acc 0.98290, 95.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.89 secs\n",
      "\u001b[1m---- Epoch 99/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06513, nli_loss 0.06513, nli_acc 0.97910, 94.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.87 secs\n",
      "\u001b[1m---- Epoch 100/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05945, nli_loss 0.05945, nli_acc 0.98240, 82.77 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.87 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(96629,13236055).jsonl\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 800 \\\n",
    "--batch_size 25 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--nli_hidden_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da32fbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 800\n",
      "   batch_size: 40\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/\n",
      "   freeze_huggingface_model: False\n",
      "   merged_input: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of BertBasedNLI ...\u001b[0m\n",
      "BertBasedNLI\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  merged_input: False\n",
      "  hidden_size: 128\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 162036\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Moderate right pleural effusion has slightly decreased since .\n",
      "Hypothesis: Moderate right pleural effusion may be residual from , or new.\n",
      "Label: contradiction\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: The chest is hyperinflated.\n",
      "Hypothesis: The lungs are clear.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Number of train entailment samples: 26442\n",
      "Number of train neutral samples: 39817\n",
      "Number of train contradiction samples: 95777\n",
      "\u001b[1mBuilding test NLI dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI+GPT-4)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_230411_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_230411_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_77_nli_acc=0.7951.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/checkpoint_77_nli_acc=0.7951.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_230411_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.15181, nli_loss 0.15181, nli_acc 0.94337, 93.73 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.7918.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.12822, nli_loss 0.12822, nli_acc 0.95300, 87.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.60 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.11074, nli_loss 0.11074, nli_acc 0.96097, 93.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nli_acc=0.7955.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.13242, nli_loss 0.13242, nli_acc 0.95091, 95.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.60 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.12146, nli_loss 0.12146, nli_acc 0.95572, 95.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.59 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.10420, nli_loss 0.10420, nli_acc 0.96441, 94.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.60 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.08950, nli_loss 0.08950, nli_acc 0.96894, 93.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.60 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07763, nli_loss 0.07763, nli_acc 0.97513, 88.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.59 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.07173, nli_loss 0.07173, nli_acc 0.97775, 94.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.60 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.07684, nli_loss 0.07684, nli_acc 0.97469, 97.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.59 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07124, nli_loss 0.07124, nli_acc 0.97859, 93.96 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.59 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06716, nli_loss 0.06716, nli_acc 0.97906, 93.48 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.59 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.10983, nli_loss 0.10983, nli_acc 0.96062, 92.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.59 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.11232, nli_loss 0.11232, nli_acc 0.95950, 93.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.58 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.09592, nli_loss 0.09592, nli_acc 0.96741, 95.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.60 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.08310, nli_loss 0.08310, nli_acc 0.97203, 90.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.59 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.06907, nli_loss 0.06907, nli_acc 0.97844, 91.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.61 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.07054, nli_loss 0.07054, nli_acc 0.97778, 97.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.59 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06585, nli_loss 0.06585, nli_acc 0.98016, 88.95 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.60 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06691, nli_loss 0.06691, nli_acc 0.97913, 94.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.60 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.10877, nli_loss 0.10877, nli_acc 0.96044, 92.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.59 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.10551, nli_loss 0.10551, nli_acc 0.96169, 91.93 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.61 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.08642, nli_loss 0.08642, nli_acc 0.97091, 88.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.59 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.06921, nli_loss 0.06921, nli_acc 0.97750, 82.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.59 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.06565, nli_loss 0.06565, nli_acc 0.97875, 87.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.60 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06338, nli_loss 0.06338, nli_acc 0.97922, 142.15 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 1.30 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07445, nli_loss 0.07445, nli_acc 0.97550, 109.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.61 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05855, nli_loss 0.05855, nli_acc 0.98228, 92.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.62 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.10991, nli_loss 0.10991, nli_acc 0.96113, 95.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.58 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.10358, nli_loss 0.10358, nli_acc 0.96234, 92.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.60 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "^C iteration 24175\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 356, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 275, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 159, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 67, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 54, in step_fn\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230925_073733_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,128)/\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 800 \\\n",
    "--batch_size 40 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--nli_hidden_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
