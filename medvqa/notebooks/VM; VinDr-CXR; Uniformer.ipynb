{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: None\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: cxrmate-rrg24-uniformer-huggingface\n",
      "   image_local_feat_size: 512\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: True\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   classification_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: aehrc/cxrmate-rrg24\n",
      "   chexpert_mlc_version: None\n",
      "   chexpert_mlc_hidden_size: 128\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: None\n",
      "   chest_imagenome_mlc_version: None\n",
      "   chest_imagenome_mlc_hidden_size: 128\n",
      "   vinbig_mlc_hidden_size: 128\n",
      "   torchxrayvision_weights_name: None\n",
      "   detectron2_model_yaml: None\n",
      "   num_regions: 144\n",
      "   roi_heads_batch_size_per_image: 128\n",
      "   rpn_batch_size_per_image: 128\n",
      "   roi_align_output_size: None\n",
      "   yolov8_model_name_or_path: None\n",
      "   yolov8_model_alias: None\n",
      "   yolov8_use_one_detector_per_dataset: False\n",
      "   yolov11_model_name_or_path: None\n",
      "   yolov11_model_alias: None\n",
      "   query_embed_size: None\n",
      "   local_attention_hidden_size: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,5,5e-6,2e-4,5,5e-6\n",
      "   batch_size: 80\n",
      "   gradient_accumulation_steps: 1\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   image_size: [384, 384]\n",
      "   horizontal_flip_prob: 0\n",
      "   mimiccxr_weight: 1\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: any_single\n",
      "   mimiccxr_balanced_sampling_mode: None\n",
      "   mimiccxr_balanced_batch_size: None\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_chest_imagenome_decent_images_only: False\n",
      "   use_amp: True\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_path: None\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: False\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: True\n",
      "   vinbig_training_data_mode: train\n",
      "   vinbig_use_validation: True\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: focal+bce+wbcbce\n",
      "   focal_loss_weight: 1\n",
      "   bce_loss_weight: 1\n",
      "   wbce_loss_weight: 1\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: False\n",
      "   predict_labels_and_bboxes_chest_imagenome: False\n",
      "   clamp_bboxes_chest_imagenome: False\n",
      "   use_anaxnet_bbox_subset: False\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   pass_pred_bbox_coords_as_input: False\n",
      "   use_gt_bboxes_as_predictions: False\n",
      "   predict_bboxes_vinbig: False\n",
      "   classify_labels_vinbig: True\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[35mvinbig_class_id_offset: 0\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of MultiPurposeVisualModule ...\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: cxrmate-rrg24-uniformer-huggingface\n",
      "  self.global_feat_size = 1024\n",
      "  self.local_feat_size = 512\n",
      "  Initializing auxiliary tasks\n",
      "    Initializing VinBig classification task\n",
      "MultilabelClassifier_v3:\n",
      "  local_feat_dim: 512\n",
      "  global_feat_dim: 1024\n",
      "  hidden_dim: 128\n",
      "  num_regions: 144\n",
      "  num_labels: 28\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,5,5e-6,2e-4,5,5e-6\n",
      "1e-06 3 0.0002 5 5e-06 0.0002 5 5e-06\n",
      "self.steps_to_restart = 5\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "Focal_BCE_WBCBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbcbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 1, max_grad_norm = None\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating VinBig visual module trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  image_size = [384, 384]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [384, 384]\n",
      "get_train_transform(): Using normal transforms\n",
      "default_prob = 0.35\n",
      "Returning augmented transforms with mode random-color-and-spatial\n",
      "get_image_transform()\n",
      "  image_size = [384, 384]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [384, 384]\n",
      "Returning transform without augmentation\n",
      "Generating train dataset and dataloader\n",
      "No finding        , #pos=10601, #neg=4399\n",
      "Other disease     , #pos=4288, #neg=10712\n",
      "Aortic enlargement, #pos=3067, #neg=11933\n",
      "Cardiomegaly      , #pos=2300, #neg=12700\n",
      "Pleural thickening, #pos=1981, #neg=13019\n",
      "Pulmonary fibrosis, #pos=1617, #neg=13383\n",
      "Lung Opacity      , #pos=1322, #neg=13678\n",
      "Other lesion      , #pos=1134, #neg=13866\n",
      "Pleural effusion  , #pos=1032, #neg=13968\n",
      "Pneumonia         , #pos=917, #neg=14083\n",
      "Nodule/Mass       , #pos=830, #neg=14170\n",
      "Tuberculosis      , #pos=750, #neg=14250\n",
      "Infiltration      , #pos=613, #neg=14387\n",
      "Calcification     , #pos=452, #neg=14548\n",
      "ILD               , #pos=386, #neg=14614\n",
      "Consolidation     , #pos=353, #neg=14647\n",
      "Lung tumor        , #pos=291, #neg=14709\n",
      "Atelectasis       , #pos=186, #neg=14814\n",
      "Mediastinal shift , #pos=150, #neg=14850\n",
      "Enlarged PA       , #pos=131, #neg=14869\n",
      "Pneumothorax      , #pos=96, #neg=14904\n",
      "Rib fracture      , #pos=90, #neg=14910\n",
      "Emphysema         , #pos=78, #neg=14922\n",
      "Lung cavity       , #pos=50, #neg=14950\n",
      "COPD              , #pos=35, #neg=14965\n",
      "Lung cyst         , #pos=32, #neg=14968\n",
      "Clavicle fracture , #pos=27, #neg=14973\n",
      "Edema             , #pos=13, #neg=14987\n",
      "len(train_indices) = 15000\n",
      "Generating val dataset and dataloader\n",
      "len(test_indices) = 3000\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [0.3]\n",
      "merged_dataset_name = vinbig\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250121_021456_vinbig_aehrc-cxrmate-rrg24-uniformer\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250121_021456_vinbig_aehrc-cxrmate-rrg24-uniformer/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250121_021456_vinbig_aehrc-cxrmate-rrg24-uniformer/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m14) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.81510, vnb_macro_prcauc 0.15458, vnb_label_loss 1.81510, 159.36 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.11257, 38.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_vnb_macro_prcauc=0.1147.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 1.70160, vnb_macro_prcauc 0.23859, vnb_label_loss 1.70160, 157.57 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.23943, 37.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_vnb_macro_prcauc=0.2394.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 1.43830, vnb_macro_prcauc 0.40929, vnb_label_loss 1.43830, 157.35 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.32616, 36.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_vnb_macro_prcauc=0.3303.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 1.18852, vnb_macro_prcauc 0.54092, vnb_label_loss 1.18852, 157.82 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.37981, 38.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_vnb_macro_prcauc=0.3879.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000096) ...\n",
      "loss 1.07633, vnb_macro_prcauc 0.60981, vnb_label_loss 1.07633, 159.06 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39202, 37.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_vnb_macro_prcauc=0.4029.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 1.03908, vnb_macro_prcauc 0.63998, vnb_label_loss 1.03908, 158.24 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39479, 37.84 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_vnb_macro_prcauc=0.4071.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 1.02710, vnb_macro_prcauc 0.64779, vnb_label_loss 1.02710, 158.64 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39198, 37.69 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 1.01812, vnb_macro_prcauc 0.66186, vnb_label_loss 1.01812, 157.64 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39255, 37.29 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.01774, vnb_macro_prcauc 0.65973, vnb_label_loss 1.01774, 158.51 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.38970, 37.20 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 1.01764, vnb_macro_prcauc 0.64495, vnb_label_loss 1.01764, 158.49 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39171, 37.34 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.96987, vnb_macro_prcauc 0.68501, vnb_label_loss 0.96987, 158.27 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.38608, 37.61 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.95044, vnb_macro_prcauc 0.69992, vnb_label_loss 0.95044, 158.69 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39433, 36.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_vnb_macro_prcauc=0.4096.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.93513, vnb_macro_prcauc 0.71273, vnb_label_loss 0.93513, 158.95 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39581, 36.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_vnb_macro_prcauc=0.4117.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.93611, vnb_macro_prcauc 0.71417, vnb_label_loss 0.93611, 160.15 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39515, 37.08 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.94330, vnb_macro_prcauc 0.69613, vnb_label_loss 0.94330, 158.57 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.38441, 37.50 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "^C iteration 4625\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_visual_module.py\", line 1502, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_visual_module.py\", line 1376, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_visual_module.py\", line 770, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 142, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/vqa.py\", line 934, in step_fn\n",
      "    output = flag_to_step_fn[flag](batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/vqa.py\", line 692, in step_fn__vinbig\n",
      "    model_output = model(**model_kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/vision/visual_modules.py\", line 948, in forward\n",
      "    local_feat_NxCxHxW = self.raw_image_encoder(raw_images)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 286, in forward\n",
      "    x = self.forward_features(x)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 277, in forward_features\n",
      "    x = blk(x)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 150, in forward\n",
      "    x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 35, in forward\n",
      "    x = self.fc2(x)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 300 \\\n",
    "--batch_size 80 \\\n",
    "--num_workers 3 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,5,5e-6,2e-4,5,5e-6\" \\\n",
    "--use_vinbig \\\n",
    "--vinbig_training_data_mode \"train\" \\\n",
    "--vinbig_use_validation \\\n",
    "--classify_labels_vinbig \\\n",
    "--binary_loss_name \"focal+bce+wbcbce\" \\\n",
    "--raw_image_encoding \"cxrmate-rrg24-uniformer-huggingface\" \\\n",
    "--huggingface_model_name \"aehrc/cxrmate-rrg24\" \\\n",
    "--image_size 384 384 \\\n",
    "--image_local_feat_size 512 \\\n",
    "--num_regions 144 \\\n",
    "--freeze_image_encoder \\\n",
    "--img_aug_mode \"random-color-and-spatial\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: None\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: cxrmate-rrg24-uniformer-huggingface\n",
      "   image_local_feat_size: 512\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   classification_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: aehrc/cxrmate-rrg24\n",
      "   chexpert_mlc_version: None\n",
      "   chexpert_mlc_hidden_size: 128\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: None\n",
      "   chest_imagenome_mlc_version: None\n",
      "   chest_imagenome_mlc_hidden_size: 128\n",
      "   vinbig_mlc_hidden_size: 128\n",
      "   torchxrayvision_weights_name: None\n",
      "   detectron2_model_yaml: None\n",
      "   num_regions: 144\n",
      "   roi_heads_batch_size_per_image: 128\n",
      "   rpn_batch_size_per_image: 128\n",
      "   roi_align_output_size: None\n",
      "   yolov8_model_name_or_path: None\n",
      "   yolov8_model_alias: None\n",
      "   yolov8_use_one_detector_per_dataset: False\n",
      "   yolov11_model_name_or_path: None\n",
      "   yolov11_model_alias: None\n",
      "   query_embed_size: None\n",
      "   local_attention_hidden_size: None\n",
      "   use_linear_head_for_classification: True\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,5,1e-6,1e-4,5,1e-6\n",
      "   batch_size: 10\n",
      "   gradient_accumulation_steps: 4\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   image_size: [384, 384]\n",
      "   horizontal_flip_prob: 0\n",
      "   mimiccxr_weight: 1\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: any_single\n",
      "   mimiccxr_balanced_sampling_mode: None\n",
      "   mimiccxr_balanced_batch_size: None\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_chest_imagenome_decent_images_only: False\n",
      "   use_amp: True\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_path: None\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: False\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: True\n",
      "   vinbig_training_data_mode: train\n",
      "   vinbig_use_validation: True\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: focal+bce+wbcbce\n",
      "   focal_loss_weight: 1\n",
      "   bce_loss_weight: 1\n",
      "   wbce_loss_weight: 1\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: False\n",
      "   predict_labels_and_bboxes_chest_imagenome: False\n",
      "   clamp_bboxes_chest_imagenome: False\n",
      "   use_anaxnet_bbox_subset: False\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   pass_pred_bbox_coords_as_input: False\n",
      "   use_gt_bboxes_as_predictions: False\n",
      "   predict_bboxes_vinbig: False\n",
      "   classify_labels_vinbig: True\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[35mvinbig_class_id_offset: 0\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of MultiPurposeVisualModule ...\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: cxrmate-rrg24-uniformer-huggingface\n",
      "  self.global_feat_size = 1024\n",
      "  self.local_feat_size = 512\n",
      "  Initializing auxiliary tasks\n",
      "    Initializing VinBig classification task\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,5,1e-6,1e-4,5,1e-6\n",
      "1e-06 3 0.0001 5 1e-06 0.0001 5 1e-06\n",
      "self.steps_to_restart = 5\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "Focal_BCE_WBCBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbcbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 1, max_grad_norm = None\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating VinBig visual module trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  image_size = [384, 384]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [384, 384]\n",
      "get_train_transform(): Using normal transforms\n",
      "default_prob = 0.35\n",
      "Returning augmented transforms with mode random-color-and-spatial\n",
      "get_image_transform()\n",
      "  image_size = [384, 384]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [384, 384]\n",
      "Returning transform without augmentation\n",
      "Generating train dataset and dataloader\n",
      "No finding        , #pos=10601, #neg=4399\n",
      "Other disease     , #pos=4288, #neg=10712\n",
      "Aortic enlargement, #pos=3067, #neg=11933\n",
      "Cardiomegaly      , #pos=2300, #neg=12700\n",
      "Pleural thickening, #pos=1981, #neg=13019\n",
      "Pulmonary fibrosis, #pos=1617, #neg=13383\n",
      "Lung Opacity      , #pos=1322, #neg=13678\n",
      "Other lesion      , #pos=1134, #neg=13866\n",
      "Pleural effusion  , #pos=1032, #neg=13968\n",
      "Pneumonia         , #pos=917, #neg=14083\n",
      "Nodule/Mass       , #pos=830, #neg=14170\n",
      "Tuberculosis      , #pos=750, #neg=14250\n",
      "Infiltration      , #pos=613, #neg=14387\n",
      "Calcification     , #pos=452, #neg=14548\n",
      "ILD               , #pos=386, #neg=14614\n",
      "Consolidation     , #pos=353, #neg=14647\n",
      "Lung tumor        , #pos=291, #neg=14709\n",
      "Atelectasis       , #pos=186, #neg=14814\n",
      "Mediastinal shift , #pos=150, #neg=14850\n",
      "Enlarged PA       , #pos=131, #neg=14869\n",
      "Pneumothorax      , #pos=96, #neg=14904\n",
      "Rib fracture      , #pos=90, #neg=14910\n",
      "Emphysema         , #pos=78, #neg=14922\n",
      "Lung cavity       , #pos=50, #neg=14950\n",
      "COPD              , #pos=35, #neg=14965\n",
      "Lung cyst         , #pos=32, #neg=14968\n",
      "Clavicle fracture , #pos=27, #neg=14973\n",
      "Edema             , #pos=13, #neg=14987\n",
      "len(train_indices) = 15000\n",
      "Generating val dataset and dataloader\n",
      "len(test_indices) = 3000\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [0.3]\n",
      "merged_dataset_name = vinbig\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250121_034338_vinbig_aehrc-cxrmate-rrg24-uniformer\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250121_034338_vinbig_aehrc-cxrmate-rrg24-uniformer/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250121_034338_vinbig_aehrc-cxrmate-rrg24-uniformer/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m14) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.64630, vnb_macro_prcauc 0.15001, vnb_label_loss 1.64630, 104.02 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.09630, 40.98 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_vnb_macro_prcauc=0.0990.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.35837, vnb_macro_prcauc 0.16538, vnb_label_loss 1.35837, 106.76 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.13992, 41.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_vnb_macro_prcauc=0.1412.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 1.04514, vnb_macro_prcauc 0.31027, vnb_label_loss 1.04514, 101.75 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.27004, 41.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_vnb_macro_prcauc=0.2721.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.83462, vnb_macro_prcauc 0.44743, vnb_label_loss 0.83462, 101.52 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.34533, 41.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_vnb_macro_prcauc=0.3504.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 0.71905, vnb_macro_prcauc 0.53542, vnb_label_loss 0.71905, 105.05 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.38619, 41.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_vnb_macro_prcauc=0.3937.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.68342, vnb_macro_prcauc 0.57154, vnb_label_loss 0.68342, 109.11 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39157, 41.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_vnb_macro_prcauc=0.4006.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.66405, vnb_macro_prcauc 0.59026, vnb_label_loss 0.66405, 108.62 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39644, 41.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_vnb_macro_prcauc=0.4061.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.67089, vnb_macro_prcauc 0.58903, vnb_label_loss 0.67089, 101.56 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39635, 41.04 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.65881, vnb_macro_prcauc 0.58465, vnb_label_loss 0.65881, 99.73 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39694, 41.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_vnb_macro_prcauc=0.4063.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.68283, vnb_macro_prcauc 0.57381, vnb_label_loss 0.68283, 108.75 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.37233, 40.44 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.64375, vnb_macro_prcauc 0.62161, vnb_label_loss 0.64375, 109.07 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.40229, 41.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_vnb_macro_prcauc=0.4133.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.62536, vnb_macro_prcauc 0.62760, vnb_label_loss 0.62536, 107.81 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39246, 41.17 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.60892, vnb_macro_prcauc 0.64180, vnb_label_loss 0.60892, 106.60 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39667, 41.83 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.61324, vnb_macro_prcauc 0.63675, vnb_label_loss 0.61324, 106.92 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39613, 41.61 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.63964, vnb_macro_prcauc 0.62099, vnb_label_loss 0.63964, 108.52 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.38672, 41.64 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.60358, vnb_macro_prcauc 0.64541, vnb_label_loss 0.60358, 108.62 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.38413, 41.69 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.59244, vnb_macro_prcauc 0.65154, vnb_label_loss 0.59244, 100.37 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39501, 41.73 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.57420, vnb_macro_prcauc 0.67593, vnb_label_loss 0.57420, 109.07 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39310, 42.00 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.57706, vnb_macro_prcauc 0.67977, vnb_label_loss 0.57706, 109.38 secs\n",
      "(2) Validation stage ...\n",
      "vnb_macro_prcauc 0.39568, 41.74 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "^C iteration 5750\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_visual_module.py\", line 1505, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_visual_module.py\", line 1379, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_visual_module.py\", line 771, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 142, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/vqa.py\", line 934, in step_fn\n",
      "    output = flag_to_step_fn[flag](batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/vqa.py\", line 692, in step_fn__vinbig\n",
      "    model_output = model(**model_kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/vision/visual_modules.py\", line 954, in forward\n",
      "    local_feat_NxCxHxW = self.raw_image_encoder(raw_images)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 286, in forward\n",
      "    x = self.forward_features(x)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 280, in forward_features\n",
      "    x = blk(x)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 149, in forward\n",
      "    x = x + self.drop_path(self.attn(self.norm1(x)))\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/aehrc/cxrmate-rrg24/81c38cf13501bbf2ab5f97696afb768b907bc675/modelling_uniformer.py\", line 77, in forward\n",
      "    attn = attn.softmax(dim=-1)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 300 \\\n",
    "--batch_size 10 \\\n",
    "--num_workers 3 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,5,1e-6,1e-4,5,1e-6\" \\\n",
    "--use_vinbig \\\n",
    "--vinbig_training_data_mode \"train\" \\\n",
    "--vinbig_use_validation \\\n",
    "--classify_labels_vinbig \\\n",
    "--binary_loss_name \"focal+bce+wbcbce\" \\\n",
    "--raw_image_encoding \"cxrmate-rrg24-uniformer-huggingface\" \\\n",
    "--huggingface_model_name \"aehrc/cxrmate-rrg24\" \\\n",
    "--image_size 384 384 \\\n",
    "--image_local_feat_size 512 \\\n",
    "--num_regions 144 \\\n",
    "--use_linear_head_for_classification \\\n",
    "--img_aug_mode \"random-color-and-spatial\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
