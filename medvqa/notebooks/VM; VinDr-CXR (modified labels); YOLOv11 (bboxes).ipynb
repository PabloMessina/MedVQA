{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 250\n",
      "   checkpoint_folder: None\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: yolov11-for-det-mlc\n",
      "   image_local_feat_size: 512\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   classification_mlp_hidden_dims: [256, 128]\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: None\n",
      "   chexpert_mlc_version: None\n",
      "   chexpert_mlc_hidden_size: 128\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: None\n",
      "   chest_imagenome_mlc_version: None\n",
      "   chest_imagenome_mlc_hidden_size: 128\n",
      "   vinbig_mlc_hidden_size: 128\n",
      "   torchxrayvision_weights_name: None\n",
      "   detectron2_model_yaml: None\n",
      "   num_regions: 169\n",
      "   roi_heads_batch_size_per_image: 128\n",
      "   rpn_batch_size_per_image: 128\n",
      "   roi_align_output_size: None\n",
      "   yolov8_model_name_or_path: None\n",
      "   yolov8_model_alias: None\n",
      "   yolov8_use_one_detector_per_dataset: False\n",
      "   yolov11_model_name_or_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vinbig/yolov11/train4/weights/best.pt\n",
      "   yolov11_model_alias: yolo11l\n",
      "   query_embed_size: 128\n",
      "   local_attention_hidden_size: 256\n",
      "   use_linear_head_for_classification: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,6,1e-6,1e-4,6,1e-6\n",
      "   train_batch_size: 100\n",
      "   val_batch_size: 100\n",
      "   gradient_accumulation_steps: 2\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   image_size: [416, 416]\n",
      "   horizontal_flip_prob: 0\n",
      "   mimiccxr_weight: 1\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: any_single\n",
      "   mimiccxr_balanced_sampling_mode: None\n",
      "   mimiccxr_balanced_batch_size: None\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_chest_imagenome_decent_images_only: False\n",
      "   use_amp: True\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_path: None\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: False\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: True\n",
      "   vinbig_training_data_mode: train\n",
      "   vinbig_use_validation: True\n",
      "   use_vinbig_with_modified_labels: True\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: focal+bce+wbcbce\n",
      "   focal_loss_weight: 1\n",
      "   bce_loss_weight: 1\n",
      "   wbce_loss_weight: 1\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: False\n",
      "   predict_labels_and_bboxes_chest_imagenome: False\n",
      "   clamp_bboxes_chest_imagenome: False\n",
      "   use_anaxnet_bbox_subset: False\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   pass_pred_bbox_coords_as_input: False\n",
      "   use_gt_bboxes_as_predictions: False\n",
      "   predict_bboxes_vinbig: True\n",
      "   classify_labels_vinbig: False\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[35mvinbig_class_id_offset: 0\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of MultiPurposeVisualModule ...\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: yolov11-for-det-mlc\n",
      "Overriding model.yaml nc=22 with nc=23\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1428757  ultralytics.nn.modules.head.Detect           [23, [256, 512, 512]]         \n",
      "YOLO11l summary: 631 layers, 25,328,213 parameters, 25,328,197 gradients, 87.4 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "From YOLOv11CustomLoss:\n",
      "device: cuda\n",
      "  Initializing auxiliary tasks\n",
      "    Skipping auxiliary tasks initialization for YOLOv11 models\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,6,1e-6,1e-4,6,1e-6\n",
      "1e-06 3 0.0001 6 1e-06 0.0001 6 1e-06\n",
      "self.steps_to_restart = 6\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "Focal_BCE_WBCBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbcbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 1, max_grad_norm = None\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating VinBig visual module trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  image_size = [416, 416]\n",
      "  mean = [0.0, 0.0, 0.0]\n",
      "  std = [1.0, 1.0, 1.0]\n",
      "  Using bounding box aware transforms\n",
      "    augmentation_mode = random-color-and-spatial\n",
      "    default_prob = 0.35\n",
      "    for_vinbig: returning vinbig transform\n",
      "get_train_transform(): Using bbox-aware transforms for VinBigData\n",
      "    Returning augmented transform with mode random-color-and-spatial\n",
      "get_image_transform()\n",
      "  image_size = [416, 416]\n",
      "  mean = [0.0, 0.0, 0.0]\n",
      "  std = [1.0, 1.0, 1.0]\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mNOTE: Improving VinDr-CXR classification labels ...\u001b[0m\n",
      "Loading bounding boxes\n",
      "Anomalous bboxes found: 29 of 37367\n",
      "\u001b[93m\u001b[1mNOTE: Improving VinDr-CXR bbox labels ...\u001b[0m\n",
      "class_id_offset: 0\n",
      "Anomalous bboxes found: 0 of 2697\n",
      "\u001b[93m\u001b[1mNOTE: Improving VinDr-CXR bbox labels ...\u001b[0m\n",
      "class_id_offset: 0\n",
      "  Loaded 18000 bounding boxes\n",
      "Generating train dataset and dataloader\n",
      "Abnormal finding  , #pos=4522, #neg=10478\n",
      "Other disease     , #pos=4377, #neg=10623\n",
      "Aortic enlargement, #pos=3098, #neg=11902\n",
      "Lung Opacity      , #pos=2709, #neg=12291\n",
      "Cardiomegaly      , #pos=2316, #neg=12684\n",
      "Pleural thickening, #pos=2010, #neg=12990\n",
      "Pulmonary fibrosis, #pos=1621, #neg=13379\n",
      "Other lesion      , #pos=1154, #neg=13846\n",
      "Pleural effusion  , #pos=1038, #neg=13962\n",
      "Pneumonia         , #pos=919, #neg=14081\n",
      "Nodule/Mass       , #pos=841, #neg=14159\n",
      "Tuberculosis      , #pos=750, #neg=14250\n",
      "Infiltration      , #pos=613, #neg=14387\n",
      "Calcification     , #pos=458, #neg=14542\n",
      "ILD               , #pos=397, #neg=14603\n",
      "Consolidation     , #pos=353, #neg=14647\n",
      "Lung tumor        , #pos=291, #neg=14709\n",
      "Atelectasis       , #pos=187, #neg=14813\n",
      "Mediastinal shift , #pos=150, #neg=14850\n",
      "Enlarged PA       , #pos=131, #neg=14869\n",
      "Pneumothorax      , #pos=96, #neg=14904\n",
      "Rib fracture      , #pos=90, #neg=14910\n",
      "Emphysema         , #pos=81, #neg=14919\n",
      "Lung cavity       , #pos=51, #neg=14949\n",
      "COPD              , #pos=36, #neg=14964\n",
      "Lung cyst         , #pos=33, #neg=14967\n",
      "Clavicle fracture , #pos=27, #neg=14973\n",
      "Edema             , #pos=13, #neg=14987\n",
      "len(train_indices) = 15000\n",
      "Generating val dataset and dataloader\n",
      "len(test_indices) = 3000\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [0.3]\n",
      "merged_dataset_name = vinbig\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "NOTE: Using only validation metrics\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250216_173251_vinbig_yolo11l(d:vinbig)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250216_173251_vinbig_yolo11l(d:vinbig)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20250216_173251_vinbig_yolo11l(d:vinbig)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 11.55999, vnb_y11_loss 11.55999, vnb_y11_box_loss 1.75062, vnb_y11_cls_loss 8.13265, vnb_y11_dfl_loss 1.67671, 180.87 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.00000, 21.16 secs\n",
      "\u001b[93mVal metrics:\u001b[0m\n",
      "\u001b[93mvnb_y11_bbox_iou: 0.0000, weight = 1.0\u001b[0m\n",
      "\u001b[93mnum = 0.0000, den = 1.0000, score = 0.0000\u001b[0m\n",
      "\u001b[93mFinal val score = 0.0000\u001b[0m\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_vnb_y11_bbox_iou=0.0000.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 9.65601, vnb_y11_loss 9.65601, vnb_y11_box_loss 1.75616, vnb_y11_cls_loss 6.22801, vnb_y11_dfl_loss 1.67184, 177.28 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.00004, 20.73 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_vnb_y11_bbox_iou=0.0000.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 7.34074, vnb_y11_loss 7.34074, vnb_y11_box_loss 1.87655, vnb_y11_cls_loss 3.82380, vnb_y11_dfl_loss 1.64039, 179.21 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.07574, 21.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_vnb_y11_bbox_iou=0.0757.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 5.84911, vnb_y11_loss 5.84911, vnb_y11_box_loss 1.79638, vnb_y11_cls_loss 2.42553, vnb_y11_dfl_loss 1.62720, 178.30 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.14136, 21.81 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_vnb_y11_bbox_iou=0.1414.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 5.54026, vnb_y11_loss 5.54026, vnb_y11_box_loss 1.72935, vnb_y11_cls_loss 2.12189, vnb_y11_dfl_loss 1.68902, 179.45 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.14455, 21.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_vnb_y11_bbox_iou=0.1446.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 5.52117, vnb_y11_loss 5.52117, vnb_y11_box_loss 1.72384, vnb_y11_cls_loss 2.10323, vnb_y11_dfl_loss 1.69409, 182.82 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.14795, 21.75 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_vnb_y11_bbox_iou=0.1480.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 5.52144, vnb_y11_loss 5.52144, vnb_y11_box_loss 1.72341, vnb_y11_cls_loss 2.09767, vnb_y11_dfl_loss 1.70036, 184.36 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15113, 21.87 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_vnb_y11_bbox_iou=0.1511.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 5.52397, vnb_y11_loss 5.52397, vnb_y11_box_loss 1.72612, vnb_y11_cls_loss 2.09415, vnb_y11_dfl_loss 1.70370, 184.66 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.14494, 22.30 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 5.52399, vnb_y11_loss 5.52399, vnb_y11_box_loss 1.72618, vnb_y11_cls_loss 2.09426, vnb_y11_dfl_loss 1.70356, 185.20 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15148, 22.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_vnb_y11_bbox_iou=0.1515.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 5.52039, vnb_y11_loss 5.52039, vnb_y11_box_loss 1.72298, vnb_y11_cls_loss 2.09269, vnb_y11_dfl_loss 1.70472, 181.19 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.14306, 21.79 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 5.47051, vnb_y11_loss 5.47051, vnb_y11_box_loss 1.71246, vnb_y11_cls_loss 2.07052, vnb_y11_dfl_loss 1.68753, 180.74 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15256, 22.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_vnb_y11_bbox_iou=0.1526.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 5.43689, vnb_y11_loss 5.43689, vnb_y11_box_loss 1.70322, vnb_y11_cls_loss 2.05114, vnb_y11_dfl_loss 1.68252, 180.22 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15691, 22.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_vnb_y11_bbox_iou=0.1569.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 5.42491, vnb_y11_loss 5.42491, vnb_y11_box_loss 1.70073, vnb_y11_cls_loss 2.04400, vnb_y11_dfl_loss 1.68018, 181.34 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15579, 22.00 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 5.40144, vnb_y11_loss 5.40144, vnb_y11_box_loss 1.69307, vnb_y11_cls_loss 2.03533, vnb_y11_dfl_loss 1.67305, 183.13 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.16470, 22.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_vnb_y11_bbox_iou=0.1647.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 5.40966, vnb_y11_loss 5.40966, vnb_y11_box_loss 1.69618, vnb_y11_cls_loss 2.03558, vnb_y11_dfl_loss 1.67791, 186.88 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15482, 21.74 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 5.39077, vnb_y11_loss 5.39077, vnb_y11_box_loss 1.69018, vnb_y11_cls_loss 2.02893, vnb_y11_dfl_loss 1.67166, 181.65 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.16393, 22.25 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 5.38832, vnb_y11_loss 5.38832, vnb_y11_box_loss 1.68935, vnb_y11_cls_loss 2.01715, vnb_y11_dfl_loss 1.68182, 180.12 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vnb_y11_bbox_iou 0.15570, 22.25 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 5.34255, vnb_y11_loss 5.34255, vnb_y11_box_loss 1.67719, vnb_y11_cls_loss 1.99075, vnb_y11_dfl_loss 1.67461, 181.29 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15585, 21.93 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 5.32080, vnb_y11_loss 5.32080, vnb_y11_box_loss 1.67402, vnb_y11_cls_loss 1.97760, vnb_y11_dfl_loss 1.66919, 180.84 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.16530, 22.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_vnb_y11_bbox_iou=0.1653.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 5.32627, vnb_y11_loss 5.32627, vnb_y11_box_loss 1.67678, vnb_y11_cls_loss 1.97904, vnb_y11_dfl_loss 1.67045, 181.36 secs\n",
      "(2) Validation stage ...\n",
      "vnb_y11_bbox_iou 0.15242, 22.08 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "   iteration 5200\r"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 250 \\\n",
    "--train_batch_size 100 \\\n",
    "--val_batch_size 100 \\\n",
    "--num_workers 3 \\\n",
    "--gradient_accumulation_steps 2 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,6,1e-6,1e-4,6,1e-6\" \\\n",
    "--use_vinbig \\\n",
    "--vinbig_training_data_mode \"train\" \\\n",
    "--vinbig_use_validation \\\n",
    "--predict_bboxes_vinbig \\\n",
    "--binary_loss_name \"focal+bce+wbcbce\" \\\n",
    "--raw_image_encoding \"yolov11-for-det-mlc\" \\\n",
    "--yolov11_model_name_or_path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/vinbig/yolov11/train4/weights/best.pt\" \\\n",
    "--yolov11_model_alias \"yolo11l\" \\\n",
    "--image_size 416 416 \\\n",
    "--image_local_feat_size 512 \\\n",
    "--num_regions 169 \\\n",
    "--classification_mlp_hidden_dims 256 128 \\\n",
    "--query_embed_size 128 \\\n",
    "--local_attention_hidden_size 256 \\\n",
    "--img_aug_mode \"random-color-and-spatial\" \\\n",
    "--use_vinbig_with_modified_labels \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
