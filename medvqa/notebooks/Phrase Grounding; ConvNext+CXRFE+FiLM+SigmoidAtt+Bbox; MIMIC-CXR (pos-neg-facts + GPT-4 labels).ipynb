{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dccb3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 30\n",
      "   batches_per_epoch: 1002\n",
      "   max_images_per_batch: 12\n",
      "   max_phrases_per_batch: 1000\n",
      "   max_phrases_per_image: 30\n",
      "   val_batch_size_factor: 4.0\n",
      "   checkpoint_folder: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_011909_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)\n",
      "   pretrained_checkpoint_folder_paths: None\n",
      "   freeze_image_encoder: False\n",
      "   raw_image_encoding: convnextmodel-huggingface\n",
      "   huggingface_model_name: facebook/convnext-small-224\n",
      "   num_regions: 169\n",
      "   image_local_feat_size: 768\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   image_encoder_dropout_p: 0\n",
      "   yolov8_model_name_or_path: None\n",
      "   yolov8_model_alias: None\n",
      "   phrase_embedding_size: 128\n",
      "   regions_width: 13\n",
      "   regions_height: 13\n",
      "   qkv_size: None\n",
      "   phrase_grounding_mode: adaptive_film_based_pooling_mlp__no_grounding\n",
      "   phrase_classifier_hidden_size: None\n",
      "   transf_d_model: None\n",
      "   transf_nhead: None\n",
      "   transf_dim_feedforward: None\n",
      "   transf_dropout: 0\n",
      "   transf_num_layers: None\n",
      "   visual_feature_proj_size: None\n",
      "   visual_grounding_hidden_size: 256\n",
      "   phrase_mlp_hidden_dims: [256, 128]\n",
      "   predict_global_alignment: False\n",
      "   alignment_proj_size: None\n",
      "   yolov11_model_name_or_path: None\n",
      "   yolov11_model_alias: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,5,1e-6,8e-5,5,1e-6\n",
      "   gradient_accumulation_steps: 3\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   attention_supervision_loss_weight: 1.0\n",
      "   phrase_classifier_loss_weight: 1.0\n",
      "   foreground_loss_weight: 1.0\n",
      "   background_loss_weight: 1.0\n",
      "   focal_loss_weight: 1.0\n",
      "   bce_loss_weight: 1.0\n",
      "   wbce_loss_weight: 1.0\n",
      "   binary_multilabel_classif_loss_name: focal+bce+npbbce\n",
      "   use_weighted_phrase_classifier_loss: False\n",
      "   cluster_and_label_weights_for_facts_filepath: None\n",
      "   use_attention_regularization_loss: False\n",
      "   use_contrastive_phrase_grounding_loss: False\n",
      "   nt_xent_temperature: 0.1\n",
      "   num_train_workers: 2\n",
      "   num_val_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   image_size: [416, 416]\n",
      "   dicom_id_to_pos_neg_facts_filepath: /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_pos_neg_facts_with_GPT4_mined_labels(nf=403416,nl=28,ngpt4=87528)(hash=741,3592138487644435704).pkl\n",
      "   iuxray_image_id_to_pos_neg_facts_filepath: None\n",
      "   mscxr_phrase2embedding_filepath: None\n",
      "   chest_imagenome_bbox_phrase_embeddings_filepath: None\n",
      "   vinbig_phrase_embeddings_filepath: None\n",
      "   chexlocalize_class_phrase_embeddings_filepath: None\n",
      "   chexpert_class_phrase_embeddings_filepath: None\n",
      "   mimiccxr_exclude_noisy_images: False\n",
      "   mimiccxr_facts_weight: 1.0\n",
      "   chest_imagenome_anatlocs_weight: 1.0\n",
      "   mscxr_weight: 1.0\n",
      "   cxrlt2024_weight: 1.0\n",
      "   vinbig_weight: 1.0\n",
      "   chexlocalize_weight: 1.0\n",
      "   chexpert_weight: 1.0\n",
      "   iuxray_weight: 1.0\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   pos_area_prior: 0.4\n",
      "   neg_area_prior: 0.0\n",
      "   use_mimiccxr_facts_for_train: True\n",
      "   use_mimiccxr_facts_for_test: True\n",
      "   use_mscxr_for_train: False\n",
      "   use_mscxr_for_test: False\n",
      "   use_chest_imagenome_for_train: False\n",
      "   use_chest_imagenome_gold_for_test: False\n",
      "   use_vinbig_for_train: False\n",
      "   use_vinbig_for_test: False\n",
      "   use_chexlocalize_for_train: False\n",
      "   use_chexlocalize_for_test: False\n",
      "   use_chexpert_for_train: False\n",
      "   use_chexpert_for_test: False\n",
      "   use_iuxray_for_train: False\n",
      "   use_iuxray_for_test: False\n",
      "   use_cxrlt2024_challenge_split: False\n",
      "   use_cxrlt2024_custom_labels: False\n",
      "   use_cxrlt2024_official_labels: False\n",
      "   use_all_cxrlt2024_official_labels_for_training: False\n",
      "   vinbig_training_data_mode: train\n",
      "   chexpert_training_data_mode: all\n",
      "   mimiccxr_balance_long_middle_short_tail: False\n",
      "   mimiccxr_long_middle_short_tail_thresholds: (0.02, 0.05)\n",
      "   mimiccxr_report_fact_nli_integrated_data_filepath: None\n",
      "   mimiccxr_use_interpret_cxr_challenge_split: False\n",
      "   mimiccxr_interpret_cxr_challenge_split_filepath: None\n",
      "   iuxray_use_interpret_cxr_challenge_split: False\n",
      "   iuxray_interpret_cxr_challenge_split_filepath: None\n",
      "   chexpert_use_interpret_cxr_challenge_split: False\n",
      "   chexpert_interpret_cxr_challenge_split_filepath: None\n",
      "   chexlocalize_use_interpret_cxr_challenge_split: False\n",
      "   chexlocalize_interpret_cxr_challenge_split_filepath: None\n",
      "   cxrlt2024_custom_dicom_id_to_pos_neg_facts_filepath: None\n",
      "   cxrlt2024_official_training_labels_for_fact_classification_filepath: None\n",
      "   cxrlt2024_do_balanced_sampling: False\n",
      "   do_visual_grounding_with_bbox_regression: False\n",
      "   do_visual_grounding_with_segmentation: False\n",
      "   replace_phrase_embeddings_with_random_vectors: False\n",
      "   use_vinbig_with_modified_labels: False\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "include_loss_weights = False\n",
      "source_image_size_mode: medium_512\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of PhraseGrounder ...\u001b[0m\n",
      "\u001b[93mWARNING: Unused kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_011909_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)', 'pretrained_checkpoint_folder_paths': None}\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: convnextmodel-huggingface\n",
      "  self.global_feat_size = 768\n",
      "  self.local_feat_size = 768\n",
      "  Initializing auxiliary tasks\n",
      "\u001b[93mUsing positional encoding for local features\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,5,1e-6,8e-5,5,1e-6\n",
      "1e-06 3 8e-05 5 1e-06 8e-05 5 1e-06\n",
      "self.steps_to_restart = 5\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_WBCBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbcbce_weight = 0.3333333333333333\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 3, max_grad_norm = None\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_WBCBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbcbce_weight = 0.3333333333333333\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating MIMIC-CXR Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  image_size = [416, 416]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [416, 416]\n",
      "get_train_transform(): Using normal transforms\n",
      "default_prob = 0.35\n",
      "Returning augmented transforms with mode random-color-and-spatial\n",
      "get_image_transform()\n",
      "  image_size = [416, 416]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [416, 416]\n",
      "Returning transform without augmentation\n",
      "\u001b[93m\u001b[1mWarning: unused kwargs in MIMICCXR_VisualModuleTrainer: {'use_yolov8': False}\u001b[0m\n",
      "len(forbidden_train_dicom_ids) = 0\n",
      "\u001b[1m\u001b[35mPreparing MIMIC-CXR-Facts datasets and dataloaders for training/testing...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mNOTE: Using strong and weak negatives for training...\u001b[0m\n",
      "fact_embeddings.shape = (403416, 128)\n",
      "Using image size mode: medium_512\n",
      "227835it [00:01, 146588.84it/s]\n",
      "Total number of images: 377110\n",
      "len(train_indices) = 368960\n",
      "len(test_indices) = 8150\n",
      "len(set(train_indices) & set(test_indices)) = 0\n",
      "avg_facts_per_image = 508.52087218126627\n",
      "train_num_facts_per_image = 30\n",
      "avg_facts_per_image = 512.7754601226994\n",
      "test_num_facts_per_image = 30\n",
      "\u001b[1mBuilding train fact dataloader...\u001b[0m\n",
      "batch_size = 12\n",
      "Normal (unbalanced) training...\n",
      "len(self.train_fact_dataloader) = 30747\n",
      "\u001b[1mBuilding test fact dataloaders...\u001b[0m\n",
      "len(self.test_fact_dataloader) = 170\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(mimiccxr_trainer.train_fact_dataloader) = 30747\n",
      "len(mimiccxr_trainer.test_fact_dataloader) = 170\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim-facts\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_24_mimfg_phrcls_loss+mimfg_prc_auc=0.7441.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_011909_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)/checkpoint_24_mimfg_phrcls_loss+mimfg_prc_auc=0.7441.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.65346, mimfg_phrcls_loss 0.65346, mimfg_prc_auc 0.89197, 277.97 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.68504, mimfg_prc_auc 0.88058, 102.98 secs\n",
      "\u001b[93mTrain metrics:\u001b[0m\n",
      "\u001b[93mmimfg_phrcls_loss: 0.6048, weight = 1.0\u001b[0m\n",
      "\u001b[93mmimfg_prc_auc: 0.8920, weight = 1.0\u001b[0m\n",
      "\u001b[93mnum = 1.4968, den = 2.0000, score = 0.7484\u001b[0m\n",
      "\u001b[93mVal metrics:\u001b[0m\n",
      "\u001b[93mmimfg_phrcls_loss: 0.5935, weight = 1.0\u001b[0m\n",
      "\u001b[93mmimfg_prc_auc: 0.8806, weight = 1.0\u001b[0m\n",
      "\u001b[93mnum = 1.4740, den = 2.0000, score = 0.7370\u001b[0m\n",
      "\u001b[93mTrain score = 0.7484, Val score = 0.7370, Final score = 0.7376\u001b[0m\n",
      "\u001b[93mw_train = 0.05, w_val = 0.95\u001b[0m\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_mimfg_phrcls_loss+mimfg_prc_auc=0.7376.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.65940, mimfg_phrcls_loss 0.65940, mimfg_prc_auc 0.88953, 279.63 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.69215, mimfg_prc_auc 0.88002, 104.02 secs\n",
      "\u001b[1m---- Epoch 3/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.65563, mimfg_phrcls_loss 0.65563, mimfg_prc_auc 0.89129, 284.80 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.68466, mimfg_prc_auc 0.87623, 104.46 secs\n",
      "\u001b[1m---- Epoch 4/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.68840, mimfg_phrcls_loss 0.68840, mimfg_prc_auc 0.87778, 284.51 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.80241, mimfg_prc_auc 0.85971, 104.41 secs\n",
      "\u001b[1m---- Epoch 5/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000033) ...\n",
      "loss 0.64847, mimfg_phrcls_loss 0.64847, mimfg_prc_auc 0.89073, 285.03 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.69571, mimfg_prc_auc 0.87902, 104.54 secs\n",
      "\u001b[1m---- Epoch 6/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.63591, mimfg_phrcls_loss 0.63591, mimfg_prc_auc 0.89667, 283.03 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.67128, mimfg_prc_auc 0.88004, 104.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_mimfg_phrcls_loss+mimfg_prc_auc=0.7399.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.61947, mimfg_phrcls_loss 0.61947, mimfg_prc_auc 0.90056, 285.69 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.65501, mimfg_prc_auc 0.88126, 104.84 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_mimfg_phrcls_loss+mimfg_prc_auc=0.7436.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62587, mimfg_phrcls_loss 0.62587, mimfg_prc_auc 0.89951, 287.03 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.65428, mimfg_prc_auc 0.88492, 104.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_mimfg_phrcls_loss+mimfg_prc_auc=0.7453.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.60929, mimfg_phrcls_loss 0.60929, mimfg_prc_auc 0.90377, 287.02 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.66138, mimfg_prc_auc 0.88347, 104.85 secs\n",
      "\u001b[1m---- Epoch 10/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.66815, mimfg_phrcls_loss 0.66815, mimfg_prc_auc 0.88458, 286.30 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.72312, mimfg_prc_auc 0.87548, 104.81 secs\n",
      "\u001b[1m---- Epoch 11/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.63576, mimfg_phrcls_loss 0.63576, mimfg_prc_auc 0.89637, 283.16 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.68410, mimfg_prc_auc 0.88148, 104.75 secs\n",
      "\u001b[1m---- Epoch 12/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.61470, mimfg_phrcls_loss 0.61470, mimfg_prc_auc 0.90246, 281.55 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.66469, mimfg_prc_auc 0.88443, 104.33 secs\n",
      "\u001b[1m---- Epoch 13/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.60014, mimfg_phrcls_loss 0.60014, mimfg_prc_auc 0.90665, 281.51 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.64360, mimfg_prc_auc 0.88653, 104.12 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_mimfg_phrcls_loss+mimfg_prc_auc=0.7484.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.59966, mimfg_phrcls_loss 0.59966, mimfg_prc_auc 0.90653, 285.31 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.63857, mimfg_prc_auc 0.88658, 103.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_mimfg_phrcls_loss+mimfg_prc_auc=0.7493.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.66320, mimfg_phrcls_loss 0.66320, mimfg_prc_auc 0.88546, 283.76 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.71722, mimfg_prc_auc 0.87123, 103.02 secs\n",
      "\u001b[1m---- Epoch 16/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.61372, mimfg_phrcls_loss 0.61372, mimfg_prc_auc 0.90065, 283.60 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.66184, mimfg_prc_auc 0.88287, 102.95 secs\n",
      "\u001b[1m---- Epoch 17/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.58792, mimfg_phrcls_loss 0.58792, mimfg_prc_auc 0.90777, 282.99 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.66232, mimfg_prc_auc 0.88560, 102.83 secs\n",
      "\u001b[1m---- Epoch 18/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.60416, mimfg_phrcls_loss 0.60416, mimfg_prc_auc 0.90458, 282.58 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.64303, mimfg_prc_auc 0.88967, 102.99 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_mimfg_phrcls_loss+mimfg_prc_auc=0.7499.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.58610, mimfg_phrcls_loss 0.58610, mimfg_prc_auc 0.90995, 282.84 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.63732, mimfg_prc_auc 0.88871, 103.40 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_mimfg_phrcls_loss+mimfg_prc_auc=0.7508.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64363, mimfg_phrcls_loss 0.64363, mimfg_prc_auc 0.89112, 283.44 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.69612, mimfg_prc_auc 0.87871, 103.18 secs\n",
      "\u001b[1m---- Epoch 21/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.60253, mimfg_phrcls_loss 0.60253, mimfg_prc_auc 0.90390, 282.86 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.61983, mimfg_prc_auc 0.88812, 102.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_mimfg_phrcls_loss+mimfg_prc_auc=0.7533.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.59354, mimfg_phrcls_loss 0.59354, mimfg_prc_auc 0.90714, 281.02 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.63210, mimfg_prc_auc 0.88995, 102.21 secs\n",
      "\u001b[1m---- Epoch 23/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.59195, mimfg_phrcls_loss 0.59195, mimfg_prc_auc 0.90782, 281.75 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.61944, mimfg_prc_auc 0.89102, 102.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_mimfg_phrcls_loss+mimfg_prc_auc=0.7549.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.57410, mimfg_phrcls_loss 0.57410, mimfg_prc_auc 0.91264, 279.72 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.62442, mimfg_prc_auc 0.89239, 101.85 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_mimfg_phrcls_loss+mimfg_prc_auc=0.7550.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.62536, mimfg_phrcls_loss 0.62536, mimfg_prc_auc 0.89650, 287.66 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.67129, mimfg_prc_auc 0.87755, 102.18 secs\n",
      "\u001b[1m---- Epoch 26/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.59978, mimfg_phrcls_loss 0.59978, mimfg_prc_auc 0.90554, 287.58 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.63749, mimfg_prc_auc 0.89085, 104.29 secs\n",
      "\u001b[1m---- Epoch 27/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.57833, mimfg_phrcls_loss 0.57833, mimfg_prc_auc 0.91087, 286.04 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.62741, mimfg_prc_auc 0.89144, 103.03 secs\n",
      "\u001b[1m---- Epoch 28/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.57943, mimfg_phrcls_loss 0.57943, mimfg_prc_auc 0.91066, 284.55 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.63416, mimfg_prc_auc 0.88971, 103.22 secs\n",
      "\u001b[1m---- Epoch 29/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.56607, mimfg_phrcls_loss 0.56607, mimfg_prc_auc 0.91527, 281.68 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.62843, mimfg_prc_auc 0.89201, 103.31 secs\n",
      "\u001b[1m---- Epoch 30/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.61785, mimfg_phrcls_loss 0.61785, mimfg_prc_auc 0.89946, 285.17 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.69500, mimfg_prc_auc 0.88365, 104.23 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_phrase_grounding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_011909_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)\" \\\n",
    "--epochs 30 \\\n",
    "--batches_per_epoch 1002 \\\n",
    "--max_images_per_batch 12 \\\n",
    "--max_phrases_per_batch 1000 \\\n",
    "--max_phrases_per_image 30 \\\n",
    "--val_batch_size_factor 4 \\\n",
    "--raw_image_encoding \"convnextmodel-huggingface\" \\\n",
    "--huggingface_model_name \"facebook/convnext-small-224\" \\\n",
    "--img_aug_mode \"random-color-and-spatial\" \\\n",
    "--image_size 416 416 \\\n",
    "--image_local_feat_size 768 \\\n",
    "--num_regions 169 \\\n",
    "--regions_width 13 \\\n",
    "--regions_height 13 \\\n",
    "--phrase_embedding_size 128 \\\n",
    "--phrase_grounding_mode \"adaptive_film_based_pooling_mlp__no_grounding\" \\\n",
    "--visual_grounding_hidden_size 256 \\\n",
    "--phrase_mlp_hidden_dims 256 128 \\\n",
    "--num_train_workers 2 \\\n",
    "--num_val_workers 2 \\\n",
    "--gradient_accumulation_steps 3 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,5,1e-6,8e-5,5,1e-6\" \\\n",
    "--binary_multilabel_classif_loss_name \"focal+bce+npbbce\" \\\n",
    "--use_mimiccxr_facts_for_train \\\n",
    "--use_mimiccxr_facts_for_test \\\n",
    "--dicom_id_to_pos_neg_facts_filepath \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_pos_neg_facts_with_GPT4_mined_labels(nf=403416,nl=28,ngpt4=87528)(hash=741,3592138487644435704).pkl\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1478dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 50\n",
      "   batches_per_epoch: 1002\n",
      "   max_images_per_batch: 12\n",
      "   max_phrases_per_batch: 1000\n",
      "   max_phrases_per_image: 30\n",
      "   val_batch_size_factor: 4.0\n",
      "   checkpoint_folder: models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_folder_paths: None\n",
      "   freeze_image_encoder: False\n",
      "   raw_image_encoding: convnextmodel-huggingface\n",
      "   huggingface_model_name: facebook/convnext-small-224\n",
      "   num_regions: 169\n",
      "   image_local_feat_size: 768\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   image_encoder_dropout_p: 0\n",
      "   yolov8_model_name_or_path: None\n",
      "   yolov8_model_alias: None\n",
      "   phrase_embedding_size: 128\n",
      "   regions_width: 13\n",
      "   regions_height: 13\n",
      "   qkv_size: None\n",
      "   phrase_grounding_mode: adaptive_film_based_pooling_mlp__no_grounding\n",
      "   phrase_classifier_hidden_size: None\n",
      "   transf_d_model: None\n",
      "   transf_nhead: None\n",
      "   transf_dim_feedforward: None\n",
      "   transf_dropout: 0\n",
      "   transf_num_layers: None\n",
      "   visual_feature_proj_size: None\n",
      "   visual_grounding_hidden_size: 256\n",
      "   phrase_mlp_hidden_dims: [256, 128]\n",
      "   predict_global_alignment: False\n",
      "   alignment_proj_size: None\n",
      "   yolov11_model_name_or_path: None\n",
      "   yolov11_model_alias: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,5,1e-6,8e-5,5,1e-6\n",
      "   gradient_accumulation_steps: 3\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   attention_supervision_loss_weight: 1.0\n",
      "   phrase_classifier_loss_weight: 1.0\n",
      "   foreground_loss_weight: 1.0\n",
      "   background_loss_weight: 1.0\n",
      "   focal_loss_weight: 1.0\n",
      "   bce_loss_weight: 1.0\n",
      "   wbce_loss_weight: 1.0\n",
      "   binary_multilabel_classif_loss_name: focal+bce+npbbce\n",
      "   use_weighted_phrase_classifier_loss: False\n",
      "   cluster_and_label_weights_for_facts_filepath: None\n",
      "   use_attention_regularization_loss: False\n",
      "   use_contrastive_phrase_grounding_loss: False\n",
      "   nt_xent_temperature: 0.1\n",
      "   num_train_workers: 2\n",
      "   num_val_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   image_size: [416, 416]\n",
      "   dicom_id_to_pos_neg_facts_filepath: /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_pos_neg_facts_with_GPT4_mined_labels(nf=403416,nl=28,ngpt4=87528)(hash=741,3592138487644435704).pkl\n",
      "   iuxray_image_id_to_pos_neg_facts_filepath: None\n",
      "   mscxr_phrase2embedding_filepath: None\n",
      "   chest_imagenome_bbox_phrase_embeddings_filepath: None\n",
      "   vinbig_phrase_embeddings_filepath: None\n",
      "   chexlocalize_class_phrase_embeddings_filepath: None\n",
      "   chexpert_class_phrase_embeddings_filepath: None\n",
      "   mimiccxr_exclude_noisy_images: False\n",
      "   mimiccxr_facts_weight: 1.0\n",
      "   chest_imagenome_anatlocs_weight: 1.0\n",
      "   mscxr_weight: 1.0\n",
      "   cxrlt2024_weight: 1.0\n",
      "   vinbig_weight: 1.0\n",
      "   chexlocalize_weight: 1.0\n",
      "   chexpert_weight: 1.0\n",
      "   iuxray_weight: 1.0\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   pos_area_prior: 0.4\n",
      "   neg_area_prior: 0.0\n",
      "   use_mimiccxr_facts_for_train: True\n",
      "   use_mimiccxr_facts_for_test: True\n",
      "   use_mscxr_for_train: False\n",
      "   use_mscxr_for_test: False\n",
      "   use_chest_imagenome_for_train: False\n",
      "   use_chest_imagenome_gold_for_test: False\n",
      "   use_vinbig_for_train: False\n",
      "   use_vinbig_for_test: False\n",
      "   use_chexlocalize_for_train: False\n",
      "   use_chexlocalize_for_test: False\n",
      "   use_chexpert_for_train: False\n",
      "   use_chexpert_for_test: False\n",
      "   use_iuxray_for_train: False\n",
      "   use_iuxray_for_test: False\n",
      "   use_cxrlt2024_challenge_split: False\n",
      "   use_cxrlt2024_custom_labels: False\n",
      "   use_cxrlt2024_official_labels: False\n",
      "   use_all_cxrlt2024_official_labels_for_training: False\n",
      "   vinbig_training_data_mode: train\n",
      "   chexpert_training_data_mode: all\n",
      "   mimiccxr_balance_long_middle_short_tail: False\n",
      "   mimiccxr_long_middle_short_tail_thresholds: (0.02, 0.05)\n",
      "   mimiccxr_report_fact_nli_integrated_data_filepath: None\n",
      "   mimiccxr_use_interpret_cxr_challenge_split: False\n",
      "   mimiccxr_interpret_cxr_challenge_split_filepath: None\n",
      "   iuxray_use_interpret_cxr_challenge_split: False\n",
      "   iuxray_interpret_cxr_challenge_split_filepath: None\n",
      "   chexpert_use_interpret_cxr_challenge_split: False\n",
      "   chexpert_interpret_cxr_challenge_split_filepath: None\n",
      "   chexlocalize_use_interpret_cxr_challenge_split: False\n",
      "   chexlocalize_interpret_cxr_challenge_split_filepath: None\n",
      "   cxrlt2024_custom_dicom_id_to_pos_neg_facts_filepath: None\n",
      "   cxrlt2024_official_training_labels_for_fact_classification_filepath: None\n",
      "   cxrlt2024_do_balanced_sampling: False\n",
      "   do_visual_grounding_with_bbox_regression: False\n",
      "   do_visual_grounding_with_segmentation: False\n",
      "   replace_phrase_embeddings_with_random_vectors: False\n",
      "   use_vinbig_with_modified_labels: False\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Resuming training ------\u001b[0m\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of PhraseGrounder ...\u001b[0m\n",
      "\u001b[93mWARNING: Unused kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_011909_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)', 'pretrained_checkpoint_folder_paths': None}\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: convnextmodel-huggingface\n",
      "  self.global_feat_size = 768\n",
      "  self.local_feat_size = 768\n",
      "  Initializing auxiliary tasks\n",
      "\u001b[93mUsing positional encoding for local features\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,5,1e-6,8e-5,5,1e-6\n",
      "1e-06 3 8e-05 5 1e-06 8e-05 5 1e-06\n",
      "self.steps_to_restart = 5\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_WBCBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbcbce_weight = 0.3333333333333333\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 3, max_grad_norm = None\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_NPBBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 npbbce_weight = 0.3333333333333333\n",
      "Focal_BCE_WBCBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbcbce_weight = 0.3333333333333333\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating MIMIC-CXR Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  image_size = [416, 416]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [416, 416]\n",
      "get_train_transform(): Using normal transforms\n",
      "default_prob = 0.35\n",
      "Returning augmented transforms with mode random-color-and-spatial\n",
      "get_image_transform()\n",
      "  image_size = [416, 416]\n",
      "  mean = [0.485, 0.456, 0.406]\n",
      "  std = [0.229, 0.224, 0.225]\n",
      "Using standard transform (only images, no bounding boxes, no masks)\n",
      "mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225], image_size = [416, 416]\n",
      "Returning transform without augmentation\n",
      "\u001b[93m\u001b[1mWarning: unused kwargs in MIMICCXR_VisualModuleTrainer: {'use_yolov8': False}\u001b[0m\n",
      "len(forbidden_train_dicom_ids) = 0\n",
      "\u001b[1m\u001b[35mPreparing MIMIC-CXR-Facts datasets and dataloaders for training/testing...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mNOTE: Using strong and weak negatives for training...\u001b[0m\n",
      "fact_embeddings.shape = (403416, 128)\n",
      "Using image size mode: medium_512\n",
      "227835it [00:01, 143596.21it/s]\n",
      "Total number of images: 377110\n",
      "len(train_indices) = 368960\n",
      "len(test_indices) = 8150\n",
      "len(set(train_indices) & set(test_indices)) = 0\n",
      "avg_facts_per_image = 508.52087218126627\n",
      "train_num_facts_per_image = 30\n",
      "avg_facts_per_image = 512.7754601226994\n",
      "test_num_facts_per_image = 30\n",
      "\u001b[1mBuilding train fact dataloader...\u001b[0m\n",
      "batch_size = 12\n",
      "Normal (unbalanced) training...\n",
      "len(self.train_fact_dataloader) = 30747\n",
      "\u001b[1mBuilding test fact dataloaders...\u001b[0m\n",
      "len(self.test_fact_dataloader) = 170\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(mimiccxr_trainer.train_fact_dataloader) = 30747\n",
      "len(mimiccxr_trainer.test_fact_dataloader) = 170\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim-facts\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_24_mimfg_phrcls_loss+mimfg_prc_auc=0.7550.pt', 'checkpoint_63_mimfg_phrcls_loss+mimfg_prc_auc=0.7714.pt']\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)/checkpoint_63_mimfg_phrcls_loss+mimfg_prc_auc=0.7714.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 64/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.51493, mimfg_phrcls_loss 0.51493, mimfg_prc_auc 0.92560, 273.28 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.57753, mimfg_prc_auc 0.90279, 103.56 secs\n",
      "\u001b[93mTrain metrics:\u001b[0m\n",
      "\u001b[93mmimfg_phrcls_loss: 0.6601, weight = 1.0\u001b[0m\n",
      "\u001b[93mmimfg_prc_auc: 0.9256, weight = 1.0\u001b[0m\n",
      "\u001b[93mnum = 1.5857, den = 2.0000, score = 0.7928\u001b[0m\n",
      "\u001b[93mVal metrics:\u001b[0m\n",
      "\u001b[93mmimfg_phrcls_loss: 0.6339, weight = 1.0\u001b[0m\n",
      "\u001b[93mmimfg_prc_auc: 0.9028, weight = 1.0\u001b[0m\n",
      "\u001b[93mnum = 1.5367, den = 2.0000, score = 0.7683\u001b[0m\n",
      "\u001b[93mTrain score = 0.7928, Val score = 0.7683, Final score = 0.7696\u001b[0m\n",
      "\u001b[93mw_train = 0.05, w_val = 0.95\u001b[0m\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_mimfg_phrcls_loss+mimfg_prc_auc=0.7696.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.56382, mimfg_phrcls_loss 0.56382, mimfg_prc_auc 0.91263, 286.07 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.65558, mimfg_prc_auc 0.88727, 105.59 secs\n",
      "\u001b[1m---- Epoch 66/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.53932, mimfg_phrcls_loss 0.53932, mimfg_prc_auc 0.92135, 290.82 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.60881, mimfg_prc_auc 0.90105, 105.86 secs\n",
      "\u001b[1m---- Epoch 67/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.51739, mimfg_phrcls_loss 0.51739, mimfg_prc_auc 0.92551, 293.47 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.57945, mimfg_prc_auc 0.90097, 106.60 secs\n",
      "\u001b[1m---- Epoch 68/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.51363, mimfg_phrcls_loss 0.51363, mimfg_prc_auc 0.92692, 295.26 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.56259, mimfg_prc_auc 0.90528, 107.53 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_68_mimfg_phrcls_loss+mimfg_prc_auc=0.7737.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 69/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.50859, mimfg_phrcls_loss 0.50859, mimfg_prc_auc 0.92810, 296.25 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.57742, mimfg_prc_auc 0.90210, 107.34 secs\n",
      "\u001b[1m---- Epoch 70/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.55256, mimfg_phrcls_loss 0.55256, mimfg_prc_auc 0.91398, 297.10 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.61693, mimfg_prc_auc 0.90039, 108.17 secs\n",
      "\u001b[1m---- Epoch 71/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.54365, mimfg_phrcls_loss 0.54365, mimfg_prc_auc 0.91814, 299.63 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.58298, mimfg_prc_auc 0.90405, 108.65 secs\n",
      "\u001b[1m---- Epoch 72/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.51907, mimfg_phrcls_loss 0.51907, mimfg_prc_auc 0.92520, 301.27 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.58603, mimfg_prc_auc 0.90310, 108.78 secs\n",
      "\u001b[1m---- Epoch 73/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50037, mimfg_phrcls_loss 0.50037, mimfg_prc_auc 0.93016, 302.50 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.57237, mimfg_prc_auc 0.90368, 109.24 secs\n",
      "\u001b[1m---- Epoch 74/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.50090, mimfg_phrcls_loss 0.50090, mimfg_prc_auc 0.93116, 302.96 secs\n",
      "(2) Validation stage ...\n",
      "mimfg_phrcls_loss 0.56402, mimfg_prc_auc 0.90503, 110.64 secs\n",
      "\u001b[1m---- Epoch 75/113\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "^C iteration 11300\n"
     ]
    }
   ],
   "source": [
    "!python ../train_phrase_grounding.py \\\n",
    "--checkpoint_folder \\\n",
    "\"models/phrase_grounding/20250215_042300_mim-facts_PhraseGrounder(facebook-convnext-small-224,AdaptiveFiLM_MLP,128,256,256-128)\" \\\n",
    "--epochs 50 \\\n",
    "--batches_per_epoch 1002 \\\n",
    "--max_images_per_batch 12 \\\n",
    "--max_phrases_per_batch 1000 \\\n",
    "--max_phrases_per_image 30 \\\n",
    "--val_batch_size_factor 4 \\\n",
    "--raw_image_encoding \"convnextmodel-huggingface\" \\\n",
    "--huggingface_model_name \"facebook/convnext-small-224\" \\\n",
    "--img_aug_mode \"random-color-and-spatial\" \\\n",
    "--image_size 416 416 \\\n",
    "--image_local_feat_size 768 \\\n",
    "--num_regions 169 \\\n",
    "--regions_width 13 \\\n",
    "--regions_height 13 \\\n",
    "--phrase_embedding_size 128 \\\n",
    "--phrase_grounding_mode \"adaptive_film_based_pooling_mlp__no_grounding\" \\\n",
    "--visual_grounding_hidden_size 256 \\\n",
    "--phrase_mlp_hidden_dims 256 128 \\\n",
    "--num_train_workers 2 \\\n",
    "--num_val_workers 2 \\\n",
    "--gradient_accumulation_steps 3 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,5,1e-6,8e-5,5,1e-6\" \\\n",
    "--binary_multilabel_classif_loss_name \"focal+bce+npbbce\" \\\n",
    "--use_mimiccxr_facts_for_train \\\n",
    "--use_mimiccxr_facts_for_test \\\n",
    "--dicom_id_to_pos_neg_facts_filepath \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr_dicom_id_to_pos_neg_facts_with_GPT4_mined_labels(nf=403416,nl=28,ngpt4=87528)(hash=741,3592138487644435704).pkl\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
