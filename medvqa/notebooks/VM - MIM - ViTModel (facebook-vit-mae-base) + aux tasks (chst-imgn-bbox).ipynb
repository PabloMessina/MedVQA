{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 30\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: None\n",
      "   mimiccxr_qa_adapted_reports_filename: qa_adapted_reports__20220904_095810.json\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: vitmodel-huggingface\n",
      "   image_local_feat_size: 768\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: CenIA/vit-mae-base-finetuned-mimic\n",
      "   chest_imagenome_bbox_hidden_size: 128\n",
      "   chest_imagenome_bbox_regressor_version: v2\n",
      "   torchxrayvision_weights_name: None\n",
      "   num_regions: 197\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,2e-4,25,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 40\n",
      "   iters_to_accumulate: 3\n",
      "   num_workers: 0\n",
      "   device: GPU\n",
      "   img_aug_mode: None\n",
      "   image_size: [224, 224]\n",
      "   mimiccxr_weight: 1.0\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_view_mode: chest_imagenome\n",
      "   chest_imagenome_labels_filename: None\n",
      "   chest_imagenome_label_names_filename: None\n",
      "   use_amp: True\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230209_000803_mim+mim(chex)_oevqa(CenIA-vit-mae-base-ft-mimic+onehot+transf)_visenc-pretr=0_dws=1.0,1.0_medtok_chx_amp\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: True\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   train_cxr14: False\n",
      "   train_vinbig: False\n",
      "   vinbig_training_data_mode: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: bce\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_gender: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_chest_imagenome: False\n",
      "   predict_bboxes_chest_imagenome: True\n",
      "   clamp_bboxes_chest_imagenome: True\n",
      "   chest_imagenome_bbox_loss_weight: 1.0\n",
      "   classify_questions: False\n",
      "   n_mined_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/chest_imagenome_train_average_bbox_coords(qa_adapted_reports__20220904_095810.json,clamped.pkl...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of MultiPurposeVisualModule ...\u001b[0m\n",
      "  self.global_feat_size = 768\n",
      "BoundingBoxRegressor_v2:\n",
      "  local_feat_dim: 768\n",
      "  global_feat_dim: 768\n",
      "  hidden_dim: 128\n",
      "  num_classes: 36\n",
      "  num_regions: 197\n",
      "MultiPurposeVisualModule: self.name=CenIA/vit-mae-base-ft-mimic\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,2e-4,25,1e-6\n",
      "1e-06 5 0.0002 25 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "get_engine(): shift_answer=False\n",
      "chest_imagenome_bbox_loss_weight:  1.0\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mCreating MIMIC-CXR visual module trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "Using Huggingface ViT model transform for CenIA/vit-mae-base-finetuned-mimic\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "get_image_transform()\n",
      "Using Huggingface ViT model transform for CenIA/vit-mae-base-finetuned-mimic\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "Loading cached detailed metadata from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/qa_adapted_reports__20220904_095810.json__detailed_metadata.pkl\n",
      "Loaded 240530 non-gold DICOM IDs from Chest Imagenome\n",
      "227835it [00:00, 471579.77it/s]\n",
      "len(self.train_indices) = 235209\n",
      "len(self.val_indices) = 1945\n",
      "Loading Chest Imagenome bounding boxes...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = mim\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230217_040800_mim_CenIA-vit-mae-base-ft-mimic\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230217_040800_mim_CenIA-vit-mae-base-ft-mimic/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_39_chf1+chf1+chf1+chf1+chf1+cD+ema+wmmp=0.4496.pt', 'checkpoint_59_chf1+chf1+chf1+chf1+chf1+cD+ema+wmmp=0.4896.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230209_000803_mim+mim(chex)_oevqa(CenIA-vit-mae-base-ft-mimic+onehot+transf)_visenc-pretr=0_dws=1.0,1.0_medtok_chx_amp/checkpoint_59_chf1+chf1+chf1+chf1+chf1+cD+ema+wmmp=0.4896.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/visual_module/20230217_040800_mim_CenIA-vit-mae-base-ft-mimic/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.78736, chestimgbbmf1 0.01977, chestimgbb_loss 0.56033, chestimgbbiou 0.65636, chestimgbbmae 0.14024, 137.28 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.03887, chestimgbbiou 0.71388, chestimgbbmae 0.11125, 20.59 secs\n",
      "Adjusting learning rate of group 0 to 2.8854e-06.\n",
      "\u001b[1m---- Epoch 2/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.39369, chestimgbbmf1 0.06106, chestimgbb_loss 0.18097, chestimgbbiou 0.74379, chestimgbbmae 0.09729, 140.08 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.08791, chestimgbbiou 0.76307, chestimgbbmae 0.08904, 20.62 secs\n",
      "Adjusting learning rate of group 0 to 8.3255e-06.\n",
      "\u001b[1m---- Epoch 3/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.05264, chestimgbbmf1 0.19776, chestimgbb_loss 0.04456, chestimgbbiou 0.80712, chestimgbbmae 0.07016, 140.02 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.31126, chestimgbbiou 0.83330, chestimgbbmae 0.06001, 21.13 secs\n",
      "Adjusting learning rate of group 0 to 2.4022e-05.\n",
      "\u001b[1m---- Epoch 4/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.03843, chestimgbbmf1 0.33636, chestimgbb_loss 0.03203, chestimgbbiou 0.83897, chestimgbbmae 0.05731, 141.90 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.40552, chestimgbbiou 0.84982, chestimgbbmae 0.05348, 20.72 secs\n",
      "Adjusting learning rate of group 0 to 6.9314e-05.\n",
      "\u001b[1m---- Epoch 5/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000069) ...\n",
      "loss 0.02181, chestimgbbmf1 0.40873, chestimgbb_loss 0.03142, chestimgbbiou 0.83681, chestimgbbmae 0.05908, 142.83 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.51391, chestimgbbiou 0.86510, chestimgbbmae 0.04772, 21.69 secs\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "\u001b[1m---- Epoch 6/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.03390, chestimgbbmf1 0.37838, chestimgbb_loss 0.05730, chestimgbbiou 0.80420, chestimgbbmae 0.07606, 140.79 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.34550, chestimgbbiou 0.82963, chestimgbbmae 0.06196, 20.74 secs\n",
      "Adjusting learning rate of group 0 to 1.6180e-04.\n",
      "\u001b[1m---- Epoch 7/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000162) ...\n",
      "loss 0.06697, chestimgbbmf1 0.70146, chestimgbb_loss 0.03246, chestimgbbiou 0.88748, chestimgbbmae 0.03932, 143.58 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.77602, chestimgbbiou 0.89615, chestimgbbmae 0.03650, 21.02 secs\n",
      "Adjusting learning rate of group 0 to 1.3090e-04.\n",
      "\u001b[1m---- Epoch 8/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000131) ...\n",
      "loss 0.01132, chestimgbbmf1 0.79327, chestimgbb_loss 0.02812, chestimgbbiou 0.90138, chestimgbbmae 0.03422, 140.70 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.78484, chestimgbbiou 0.89761, chestimgbbmae 0.03593, 20.65 secs\n",
      "Adjusting learning rate of group 0 to 1.0590e-04.\n",
      "\u001b[1m---- Epoch 9/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000106) ...\n",
      "loss 0.02218, chestimgbbmf1 0.79260, chestimgbb_loss 0.02662, chestimgbbiou 0.90067, chestimgbbmae 0.03451, 142.70 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.79815, chestimgbbiou 0.90010, chestimgbbmae 0.03490, 20.69 secs\n",
      "Adjusting learning rate of group 0 to 8.5677e-05.\n",
      "\u001b[1m---- Epoch 10/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000086) ...\n",
      "loss 0.02484, chestimgbbmf1 0.81963, chestimgbb_loss 0.02199, chestimgbbiou 0.90582, chestimgbbmae 0.03249, 140.54 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.81759, chestimgbbiou 0.90351, chestimgbbmae 0.03376, 20.72 secs\n",
      "Adjusting learning rate of group 0 to 6.9314e-05.\n",
      "\u001b[1m---- Epoch 11/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000069) ...\n",
      "loss 0.04745, chestimgbbmf1 0.83315, chestimgbb_loss 0.02246, chestimgbbiou 0.90795, chestimgbbmae 0.03186, 143.17 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.81737, chestimgbbiou 0.90370, chestimgbbmae 0.03369, 20.92 secs\n",
      "Adjusting learning rate of group 0 to 5.6077e-05.\n",
      "\u001b[1m---- Epoch 12/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.04950, chestimgbbmf1 0.83905, chestimgbb_loss 0.02165, chestimgbbiou 0.90959, chestimgbbmae 0.03118, 140.94 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.82978, chestimgbbiou 0.90617, chestimgbbmae 0.03274, 20.74 secs\n",
      "Adjusting learning rate of group 0 to 4.5367e-05.\n",
      "\u001b[1m---- Epoch 13/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000045) ...\n",
      "loss 0.02624, chestimgbbmf1 0.84646, chestimgbb_loss 0.02152, chestimgbbiou 0.91130, chestimgbbmae 0.03062, 142.60 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.84306, chestimgbbiou 0.90880, chestimgbbmae 0.03183, 20.85 secs\n",
      "Adjusting learning rate of group 0 to 3.6703e-05.\n",
      "\u001b[1m---- Epoch 14/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000037) ...\n",
      "loss 0.01223, chestimgbbmf1 0.85656, chestimgbb_loss 0.02089, chestimgbbiou 0.91300, chestimgbbmae 0.03003, 141.23 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.83899, chestimgbbiou 0.90803, chestimgbbmae 0.03200, 20.75 secs\n",
      "Adjusting learning rate of group 0 to 2.9693e-05.\n",
      "\u001b[1m---- Epoch 15/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 0.01558, chestimgbbmf1 0.86042, chestimgbb_loss 0.01943, chestimgbbiou 0.91416, chestimgbbmae 0.02956, 142.11 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.84439, chestimgbbiou 0.90936, chestimgbbmae 0.03154, 20.86 secs\n",
      "Adjusting learning rate of group 0 to 2.4022e-05.\n",
      "\u001b[1m---- Epoch 16/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n",
      "loss 0.00788, chestimgbbmf1 0.86519, chestimgbb_loss 0.01948, chestimgbbiou 0.91479, chestimgbbmae 0.02938, 141.09 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85380, chestimgbbiou 0.91159, chestimgbbmae 0.03073, 22.75 secs\n",
      "Adjusting learning rate of group 0 to 1.9435e-05.\n",
      "\u001b[1m---- Epoch 17/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.00728, chestimgbbmf1 0.86876, chestimgbb_loss 0.02010, chestimgbbiou 0.91593, chestimgbbmae 0.02897, 141.72 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85535, chestimgbbiou 0.91188, chestimgbbmae 0.03062, 20.67 secs\n",
      "Adjusting learning rate of group 0 to 1.5723e-05.\n",
      "\u001b[1m---- Epoch 18/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.05318, chestimgbbmf1 0.86988, chestimgbb_loss 0.01897, chestimgbbiou 0.91645, chestimgbbmae 0.02876, 142.32 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85866, chestimgbbiou 0.91249, chestimgbbmae 0.03046, 21.37 secs\n",
      "Adjusting learning rate of group 0 to 1.2720e-05.\n",
      "\u001b[1m---- Epoch 19/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.01015, chestimgbbmf1 0.87141, chestimgbb_loss 0.01924, chestimgbbiou 0.91656, chestimgbbmae 0.02872, 140.82 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85226, chestimgbbiou 0.91100, chestimgbbmae 0.03098, 20.81 secs\n",
      "Adjusting learning rate of group 0 to 1.0291e-05.\n",
      "\u001b[1m---- Epoch 20/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.02824, chestimgbbmf1 0.87397, chestimgbb_loss 0.01753, chestimgbbiou 0.91791, chestimgbbmae 0.02818, 142.32 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86052, chestimgbbiou 0.91307, chestimgbbmae 0.03023, 20.75 secs\n",
      "Adjusting learning rate of group 0 to 8.3255e-06.\n",
      "\u001b[1m---- Epoch 21/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.04775, chestimgbbmf1 0.87522, chestimgbb_loss 0.01698, chestimgbbiou 0.91806, chestimgbbmae 0.02809, 137.57 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86161, chestimgbbiou 0.91340, chestimgbbmae 0.03013, 20.27 secs\n",
      "Adjusting learning rate of group 0 to 6.7355e-06.\n",
      "\u001b[1m---- Epoch 22/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.02164, chestimgbbmf1 0.87624, chestimgbb_loss 0.01762, chestimgbbiou 0.91773, chestimgbbmae 0.02830, 137.40 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85839, chestimgbbiou 0.91264, chestimgbbmae 0.03038, 20.27 secs\n",
      "Adjusting learning rate of group 0 to 5.4492e-06.\n",
      "\u001b[1m---- Epoch 23/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.02645, chestimgbbmf1 0.87624, chestimgbb_loss 0.01703, chestimgbbiou 0.91857, chestimgbbmae 0.02788, 138.23 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.85735, chestimgbbiou 0.91234, chestimgbbmae 0.03048, 20.26 secs\n",
      "Adjusting learning rate of group 0 to 4.4085e-06.\n",
      "\u001b[1m---- Epoch 24/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.00583, chestimgbbmf1 0.87756, chestimgbb_loss 0.01783, chestimgbbiou 0.91837, chestimgbbmae 0.02808, 137.26 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86056, chestimgbbiou 0.91308, chestimgbbmae 0.03022, 20.26 secs\n",
      "Adjusting learning rate of group 0 to 3.5665e-06.\n",
      "\u001b[1m---- Epoch 25/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.01802, chestimgbbmf1 0.87760, chestimgbb_loss 0.01778, chestimgbbiou 0.91851, chestimgbbmae 0.02799, 137.42 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86476, chestimgbbiou 0.91428, chestimgbbmae 0.02980, 20.29 secs\n",
      "Adjusting learning rate of group 0 to 2.8854e-06.\n",
      "\u001b[1m---- Epoch 26/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.00348, chestimgbbmf1 0.87783, chestimgbb_loss 0.01799, chestimgbbiou 0.91843, chestimgbbmae 0.02802, 137.27 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86223, chestimgbbiou 0.91366, chestimgbbmae 0.03001, 20.25 secs\n",
      "Adjusting learning rate of group 0 to 2.3343e-06.\n",
      "\u001b[1m---- Epoch 27/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.01311, chestimgbbmf1 0.87941, chestimgbb_loss 0.01750, chestimgbbiou 0.91889, chestimgbbmae 0.02784, 137.10 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86301, chestimgbbiou 0.91397, chestimgbbmae 0.02991, 20.44 secs\n",
      "Adjusting learning rate of group 0 to 1.8885e-06.\n",
      "\u001b[1m---- Epoch 28/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.01750, chestimgbbmf1 0.88226, chestimgbb_loss 0.01728, chestimgbbiou 0.91967, chestimgbbmae 0.02756, 137.34 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86335, chestimgbbiou 0.91389, chestimgbbmae 0.02993, 20.26 secs\n",
      "Adjusting learning rate of group 0 to 1.5279e-06.\n",
      "\u001b[1m---- Epoch 29/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.01114, chestimgbbmf1 0.87941, chestimgbb_loss 0.01661, chestimgbbiou 0.91894, chestimgbbmae 0.02780, 137.10 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86520, chestimgbbiou 0.91450, chestimgbbmae 0.02972, 20.25 secs\n",
      "Adjusting learning rate of group 0 to 1.2361e-06.\n",
      "\u001b[1m---- Epoch 30/30\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.02375, chestimgbbmf1 0.87922, chestimgbb_loss 0.01701, chestimgbbiou 0.91907, chestimgbbmae 0.02775, 137.39 secs\n",
      "(2) Validation stage ...\n",
      "chestimgbbmf1 0.86484, chestimgbbiou 0.91427, chestimgbbmae 0.02979, 20.29 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_visual_module.py \\\n",
    "        --pretrained-checkpoint-folder-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230209_000803_mim+mim(chex)_oevqa(CenIA-vit-mae-base-ft-mimic+onehot+transf)_visenc-pretr=0_dws=1.0,1.0_medtok_chx_amp\" \\\n",
    "        --epochs 30 \\\n",
    "        --batches-per-epoch 300 \\\n",
    "        --batch-size 40 \\\n",
    "        --num-workers 0 \\\n",
    "        --iters-to-accumulate 3 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,2e-4,25,1e-6\" \\\n",
    "        --use-mimiccxr \\\n",
    "        --mimiccxr-weight 1.0 \\\n",
    "        --mimiccxr-qa-adapted-reports-filename \"qa_adapted_reports__20220904_095810.json\" \\\n",
    "        --mimiccxr-view-mode \"chest_imagenome\" \\\n",
    "        --predict-bboxes-chest-imagenome \\\n",
    "        --clamp-bboxes-chest-imagenome \\\n",
    "        --chest-imagenome-bbox-regressor-version \"v2\" \\\n",
    "        --raw-image-encoding \"vitmodel-huggingface\" \\\n",
    "        --huggingface-model-name \"CenIA/vit-mae-base-finetuned-mimic\" \\\n",
    "        --image-size 224 224 \\\n",
    "        --image-local-feat-size 768 \\\n",
    "        --num-regions 197 \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
