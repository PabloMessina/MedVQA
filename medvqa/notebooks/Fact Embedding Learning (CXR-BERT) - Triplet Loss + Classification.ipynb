{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c052a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 60\n",
      "   batches_per_epoch: 300\n",
      "   batch_size: 200\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 4\n",
      "   override_lr: False\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(1605733,297669,1315702,4001000,20002000,1000,2000).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 1.0, 1.0], 'observations': [1.0, 1.0, 1.0, 1.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).improved_comparison(6526297).jsonl\n",
      "   paraphrases_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl']\n",
      "   dataset_name: MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\n",
      "   triplets_weight: 1.0\n",
      "   classification_weight: 1.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: True\n",
      "  n_categories: 6\n",
      "  classify_health_status: True\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: True\n",
      "  n_comparison_statuses: 15\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(1605733,297669,1315702,4001000,20002000,1000,2000).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1504492\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1493944\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1315582\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 5382918\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 5372563\n",
      "\tWeight: 1.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4440269\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 4909633\n",
      "\tWeight: 1.0\n",
      "----\n",
      "\u001b[1mBuilding train classification dataset and dataloader...\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "Number of sentences with paraphrases: 59901\n",
      "Number of paraphrases: 599166\n",
      "Loading integrated facts metadata...\n",
      "Number of facts: 1455640\n",
      "Number of improved comparisons: 521435/578733\n",
      "Category: anatomical finding -> 1015229\n",
      "Category: tubes and lines -> 179581\n",
      "Category: technical assessment -> 97422\n",
      "Category: disease -> 82909\n",
      "Category: device -> 78769\n",
      "Category: other -> 1730\n",
      "Health status: abnormal -> 830476\n",
      "Health status: unknown -> 365562\n",
      "Health status: normal -> 168102\n",
      "Health status: ambiguous -> 90968\n",
      "Health status: other -> 532\n",
      "Comparison status: no comparison -> 821179\n",
      "Comparison status: stable/unchanged -> 161806\n",
      "Comparison status: worsened -> 78594\n",
      "Comparison status: improved -> 61567\n",
      "Comparison status: resolved -> 58491\n",
      "Comparison status: new finding -> 54389\n",
      "Comparison status: position changed -> 49909\n",
      "Comparison status: unclear comparison -> 48953\n",
      "Comparison status: increase -> 36937\n",
      "Comparison status: decrease -> 24308\n",
      "Comparison status: progressed -> 19529\n",
      "Comparison status: smaller -> 15232\n",
      "Comparison status: larger -> 14403\n",
      "Comparison status: reappeared -> 10084\n",
      "Comparison status: other -> 259\n",
      "\u001b[1mExample fact: no focal infiltrate in the remaining aerated lung\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: normal\n",
      "Comparison status: stable/unchanged\n",
      "\u001b[1mExample fact: thickening of the wall around the bronchi without clear focal consolidation\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: opacification shows no difference\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: stable/unchanged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison status: no comparison\n",
      "\tNumber of samples: 821179\n",
      "\tWeight: 7584.223051637783\n",
      "Comparison status: worsened\n",
      "\tNumber of samples: 78594\n",
      "\tWeight: 4300.633270502103\n",
      "Comparison status: resolved\n",
      "\tNumber of samples: 58491\n",
      "\tWeight: 3971.2797023757703\n",
      "Comparison status: stable/unchanged\n",
      "\tNumber of samples: 161806\n",
      "\tWeight: 5181.224495202725\n",
      "Comparison status: progressed\n",
      "\tNumber of samples: 19529\n",
      "\tWeight: 2895.6699714847914\n",
      "Comparison status: larger\n",
      "\tNumber of samples: 14403\n",
      "\tWeight: 2636.1253812589207\n",
      "Comparison status: improved\n",
      "\tNumber of samples: 61567\n",
      "\tWeight: 4027.1690868518713\n",
      "Comparison status: reappeared\n",
      "\tNumber of samples: 10084\n",
      "\tWeight: 2352.5204683090246\n",
      "Comparison status: smaller\n",
      "\tNumber of samples: 15232\n",
      "\tWeight: 2682.6163398845774\n",
      "Comparison status: position changed\n",
      "\tNumber of samples: 49909\n",
      "\tWeight: 3801.537897984162\n",
      "Comparison status: new finding\n",
      "\tNumber of samples: 54389\n",
      "\tWeight: 3892.8821154862517\n",
      "Comparison status: unclear comparison\n",
      "\tNumber of samples: 48953\n",
      "\tWeight: 3781.184791746964\n",
      "Comparison status: increase\n",
      "\tNumber of samples: 36937\n",
      "\tWeight: 3492.9744015057554\n",
      "Comparison status: decrease\n",
      "\tNumber of samples: 24308\n",
      "\tWeight: 3092.4456077564887\n",
      "Comparison status: other\n",
      "\tNumber of samples: 259\n",
      "\tWeight: 515.2339764293102\n",
      "----\n",
      "\u001b[1mBuilding val dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 334\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 334\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 334\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob3\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_121846_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_121846_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_121846_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 3.30874, triplet_loss 0.51524, c_loss 1.78307, hs_loss 1.61036, cs_loss 2.70880, cacc 0.30677, hsacc 0.09180, csacc 0.09557, 46.92 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.93114, tacc(al1) 0.63174, tacc(al2) 0.99401, tacc(ob0) 0.97800, tacc(ob1) 0.73800, tacc(ob2) 1.00000, tacc(ob3) 0.78800, 2.02 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.7957.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 3.26763, triplet_loss 0.48881, c_loss 1.74758, hs_loss 1.59622, cs_loss 2.70265, cacc 0.41303, hsacc 0.10367, csacc 0.11043, 45.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96707, tacc(al1) 0.80838, tacc(al2) 0.93413, tacc(ob0) 0.98200, tacc(ob1) 0.88000, tacc(ob2) 0.95400, tacc(ob3) 0.88800, 2.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8455.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 3.14299, triplet_loss 0.47320, c_loss 1.59204, hs_loss 1.54192, cs_loss 2.67882, cacc 0.68550, hsacc 0.27730, csacc 0.16237, 45.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.85629, tacc(al2) 0.86527, tacc(ob0) 0.97800, tacc(ob1) 0.91600, tacc(ob2) 0.86000, tacc(ob3) 0.95400, 2.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8622.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 2.90082, triplet_loss 0.46340, c_loss 1.38650, hs_loss 1.34997, cs_loss 2.60178, cacc 0.81117, hsacc 0.73477, csacc 0.26757, 46.18 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.88323, tacc(al2) 0.85030, tacc(ob0) 0.96400, tacc(ob1) 0.90600, tacc(ob2) 0.83200, tacc(ob3) 0.93800, 2.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8776.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 2.72651, triplet_loss 0.44969, c_loss 1.27734, hs_loss 1.23100, cs_loss 2.49500, cacc 0.81467, hsacc 0.78547, csacc 0.41340, 46.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.87425, tacc(al2) 0.84132, tacc(ob0) 0.95600, tacc(ob1) 0.91200, tacc(ob2) 0.81800, tacc(ob3) 0.94400, 2.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8807.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 2.65200, triplet_loss 0.44628, c_loss 1.23170, hs_loss 1.17821, cs_loss 2.44781, cacc 0.82463, hsacc 0.80477, csacc 0.44623, 46.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.89820, tacc(al2) 0.84431, tacc(ob0) 0.96400, tacc(ob1) 0.92200, tacc(ob2) 0.81400, tacc(ob3) 0.93800, 2.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8876.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 2.61674, triplet_loss 0.44323, c_loss 1.20662, hs_loss 1.16190, cs_loss 2.42173, cacc 0.82963, hsacc 0.81067, csacc 0.46317, 45.81 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.89521, tacc(al2) 0.85629, tacc(ob0) 0.97600, tacc(ob1) 0.91800, tacc(ob2) 0.80200, tacc(ob3) 0.94600, 2.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8902.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 2.59577, triplet_loss 0.44197, c_loss 1.19765, hs_loss 1.14413, cs_loss 2.40778, cacc 0.81963, hsacc 0.81453, csacc 0.47257, 46.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.90120, tacc(al2) 0.85629, tacc(ob0) 0.97000, tacc(ob1) 0.92200, tacc(ob2) 0.81000, tacc(ob3) 0.94800, 2.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8917.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.58949, triplet_loss 0.43914, c_loss 1.19889, hs_loss 1.13891, cs_loss 2.40204, cacc 0.82560, hsacc 0.81913, csacc 0.47167, 46.64 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.89820, tacc(al2) 0.85030, tacc(ob0) 0.97000, tacc(ob1) 0.92200, tacc(ob2) 0.81200, tacc(ob3) 0.94200, 2.27 secs\n",
      "\u001b[1m---- Epoch 10/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.58041, triplet_loss 0.43944, c_loss 1.18628, hs_loss 1.13947, cs_loss 2.39564, cacc 0.82667, hsacc 0.81717, csacc 0.47823, 45.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.85329, tacc(ob0) 0.97000, tacc(ob1) 0.92400, tacc(ob2) 0.81200, tacc(ob3) 0.94400, 2.33 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8922.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.57875, triplet_loss 0.44064, c_loss 1.18437, hs_loss 1.13794, cs_loss 2.39455, cacc 0.82403, hsacc 0.81977, csacc 0.47980, 46.83 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.85329, tacc(ob0) 0.96800, tacc(ob1) 0.92400, tacc(ob2) 0.81200, tacc(ob3) 0.94400, 2.31 secs\n",
      "\u001b[1m---- Epoch 12/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.57977, triplet_loss 0.43893, c_loss 1.18934, hs_loss 1.13657, cs_loss 2.39469, cacc 0.82183, hsacc 0.81307, csacc 0.47643, 47.06 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.85329, tacc(ob0) 0.97000, tacc(ob1) 0.92200, tacc(ob2) 0.81800, tacc(ob3) 0.94200, 2.36 secs\n",
      "\u001b[1m---- Epoch 13/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 2.55381, triplet_loss 0.44877, c_loss 1.15979, hs_loss 1.12649, cs_loss 2.37257, cacc 0.81673, hsacc 0.79703, csacc 0.46553, 46.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97904, tacc(al1) 0.89222, tacc(al2) 0.82934, tacc(ob0) 0.96400, tacc(ob1) 0.91600, tacc(ob2) 0.81600, tacc(ob3) 0.94200, 2.38 secs\n",
      "\u001b[1m---- Epoch 14/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 2.45265, triplet_loss 0.44629, c_loss 1.08928, hs_loss 1.06318, cs_loss 2.30655, cacc 0.82320, hsacc 0.81060, csacc 0.48123, 47.24 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.90120, tacc(al2) 0.85329, tacc(ob0) 0.97200, tacc(ob1) 0.93200, tacc(ob2) 0.80400, tacc(ob3) 0.94800, 3.00 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8924.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 2.40241, triplet_loss 0.44280, c_loss 1.06929, hs_loss 1.02933, cs_loss 2.26339, cacc 0.81853, hsacc 0.82703, csacc 0.50027, 48.08 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97904, tacc(al1) 0.91018, tacc(al2) 0.86527, tacc(ob0) 0.96600, tacc(ob1) 0.93000, tacc(ob2) 0.81400, tacc(ob3) 0.95200, 2.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8965.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 2.37547, triplet_loss 0.44082, c_loss 1.04573, hs_loss 1.01696, cs_loss 2.24742, cacc 0.82193, hsacc 0.82867, csacc 0.50243, 49.55 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.91916, tacc(al2) 0.85629, tacc(ob0) 0.97000, tacc(ob1) 0.93400, tacc(ob2) 0.81800, tacc(ob3) 0.94600, 3.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.8979.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 2.36183, triplet_loss 0.44053, c_loss 1.03229, hs_loss 1.01204, cs_loss 2.23880, cacc 0.82353, hsacc 0.83143, csacc 0.50707, 46.72 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.91617, tacc(al2) 0.85629, tacc(ob0) 0.96400, tacc(ob1) 0.92800, tacc(ob2) 0.81600, tacc(ob3) 0.93800, 3.70 secs\n",
      "\u001b[1m---- Epoch 18/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 2.35833, triplet_loss 0.43981, c_loss 1.03621, hs_loss 1.01111, cs_loss 2.22954, cacc 0.82063, hsacc 0.82933, csacc 0.51460, 47.40 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97904, tacc(al1) 0.91617, tacc(al2) 0.85329, tacc(ob0) 0.96600, tacc(ob1) 0.92600, tacc(ob2) 0.81400, tacc(ob3) 0.94200, 3.32 secs\n",
      "\u001b[1m---- Epoch 19/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.35911, triplet_loss 0.43838, c_loss 1.03931, hs_loss 1.00761, cs_loss 2.23292, cacc 0.81920, hsacc 0.83157, csacc 0.50767, 51.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.91018, tacc(al2) 0.85928, tacc(ob0) 0.96600, tacc(ob1) 0.92600, tacc(ob2) 0.81600, tacc(ob3) 0.93600, 4.06 secs\n",
      "\u001b[1m---- Epoch 20/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.35209, triplet_loss 0.43925, c_loss 1.03195, hs_loss 1.00208, cs_loss 2.23090, cacc 0.82413, hsacc 0.83050, csacc 0.51193, 53.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.91018, tacc(al2) 0.85329, tacc(ob0) 0.96600, tacc(ob1) 0.92400, tacc(ob2) 0.81600, tacc(ob3) 0.93600, 3.66 secs\n",
      "\u001b[1m---- Epoch 21/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 2.33597, triplet_loss 0.44804, c_loss 1.00592, hs_loss 1.00197, cs_loss 2.21600, cacc 0.81990, hsacc 0.81687, csacc 0.49387, 55.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.90719, tacc(al2) 0.82335, tacc(ob0) 0.96400, tacc(ob1) 0.92000, tacc(ob2) 0.82600, tacc(ob3) 0.93200, 4.09 secs\n",
      "\u001b[1m---- Epoch 22/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 2.25410, triplet_loss 0.44373, c_loss 0.95606, hs_loss 0.95207, cs_loss 2.15633, cacc 0.81927, hsacc 0.82653, csacc 0.50673, 55.40 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.88323, tacc(al2) 0.85030, tacc(ob0) 0.96800, tacc(ob1) 0.93000, tacc(ob2) 0.82000, tacc(ob3) 0.93400, 4.83 secs\n",
      "\u001b[1m---- Epoch 23/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 2.21247, triplet_loss 0.44120, c_loss 0.93528, hs_loss 0.92969, cs_loss 2.11877, cacc 0.82003, hsacc 0.83287, csacc 0.52143, 57.91 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.90419, tacc(al2) 0.85030, tacc(ob0) 0.96800, tacc(ob1) 0.93600, tacc(ob2) 0.81800, tacc(ob3) 0.95200, 3.73 secs\n",
      "\u001b[1m---- Epoch 24/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 2.18112, triplet_loss 0.43872, c_loss 0.91575, hs_loss 0.90915, cs_loss 2.09862, cacc 0.82417, hsacc 0.83890, csacc 0.53007, 58.64 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.91317, tacc(al2) 0.86228, tacc(ob0) 0.97000, tacc(ob1) 0.92400, tacc(ob2) 0.82800, tacc(ob3) 0.95400, 3.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9010.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 2.17165, triplet_loss 0.43793, c_loss 0.91135, hs_loss 0.90281, cs_loss 2.09120, cacc 0.82327, hsacc 0.84237, csacc 0.53370, 59.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.86826, tacc(ob0) 0.96800, tacc(ob1) 0.92400, tacc(ob2) 0.82200, tacc(ob3) 0.95200, 3.78 secs\n",
      "\u001b[1m---- Epoch 26/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 2.15762, triplet_loss 0.43883, c_loss 0.89491, hs_loss 0.89269, cs_loss 2.08881, cacc 0.82480, hsacc 0.84930, csacc 0.53143, 59.68 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90419, tacc(al2) 0.86527, tacc(ob0) 0.96800, tacc(ob1) 0.93000, tacc(ob2) 0.82200, tacc(ob3) 0.95000, 3.71 secs\n",
      "\u001b[1m---- Epoch 27/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.16163, triplet_loss 0.43850, c_loss 0.90266, hs_loss 0.89558, cs_loss 2.08652, cacc 0.82550, hsacc 0.84567, csacc 0.53610, 58.02 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.91018, tacc(al2) 0.86228, tacc(ob0) 0.96800, tacc(ob1) 0.93000, tacc(ob2) 0.82400, tacc(ob3) 0.95200, 3.86 secs\n",
      "\u001b[1m---- Epoch 28/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.15775, triplet_loss 0.43918, c_loss 0.90025, hs_loss 0.89438, cs_loss 2.08170, cacc 0.82337, hsacc 0.84797, csacc 0.53753, 59.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.91018, tacc(al2) 0.86228, tacc(ob0) 0.96800, tacc(ob1) 0.92800, tacc(ob2) 0.82200, tacc(ob3) 0.95000, 4.34 secs\n",
      "\u001b[1m---- Epoch 29/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 2.15830, triplet_loss 0.44501, c_loss 0.89348, hs_loss 0.90575, cs_loss 2.07236, cacc 0.82293, hsacc 0.82547, csacc 0.51907, 60.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97904, tacc(al1) 0.88323, tacc(al2) 0.85329, tacc(ob0) 0.96800, tacc(ob1) 0.92600, tacc(ob2) 0.82800, tacc(ob3) 0.93200, 3.51 secs\n",
      "\u001b[1m---- Epoch 30/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 2.08085, triplet_loss 0.44310, c_loss 0.84077, hs_loss 0.86213, cs_loss 2.01570, cacc 0.83107, hsacc 0.83963, csacc 0.53200, 58.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.90120, tacc(al2) 0.85329, tacc(ob0) 0.97200, tacc(ob1) 0.93200, tacc(ob2) 0.81600, tacc(ob3) 0.95000, 3.75 secs\n",
      "\u001b[1m---- Epoch 31/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 2.03104, triplet_loss 0.44175, c_loss 0.80783, hs_loss 0.82893, cs_loss 1.98359, cacc 0.82580, hsacc 0.84797, csacc 0.53863, 56.79 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.84431, tacc(ob0) 0.96800, tacc(ob1) 0.94400, tacc(ob2) 0.81800, tacc(ob3) 0.93600, 4.00 secs\n",
      "\u001b[1m---- Epoch 32/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 2.01524, triplet_loss 0.44027, c_loss 0.81240, hs_loss 0.81865, cs_loss 1.95917, cacc 0.82963, hsacc 0.85137, csacc 0.55337, 56.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90719, tacc(al2) 0.84731, tacc(ob0) 0.97200, tacc(ob1) 0.94600, tacc(ob2) 0.82600, tacc(ob3) 0.95000, 3.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9016.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 2.00181, triplet_loss 0.43751, c_loss 0.80241, hs_loss 0.80804, cs_loss 1.95565, cacc 0.82257, hsacc 0.85263, csacc 0.54737, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90419, tacc(al2) 0.85030, tacc(ob0) 0.97800, tacc(ob1) 0.94400, tacc(ob2) 0.82400, tacc(ob3) 0.94800, 3.54 secs\n",
      "\u001b[1m---- Epoch 34/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 2.00107, triplet_loss 0.43710, c_loss 0.79517, hs_loss 0.81694, cs_loss 1.95294, cacc 0.83337, hsacc 0.85700, csacc 0.55223, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.85030, tacc(ob0) 0.97400, tacc(ob1) 0.93800, tacc(ob2) 0.82200, tacc(ob3) 0.95000, 3.60 secs\n",
      "\u001b[1m---- Epoch 35/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.99653, triplet_loss 0.43626, c_loss 0.80400, hs_loss 0.80240, cs_loss 1.95040, cacc 0.82877, hsacc 0.85333, csacc 0.55107, 59.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90719, tacc(al2) 0.85030, tacc(ob0) 0.97600, tacc(ob1) 0.94600, tacc(ob2) 0.82200, tacc(ob3) 0.94800, 3.88 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_35_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9017.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 36/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.00342, triplet_loss 0.43751, c_loss 0.80858, hs_loss 0.81055, cs_loss 1.95019, cacc 0.83067, hsacc 0.85637, csacc 0.55353, 59.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.91018, tacc(al2) 0.85030, tacc(ob0) 0.97600, tacc(ob1) 0.94600, tacc(ob2) 0.82200, tacc(ob3) 0.94800, 4.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_36_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9024.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 37/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 1.99674, triplet_loss 0.44340, c_loss 0.78677, hs_loss 0.82067, cs_loss 1.94264, cacc 0.82833, hsacc 0.83803, csacc 0.53487, 61.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.90120, tacc(al2) 0.85928, tacc(ob0) 0.96600, tacc(ob1) 0.93200, tacc(ob2) 0.79800, tacc(ob3) 0.93600, 4.53 secs\n",
      "\u001b[1m---- Epoch 38/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 1.92976, triplet_loss 0.44055, c_loss 0.74750, hs_loss 0.78457, cs_loss 1.88691, cacc 0.82703, hsacc 0.84583, csacc 0.54697, 61.51 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.91018, tacc(al2) 0.85329, tacc(ob0) 0.97000, tacc(ob1) 0.93800, tacc(ob2) 0.81000, tacc(ob3) 0.94200, 3.85 secs\n",
      "\u001b[1m---- Epoch 39/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 1.88441, triplet_loss 0.44130, c_loss 0.71822, hs_loss 0.76220, cs_loss 1.84709, cacc 0.83383, hsacc 0.84967, csacc 0.56077, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.89820, tacc(al2) 0.85928, tacc(ob0) 0.97200, tacc(ob1) 0.93000, tacc(ob2) 0.82400, tacc(ob3) 0.94800, 3.48 secs\n",
      "\u001b[1m---- Epoch 40/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 1.86572, triplet_loss 0.43934, c_loss 0.70694, hs_loss 0.74485, cs_loss 1.84032, cacc 0.83540, hsacc 0.86337, csacc 0.55683, 59.65 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.85928, tacc(ob0) 0.97200, tacc(ob1) 0.94600, tacc(ob2) 0.82600, tacc(ob3) 0.94600, 3.86 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9026.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 1.86440, triplet_loss 0.43833, c_loss 0.71942, hs_loss 0.74496, cs_loss 1.82610, cacc 0.83193, hsacc 0.85757, csacc 0.56013, 57.57 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90120, tacc(al2) 0.85329, tacc(ob0) 0.97400, tacc(ob1) 0.94400, tacc(ob2) 0.81800, tacc(ob3) 0.95600, 4.89 secs\n",
      "\u001b[1m---- Epoch 42/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.85429, triplet_loss 0.43708, c_loss 0.71111, hs_loss 0.73524, cs_loss 1.82514, cacc 0.83090, hsacc 0.86123, csacc 0.56383, 57.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90719, tacc(al2) 0.85629, tacc(ob0) 0.97400, tacc(ob1) 0.94400, tacc(ob2) 0.82200, tacc(ob3) 0.95400, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_42_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9035.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 43/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.84432, triplet_loss 0.43756, c_loss 0.70388, hs_loss 0.72819, cs_loss 1.81902, cacc 0.83430, hsacc 0.86360, csacc 0.56647, 56.27 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90419, tacc(al2) 0.85329, tacc(ob0) 0.97400, tacc(ob1) 0.93800, tacc(ob2) 0.82200, tacc(ob3) 0.95600, 3.63 secs\n",
      "\u001b[1m---- Epoch 44/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.83997, triplet_loss 0.43681, c_loss 0.69315, hs_loss 0.72881, cs_loss 1.82117, cacc 0.83680, hsacc 0.86337, csacc 0.56517, 55.88 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.90419, tacc(al2) 0.85329, tacc(ob0) 0.97600, tacc(ob1) 0.94000, tacc(ob2) 0.82000, tacc(ob3) 0.95600, 3.82 secs\n",
      "\u001b[1m---- Epoch 45/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 1.85557, triplet_loss 0.44289, c_loss 0.70259, hs_loss 0.74573, cs_loss 1.81993, cacc 0.83440, hsacc 0.84660, csacc 0.54797, 59.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98503, tacc(al1) 0.88922, tacc(al2) 0.82036, tacc(ob0) 0.97400, tacc(ob1) 0.93000, tacc(ob2) 0.80200, tacc(ob3) 0.94800, 3.47 secs\n",
      "\u001b[1m---- Epoch 46/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 1.80287, triplet_loss 0.44285, c_loss 0.67007, hs_loss 0.71574, cs_loss 1.77709, cacc 0.83427, hsacc 0.85280, csacc 0.55377, 56.07 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97904, tacc(al1) 0.91617, tacc(al2) 0.84431, tacc(ob0) 0.97800, tacc(ob1) 0.93200, tacc(ob2) 0.81000, tacc(ob3) 0.95200, 4.19 secs\n",
      "\u001b[1m---- Epoch 47/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 1.76265, triplet_loss 0.44175, c_loss 0.64521, hs_loss 0.70520, cs_loss 1.73314, cacc 0.83723, hsacc 0.85850, csacc 0.56943, 59.94 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.91916, tacc(al2) 0.84731, tacc(ob0) 0.97400, tacc(ob1) 0.93800, tacc(ob2) 0.81800, tacc(ob3) 0.95400, 4.24 secs\n",
      "\u001b[1m---- Epoch 48/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 1.73889, triplet_loss 0.43937, c_loss 0.64131, hs_loss 0.68303, cs_loss 1.71408, cacc 0.83810, hsacc 0.86627, csacc 0.58063, 59.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98204, tacc(al1) 0.91916, tacc(al2) 0.85030, tacc(ob0) 0.98000, tacc(ob1) 0.94200, tacc(ob2) 0.80800, tacc(ob3) 0.94800, 4.35 secs\n",
      "\u001b[1m---- Epoch 49/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 1.73613, triplet_loss 0.43871, c_loss 0.63660, hs_loss 0.68891, cs_loss 1.70804, cacc 0.84170, hsacc 0.86540, csacc 0.57863, 60.59 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.92216, tacc(al2) 0.86527, tacc(ob0) 0.97600, tacc(ob1) 0.94200, tacc(ob2) 0.82200, tacc(ob3) 0.94600, 4.82 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9069.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.71973, triplet_loss 0.43663, c_loss 0.62033, hs_loss 0.68190, cs_loss 1.70061, cacc 0.84527, hsacc 0.86643, csacc 0.58027, 61.50 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.91916, tacc(al2) 0.86228, tacc(ob0) 0.98000, tacc(ob1) 0.94400, tacc(ob2) 0.82000, tacc(ob3) 0.94600, 4.36 secs\n",
      "\u001b[1m---- Epoch 51/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.71905, triplet_loss 0.43699, c_loss 0.62128, hs_loss 0.68017, cs_loss 1.69966, cacc 0.84320, hsacc 0.86847, csacc 0.58180, 61.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.91617, tacc(al2) 0.86228, tacc(ob0) 0.97800, tacc(ob1) 0.94400, tacc(ob2) 0.82000, tacc(ob3) 0.94600, 4.28 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 52/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.73005, triplet_loss 0.43816, c_loss 0.64683, hs_loss 0.67365, cs_loss 1.70146, cacc 0.84183, hsacc 0.86423, csacc 0.57947, 58.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.91317, tacc(al2) 0.86527, tacc(ob0) 0.97600, tacc(ob1) 0.94200, tacc(ob2) 0.82200, tacc(ob3) 0.94600, 3.50 secs\n",
      "\u001b[1m---- Epoch 53/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 1.74213, triplet_loss 0.44419, c_loss 0.63594, hs_loss 0.69635, cs_loss 1.70778, cacc 0.84293, hsacc 0.84710, csacc 0.56440, 56.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97605, tacc(al1) 0.88922, tacc(al2) 0.83832, tacc(ob0) 0.97600, tacc(ob1) 0.93200, tacc(ob2) 0.80600, tacc(ob3) 0.94400, 4.16 secs\n",
      "\u001b[1m---- Epoch 54/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 1.68754, triplet_loss 0.44400, c_loss 0.59424, hs_loss 0.66842, cs_loss 1.66843, cacc 0.85183, hsacc 0.85990, csacc 0.56713, 53.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.89521, tacc(al2) 0.84731, tacc(ob0) 0.97600, tacc(ob1) 0.93600, tacc(ob2) 0.82400, tacc(ob3) 0.94800, 4.33 secs\n",
      "\u001b[1m---- Epoch 55/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 1.64633, triplet_loss 0.43859, c_loss 0.58309, hs_loss 0.64233, cs_loss 1.62865, cacc 0.84900, hsacc 0.86727, csacc 0.58400, 56.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.89521, tacc(al2) 0.85030, tacc(ob0) 0.98400, tacc(ob1) 0.94400, tacc(ob2) 0.82200, tacc(ob3) 0.95800, 3.41 secs\n",
      "\u001b[1m---- Epoch 56/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 1.62899, triplet_loss 0.44018, c_loss 0.57785, hs_loss 0.62978, cs_loss 1.61016, cacc 0.85163, hsacc 0.86783, csacc 0.58743, 60.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.90120, tacc(al2) 0.85928, tacc(ob0) 0.97800, tacc(ob1) 0.94600, tacc(ob2) 0.81800, tacc(ob3) 0.95800, 3.79 secs\n",
      "\u001b[1m---- Epoch 57/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 1.62349, triplet_loss 0.43930, c_loss 0.56961, hs_loss 0.63802, cs_loss 1.60006, cacc 0.85763, hsacc 0.87150, csacc 0.59087, 61.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.90419, tacc(al2) 0.85629, tacc(ob0) 0.97800, tacc(ob1) 0.94600, tacc(ob2) 0.82200, tacc(ob3) 0.95400, 4.20 secs\n",
      "\u001b[1m---- Epoch 58/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.61282, triplet_loss 0.43867, c_loss 0.56260, hs_loss 0.62577, cs_loss 1.59860, cacc 0.85560, hsacc 0.86977, csacc 0.59100, 59.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.90419, tacc(al2) 0.85629, tacc(ob0) 0.97800, tacc(ob1) 0.94600, tacc(ob2) 0.82000, tacc(ob3) 0.95400, 3.66 secs\n",
      "\u001b[1m---- Epoch 59/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "   iteration 17475\r"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "    --epochs 60 \\\n",
    "    --batches_per_epoch 300 \\\n",
    "    --batch_size 200 \\\n",
    "    --num_workers 3 \\\n",
    "    --iters_to_accumulate 4 \\\n",
    "    --optimizer_name \"adamw\" \\\n",
    "    --scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "    --lr 1e-6 \\\n",
    "    --warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "    --triplets_filepath \\\n",
    "        \"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(1605733,297669,1315702,4001000,20002000,1000,2000).pkl\" \\\n",
    "    --triplet_rule_weights \\\n",
    "        \"{'anatomical_locations': [1., 1., 1.], 'observations': [1., 1., 1., 1.]}\" \\\n",
    "    --integrated_facts_metadata_jsonl_filepath \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).improved_comparison(6526297).jsonl\" \\\n",
    "    --paraphrases_jsonl_filepaths \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "    --dataset_name \"MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\" \\\n",
    "    --huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "    --embedding_size 128 \\\n",
    "    --use_amp \\\n",
    "    --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c822c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "    --pretrained_checkpoint_folder_path \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_121846_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "    --epochs 100 \\\n",
    "    --batches_per_epoch 400 \\\n",
    "    --batch_size 200 \\\n",
    "    --num_workers 3 \\\n",
    "    --iters_to_accumulate 4 \\\n",
    "    --optimizer_name \"adamw\" \\\n",
    "    --scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "    --lr 1e-6 \\\n",
    "    --warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "    --triplets_filepath \\\n",
    "        \"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(1605733,297669,1315702,4001000,20002000,1000,2000).pkl\" \\\n",
    "    --triplet_rule_weights \\\n",
    "        \"{'anatomical_locations': [1., 1., 1.], 'observations': [1., 1., 1., 1.]}\" \\\n",
    "    --integrated_facts_metadata_jsonl_filepath \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).improved_comparison(6526297).jsonl\" \\\n",
    "    --paraphrases_jsonl_filepaths \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "    --dataset_name \"MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\" \\\n",
    "    --huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "    --embedding_size 128 \\\n",
    "    --use_amp \\\n",
    "    --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d79f2f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 40\n",
      "   batches_per_epoch: 400\n",
      "   batch_size: 200\n",
      "   checkpoint_folder: models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   huggingface_model_name: None\n",
      "   embedding_size: 128\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 0.001\n",
      "   scheduler: reduce-lr-on-plateau\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: None\n",
      "   iters_to_accumulate: 1\n",
      "   override_lr: False\n",
      "   triplets_filepath: None\n",
      "   triplet_rule_weights: None\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   dataset_name: None\n",
      "   triplets_weight: 1.0\n",
      "   classification_weight: 1.0\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Resuming training ------\u001b[0m\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: True\n",
      "  n_categories: 6\n",
      "  classify_health_status: True\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: True\n",
      "  n_comparison_statuses: 15\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(1605733,297669,1315702,4001000,20002000,1000,2000).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1504492\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1493944\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1315582\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 5382918\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 5372563\n",
      "\tWeight: 1.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4440269\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 4909633\n",
      "\tWeight: 1.0\n",
      "----\n",
      "\u001b[1mBuilding train classification dataset and dataloader...\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "Number of sentences with paraphrases: 59901\n",
      "Number of paraphrases: 599166\n",
      "Loading integrated facts metadata...\n",
      "Number of facts: 1455640\n",
      "Number of improved comparisons: 521435/578733\n",
      "Category: anatomical finding -> 1015229\n",
      "Category: tubes and lines -> 179581\n",
      "Category: technical assessment -> 97422\n",
      "Category: disease -> 82909\n",
      "Category: device -> 78769\n",
      "Category: other -> 1730\n",
      "Health status: abnormal -> 830476\n",
      "Health status: unknown -> 365562\n",
      "Health status: normal -> 168102\n",
      "Health status: ambiguous -> 90968\n",
      "Health status: other -> 532\n",
      "Comparison status: no comparison -> 821179\n",
      "Comparison status: stable/unchanged -> 161806\n",
      "Comparison status: worsened -> 78594\n",
      "Comparison status: improved -> 61567\n",
      "Comparison status: resolved -> 58491\n",
      "Comparison status: new finding -> 54389\n",
      "Comparison status: position changed -> 49909\n",
      "Comparison status: unclear comparison -> 48953\n",
      "Comparison status: increase -> 36937\n",
      "Comparison status: decrease -> 24308\n",
      "Comparison status: progressed -> 19529\n",
      "Comparison status: smaller -> 15232\n",
      "Comparison status: larger -> 14403\n",
      "Comparison status: reappeared -> 10084\n",
      "Comparison status: other -> 259\n",
      "\u001b[1mExample fact: diffuse opacities in the left lung\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: stable/unchanged\n",
      "\u001b[1mExample fact: no evidence of acute trauma\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: normal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: existing prominent hiatal hernia\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: cluster of microcalcifications\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: endotracheal tube tip is positioned roughly 3.4 cm from the carina\u001b[0m\n",
      "Category: tubes and lines\n",
      "Health status: unknown\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: potential for an early infiltrate\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: ambiguous\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: AP view of the chest compared to previous exam from earlier the same day at 3:58 a.m.\u001b[0m\n",
      "Category: technical assessment\n",
      "Health status: unknown\n",
      "Comparison status: resolved\n",
      "\u001b[1mExample fact: decreased pre-existing effusion\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: decrease\n",
      "\u001b[1mExample fact: old healed rib fractures\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: multifocal airspace opacities\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison status: no comparison\n",
      "\tNumber of samples: 821179\n",
      "\tWeight: 7584.223051637783\n",
      "Comparison status: worsened\n",
      "\tNumber of samples: 78594\n",
      "\tWeight: 4300.633270502103\n",
      "Comparison status: resolved\n",
      "\tNumber of samples: 58491\n",
      "\tWeight: 3971.2797023757703\n",
      "Comparison status: stable/unchanged\n",
      "\tNumber of samples: 161806\n",
      "\tWeight: 5181.224495202725\n",
      "Comparison status: progressed\n",
      "\tNumber of samples: 19529\n",
      "\tWeight: 2895.6699714847914\n",
      "Comparison status: larger\n",
      "\tNumber of samples: 14403\n",
      "\tWeight: 2636.1253812589207\n",
      "Comparison status: improved\n",
      "\tNumber of samples: 61567\n",
      "\tWeight: 4027.1690868518713\n",
      "Comparison status: reappeared\n",
      "\tNumber of samples: 10084\n",
      "\tWeight: 2352.5204683090246\n",
      "Comparison status: smaller\n",
      "\tNumber of samples: 15232\n",
      "\tWeight: 2682.6163398845774\n",
      "Comparison status: position changed\n",
      "\tNumber of samples: 49909\n",
      "\tWeight: 3801.537897984162\n",
      "Comparison status: new finding\n",
      "\tNumber of samples: 54389\n",
      "\tWeight: 3892.8821154862517\n",
      "Comparison status: unclear comparison\n",
      "\tNumber of samples: 48953\n",
      "\tWeight: 3781.184791746964\n",
      "Comparison status: increase\n",
      "\tNumber of samples: 36937\n",
      "\tWeight: 3492.9744015057554\n",
      "Comparison status: decrease\n",
      "\tNumber of samples: 24308\n",
      "\tWeight: 3092.4456077564887\n",
      "Comparison status: other\n",
      "\tNumber of samples: 259\n",
      "\tWeight: 515.2339764293102\n",
      "----\n",
      "\u001b[1mBuilding val dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 334\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 334\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 334\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 500\n",
      "\tRule ID: ob3\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_42_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9201.pt']\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_42_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9201.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 43/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.30076, triplet_loss 0.43789, c_loss 0.40257, hs_loss 0.47345, cs_loss 1.28760, cacc 0.89633, hsacc 0.89080, csacc 0.64435, 62.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.93413, tacc(al2) 0.87425, tacc(ob0) 0.98200, tacc(ob1) 0.94600, tacc(ob2) 0.84600, tacc(ob3) 0.96600, 2.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_43_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9218.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 44/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.30242, triplet_loss 0.43684, c_loss 0.40075, hs_loss 0.48043, cs_loss 1.28682, cacc 0.89397, hsacc 0.88705, csacc 0.64390, 61.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92814, tacc(al2) 0.87425, tacc(ob0) 0.98200, tacc(ob1) 0.94600, tacc(ob2) 0.84400, tacc(ob3) 0.96600, 2.45 secs\n",
      "\u001b[1m---- Epoch 45/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.32254, triplet_loss 0.43825, c_loss 0.41024, hs_loss 0.49234, cs_loss 1.30424, cacc 0.89318, hsacc 0.88153, csacc 0.63215, 62.20 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.92515, tacc(al2) 0.85629, tacc(ob0) 0.97600, tacc(ob1) 0.94800, tacc(ob2) 0.85400, tacc(ob3) 0.97000, 2.65 secs\n",
      "\u001b[1m---- Epoch 46/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.29931, triplet_loss 0.43965, c_loss 0.39799, hs_loss 0.48437, cs_loss 1.27661, cacc 0.89230, hsacc 0.88328, csacc 0.63865, 61.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.93114, tacc(al2) 0.86527, tacc(ob0) 0.98200, tacc(ob1) 0.95200, tacc(ob2) 0.84200, tacc(ob3) 0.97000, 2.68 secs\n",
      "\u001b[1m---- Epoch 47/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.27259, triplet_loss 0.43792, c_loss 0.37353, hs_loss 0.48101, cs_loss 1.25272, cacc 0.89995, hsacc 0.88735, csacc 0.64640, 63.18 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92814, tacc(al2) 0.87126, tacc(ob0) 0.98000, tacc(ob1) 0.94800, tacc(ob2) 0.84000, tacc(ob3) 0.96600, 2.67 secs\n",
      "\u001b[1m---- Epoch 48/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.25436, triplet_loss 0.43769, c_loss 0.36909, hs_loss 0.45800, cs_loss 1.24394, cacc 0.89917, hsacc 0.89015, csacc 0.64928, 63.92 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92814, tacc(al2) 0.86527, tacc(ob0) 0.97800, tacc(ob1) 0.94600, tacc(ob2) 0.84200, tacc(ob3) 0.97000, 2.72 secs\n",
      "\u001b[1m---- Epoch 49/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.25144, triplet_loss 0.43756, c_loss 0.36485, hs_loss 0.45861, cs_loss 1.24186, cacc 0.90360, hsacc 0.89140, csacc 0.64667, 62.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93413, tacc(al2) 0.86826, tacc(ob0) 0.98200, tacc(ob1) 0.95000, tacc(ob2) 0.84000, tacc(ob3) 0.96800, 2.83 secs\n",
      "\u001b[1m---- Epoch 50/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.25093, triplet_loss 0.43736, c_loss 0.37261, hs_loss 0.46048, cs_loss 1.23141, cacc 0.90428, hsacc 0.88985, csacc 0.65410, 63.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93413, tacc(al2) 0.86527, tacc(ob0) 0.98400, tacc(ob1) 0.95000, tacc(ob2) 0.84200, tacc(ob3) 0.97000, 2.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_50_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9224.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 51/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.25674, triplet_loss 0.43662, c_loss 0.37957, hs_loss 0.46134, cs_loss 1.23595, cacc 0.90363, hsacc 0.89118, csacc 0.64770, 64.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93413, tacc(al2) 0.86826, tacc(ob0) 0.98200, tacc(ob1) 0.95200, tacc(ob2) 0.84000, tacc(ob3) 0.97200, 3.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_51_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9226.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 52/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.24977, triplet_loss 0.43837, c_loss 0.37034, hs_loss 0.46390, cs_loss 1.22692, cacc 0.90285, hsacc 0.88733, csacc 0.65458, 64.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93413, tacc(al2) 0.86527, tacc(ob0) 0.98400, tacc(ob1) 0.95200, tacc(ob2) 0.84000, tacc(ob3) 0.97200, 3.53 secs\n",
      "\u001b[1m---- Epoch 53/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.27032, triplet_loss 0.43836, c_loss 0.39067, hs_loss 0.47348, cs_loss 1.23813, cacc 0.89692, hsacc 0.88448, csacc 0.64502, 69.28 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.91916, tacc(al2) 0.85629, tacc(ob0) 0.97800, tacc(ob1) 0.95000, tacc(ob2) 0.85400, tacc(ob3) 0.96200, 4.17 secs\n",
      "\u001b[1m---- Epoch 54/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.25403, triplet_loss 0.44028, c_loss 0.37739, hs_loss 0.47403, cs_loss 1.21638, cacc 0.90365, hsacc 0.87950, csacc 0.64682, 71.15 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tacc(al0) 0.99102, tacc(al1) 0.93114, tacc(al2) 0.85629, tacc(ob0) 0.98400, tacc(ob1) 0.95000, tacc(ob2) 0.83800, tacc(ob3) 0.96600, 4.60 secs\n",
      "\u001b[1m---- Epoch 55/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.22044, triplet_loss 0.43780, c_loss 0.35657, hs_loss 0.44616, cs_loss 1.20035, cacc 0.90845, hsacc 0.88833, csacc 0.65220, 76.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.93114, tacc(al2) 0.85928, tacc(ob0) 0.98000, tacc(ob1) 0.95200, tacc(ob2) 0.83600, tacc(ob3) 0.96400, 4.98 secs\n",
      "\u001b[1m---- Epoch 56/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.20871, triplet_loss 0.43729, c_loss 0.35324, hs_loss 0.43534, cs_loss 1.19155, cacc 0.90570, hsacc 0.89280, csacc 0.65360, 82.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.93114, tacc(al2) 0.86826, tacc(ob0) 0.98200, tacc(ob1) 0.95600, tacc(ob2) 0.84000, tacc(ob3) 0.97200, 4.60 secs\n",
      "\u001b[1m---- Epoch 57/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.21414, triplet_loss 0.43828, c_loss 0.36304, hs_loss 0.44098, cs_loss 1.18597, cacc 0.90785, hsacc 0.89245, csacc 0.65715, 88.07 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.92515, tacc(al2) 0.86826, tacc(ob0) 0.98400, tacc(ob1) 0.95600, tacc(ob2) 0.83600, tacc(ob3) 0.97400, 5.72 secs\n",
      "\u001b[1m---- Epoch 58/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.21604, triplet_loss 0.43744, c_loss 0.35496, hs_loss 0.45651, cs_loss 1.18318, cacc 0.90510, hsacc 0.88975, csacc 0.65682, 90.49 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92515, tacc(al2) 0.86826, tacc(ob0) 0.98400, tacc(ob1) 0.95400, tacc(ob2) 0.84000, tacc(ob3) 0.97200, 6.06 secs\n",
      "\u001b[1m---- Epoch 59/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.21074, triplet_loss 0.43813, c_loss 0.35839, hs_loss 0.44758, cs_loss 1.17737, cacc 0.90680, hsacc 0.88833, csacc 0.65910, 95.59 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.93114, tacc(al2) 0.86826, tacc(ob0) 0.98400, tacc(ob1) 0.95400, tacc(ob2) 0.84200, tacc(ob3) 0.97200, 5.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_59_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9226.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 60/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.20991, triplet_loss 0.43938, c_loss 0.34914, hs_loss 0.45643, cs_loss 1.17487, cacc 0.91078, hsacc 0.88875, csacc 0.66102, 87.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92814, tacc(al2) 0.86826, tacc(ob0) 0.98400, tacc(ob1) 0.95400, tacc(ob2) 0.84200, tacc(ob3) 0.97200, 5.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_60_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9228.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 61/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.22890, triplet_loss 0.43845, c_loss 0.35279, hs_loss 0.46426, cs_loss 1.20231, cacc 0.90368, hsacc 0.88180, csacc 0.64530, 76.75 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.92814, tacc(al2) 0.87126, tacc(ob0) 0.98400, tacc(ob1) 0.95800, tacc(ob2) 0.83400, tacc(ob3) 0.96800, 4.45 secs\n",
      "\u001b[1m---- Epoch 62/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.19884, triplet_loss 0.44027, c_loss 0.34742, hs_loss 0.44248, cs_loss 1.16751, cacc 0.91005, hsacc 0.88933, csacc 0.65528, 77.59 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92515, tacc(al2) 0.87725, tacc(ob0) 0.97800, tacc(ob1) 0.94200, tacc(ob2) 0.83200, tacc(ob3) 0.96800, 4.58 secs\n",
      "\u001b[1m---- Epoch 63/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.18842, triplet_loss 0.43821, c_loss 0.34605, hs_loss 0.43630, cs_loss 1.15628, cacc 0.91033, hsacc 0.88758, csacc 0.65912, 74.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92515, tacc(al2) 0.88024, tacc(ob0) 0.97600, tacc(ob1) 0.93800, tacc(ob2) 0.83800, tacc(ob3) 0.97000, 4.02 secs\n",
      "\u001b[1m---- Epoch 64/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.17554, triplet_loss 0.43785, c_loss 0.34140, hs_loss 0.42058, cs_loss 1.15125, cacc 0.91635, hsacc 0.89045, csacc 0.66030, 83.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.92515, tacc(al2) 0.88024, tacc(ob0) 0.97800, tacc(ob1) 0.94400, tacc(ob2) 0.84000, tacc(ob3) 0.96800, 6.03 secs\n",
      "\u001b[1m---- Epoch 65/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.16893, triplet_loss 0.43824, c_loss 0.34345, hs_loss 0.42062, cs_loss 1.13556, cacc 0.91350, hsacc 0.89463, csacc 0.66560, 88.48 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93114, tacc(al2) 0.88623, tacc(ob0) 0.98000, tacc(ob1) 0.93800, tacc(ob2) 0.83600, tacc(ob3) 0.97200, 6.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_65_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9230.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 66/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.16223, triplet_loss 0.43815, c_loss 0.33068, hs_loss 0.42209, cs_loss 1.13355, cacc 0.91568, hsacc 0.88955, csacc 0.66797, 99.09 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93114, tacc(al2) 0.88323, tacc(ob0) 0.98000, tacc(ob1) 0.94200, tacc(ob2) 0.84000, tacc(ob3) 0.97400, 5.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_66_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9239.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 67/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.15856, triplet_loss 0.43880, c_loss 0.33293, hs_loss 0.41905, cs_loss 1.12634, cacc 0.91360, hsacc 0.89210, csacc 0.67322, 102.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93114, tacc(al2) 0.88623, tacc(ob0) 0.98000, tacc(ob1) 0.94000, tacc(ob2) 0.83800, tacc(ob3) 0.97000, 6.22 secs\n",
      "\u001b[1m---- Epoch 68/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.16147, triplet_loss 0.43961, c_loss 0.33210, hs_loss 0.42566, cs_loss 1.12556, cacc 0.91343, hsacc 0.89092, csacc 0.67027, 84.68 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99401, tacc(al1) 0.93114, tacc(al2) 0.88623, tacc(ob0) 0.98000, tacc(ob1) 0.94000, tacc(ob2) 0.84000, tacc(ob3) 0.96800, 6.13 secs\n",
      "\u001b[1m---- Epoch 69/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.17491, triplet_loss 0.44224, c_loss 0.33448, hs_loss 0.43291, cs_loss 1.14019, cacc 0.91487, hsacc 0.88940, csacc 0.65885, 80.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99701, tacc(al1) 0.91617, tacc(al2) 0.88024, tacc(ob0) 0.98400, tacc(ob1) 0.94600, tacc(ob2) 0.83600, tacc(ob3) 0.97400, 5.30 secs\n",
      "\u001b[1m---- Epoch 70/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.15876, triplet_loss 0.43869, c_loss 0.32981, hs_loss 0.42265, cs_loss 1.12636, cacc 0.91030, hsacc 0.88658, csacc 0.66335, 84.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92515, tacc(al2) 0.87425, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.84200, tacc(ob3) 0.97400, 5.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_70_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9242.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 71/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.13995, triplet_loss 0.43881, c_loss 0.31892, hs_loss 0.41338, cs_loss 1.10879, cacc 0.91607, hsacc 0.89225, csacc 0.66880, 83.88 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.91916, tacc(al2) 0.87425, tacc(ob0) 0.98000, tacc(ob1) 0.95600, tacc(ob2) 0.84600, tacc(ob3) 0.97400, 6.25 secs\n",
      "\u001b[1m---- Epoch 72/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.13319, triplet_loss 0.43940, c_loss 0.32206, hs_loss 0.41482, cs_loss 1.09009, cacc 0.91770, hsacc 0.88955, csacc 0.67273, 91.52 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98802, tacc(al1) 0.92515, tacc(al2) 0.88024, tacc(ob0) 0.98200, tacc(ob1) 0.94800, tacc(ob2) 0.85200, tacc(ob3) 0.97600, 6.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_72_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9250.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 73/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.12491, triplet_loss 0.43847, c_loss 0.31682, hs_loss 0.40927, cs_loss 1.08526, cacc 0.92025, hsacc 0.89302, csacc 0.67437, 126.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99102, tacc(al1) 0.92216, tacc(al2) 0.86826, tacc(ob0) 0.98200, tacc(ob1) 0.94600, tacc(ob2) 0.85000, tacc(ob3) 0.97600, 11.55 secs\n",
      "\u001b[1m---- Epoch 74/82\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "^C iteration 12600\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "    --checkpoint_folder \"models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "    --epochs 40 \\\n",
    "    --batches_per_epoch 400 \\\n",
    "    --batch_size 200 \\\n",
    "    --num_workers 2 \\\n",
    "    --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf75b2e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 400\n",
      "   batch_size: 80\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 4\n",
      "   override_lr: False\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(2286627,297669,1996654,2501000,1000).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [0.8, 1.6, 0.8], 'observations': [1.0, 2.0, 1.0, 1.0, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).improved_comparison(6526297).jsonl\n",
      "   paraphrases_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl']\n",
      "   dataset_name: MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\n",
      "   triplets_weight: 1.0\n",
      "   classification_weight: 1.0\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: True\n",
      "  n_categories: 6\n",
      "  classify_health_status: True\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: True\n",
      "  n_comparison_statuses: 15\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(2286627,297669,1996654,2501000,1000).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 2880494\n",
      "\tWeight: 0.8\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 2868295\n",
      "\tWeight: 1.6\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 2001430\n",
      "\tWeight: 0.8\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3769449\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3762709\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 2217029\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2473084\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other - Hard negative\"\n",
      "\tNumber of triplets: 4429094\n",
      "\tWeight: 2.0\n",
      "----\n",
      "\u001b[1mBuilding train classification dataset and dataloader...\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n",
      "Number of sentences with paraphrases: 109887\n",
      "Number of paraphrases: 1299918\n",
      "Loading integrated facts metadata...\n",
      "Number of facts: 2188212\n",
      "Number of improved comparisons: 521435/578733\n",
      "Category: anatomical finding -> 1564922\n",
      "Category: tubes and lines -> 235687\n",
      "Category: technical assessment -> 150516\n",
      "Category: disease -> 122596\n",
      "Category: device -> 111660\n",
      "Category: other -> 2831\n",
      "Health status: abnormal -> 1283130\n",
      "Health status: unknown -> 520758\n",
      "Health status: normal -> 246373\n",
      "Health status: ambiguous -> 136998\n",
      "Health status: other -> 953\n",
      "Comparison status: no comparison -> 1204429\n",
      "Comparison status: stable/unchanged -> 248878\n",
      "Comparison status: worsened -> 125579\n",
      "Comparison status: improved -> 99848\n",
      "Comparison status: resolved -> 91368\n",
      "Comparison status: new finding -> 83889\n",
      "Comparison status: unclear comparison -> 75262\n",
      "Comparison status: position changed -> 67257\n",
      "Comparison status: increase -> 58489\n",
      "Comparison status: decrease -> 38620\n",
      "Comparison status: progressed -> 31572\n",
      "Comparison status: smaller -> 24358\n",
      "Comparison status: larger -> 22410\n",
      "Comparison status: reappeared -> 15813\n",
      "Comparison status: other -> 440\n",
      "\u001b[1mExample fact: prominent irregular opacification along the right lateral chest wall\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: Complete disappearance of earlier noted consolidations in the right lower lobe\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: normal\n",
      "Comparison status: resolved\n",
      "\u001b[1mExample fact: fluid collection in the pleural space at the lower part\u001b[0m\n",
      "Category: tubes and lines\n",
      "Health status: unknown\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: greater than normal fluid in the left side\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: worsened\n",
      "\u001b[1mExample fact: Potential slight worsening of atelectasis in the left base\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: worsened\n",
      "\u001b[1mExample fact: prior surgical procedure in the mid trachea\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: unknown\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: medial opacity\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: unobstructed central airways\u001b[0m\n",
      "Category: technical assessment\n",
      "Health status: unknown\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: opacification not well localized on the lateral view\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: ambiguous\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: There has been a progression in the parenchymal consolidations compared to the previous imaging\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: increase\n",
      "Comparison status: no comparison\n",
      "\tNumber of samples: 1204429\n",
      "\tWeight: 8242.307524442282\n",
      "Comparison status: worsened\n",
      "\tNumber of samples: 125579\n",
      "\tWeight: 4859.644675613537\n",
      "Comparison status: resolved\n",
      "\tNumber of samples: 91368\n",
      "\tWeight: 4475.3220512437665\n",
      "Comparison status: stable/unchanged\n",
      "\tNumber of samples: 248878\n",
      "\tWeight: 5759.479652244662\n",
      "Comparison status: progressed\n",
      "\tNumber of samples: 31572\n",
      "\tWeight: 3338.9210042448967\n",
      "Comparison status: larger\n",
      "\tNumber of samples: 22410\n",
      "\tWeight: 3018.358277880451\n",
      "Comparison status: improved\n",
      "\tNumber of samples: 99848\n",
      "\tWeight: 4580.4541460194605\n",
      "Comparison status: reappeared\n",
      "\tNumber of samples: 15813\n",
      "\tWeight: 2714.0180702391563\n",
      "Comparison status: smaller\n",
      "\tNumber of samples: 24358\n",
      "\tWeight: 3094.333715211565\n",
      "Comparison status: position changed\n",
      "\tNumber of samples: 67257\n",
      "\tWeight: 4124.787930176123\n",
      "Comparison status: new finding\n",
      "\tNumber of samples: 83889\n",
      "\tWeight: 4375.692166468171\n",
      "Comparison status: unclear comparison\n",
      "\tNumber of samples: 75262\n",
      "\tWeight: 4251.23985322021\n",
      "Comparison status: increase\n",
      "\tNumber of samples: 58489\n",
      "\tWeight: 3971.242589056373\n",
      "Comparison status: decrease\n",
      "\tNumber of samples: 38620\n",
      "\tWeight: 3537.55808591334\n",
      "Comparison status: other\n",
      "\tNumber of samples: 440\n",
      "\tWeight: 677.1506551206787\n",
      "----\n",
      "\u001b[1mBuilding val dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_045133_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_045133_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_72_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9250.pt', 'checkpoint_42_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9201.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_72_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)=0.9250.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_045133_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.22880, triplet_loss 0.45542, c_loss 0.33476, hs_loss 0.45061, cs_loss 1.21680, cacc 0.91681, hsacc 0.87294, csacc 0.64075, 51.07 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99000, tacc(al1) 0.92400, tacc(al2) 0.87500, tacc(ob0) 0.97400, tacc(ob1) 0.94000, tacc(ob2) 0.86900, tacc(ob3) 0.98000, tacc(ob4) 0.90900, 6.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9204.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.22613, triplet_loss 0.45501, c_loss 0.32306, hs_loss 0.45390, cs_loss 1.22029, cacc 0.91175, hsacc 0.87313, csacc 0.62731, 49.04 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99200, tacc(al1) 0.92600, tacc(al2) 0.87800, tacc(ob0) 0.97500, tacc(ob1) 0.94200, tacc(ob2) 0.86200, tacc(ob3) 0.98000, tacc(ob4) 0.90700, 6.59 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 1.21733, triplet_loss 0.45470, c_loss 0.32814, hs_loss 0.46505, cs_loss 1.18677, cacc 0.91144, hsacc 0.87100, csacc 0.64094, 49.05 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99100, tacc(al1) 0.92600, tacc(al2) 0.87900, tacc(ob0) 0.97700, tacc(ob1) 0.93800, tacc(ob2) 0.86400, tacc(ob3) 0.97700, tacc(ob4) 0.92000, 6.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9214.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.23806, triplet_loss 0.45816, c_loss 0.34126, hs_loss 0.47589, cs_loss 1.20081, cacc 0.91181, hsacc 0.86700, csacc 0.63294, 46.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.92300, tacc(al2) 0.86500, tacc(ob0) 0.97000, tacc(ob1) 0.94200, tacc(ob2) 0.86700, tacc(ob3) 0.98100, tacc(ob4) 0.90600, 6.76 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 1.21442, triplet_loss 0.45583, c_loss 0.32931, hs_loss 0.47450, cs_loss 1.16921, cacc 0.90731, hsacc 0.86744, csacc 0.63013, 50.57 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.91800, tacc(al2) 0.86600, tacc(ob0) 0.97700, tacc(ob1) 0.94500, tacc(ob2) 0.86600, tacc(ob3) 0.97800, tacc(ob4) 0.91400, 6.75 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 1.19710, triplet_loss 0.45186, c_loss 0.33631, hs_loss 0.44423, cs_loss 1.16179, cacc 0.91206, hsacc 0.87175, csacc 0.63913, 50.22 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.92300, tacc(al2) 0.86400, tacc(ob0) 0.97600, tacc(ob1) 0.94500, tacc(ob2) 0.86400, tacc(ob3) 0.97600, tacc(ob4) 0.91600, 6.79 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 1.17031, triplet_loss 0.45266, c_loss 0.30281, hs_loss 0.43928, cs_loss 1.14588, cacc 0.91225, hsacc 0.87175, csacc 0.64538, 50.61 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.92100, tacc(al2) 0.86200, tacc(ob0) 0.97600, tacc(ob1) 0.95000, tacc(ob2) 0.86600, tacc(ob3) 0.97900, tacc(ob4) 0.91700, 6.74 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 1.17447, triplet_loss 0.45061, c_loss 0.30848, hs_loss 0.45150, cs_loss 1.13834, cacc 0.91344, hsacc 0.87156, csacc 0.65069, 50.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.92200, tacc(al2) 0.86400, tacc(ob0) 0.97700, tacc(ob1) 0.94500, tacc(ob2) 0.87000, tacc(ob3) 0.98000, tacc(ob4) 0.91700, 6.83 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 1.16579, triplet_loss 0.44926, c_loss 0.30670, hs_loss 0.44565, cs_loss 1.12996, cacc 0.91506, hsacc 0.87294, csacc 0.64581, 51.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.92500, tacc(al2) 0.86200, tacc(ob0) 0.97600, tacc(ob1) 0.94700, tacc(ob2) 0.87000, tacc(ob3) 0.98000, tacc(ob4) 0.91600, 6.93 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.17173, triplet_loss 0.45031, c_loss 0.29664, hs_loss 0.45029, cs_loss 1.14622, cacc 0.91569, hsacc 0.87106, csacc 0.63925, 51.50 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.92400, tacc(al2) 0.86300, tacc(ob0) 0.97800, tacc(ob1) 0.94800, tacc(ob2) 0.87200, tacc(ob3) 0.98100, tacc(ob4) 0.91500, 6.98 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.16001, triplet_loss 0.45085, c_loss 0.29569, hs_loss 0.45086, cs_loss 1.12262, cacc 0.91669, hsacc 0.86769, csacc 0.64431, 51.28 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.92500, tacc(al2) 0.86200, tacc(ob0) 0.97800, tacc(ob1) 0.94600, tacc(ob2) 0.87100, tacc(ob3) 0.98000, tacc(ob4) 0.91300, 6.97 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.17202, triplet_loss 0.45140, c_loss 0.31561, hs_loss 0.44551, cs_loss 1.13152, cacc 0.91894, hsacc 0.86806, csacc 0.64900, 51.05 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.92500, tacc(al2) 0.86100, tacc(ob0) 0.97800, tacc(ob1) 0.94700, tacc(ob2) 0.87100, tacc(ob3) 0.98000, tacc(ob4) 0.91500, 7.04 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.19389, triplet_loss 0.45340, c_loss 0.32025, hs_loss 0.45822, cs_loss 1.15592, cacc 0.90725, hsacc 0.86475, csacc 0.63775, 50.88 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.92400, tacc(al2) 0.86600, tacc(ob0) 0.98000, tacc(ob1) 0.94400, tacc(ob2) 0.86600, tacc(ob3) 0.97700, tacc(ob4) 0.91100, 7.01 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.18089, triplet_loss 0.45596, c_loss 0.30729, hs_loss 0.45154, cs_loss 1.14700, cacc 0.91450, hsacc 0.86725, csacc 0.63444, 50.66 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.92400, tacc(al2) 0.84900, tacc(ob0) 0.97800, tacc(ob1) 0.95000, tacc(ob2) 0.86300, tacc(ob3) 0.97500, tacc(ob4) 0.91600, 7.12 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.16436, triplet_loss 0.45139, c_loss 0.28146, hs_loss 0.46121, cs_loss 1.13466, cacc 0.91919, hsacc 0.86638, csacc 0.64044, 50.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93100, tacc(al2) 0.85800, tacc(ob0) 0.97700, tacc(ob1) 0.95200, tacc(ob2) 0.86400, tacc(ob3) 0.97700, tacc(ob4) 0.91800, 6.90 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.15318, triplet_loss 0.45213, c_loss 0.30530, hs_loss 0.43358, cs_loss 1.11534, cacc 0.91706, hsacc 0.87862, csacc 0.64463, 50.38 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92600, tacc(al2) 0.85400, tacc(ob0) 0.97700, tacc(ob1) 0.95100, tacc(ob2) 0.86800, tacc(ob3) 0.97800, tacc(ob4) 0.91300, 7.07 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.13593, triplet_loss 0.45063, c_loss 0.28775, hs_loss 0.42331, cs_loss 1.11018, cacc 0.91894, hsacc 0.88175, csacc 0.64856, 52.52 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92900, tacc(al2) 0.85300, tacc(ob0) 0.97600, tacc(ob1) 0.94900, tacc(ob2) 0.87400, tacc(ob3) 0.97700, tacc(ob4) 0.91600, 7.04 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.13219, triplet_loss 0.45215, c_loss 0.28875, hs_loss 0.41612, cs_loss 1.10736, cacc 0.91750, hsacc 0.88269, csacc 0.65706, 52.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92900, tacc(al2) 0.85500, tacc(ob0) 0.97700, tacc(ob1) 0.94700, tacc(ob2) 0.87100, tacc(ob3) 0.97600, tacc(ob4) 0.91300, 7.08 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.13588, triplet_loss 0.45105, c_loss 0.29071, hs_loss 0.43660, cs_loss 1.09340, cacc 0.91300, hsacc 0.87287, csacc 0.66038, 51.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92700, tacc(al2) 0.85600, tacc(ob0) 0.97700, tacc(ob1) 0.94800, tacc(ob2) 0.87500, tacc(ob3) 0.97500, tacc(ob4) 0.91500, 6.92 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.13131, triplet_loss 0.45040, c_loss 0.27947, hs_loss 0.42761, cs_loss 1.10515, cacc 0.92012, hsacc 0.87894, csacc 0.65125, 52.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92800, tacc(al2) 0.85600, tacc(ob0) 0.97700, tacc(ob1) 0.94900, tacc(ob2) 0.87200, tacc(ob3) 0.97500, tacc(ob4) 0.91400, 6.97 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.17986, triplet_loss 0.45392, c_loss 0.32257, hs_loss 0.43843, cs_loss 1.14480, cacc 0.90663, hsacc 0.87206, csacc 0.63738, 52.11 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.92700, tacc(al2) 0.86200, tacc(ob0) 0.98000, tacc(ob1) 0.95100, tacc(ob2) 0.85500, tacc(ob3) 0.97800, tacc(ob4) 0.92000, 7.08 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.17535, triplet_loss 0.45183, c_loss 0.30876, hs_loss 0.45072, cs_loss 1.13938, cacc 0.90987, hsacc 0.86581, csacc 0.63631, 51.37 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.92900, tacc(al2) 0.84800, tacc(ob0) 0.97400, tacc(ob1) 0.94700, tacc(ob2) 0.86500, tacc(ob3) 0.97700, tacc(ob4) 0.92000, 7.06 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.13125, triplet_loss 0.45401, c_loss 0.27285, hs_loss 0.43229, cs_loss 1.10336, cacc 0.91587, hsacc 0.87519, csacc 0.64838, 51.90 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.92800, tacc(al2) 0.85100, tacc(ob0) 0.98000, tacc(ob1) 0.95300, tacc(ob2) 0.86700, tacc(ob3) 0.98100, tacc(ob4) 0.91900, 6.99 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.11983, triplet_loss 0.45285, c_loss 0.28006, hs_loss 0.41901, cs_loss 1.08774, cacc 0.91650, hsacc 0.87369, csacc 0.65400, 51.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.92800, tacc(al2) 0.85400, tacc(ob0) 0.98100, tacc(ob1) 0.95500, tacc(ob2) 0.86400, tacc(ob3) 0.97900, tacc(ob4) 0.92200, 7.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9217.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.11527, triplet_loss 0.45220, c_loss 0.28049, hs_loss 0.42601, cs_loss 1.07184, cacc 0.91669, hsacc 0.87256, csacc 0.65750, 51.78 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.92700, tacc(al2) 0.85300, tacc(ob0) 0.97800, tacc(ob1) 0.95400, tacc(ob2) 0.86800, tacc(ob3) 0.98100, tacc(ob4) 0.92000, 7.10 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.12586, triplet_loss 0.45258, c_loss 0.28219, hs_loss 0.43620, cs_loss 1.08075, cacc 0.91712, hsacc 0.87287, csacc 0.65675, 52.20 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.92800, tacc(al2) 0.85400, tacc(ob0) 0.97800, tacc(ob1) 0.95400, tacc(ob2) 0.86700, tacc(ob3) 0.97900, tacc(ob4) 0.91800, 7.04 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.12272, triplet_loss 0.45267, c_loss 0.29502, hs_loss 0.41948, cs_loss 1.07827, cacc 0.91575, hsacc 0.87387, csacc 0.65625, 51.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.92800, tacc(al2) 0.85400, tacc(ob0) 0.97800, tacc(ob1) 0.95600, tacc(ob2) 0.86900, tacc(ob3) 0.98000, tacc(ob4) 0.92100, 7.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_27_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9220.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.10789, triplet_loss 0.45309, c_loss 0.27983, hs_loss 0.41738, cs_loss 1.06549, cacc 0.91225, hsacc 0.87644, csacc 0.66119, 50.92 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.92800, tacc(al2) 0.85600, tacc(ob0) 0.97800, tacc(ob1) 0.95700, tacc(ob2) 0.86800, tacc(ob3) 0.98000, tacc(ob4) 0.92100, 6.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_28_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9224.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.13706, triplet_loss 0.45370, c_loss 0.27350, hs_loss 0.43787, cs_loss 1.10905, cacc 0.91300, hsacc 0.86919, csacc 0.64169, 51.09 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.93400, tacc(al2) 0.85100, tacc(ob0) 0.98000, tacc(ob1) 0.95600, tacc(ob2) 0.86200, tacc(ob3) 0.97700, tacc(ob4) 0.91000, 7.00 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.12897, triplet_loss 0.45260, c_loss 0.27210, hs_loss 0.43566, cs_loss 1.09759, cacc 0.91831, hsacc 0.86681, csacc 0.64925, 52.11 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.93100, tacc(al2) 0.84300, tacc(ob0) 0.97900, tacc(ob1) 0.95700, tacc(ob2) 0.87100, tacc(ob3) 0.98000, tacc(ob4) 0.91900, 6.87 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.11707, triplet_loss 0.44936, c_loss 0.26707, hs_loss 0.43005, cs_loss 1.08768, cacc 0.92344, hsacc 0.87050, csacc 0.64794, 51.78 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93100, tacc(al2) 0.85600, tacc(ob0) 0.98100, tacc(ob1) 0.95600, tacc(ob2) 0.86900, tacc(ob3) 0.97500, tacc(ob4) 0.92300, 6.99 secs\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.09527, triplet_loss 0.45047, c_loss 0.27041, hs_loss 0.41700, cs_loss 1.05265, cacc 0.91975, hsacc 0.87469, csacc 0.66538, 50.70 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93000, tacc(al2) 0.84600, tacc(ob0) 0.97900, tacc(ob1) 0.95200, tacc(ob2) 0.86000, tacc(ob3) 0.97700, tacc(ob4) 0.92200, 7.16 secs\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.09625, triplet_loss 0.45081, c_loss 0.26727, hs_loss 0.42164, cs_loss 1.05278, cacc 0.92525, hsacc 0.87456, csacc 0.66544, 52.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.92900, tacc(al2) 0.85100, tacc(ob0) 0.98200, tacc(ob1) 0.95500, tacc(ob2) 0.85700, tacc(ob3) 0.97700, tacc(ob4) 0.92200, 7.16 secs\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.08732, triplet_loss 0.45094, c_loss 0.27049, hs_loss 0.41070, cs_loss 1.04251, cacc 0.92506, hsacc 0.87931, csacc 0.66994, 51.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.92600, tacc(al2) 0.85200, tacc(ob0) 0.98100, tacc(ob1) 0.95500, tacc(ob2) 0.86100, tacc(ob3) 0.97800, tacc(ob4) 0.92100, 7.17 secs\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.08436, triplet_loss 0.45207, c_loss 0.25936, hs_loss 0.40511, cs_loss 1.05220, cacc 0.92388, hsacc 0.87356, csacc 0.66212, 50.69 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.92700, tacc(al2) 0.85000, tacc(ob0) 0.98000, tacc(ob1) 0.95600, tacc(ob2) 0.85700, tacc(ob3) 0.97800, tacc(ob4) 0.92000, 7.12 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.09287, triplet_loss 0.44854, c_loss 0.26164, hs_loss 0.42090, cs_loss 1.05467, cacc 0.92381, hsacc 0.87687, csacc 0.66419, 51.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.92700, tacc(al2) 0.84900, tacc(ob0) 0.98100, tacc(ob1) 0.95600, tacc(ob2) 0.85700, tacc(ob3) 0.97800, tacc(ob4) 0.92000, 7.19 secs\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.13382, triplet_loss 0.45436, c_loss 0.28107, hs_loss 0.45169, cs_loss 1.08053, cacc 0.91456, hsacc 0.86325, csacc 0.65250, 52.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93000, tacc(al2) 0.85200, tacc(ob0) 0.97600, tacc(ob1) 0.95100, tacc(ob2) 0.86200, tacc(ob3) 0.97300, tacc(ob4) 0.91600, 7.05 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.10889, triplet_loss 0.45147, c_loss 0.27482, hs_loss 0.41824, cs_loss 1.07326, cacc 0.91938, hsacc 0.87075, csacc 0.65244, 51.60 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93800, tacc(al2) 0.85300, tacc(ob0) 0.98300, tacc(ob1) 0.94900, tacc(ob2) 0.87600, tacc(ob3) 0.97800, tacc(ob4) 0.91800, 7.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_38_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9227.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.10692, triplet_loss 0.45180, c_loss 0.26055, hs_loss 0.42731, cs_loss 1.07417, cacc 0.92319, hsacc 0.87313, csacc 0.65212, 50.64 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93100, tacc(al2) 0.84700, tacc(ob0) 0.98200, tacc(ob1) 0.95500, tacc(ob2) 0.87100, tacc(ob3) 0.97500, tacc(ob4) 0.92100, 6.92 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.09332, triplet_loss 0.45135, c_loss 0.26455, hs_loss 0.41365, cs_loss 1.05710, cacc 0.92000, hsacc 0.87613, csacc 0.65463, 51.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.93300, tacc(al2) 0.84400, tacc(ob0) 0.98300, tacc(ob1) 0.95200, tacc(ob2) 0.86800, tacc(ob3) 0.97600, tacc(ob4) 0.92700, 6.81 secs\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.08246, triplet_loss 0.44927, c_loss 0.27170, hs_loss 0.40423, cs_loss 1.03973, cacc 0.91912, hsacc 0.87981, csacc 0.66912, 51.57 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93400, tacc(al2) 0.84000, tacc(ob0) 0.98300, tacc(ob1) 0.95200, tacc(ob2) 0.87100, tacc(ob3) 0.97600, tacc(ob4) 0.92400, 6.98 secs\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.06859, triplet_loss 0.45072, c_loss 0.25365, hs_loss 0.40851, cs_loss 1.02431, cacc 0.92275, hsacc 0.87987, csacc 0.67050, 51.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93400, tacc(al2) 0.84100, tacc(ob0) 0.98200, tacc(ob1) 0.95300, tacc(ob2) 0.87200, tacc(ob3) 0.97500, tacc(ob4) 0.92400, 6.97 secs\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.07806, triplet_loss 0.45049, c_loss 0.27202, hs_loss 0.41278, cs_loss 1.02083, cacc 0.92081, hsacc 0.87619, csacc 0.67063, 51.75 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93500, tacc(al2) 0.83900, tacc(ob0) 0.98200, tacc(ob1) 0.95200, tacc(ob2) 0.87100, tacc(ob3) 0.97600, tacc(ob4) 0.92300, 6.98 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.07574, triplet_loss 0.45108, c_loss 0.26366, hs_loss 0.40550, cs_loss 1.03125, cacc 0.92337, hsacc 0.87575, csacc 0.66744, 51.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93500, tacc(al2) 0.83700, tacc(ob0) 0.98200, tacc(ob1) 0.95300, tacc(ob2) 0.87100, tacc(ob3) 0.97600, tacc(ob4) 0.92400, 7.07 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.11295, triplet_loss 0.45142, c_loss 0.27732, hs_loss 0.43509, cs_loss 1.06206, cacc 0.92325, hsacc 0.86956, csacc 0.65512, 51.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.93500, tacc(al2) 0.85000, tacc(ob0) 0.98200, tacc(ob1) 0.94500, tacc(ob2) 0.86200, tacc(ob3) 0.97600, tacc(ob4) 0.91800, 6.94 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.10116, triplet_loss 0.45179, c_loss 0.26729, hs_loss 0.41569, cs_loss 1.06755, cacc 0.92600, hsacc 0.87038, csacc 0.64981, 52.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93200, tacc(al2) 0.85000, tacc(ob0) 0.98600, tacc(ob1) 0.95700, tacc(ob2) 0.86300, tacc(ob3) 0.98200, tacc(ob4) 0.92500, 7.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_46_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9227.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.07100, triplet_loss 0.45163, c_loss 0.25116, hs_loss 0.41182, cs_loss 1.02739, cacc 0.92463, hsacc 0.87475, csacc 0.66663, 49.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.93500, tacc(al2) 0.84800, tacc(ob0) 0.98400, tacc(ob1) 0.95800, tacc(ob2) 0.87300, tacc(ob3) 0.98100, tacc(ob4) 0.92300, 7.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9244.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.06426, triplet_loss 0.44994, c_loss 0.25581, hs_loss 0.40190, cs_loss 1.02088, cacc 0.92519, hsacc 0.88119, csacc 0.66887, 49.46 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.93700, tacc(al2) 0.84600, tacc(ob0) 0.98600, tacc(ob1) 0.95400, tacc(ob2) 0.86500, tacc(ob3) 0.98000, tacc(ob4) 0.92900, 7.05 secs\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.06009, triplet_loss 0.45191, c_loss 0.24778, hs_loss 0.40771, cs_loss 1.01278, cacc 0.92925, hsacc 0.88138, csacc 0.66506, 49.82 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93900, tacc(al2) 0.84400, tacc(ob0) 0.98500, tacc(ob1) 0.95700, tacc(ob2) 0.87000, tacc(ob3) 0.98300, tacc(ob4) 0.92400, 7.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9245.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.05516, triplet_loss 0.45195, c_loss 0.24786, hs_loss 0.40658, cs_loss 1.00393, cacc 0.92806, hsacc 0.87625, csacc 0.67394, 49.80 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93800, tacc(al2) 0.84300, tacc(ob0) 0.98500, tacc(ob1) 0.95400, tacc(ob2) 0.87400, tacc(ob3) 0.98200, tacc(ob4) 0.92500, 7.09 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.06738, triplet_loss 0.45054, c_loss 0.26000, hs_loss 0.40620, cs_loss 1.01802, cacc 0.92400, hsacc 0.87750, csacc 0.66981, 50.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.94000, tacc(al2) 0.84200, tacc(ob0) 0.98500, tacc(ob1) 0.95600, tacc(ob2) 0.87400, tacc(ob3) 0.98200, tacc(ob4) 0.92500, 6.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_51_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9246.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.05972, triplet_loss 0.44911, c_loss 0.25537, hs_loss 0.39583, cs_loss 1.01914, cacc 0.92650, hsacc 0.87881, csacc 0.66700, 49.59 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.94100, tacc(al2) 0.84200, tacc(ob0) 0.98500, tacc(ob1) 0.95600, tacc(ob2) 0.87400, tacc(ob3) 0.98200, tacc(ob4) 0.92600, 6.88 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_52_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9249.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.07694, triplet_loss 0.45195, c_loss 0.23681, hs_loss 0.41892, cs_loss 1.04620, cacc 0.92237, hsacc 0.87619, csacc 0.65500, 48.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98900, tacc(al1) 0.93700, tacc(al2) 0.84600, tacc(ob0) 0.98600, tacc(ob1) 0.94300, tacc(ob2) 0.87200, tacc(ob3) 0.98000, tacc(ob4) 0.92000, 6.97 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.07854, triplet_loss 0.45112, c_loss 0.25840, hs_loss 0.40430, cs_loss 1.04327, cacc 0.92363, hsacc 0.87275, csacc 0.65863, 51.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98900, tacc(al1) 0.94500, tacc(al2) 0.84200, tacc(ob0) 0.98400, tacc(ob1) 0.95400, tacc(ob2) 0.87600, tacc(ob3) 0.98200, tacc(ob4) 0.92800, 6.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_54_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9256.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.05516, triplet_loss 0.45111, c_loss 0.24512, hs_loss 0.40158, cs_loss 1.01251, cacc 0.92250, hsacc 0.87213, csacc 0.66712, 51.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.99000, tacc(al1) 0.94000, tacc(al2) 0.84000, tacc(ob0) 0.98600, tacc(ob1) 0.95700, tacc(ob2) 0.87400, tacc(ob3) 0.97800, tacc(ob4) 0.92800, 7.09 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.05116, triplet_loss 0.44891, c_loss 0.24996, hs_loss 0.40483, cs_loss 0.99862, cacc 0.92250, hsacc 0.87500, csacc 0.67150, 52.16 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98900, tacc(al1) 0.94200, tacc(al2) 0.84500, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.87100, tacc(ob3) 0.98100, tacc(ob4) 0.92600, 7.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_56_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9259.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.04102, triplet_loss 0.45251, c_loss 0.24225, hs_loss 0.39145, cs_loss 0.99584, cacc 0.92056, hsacc 0.88087, csacc 0.67537, 50.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98900, tacc(al1) 0.94200, tacc(al2) 0.84200, tacc(ob0) 0.98500, tacc(ob1) 0.95700, tacc(ob2) 0.86400, tacc(ob3) 0.98100, tacc(ob4) 0.92800, 7.10 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.04743, triplet_loss 0.45049, c_loss 0.23738, hs_loss 0.40454, cs_loss 1.00244, cacc 0.92575, hsacc 0.88419, csacc 0.67050, 52.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98800, tacc(al1) 0.94200, tacc(al2) 0.84100, tacc(ob0) 0.98700, tacc(ob1) 0.95600, tacc(ob2) 0.86600, tacc(ob3) 0.98100, tacc(ob4) 0.93000, 7.14 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.04710, triplet_loss 0.44925, c_loss 0.25657, hs_loss 0.39626, cs_loss 0.99212, cacc 0.92950, hsacc 0.87656, csacc 0.67106, 51.81 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98900, tacc(al1) 0.94200, tacc(al2) 0.84000, tacc(ob0) 0.98600, tacc(ob1) 0.95700, tacc(ob2) 0.86600, tacc(ob3) 0.98200, tacc(ob4) 0.93000, 6.83 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.04702, triplet_loss 0.44970, c_loss 0.24903, hs_loss 0.39334, cs_loss 1.00196, cacc 0.92506, hsacc 0.87400, csacc 0.67138, 51.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98900, tacc(al1) 0.94200, tacc(al2) 0.84000, tacc(ob0) 0.98500, tacc(ob1) 0.95700, tacc(ob2) 0.86600, tacc(ob3) 0.98100, tacc(ob4) 0.93000, 6.95 secs\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.08219, triplet_loss 0.45488, c_loss 0.25121, hs_loss 0.41368, cs_loss 1.04460, cacc 0.93137, hsacc 0.87300, csacc 0.65506, 51.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.94200, tacc(al2) 0.84000, tacc(ob0) 0.98300, tacc(ob1) 0.95200, tacc(ob2) 0.86400, tacc(ob3) 0.97700, tacc(ob4) 0.92500, 7.02 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.07309, triplet_loss 0.45376, c_loss 0.25706, hs_loss 0.40901, cs_loss 1.02635, cacc 0.92400, hsacc 0.86950, csacc 0.65844, 52.58 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.93700, tacc(al2) 0.84800, tacc(ob0) 0.98100, tacc(ob1) 0.95800, tacc(ob2) 0.86200, tacc(ob3) 0.97800, tacc(ob4) 0.92100, 6.99 secs\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.04928, triplet_loss 0.45267, c_loss 0.24660, hs_loss 0.39926, cs_loss 1.00002, cacc 0.92619, hsacc 0.87438, csacc 0.67156, 51.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.94000, tacc(al2) 0.84300, tacc(ob0) 0.98300, tacc(ob1) 0.96000, tacc(ob2) 0.86200, tacc(ob3) 0.98100, tacc(ob4) 0.92800, 7.00 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.03947, triplet_loss 0.45060, c_loss 0.24091, hs_loss 0.39275, cs_loss 0.99467, cacc 0.92919, hsacc 0.87950, csacc 0.67319, 51.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.93800, tacc(al2) 0.84200, tacc(ob0) 0.98400, tacc(ob1) 0.95700, tacc(ob2) 0.86500, tacc(ob3) 0.97800, tacc(ob4) 0.93200, 6.95 secs\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.02626, triplet_loss 0.44894, c_loss 0.23806, hs_loss 0.38842, cs_loss 0.97710, cacc 0.93137, hsacc 0.87981, csacc 0.68244, 50.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93500, tacc(al2) 0.84500, tacc(ob0) 0.98300, tacc(ob1) 0.95700, tacc(ob2) 0.86400, tacc(ob3) 0.97900, tacc(ob4) 0.92700, 7.15 secs\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.01145, triplet_loss 0.45074, c_loss 0.23362, hs_loss 0.37531, cs_loss 0.96323, cacc 0.93019, hsacc 0.87900, csacc 0.67994, 52.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93600, tacc(al2) 0.84500, tacc(ob0) 0.98300, tacc(ob1) 0.95800, tacc(ob2) 0.86300, tacc(ob3) 0.98000, tacc(ob4) 0.92800, 7.13 secs\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.04122, triplet_loss 0.45075, c_loss 0.23681, hs_loss 0.39631, cs_loss 0.99857, cacc 0.93181, hsacc 0.87969, csacc 0.67544, 52.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98800, tacc(al1) 0.93500, tacc(al2) 0.84400, tacc(ob0) 0.98300, tacc(ob1) 0.95900, tacc(ob2) 0.86400, tacc(ob3) 0.97800, tacc(ob4) 0.92700, 7.17 secs\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.02579, triplet_loss 0.45018, c_loss 0.23233, hs_loss 0.38182, cs_loss 0.98726, cacc 0.93012, hsacc 0.87787, csacc 0.67050, 52.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93600, tacc(al2) 0.84300, tacc(ob0) 0.98300, tacc(ob1) 0.95900, tacc(ob2) 0.86500, tacc(ob3) 0.97900, tacc(ob4) 0.92800, 7.12 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.04725, triplet_loss 0.45150, c_loss 0.23109, hs_loss 0.40155, cs_loss 1.01035, cacc 0.92600, hsacc 0.87556, csacc 0.66275, 52.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.93600, tacc(al2) 0.84400, tacc(ob0) 0.98200, tacc(ob1) 0.95000, tacc(ob2) 0.86600, tacc(ob3) 0.98100, tacc(ob4) 0.92100, 7.12 secs\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.04464, triplet_loss 0.45205, c_loss 0.24293, hs_loss 0.40208, cs_loss 0.99222, cacc 0.92875, hsacc 0.87262, csacc 0.67231, 51.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.94200, tacc(al2) 0.84700, tacc(ob0) 0.98700, tacc(ob1) 0.95800, tacc(ob2) 0.87000, tacc(ob3) 0.98100, tacc(ob4) 0.92700, 7.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_70_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9261.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.04569, triplet_loss 0.45157, c_loss 0.23818, hs_loss 0.41803, cs_loss 0.98359, cacc 0.92888, hsacc 0.87356, csacc 0.67656, 51.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.93800, tacc(al2) 0.83700, tacc(ob0) 0.98500, tacc(ob1) 0.95700, tacc(ob2) 0.86600, tacc(ob3) 0.98100, tacc(ob4) 0.93100, 6.87 secs\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 1.03548, triplet_loss 0.45179, c_loss 0.24578, hs_loss 0.40222, cs_loss 0.97115, cacc 0.92888, hsacc 0.87862, csacc 0.67700, 51.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93600, tacc(al2) 0.84100, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.86400, tacc(ob3) 0.98500, tacc(ob4) 0.93000, 7.07 secs\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.01442, triplet_loss 0.44898, c_loss 0.23328, hs_loss 0.38678, cs_loss 0.95981, cacc 0.93100, hsacc 0.88419, csacc 0.68306, 52.52 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.93400, tacc(al2) 0.84200, tacc(ob0) 0.98600, tacc(ob1) 0.95800, tacc(ob2) 0.86700, tacc(ob3) 0.98300, tacc(ob4) 0.93200, 6.85 secs\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.02227, triplet_loss 0.44718, c_loss 0.23213, hs_loss 0.37584, cs_loss 0.98938, cacc 0.93150, hsacc 0.88638, csacc 0.66794, 51.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93300, tacc(al2) 0.84100, tacc(ob0) 0.98600, tacc(ob1) 0.95800, tacc(ob2) 0.86500, tacc(ob3) 0.98200, tacc(ob4) 0.93200, 6.99 secs\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.00744, triplet_loss 0.44822, c_loss 0.23733, hs_loss 0.37305, cs_loss 0.95628, cacc 0.92988, hsacc 0.88231, csacc 0.68531, 51.03 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93500, tacc(al2) 0.84000, tacc(ob0) 0.98600, tacc(ob1) 0.95700, tacc(ob2) 0.86600, tacc(ob3) 0.98200, tacc(ob4) 0.93200, 7.06 secs\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.01087, triplet_loss 0.44831, c_loss 0.22926, hs_loss 0.37883, cs_loss 0.96534, cacc 0.92788, hsacc 0.88575, csacc 0.67937, 51.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98800, tacc(al1) 0.93400, tacc(al2) 0.83800, tacc(ob0) 0.98700, tacc(ob1) 0.95900, tacc(ob2) 0.86700, tacc(ob3) 0.98200, tacc(ob4) 0.93300, 7.11 secs\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.05132, triplet_loss 0.45257, c_loss 0.25263, hs_loss 0.40186, cs_loss 0.99557, cacc 0.92412, hsacc 0.87250, csacc 0.66769, 51.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.93500, tacc(al2) 0.85000, tacc(ob0) 0.98700, tacc(ob1) 0.95500, tacc(ob2) 0.86700, tacc(ob3) 0.98200, tacc(ob4) 0.92400, 7.00 secs\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 1.04793, triplet_loss 0.44881, c_loss 0.23701, hs_loss 0.42138, cs_loss 0.98866, cacc 0.92206, hsacc 0.86800, csacc 0.66769, 51.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.93100, tacc(al2) 0.85300, tacc(ob0) 0.98800, tacc(ob1) 0.95500, tacc(ob2) 0.87000, tacc(ob3) 0.98100, tacc(ob4) 0.93700, 7.05 secs\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.01997, triplet_loss 0.44904, c_loss 0.23358, hs_loss 0.37542, cs_loss 0.98191, cacc 0.92563, hsacc 0.88106, csacc 0.66944, 49.41 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tacc(al0) 0.98300, tacc(al1) 0.93200, tacc(al2) 0.84300, tacc(ob0) 0.98600, tacc(ob1) 0.95800, tacc(ob2) 0.85900, tacc(ob3) 0.98100, tacc(ob4) 0.93500, 7.10 secs\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.99661, triplet_loss 0.44946, c_loss 0.22802, hs_loss 0.36781, cs_loss 0.94793, cacc 0.92750, hsacc 0.87975, csacc 0.68294, 49.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.93300, tacc(al2) 0.84500, tacc(ob0) 0.98300, tacc(ob1) 0.95700, tacc(ob2) 0.86200, tacc(ob3) 0.98100, tacc(ob4) 0.93100, 7.14 secs\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.01322, triplet_loss 0.44986, c_loss 0.22876, hs_loss 0.38458, cs_loss 0.96324, cacc 0.93288, hsacc 0.87925, csacc 0.68194, 50.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.93000, tacc(al2) 0.84700, tacc(ob0) 0.98400, tacc(ob1) 0.95600, tacc(ob2) 0.86200, tacc(ob3) 0.98200, tacc(ob4) 0.93500, 7.01 secs\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.99694, triplet_loss 0.44924, c_loss 0.22994, hs_loss 0.36981, cs_loss 0.94490, cacc 0.92844, hsacc 0.88281, csacc 0.67850, 52.16 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.93000, tacc(al2) 0.84400, tacc(ob0) 0.98500, tacc(ob1) 0.95700, tacc(ob2) 0.86200, tacc(ob3) 0.98200, tacc(ob4) 0.93500, 7.07 secs\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.99561, triplet_loss 0.44858, c_loss 0.22177, hs_loss 0.37209, cs_loss 0.94878, cacc 0.92819, hsacc 0.88294, csacc 0.68306, 51.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98500, tacc(al1) 0.93100, tacc(al2) 0.84400, tacc(ob0) 0.98400, tacc(ob1) 0.95700, tacc(ob2) 0.86700, tacc(ob3) 0.98200, tacc(ob4) 0.93500, 6.96 secs\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "Current run is terminating due to exception: CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 10.76 GiB total capacity; 8.71 GiB already allocated; 216.56 MiB free; 9.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 10.76 GiB total capacity; 8.71 GiB already allocated; 216.56 MiB free; 9.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 463, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 374, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 231, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1087, in _run_once_on_dataset_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 124, in step_fn_wrapper\n",
      "    output = step_fn__triplet_ranking(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 45, in step_fn__triplet_ranking\n",
      "    n_embeddings = model(input_ids=n_input_ids, attention_mask=n_attention_mask)['text_embeddings']\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/fact_encoder.py\", line 58, in forward\n",
      "    text_embeddings = self.model.get_projected_text_embeddings(input_ids=input_ids, attention_mask=attention_mask)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1373, in forward\n",
      "    prediction_scores = self.cls(sequence_output)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 710, in forward\n",
      "    prediction_scores = self.predictions(sequence_output)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 700, in forward\n",
      "    hidden_states = self.decoder(hidden_states)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 10.76 GiB total capacity; 8.71 GiB already allocated; 216.56 MiB free; 9.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 400 \\\n",
    "--batch_size 80 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 4 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(2286627,297669,1996654,2501000,1000).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [0.8, 1.6, 0.8], 'observations': [1., 2., 1., 1., 2.]}\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).improved_comparison(6526297).jsonl\" \\\n",
    "--paraphrases_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "--dataset_name \"MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132aa588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 1000\n",
      "   batch_size: 75\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_045133_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 8\n",
      "   override_lr: False\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(2286627,297669,1996654,2501000,1000).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [0.8, 1.6, 0.8], 'observations': [1.0, 2.0, 1.0, 1.0, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).improved_comparison(6526297).jsonl\n",
      "   paraphrases_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl']\n",
      "   dataset_name: MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\n",
      "   triplets_weight: 1.0\n",
      "   classification_weight: 1.0\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: True\n",
      "  n_categories: 6\n",
      "  classify_health_status: True\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: True\n",
      "  n_comparison_statuses: 15\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 8\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(2286627,297669,1996654,2501000,1000).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 2880494\n",
      "\tWeight: 0.8\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 2868295\n",
      "\tWeight: 1.6\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 2001430\n",
      "\tWeight: 0.8\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3769449\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3762709\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 2217029\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2473084\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other - Hard negative\"\n",
      "\tNumber of triplets: 4429094\n",
      "\tWeight: 2.0\n",
      "----\n",
      "\u001b[1mBuilding train classification dataset and dataloader...\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "Loading paraphrases from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl...\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n",
      "Number of sentences with paraphrases: 109887\n",
      "Number of paraphrases: 1299918\n",
      "Loading integrated facts metadata...\n",
      "Number of facts: 2188212\n",
      "Number of improved comparisons: 521435/578733\n",
      "Category: anatomical finding -> 1564922\n",
      "Category: tubes and lines -> 235687\n",
      "Category: technical assessment -> 150516\n",
      "Category: disease -> 122596\n",
      "Category: device -> 111660\n",
      "Category: other -> 2831\n",
      "Health status: abnormal -> 1283130\n",
      "Health status: unknown -> 520758\n",
      "Health status: normal -> 246373\n",
      "Health status: ambiguous -> 136998\n",
      "Health status: other -> 953\n",
      "Comparison status: no comparison -> 1204429\n",
      "Comparison status: stable/unchanged -> 248878\n",
      "Comparison status: worsened -> 125579\n",
      "Comparison status: improved -> 99848\n",
      "Comparison status: resolved -> 91368\n",
      "Comparison status: new finding -> 83889\n",
      "Comparison status: unclear comparison -> 75262\n",
      "Comparison status: position changed -> 67257\n",
      "Comparison status: increase -> 58489\n",
      "Comparison status: decrease -> 38620\n",
      "Comparison status: progressed -> 31572\n",
      "Comparison status: smaller -> 24358\n",
      "Comparison status: larger -> 22410\n",
      "Comparison status: reappeared -> 15813\n",
      "Comparison status: other -> 440\n",
      "\u001b[1mExample fact: increased opacification in the left lower zone has decreased\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: decrease\n",
      "\u001b[1mExample fact: X-ray does not show any indication of tooth aspiration\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: normal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: the proximal bronchus intermedius is partially obstructed by the tip of the Dobbhoff feeding tube\u001b[0m\n",
      "Category: tubes and lines\n",
      "Health status: unknown\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: all other lines in standard positions\u001b[0m\n",
      "Category: tubes and lines\n",
      "Health status: normal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: Multifocal infiltrates are observed in the alveoli\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: Consolidation seen, possibly secondary to aspiration\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: ambiguous\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: postoperative changes projecting over the neck\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: unknown\n",
      "Comparison status: no comparison\n",
      "\u001b[1mExample fact: slightly improved atelectasis\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: improved\n",
      "\u001b[1mExample fact: stable small bibasilar effusions\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: stable/unchanged\n",
      "\u001b[1mExample fact: patchy relatively confluent opacities\u001b[0m\n",
      "Category: anatomical finding\n",
      "Health status: abnormal\n",
      "Comparison status: no comparison\n",
      "Comparison status: no comparison\n",
      "\tNumber of samples: 1204429\n",
      "\tWeight: 8242.307524442282\n",
      "Comparison status: worsened\n",
      "\tNumber of samples: 125579\n",
      "\tWeight: 4859.644675613537\n",
      "Comparison status: resolved\n",
      "\tNumber of samples: 91368\n",
      "\tWeight: 4475.3220512437665\n",
      "Comparison status: stable/unchanged\n",
      "\tNumber of samples: 248878\n",
      "\tWeight: 5759.479652244662\n",
      "Comparison status: progressed\n",
      "\tNumber of samples: 31572\n",
      "\tWeight: 3338.9210042448967\n",
      "Comparison status: larger\n",
      "\tNumber of samples: 22410\n",
      "\tWeight: 3018.358277880451\n",
      "Comparison status: improved\n",
      "\tNumber of samples: 99848\n",
      "\tWeight: 4580.4541460194605\n",
      "Comparison status: reappeared\n",
      "\tNumber of samples: 15813\n",
      "\tWeight: 2714.0180702391563\n",
      "Comparison status: smaller\n",
      "\tNumber of samples: 24358\n",
      "\tWeight: 3094.333715211565\n",
      "Comparison status: position changed\n",
      "\tNumber of samples: 67257\n",
      "\tWeight: 4124.787930176123\n",
      "Comparison status: new finding\n",
      "\tNumber of samples: 83889\n",
      "\tWeight: 4375.692166468171\n",
      "Comparison status: unclear comparison\n",
      "\tNumber of samples: 75262\n",
      "\tWeight: 4251.23985322021\n",
      "Comparison status: increase\n",
      "\tNumber of samples: 58489\n",
      "\tWeight: 3971.242589056373\n",
      "Comparison status: decrease\n",
      "\tNumber of samples: 38620\n",
      "\tWeight: 3537.55808591334\n",
      "Comparison status: other\n",
      "\tNumber of samples: 440\n",
      "\tWeight: 677.1506551206787\n",
      "----\n",
      "\u001b[1mBuilding val dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_061431_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_061431_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_70_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9261.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_045133_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_70_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9261.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_061431_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.02588, triplet_loss 0.45110, c_loss 0.24075, hs_loss 0.38914, cs_loss 0.97076, cacc 0.93293, hsacc 0.87875, csacc 0.68053, 117.42 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.94000, tacc(al2) 0.84400, tacc(ob0) 0.98700, tacc(ob1) 0.95900, tacc(ob2) 0.87200, tacc(ob3) 0.98300, tacc(ob4) 0.92800, 6.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_cacc+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)=0.9268.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.02187, triplet_loss 0.44861, c_loss 0.24315, hs_loss 0.37860, cs_loss 0.97338, cacc 0.92936, hsacc 0.88147, csacc 0.68067, 121.18 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93800, tacc(al2) 0.84300, tacc(ob0) 0.98600, tacc(ob1) 0.95700, tacc(ob2) 0.86900, tacc(ob3) 0.98000, tacc(ob4) 0.92900, 7.11 secs\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 1.01520, triplet_loss 0.44918, c_loss 0.22307, hs_loss 0.38401, cs_loss 0.97412, cacc 0.93083, hsacc 0.87963, csacc 0.67568, 123.31 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93200, tacc(al2) 0.84300, tacc(ob0) 0.98100, tacc(ob1) 0.95800, tacc(ob2) 0.86700, tacc(ob3) 0.97900, tacc(ob4) 0.92700, 7.09 secs\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 1.02659, triplet_loss 0.45243, c_loss 0.22752, hs_loss 0.38916, cs_loss 0.98407, cacc 0.92824, hsacc 0.87603, csacc 0.67187, 120.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93300, tacc(al2) 0.84400, tacc(ob0) 0.98100, tacc(ob1) 0.95700, tacc(ob2) 0.87300, tacc(ob3) 0.97200, tacc(ob4) 0.93400, 7.08 secs\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 1.01621, triplet_loss 0.45117, c_loss 0.22220, hs_loss 0.38565, cs_loss 0.97340, cacc 0.93075, hsacc 0.87803, csacc 0.67757, 122.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.93900, tacc(al2) 0.83200, tacc(ob0) 0.98200, tacc(ob1) 0.95400, tacc(ob2) 0.86700, tacc(ob3) 0.98400, tacc(ob4) 0.92900, 6.93 secs\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.99523, triplet_loss 0.45130, c_loss 0.21911, hs_loss 0.36708, cs_loss 0.95297, cacc 0.93029, hsacc 0.88331, csacc 0.68363, 123.83 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93900, tacc(al2) 0.83200, tacc(ob0) 0.98100, tacc(ob1) 0.95900, tacc(ob2) 0.87100, tacc(ob3) 0.98500, tacc(ob4) 0.93300, 6.92 secs\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.98757, triplet_loss 0.44894, c_loss 0.22577, hs_loss 0.36010, cs_loss 0.94034, cacc 0.93171, hsacc 0.88621, csacc 0.68867, 124.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93300, tacc(al2) 0.83100, tacc(ob0) 0.98300, tacc(ob1) 0.95800, tacc(ob2) 0.87200, tacc(ob3) 0.98300, tacc(ob4) 0.93000, 7.07 secs\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.98659, triplet_loss 0.44794, c_loss 0.21413, hs_loss 0.37335, cs_loss 0.93777, cacc 0.93523, hsacc 0.88021, csacc 0.68584, 124.91 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98700, tacc(al1) 0.93900, tacc(al2) 0.83200, tacc(ob0) 0.98200, tacc(ob1) 0.95600, tacc(ob2) 0.87300, tacc(ob3) 0.97900, tacc(ob4) 0.93600, 6.91 secs\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.97228, triplet_loss 0.44771, c_loss 0.21665, hs_loss 0.36024, cs_loss 0.91997, cacc 0.93261, hsacc 0.88643, csacc 0.69304, 123.30 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98800, tacc(al1) 0.94000, tacc(al2) 0.82900, tacc(ob0) 0.98300, tacc(ob1) 0.95800, tacc(ob2) 0.87200, tacc(ob3) 0.98000, tacc(ob4) 0.93100, 6.88 secs\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.97903, triplet_loss 0.44929, c_loss 0.21318, hs_loss 0.36945, cs_loss 0.92613, cacc 0.93331, hsacc 0.88475, csacc 0.69357, 120.04 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98800, tacc(al1) 0.94000, tacc(al2) 0.83100, tacc(ob0) 0.98300, tacc(ob1) 0.95700, tacc(ob2) 0.87200, tacc(ob3) 0.98000, tacc(ob4) 0.93300, 6.85 secs\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "   iteration 10175\r"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230721_045133_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 1000 \\\n",
    "--batch_size 75 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 8 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(2286627,297669,1996654,2501000,1000).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [0.8, 1.6, 0.8], 'observations': [1., 2., 1., 1., 2.]}\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).improved_comparison(6526297).jsonl\" \\\n",
    "--paraphrases_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "--dataset_name \"MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cf97a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR(GPT3.5+CXR-Bert-based-labels)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\r\n"
     ]
    }
   ],
   "source": [
    "!cd /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230719_132018_MIMIC-CXR\\(GPT3.5+CXR-Bert-based-labels\\)_FactEncoder\\(microsoft-BiomedVLP-CXR-BERT-specialized\\) && pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
