{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.train_mae import debug_main\n",
    "import shlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 1\n",
      "   batches_per_epoch: 20\n",
      "   checkpoint_folder: None\n",
      "   pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "   optimizer_name: adamw\n",
      "   num_accumulation_steps: 1\n",
      "   lr: 1e-06\n",
      "   override_lr: False\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,4,4e-4,76,5e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   num_workers: 3\n",
      "   batch_size: 80\n",
      "   device: GPU\n",
      "   padchest_weight: 1.0\n",
      "   use_amp: True\n",
      "   use_padchest: True\n",
      "   padchest_training_data_mode: all\n",
      "   padchest_use_validation: True\n",
      "   padchest_train_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\n",
      "   padchest_val_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\n",
      "   padchest_test_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\n",
      "   save: False\n",
      "\u001b[34m------------------- Training ViTMAE from scratch -------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mDevice: cuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of ViTMAEForPreTraining ...\u001b[0m\n",
      "pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mCreating optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mCreating learning rate scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,4,4e-4,76,5e-6\n",
      "1e-06 4 0.0004 76 5e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mCreating instance of ViTFeatureExtractor ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mCreating image transform ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating PadChest MAE trainer ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pamessina/medvqa/medvqa/train_mae.py:387: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug=debug,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset and dataloader ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:40<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Train dataset size: 1000000000000000000\n",
      "Creating val dataset and dataloader ...\n",
      "Done!\n",
      "Val dataset size: 5669\n"
     ]
    }
   ],
   "source": [
    "output = debug_main(args=shlex.split(\n",
    "    ' '.join([        \n",
    "        '--epochs 1',\n",
    "        '--batches-per-epoch 20',\n",
    "        '--batch-size 80',\n",
    "        '--num-accumulation-steps 1',\n",
    "        '--num-workers 3',\n",
    "        '--optimizer-name \"adamw\"',\n",
    "        '--scheduler \"warmup+decay\"',\n",
    "        '--lr 1e-6',\n",
    "        '--warmup-and-decay-args \"1e-6,4,4e-4,76,5e-6\"',\n",
    "        '--use-padchest',\n",
    "        '--padchest-use-validation',\n",
    "        '--padchest-weight 1',\n",
    "        '--padchest-train-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\"',\n",
    "        '--padchest-val-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\"',\n",
    "        '--padchest-test-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\"',\n",
    "        '--use-amp',\n",
    "        '--no-save',\n",
    "    ])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['padchest_mae_trainer'].train_dataset[0]['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'i': tensor([[[ 0.8104,  0.8447,  0.8961,  ..., -1.0219, -1.0733, -1.1075],\n",
       "          [ 0.8789,  0.8961,  0.9132,  ..., -1.0390, -1.0904, -1.1247],\n",
       "          [ 0.9474,  0.9646,  0.9303,  ..., -1.0562, -1.0904, -1.1247],\n",
       "          ...,\n",
       "          [ 0.5364,  0.5536,  0.5878,  ...,  1.2899,  1.2043,  1.1358],\n",
       "          [ 0.5022,  0.5364,  0.5878,  ...,  1.4954,  1.4440,  1.4098],\n",
       "          [ 0.5022,  0.5193,  0.5707,  ...,  1.7523,  1.7009,  1.6667]],\n",
       " \n",
       "         [[ 0.9580,  0.9930,  1.0455,  ..., -0.9153, -0.9678, -1.0028],\n",
       "          [ 1.0280,  1.0455,  1.0630,  ..., -0.9328, -0.9853, -1.0203],\n",
       "          [ 1.0980,  1.1155,  1.0805,  ..., -0.9503, -0.9853, -1.0203],\n",
       "          ...,\n",
       "          [ 0.6779,  0.6954,  0.7304,  ...,  1.4482,  1.3606,  1.2906],\n",
       "          [ 0.6429,  0.6779,  0.7304,  ...,  1.6583,  1.6057,  1.5707],\n",
       "          [ 0.6429,  0.6604,  0.7129,  ...,  1.9209,  1.8683,  1.8333]],\n",
       " \n",
       "         [[ 1.1759,  1.2108,  1.2631,  ..., -0.6890, -0.7413, -0.7761],\n",
       "          [ 1.2457,  1.2631,  1.2805,  ..., -0.7064, -0.7587, -0.7936],\n",
       "          [ 1.3154,  1.3328,  1.2980,  ..., -0.7238, -0.7587, -0.7936],\n",
       "          ...,\n",
       "          [ 0.8971,  0.9145,  0.9494,  ...,  1.6640,  1.5768,  1.5071],\n",
       "          [ 0.8622,  0.8971,  0.9494,  ...,  1.8731,  1.8208,  1.7860],\n",
       "          [ 0.8622,  0.8797,  0.9319,  ...,  2.1346,  2.0823,  2.0474]]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['padchest_mae_trainer'].val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 400\n",
      "   checkpoint_folder: None\n",
      "   pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "   optimizer_name: adamw\n",
      "   num_accumulation_steps: 4\n",
      "   lr: 1e-06\n",
      "   override_lr: False\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,8e-5,95,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   num_workers: 4\n",
      "   batch_size: 120\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   use_mimiccxr: True\n",
      "   use_iuxray: True\n",
      "   use_chexpert: True\n",
      "   use_cxr14: True\n",
      "   use_vinbig: True\n",
      "   use_padchest: True\n",
      "   mimiccxr_weight: 1.0\n",
      "   iuxray_weight: 0.05\n",
      "   chexpert_weight: 0.6\n",
      "   cxr14_weight: 0.4\n",
      "   vinbig_weight: 0.4\n",
      "   padchest_weight: 0.5\n",
      "   mimiccxr_qa_adapted_reports_filename: qa_adapted_reports__20220904_095810.json\n",
      "   mimiccxr_chexpert_labels_filename: chexpert_labels_per_report__20220904_113605.pkl\n",
      "   iuxray_qa_adapted_reports_filename: qa_adapted_reports__20220904_091601.json\n",
      "   iuxray_chexpert_labels_filename: chexpert_labels_per_report__20220904_113427.pkl\n",
      "   padchest_training_data_mode: all\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\n",
      "   padchest_val_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\n",
      "   padchest_test_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\n",
      "   save: True\n",
      "\u001b[34m------------------- Training ViTMAE from scratch -------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mDevice: cuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mCreating instance of ViTMAEForPreTraining ...\u001b[0m\n",
      "pretrained_model_name_or_path: facebook/vit-mae-base\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mCreating optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mCreating learning rate scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,8e-5,95,1e-6\n",
      "1e-06 5 8e-05 95 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mCreating instance of ViTFeatureExtractor ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mCreating image transform ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR MAE trainer ...\u001b[0m\n",
      "Creating train dataset and dataloader ...\n",
      "100%|███████████████████████████████████████████| 13/13 [00:05<00:00,  2.32it/s]\n",
      "Done!\n",
      "len(train_indices) = 368953\n",
      "Creating val dataset and dataloader ...\n",
      "Done!\n",
      "len(val_indices) = 2991\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating IU-XRAY MAE trainer ...\u001b[0m\n",
      "Creating train dataset and dataloader ...\n",
      "100%|██████████████████████████████████████████| 13/13 [00:00<00:00, 117.86it/s]\n",
      "Done!\n",
      "len(train_indices) = 7321\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating CheXpert MAE trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/workspace/chexpert/CheXpert-v1.0-small/train-val.csv\n",
      "Loading images\n",
      "Loading chexpert labels\n",
      "Creating train dataset and dataloader ...\n",
      "100%|███████████████████████████████████████████| 13/13 [00:03<00:00,  3.77it/s]\n",
      "Done!\n",
      "len(train_indices) = 223648\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mCreating CXR14 MAE trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/data/chest-x-ray-8/metadata/Data_Entry_2017_v2020.csv\n",
      "Loading images\n",
      "Loading cxr14 labels\n",
      "Creating train dataset and dataloader ...\n",
      "100%|███████████████████████████████████████████| 15/15 [00:01<00:00,  7.58it/s]\n",
      "Done!\n",
      "len(train_indices) = 112120\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mCreating VinBig MAE trainer ...\u001b[0m\n",
      "Creating train dataset and dataloader ...\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 46.58it/s]\n",
      "Done!\n",
      "len(train_indices) = 18000\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mCreating PadChest MAE trainer ...\u001b[0m\n",
      "../train_mae.py:575: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug=debug,\n",
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n",
      "Creating train dataset and dataloader ...\n",
      "100%|█████████████████████████████████████████| 193/193 [00:39<00:00,  4.87it/s]\n",
      "Done!\n",
      "len(train_indices) = 160692\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 6\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0, 0.05, 0.6, 0.4, 0.4, 0.5]\n",
      "merged_dataset_name = mim+iux+chx+cxr14+vinbig+padchest\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m15) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m16) \u001b[0m\u001b[34mCreating learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m17) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m18) \u001b[0m\u001b[34mCreating log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m19) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06789, 150.95 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04712, 3.52 secs\n",
      "Adjusting learning rate of group 0 to 2.4022e-06.\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06578, 135.28 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.05045, 3.52 secs\n",
      "Adjusting learning rate of group 0 to 5.7708e-06.\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.06569, 135.84 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04530, 3.49 secs\n",
      "Adjusting learning rate of group 0 to 1.3863e-05.\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.05706, 142.54 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04810, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 3.3302e-05.\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000033) ...\n",
      "loss 0.06256, 146.78 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04335, 3.51 secs\n",
      "Adjusting learning rate of group 0 to 8.0000e-05.\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.04247, 149.00 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04403, 3.56 secs\n",
      "Adjusting learning rate of group 0 to 7.6394e-05.\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000076) ...\n",
      "loss 0.04153, 143.40 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04128, 3.57 secs\n",
      "Adjusting learning rate of group 0 to 7.2950e-05.\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000073) ...\n",
      "loss 0.07552, 141.13 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04561, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 6.9661e-05.\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000070) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.08270, 149.42 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04084, 3.55 secs\n",
      "Adjusting learning rate of group 0 to 6.6521e-05.\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000067) ...\n",
      "loss 0.03145, 148.71 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04638, 3.62 secs\n",
      "Adjusting learning rate of group 0 to 6.3522e-05.\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000064) ...\n",
      "loss 0.04253, 146.65 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03968, 3.56 secs\n",
      "Adjusting learning rate of group 0 to 6.0659e-05.\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000061) ...\n",
      "loss 0.03853, 148.04 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03904, 3.64 secs\n",
      "Adjusting learning rate of group 0 to 5.7925e-05.\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000058) ...\n",
      "loss 0.03630, 146.06 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04106, 3.58 secs\n",
      "Adjusting learning rate of group 0 to 5.5313e-05.\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000055) ...\n",
      "loss 0.07582, 144.61 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03857, 3.63 secs\n",
      "Adjusting learning rate of group 0 to 5.2820e-05.\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.08059, 145.10 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04130, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 5.0439e-05.\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000050) ...\n",
      "loss 0.05066, 145.76 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04818, 3.55 secs\n",
      "Adjusting learning rate of group 0 to 4.8165e-05.\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000048) ...\n",
      "loss 0.05648, 147.10 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03990, 3.52 secs\n",
      "Adjusting learning rate of group 0 to 4.5994e-05.\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.04326, 143.68 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03989, 3.64 secs\n",
      "Adjusting learning rate of group 0 to 4.3920e-05.\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.04450, 147.06 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04209, 3.51 secs\n",
      "Adjusting learning rate of group 0 to 4.1941e-05.\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000042) ...\n",
      "loss 0.05006, 141.14 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04135, 3.58 secs\n",
      "Adjusting learning rate of group 0 to 4.0050e-05.\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000040) ...\n",
      "loss 0.02953, 148.33 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03890, 3.58 secs\n",
      "Adjusting learning rate of group 0 to 3.8245e-05.\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000038) ...\n",
      "loss 0.07275, 148.44 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03743, 3.60 secs\n",
      "Adjusting learning rate of group 0 to 3.6521e-05.\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000037) ...\n",
      "loss 0.07596, 142.61 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03873, 3.50 secs\n",
      "Adjusting learning rate of group 0 to 3.4874e-05.\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000035) ...\n",
      "loss 0.02849, 144.68 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04176, 3.60 secs\n",
      "Adjusting learning rate of group 0 to 3.3302e-05.\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000033) ...\n",
      "loss 0.02648, 141.86 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04280, 3.54 secs\n",
      "Adjusting learning rate of group 0 to 3.1801e-05.\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.02492, 145.63 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03888, 3.54 secs\n",
      "Adjusting learning rate of group 0 to 3.0367e-05.\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 0.02541, 146.28 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04140, 3.54 secs\n",
      "Adjusting learning rate of group 0 to 2.8998e-05.\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000029) ...\n",
      "loss 0.02766, 149.62 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03579, 3.64 secs\n",
      "Adjusting learning rate of group 0 to 2.7691e-05.\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.07155, 143.72 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03812, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 2.6443e-05.\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000026) ...\n",
      "loss 0.04602, 140.93 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03993, 3.62 secs\n",
      "Adjusting learning rate of group 0 to 2.5251e-05.\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000025) ...\n",
      "loss 0.04192, 149.76 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03776, 3.58 secs\n",
      "Adjusting learning rate of group 0 to 2.4113e-05.\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n",
      "loss 0.04189, 147.63 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03858, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 2.3026e-05.\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.05049, 143.80 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03747, 3.54 secs\n",
      "Adjusting learning rate of group 0 to 2.1988e-05.\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.04283, 143.64 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03462, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 2.0996e-05.\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.03469, 140.73 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03615, 3.58 secs\n",
      "Adjusting learning rate of group 0 to 2.0050e-05.\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 0.03391, 145.67 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03716, 3.56 secs\n",
      "Adjusting learning rate of group 0 to 1.9146e-05.\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.07446, 140.96 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03722, 3.53 secs\n",
      "Adjusting learning rate of group 0 to 1.8283e-05.\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.06867, 148.41 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03877, 3.58 secs\n",
      "Adjusting learning rate of group 0 to 1.7459e-05.\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.02570, 141.78 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03704, 3.60 secs\n",
      "Adjusting learning rate of group 0 to 1.6672e-05.\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000017) ...\n",
      "loss 0.03480, 145.45 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03488, 3.57 secs\n",
      "Adjusting learning rate of group 0 to 1.5920e-05.\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.03619, 140.56 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03664, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 1.5203e-05.\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.03812, 145.73 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03791, 3.50 secs\n",
      "Adjusting learning rate of group 0 to 1.4517e-05.\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.07326, 149.28 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03920, 3.56 secs\n",
      "Adjusting learning rate of group 0 to 1.3863e-05.\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.06922, 141.38 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03666, 3.62 secs\n",
      "Adjusting learning rate of group 0 to 1.3238e-05.\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.04139, 141.24 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03824, 3.52 secs\n",
      "Adjusting learning rate of group 0 to 1.2641e-05.\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.05392, 143.05 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03774, 3.56 secs\n",
      "Adjusting learning rate of group 0 to 1.2071e-05.\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.05250, 145.81 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03767, 3.66 secs\n",
      "Adjusting learning rate of group 0 to 1.1527e-05.\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.04126, 139.34 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03692, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 1.1008e-05.\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.04769, 142.44 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03646, 3.57 secs\n",
      "Adjusting learning rate of group 0 to 1.0511e-05.\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 0.02744, 148.73 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04024, 3.62 secs\n",
      "Adjusting learning rate of group 0 to 1.0038e-05.\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07455, 148.32 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03401, 3.57 secs\n",
      "Adjusting learning rate of group 0 to 9.5850e-06.\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07515, 147.93 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.04037, 3.60 secs\n",
      "Adjusting learning rate of group 0 to 9.1530e-06.\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.02991, 148.63 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03639, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 8.7403e-06.\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.02590, 144.47 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03800, 3.62 secs\n",
      "Adjusting learning rate of group 0 to 8.3463e-06.\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.02896, 144.46 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04250, 3.57 secs\n",
      "Adjusting learning rate of group 0 to 7.9701e-06.\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.02791, 146.80 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03751, 3.56 secs\n",
      "Adjusting learning rate of group 0 to 7.6108e-06.\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.02671, 151.07 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03635, 3.54 secs\n",
      "Adjusting learning rate of group 0 to 7.2677e-06.\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.06908, 149.26 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03811, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 6.9401e-06.\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.04583, 138.43 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03742, 3.56 secs\n",
      "Adjusting learning rate of group 0 to 6.6273e-06.\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.04496, 149.25 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03723, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 6.3285e-06.\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.04881, 149.54 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03691, 3.55 secs\n",
      "Adjusting learning rate of group 0 to 6.0432e-06.\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.04149, 141.66 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03605, 3.54 secs\n",
      "Adjusting learning rate of group 0 to 5.7708e-06.\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.04213, 142.50 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03572, 3.64 secs\n",
      "Adjusting learning rate of group 0 to 5.5107e-06.\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.03506, 149.38 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03672, 3.61 secs\n",
      "Adjusting learning rate of group 0 to 5.2622e-06.\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.03190, 147.07 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03571, 3.53 secs\n",
      "Adjusting learning rate of group 0 to 5.0250e-06.\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.06520, 142.20 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03699, 3.55 secs\n",
      "Adjusting learning rate of group 0 to 4.7985e-06.\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.06871, 145.52 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03600, 3.61 secs\n",
      "Adjusting learning rate of group 0 to 4.5822e-06.\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.02773, 141.18 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03670, 3.63 secs\n",
      "Adjusting learning rate of group 0 to 4.3756e-06.\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.03588, 143.08 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03501, 3.50 secs\n",
      "Adjusting learning rate of group 0 to 4.1784e-06.\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.03564, 141.78 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.04021, 3.66 secs\n",
      "Adjusting learning rate of group 0 to 3.9900e-06.\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.03358, 142.38 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03695, 3.62 secs\n",
      "Adjusting learning rate of group 0 to 3.8102e-06.\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06824, 141.62 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03622, 3.57 secs\n",
      "Adjusting learning rate of group 0 to 3.4744e-06.\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.04111, 146.89 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03650, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 3.3178e-06.\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.04802, 140.57 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03758, 3.59 secs\n",
      "Adjusting learning rate of group 0 to 3.1682e-06.\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.04318, 147.53 secs\n",
      "(2) Validation stage ...\n",
      "loss 0.03696, 3.54 secs\n",
      "Adjusting learning rate of group 0 to 3.0254e-06.\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "^C\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"../train_mae.py\", line 669, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"../train_mae.py\", line 575, in train_from_scratch\n",
      "    debug=debug,\n",
      "  File \"../train_mae.py\", line 439, in _train_model\n",
      "    epoch_length = batches_per_epoch)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 701, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 774, in _internal_run\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 469, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 744, in _internal_run\n",
      "    time_taken = self._run_once_on_dataset()\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 835, in _run_once_on_dataset\n",
      "    self._fire_event(Events.ITERATION_COMPLETED)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/engine/engine.py\", line 424, in _fire_event\n",
      "    func(*first, *(event_args + others), **kwargs)\n",
      "  File \"/home/pamessina/venv/lib/python3.6/site-packages/ignite/metrics/metric.py\", line 351, in completed\n",
      "    result = result.item()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_mae.py \\\n",
    "        --epochs 100 \\\n",
    "        --batches-per-epoch 400 \\\n",
    "        --batch-size 120 \\\n",
    "        --num-accumulation-steps 4 \\\n",
    "        --num-workers 4 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,8e-5,95,1e-6\" \\\n",
    "        --use-mimiccxr \\\n",
    "        --use-iuxray \\\n",
    "        --use-chexpert \\\n",
    "        --use-cxr14 \\\n",
    "        --use-vinbig \\\n",
    "        --use-padchest \\\n",
    "        --mimiccxr-qa-adapted-reports-filename \"qa_adapted_reports__20220904_095810.json\" \\\n",
    "        --mimiccxr-chexpert-labels-filename \"chexpert_labels_per_report__20220904_113605.pkl\" \\\n",
    "        --iuxray-qa-adapted-reports-filename \"qa_adapted_reports__20220904_091601.json\" \\\n",
    "        --iuxray-chexpert-labels-filename \"chexpert_labels_per_report__20220904_113427.pkl\" \\\n",
    "        --padchest-train-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-val-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-test-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\" \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
