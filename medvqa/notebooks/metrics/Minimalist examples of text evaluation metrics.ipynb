{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e4ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient\",\n",
    "]\n",
    "\n",
    "gen_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The fast brown fox leaped over the lazy dog.\",\n",
    "    \"Being or not being, that's what's being questioned.\",\n",
    "    \"Everything that sparkles isn't necessarily gold.\",\n",
    "    \"The patient has cardiomegaly\",\n",
    "    \"The patient has no cardiomegaly\",\n",
    "    \"The subject\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c035ed5",
   "metadata": {},
   "source": [
    "### Cider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcac9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.cider import cider_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad3239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 4.050422474728564\n",
      "scores: [10.          3.30879368  0.74854162  0.73773126 10.          3.40067913\n",
      "  0.15721163]\n"
     ]
    }
   ],
   "source": [
    "scorer = cider_scorer.CiderScorer(n=4)\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    scorer += (gen, [gt])\n",
    "mean_score, scores = scorer.compute_score()\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d8812",
   "metadata": {},
   "source": [
    "### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689fca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.rouge import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c66ddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 0.6765759215706109\n",
      "scores: [1.0, 0.7777777777777778, 0.21785714285714283, 0.3333333333333333, 1.0, 0.9070631970260222, 0.5]\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge.Rouge()\n",
    "scores = []\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    score = scorer.calc_score([gen], [gt])\n",
    "    scores.append(score)\n",
    "mean_score = sum(scores) / len(scores)\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2061a",
   "metadata": {},
   "source": [
    "### Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93975c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu import bleu_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d895698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: [0.6589154193334064, 0.5673484877926362, 0.506750741961736, 0.45857604536602137]\n",
      "scores:\n",
      "b1: 1.0000, b2: 1.0000, b3: 1.0000, b4: 1.0000, b: 1.0000\n",
      "b1: 0.7778, b2: 0.6236, b3: 0.4807, b4: 0.3689, b: 0.5628\n",
      "b1: 0.1947, b2: 0.1472, b3: 0.0000, b4: 0.0000, b: 0.0855\n",
      "b1: 0.3333, b2: 0.0000, b3: 0.0000, b4: 0.0000, b: 0.0833\n",
      "b1: 1.0000, b2: 1.0000, b3: 1.0000, b4: 1.0000, b: 1.0000\n",
      "b1: 0.8000, b2: 0.6325, b3: 0.5109, b4: 0.0001, b: 0.4859\n",
      "b1: 0.5000, b2: 0.0000, b3: 0.0000, b4: 0.0000, b: 0.1250\n"
     ]
    }
   ],
   "source": [
    "scorer = bleu_scorer.BleuScorer(n=4)\n",
    "for gen, gt in zip(gen_texts, gt_texts):\n",
    "    scorer += (gen, [gt])\n",
    "mean_score, scores = scorer.compute_score()\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:')\n",
    "for i in range(len(gen_texts)):\n",
    "    b1 = scores[0][i]\n",
    "    b2 = scores[1][i]\n",
    "    b3 = scores[2][i]\n",
    "    b4 = scores[3][i]\n",
    "    b = (b1+b2+b3+b4)/4\n",
    "    print(f'b1: {b1:.4f}, b2: {b2:.4f}, b3: {b3:.4f}, b4: {b4:.4f}, b: {b:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317d9c6",
   "metadata": {},
   "source": [
    "### Meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a8eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371b3d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_score: 0.7408071940279891\n",
      "scores: [0.9995, 0.9995, 0.5852864583333333, 0.4445422535211268, 0.9921875, 0.9146341463414633, 0.25]\n"
     ]
    }
   ],
   "source": [
    "tokenized_gt_texts = [word_tokenize(x) for x in gt_texts]\n",
    "tokenized_gen_texts = [word_tokenize(x) for x in gen_texts]\n",
    "scores = []\n",
    "for gen, gt in zip(tokenized_gen_texts, tokenized_gt_texts):\n",
    "    score = meteor_score([gt], gen)\n",
    "    scores.append(score)\n",
    "mean_score = sum(scores) / len(scores)\n",
    "print('mean_score:', mean_score)\n",
    "print('scores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a1aac",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c46f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38dba60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertScore(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertScore, self).__init__()\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(model_type='distilbert-base-uncased',\n",
    "                                          num_layers=5,\n",
    "                                          batch_size=64,\n",
    "                                          nthreads=4,\n",
    "                                          all_layers=False,\n",
    "                                          idf=False,\n",
    "                                          device=None,\n",
    "                                          lang='en',\n",
    "                                          rescale_with_baseline=True,\n",
    "                                          baseline_path=None)\n",
    "\n",
    "    def forward(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=64,\n",
    "        )\n",
    "        return torch.mean(f).item(), f.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c7b784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a2f1eb3a9c4d9082b0c890cea7b12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ada9db76f243aea5795ba9a2a85e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c080719b795449ad9c16db6462abb02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe11eb753a2e4ce8ba9a00b266dcbdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_score = BertScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccee4a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802782416343689\n",
      "[0.9999998211860657, 0.9180418848991394, 0.5307456851005554, 0.7448107004165649, 1.0, 0.9339528679847717, 0.491926372051239]\n"
     ]
    }
   ],
   "source": [
    "x, y = bert_score(gen_texts, gt_texts)\n",
    "print(x) # average f1\n",
    "print(y) # instance f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e51a9c",
   "metadata": {},
   "source": [
    "### CheXbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1650738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f1chexbert import F1CheXbert\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aced057",
   "metadata": {},
   "outputs": [],
   "source": [
    "_B = 1777771\n",
    "_M = [999727999, 1070777777]\n",
    "_MASK32 = (1 << 32) - 1\n",
    "_MAXLEN = 1000000\n",
    "_POW = [[None] * _MAXLEN, [None] * _MAXLEN] # _POW[k][i] = _B^i mod _M[k]\n",
    "for k in range(2):\n",
    "    _POW[k][0] = 1\n",
    "    for i in range(1, _MAXLEN):\n",
    "        _POW[k][i] = (_POW[k][i - 1] * _B) % _M[k]\n",
    "\n",
    "def hash_string(s):\n",
    "    hh = 0\n",
    "    for k in range(2):\n",
    "        h = 0\n",
    "        for c in s:\n",
    "            h = (h * _B + ord(c)) % _M[k]\n",
    "        hh = (hh << 32) | h\n",
    "    return (len(s), hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07ef075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEXBERT_LABELS = [\n",
    "    \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\", \"Lung Lesion\", \"Edema\",\n",
    "    \"Consolidation\", \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\", \"Pleural Effusion\", \"Pleural Other\",\n",
    "    \"Fracture\", \"Support Devices\", \"No Finding\",\n",
    "]\n",
    "\n",
    "def merge_labels(labels_list):        \n",
    "    merged = np.zeros((len(CHEXBERT_LABELS),), np.int8)\n",
    "    merged[-1] = 1 # default to no findings\n",
    "    for labels in labels_list:\n",
    "        if labels[-1] == 0: # there is a finding\n",
    "            merged[-1] = 0\n",
    "        for i in range(0, len(labels)-1): # iterate over all labels except the last one\n",
    "            if labels[i] == 1:\n",
    "                merged[i] = 1\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a0a754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXbertLabeler(F1CheXbert):\n",
    "\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__(device=device)\n",
    "        self.cache = dict()\n",
    "\n",
    "    def get_labels(self, texts):\n",
    "\n",
    "        output_labels = [None] * len(texts)\n",
    "        dirty_count = 0\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), total=len(texts), mininterval=2):\n",
    "            text_hash = hash_string(text)\n",
    "            if text_hash in self.cache:\n",
    "                output_labels[i] = self.cache[text_hash]\n",
    "                continue\n",
    "            sentences = sent_tokenize(text)\n",
    "            sentence_labels = []\n",
    "            for sentence in sentences:\n",
    "                hash = hash_string(sentence)\n",
    "                labels = self.cache.get(hash, None)\n",
    "                if labels is None:\n",
    "                    labels = self.get_label(sentence)\n",
    "                    self.cache[hash] = labels\n",
    "                    dirty_count += 1\n",
    "                sentence_labels.append(labels)\n",
    "            output_labels[i] = merge_labels(sentence_labels)\n",
    "            self.cache[text_hash] = output_labels[i]\n",
    "        \n",
    "        return np.array(output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1073bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(gt_binary_labels, gen_binary_labels):\n",
    "    tp = np.sum((gen_binary_labels == 1) & (gt_binary_labels == 1))\n",
    "    fp = np.sum((gen_binary_labels == 1) & (gt_binary_labels == 0))\n",
    "    fn = np.sum((gen_binary_labels == 0) & (gt_binary_labels == 1))\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def compute_chexbert_f1_scores(labeler, gen_texts, gt_texts):\n",
    "    gen_labels = labeler.get_labels(gen_texts)\n",
    "    gt_labels = labeler.get_labels(gt_texts)\n",
    "    print(gen_labels.shape)\n",
    "    print(gt_labels.shape)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    f1_scores = np.zeros(len(gen_texts))\n",
    "    for i in range(len(gen_texts)):\n",
    "        f1_scores[i] = f1(gt_labels[i], gen_labels[i])\n",
    "    metrics['instance_f1s'] = f1_scores\n",
    "    \n",
    "    metrics['sample_avg_f1'] = f1_scores.mean()\n",
    "    \n",
    "    metrics['micro_avg_f1'] = f1(gt_labels.flatten(), gen_labels.flatten())\n",
    "    \n",
    "    metrics['class_f1s'] = {}\n",
    "    class_f1s = np.empty((len(CHEXBERT_LABELS),))\n",
    "    for i in range(len(CHEXBERT_LABELS)):\n",
    "        class_f1s[i] = f1(gt_labels.T[i], gen_labels.T[i])   \n",
    "        metrics['class_f1s'][CHEXBERT_LABELS[i]] = class_f1s[i]\n",
    "        \n",
    "    metrics['macro_avg_f1'] = class_f1s.mean()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f79e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 85.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 193.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 14)\n",
      "(7, 14)\n",
      "{'class_f1s': {'Atelectasis': 0.0,\n",
      "               'Cardiomegaly': 0.6666666666666666,\n",
      "               'Consolidation': 0.0,\n",
      "               'Edema': 0.0,\n",
      "               'Enlarged Cardiomediastinum': 0.0,\n",
      "               'Fracture': 0.0,\n",
      "               'Lung Lesion': 0.0,\n",
      "               'Lung Opacity': 0.0,\n",
      "               'No Finding': 1.0,\n",
      "               'Pleural Effusion': 0.0,\n",
      "               'Pleural Other': 0.0,\n",
      "               'Pneumonia': 0.0,\n",
      "               'Pneumothorax': 0.0,\n",
      "               'Support Devices': 0.0},\n",
      " 'instance_f1s': array([1., 1., 0., 1., 1., 0., 1.]),\n",
      " 'macro_avg_f1': 0.11904761904761904,\n",
      " 'micro_avg_f1': 0.9090909090909091,\n",
      " 'sample_avg_f1': 0.7142857142857143}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labeler = CheXbertLabeler()\n",
    "f1_scores = compute_chexbert_f1_scores(labeler, gen_texts, gt_texts)\n",
    "pprint(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f700af",
   "metadata": {},
   "source": [
    "### RadGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca10a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radgraph import RadGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f226dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_relations(data):\n",
    "    entities = data['entities']\n",
    "    n = len(entities)\n",
    "    e_strings = [None] * n\n",
    "    hash2count = dict()\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        e_strings[i] = f\"{e['tokens']}|{e['label']}\" # tokens|label\n",
    "        h = hash_string(e_strings[i])\n",
    "        hash2count[h] = hash2count.get(h, 0) + 1\n",
    "    for k, e in entities.items():\n",
    "        i = int(k)-1\n",
    "        for r in e['relations']:\n",
    "            j = int(r[1])-1\n",
    "            rel_s1 = f\"{e_strings[i]}|{r[0]}|{e_strings[j]}\" # e1|rel|e2\n",
    "            rel_s2 = f\"{e_strings[i]}|{e_strings[j]}\" # e1|e2\n",
    "            h1 = hash_string(rel_s1)\n",
    "            h2 = hash_string(rel_s2)\n",
    "            hash2count[h1] = hash2count.get(h1, 0) + 1\n",
    "            hash2count[h2] = hash2count.get(h2, 0) + 1\n",
    "    return hash2count\n",
    "\n",
    "def jaccard_between_dicts(d1, d2):\n",
    "    if len(d1) == 0 and len(d2) == 0:\n",
    "        return 1 # both dicts are empty -> perfect match\n",
    "    inters_size = 0\n",
    "    for k, c in d1.items():\n",
    "        inters_size += min(c, d2.get(k, 0))\n",
    "    union_size = sum(d1.values()) + sum(d2.values()) - inters_size\n",
    "    assert union_size > 0\n",
    "    return inters_size / union_size\n",
    "\n",
    "def f1_between_dicts(gt_dict, pred_dict):\n",
    "    if len(gt_dict) == 0 and len(pred_dict) == 0:\n",
    "        return 1 # both dicts are empty -> perfect match\n",
    "    inters_size = 0\n",
    "    for k, c in gt_dict.items():\n",
    "        inters_size += min(c, pred_dict.get(k, 0))\n",
    "    pred_sum = sum(pred_dict.values())\n",
    "    gt_sum = sum(gt_dict.values())\n",
    "    p = inters_size / pred_sum if pred_sum > 0 else 0\n",
    "    r = inters_size / gt_sum if gt_sum > 0 else 0\n",
    "    return 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "def eval_radgraph(gen_texts, gt_texts):\n",
    "    # Create instance of RadGraph\n",
    "    radgraph = RadGraph()\n",
    "    \n",
    "    # Collect unique sentences\n",
    "    sentences = set()\n",
    "    sentences.update(gen_texts)\n",
    "    sentences.update(gt_texts)\n",
    "    sentences = list(sentences)\n",
    "    s2idx = {s: i for i, s in enumerate(sentences)} # Map sentence to index\n",
    "\n",
    "    # Compute RadGraph annotations\n",
    "    annotations = radgraph(sentences)\n",
    "\n",
    "    # Extract entities and relations\n",
    "    ent_rel_dicts = [None] * len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        ent_rel_dicts[i] = extract_entities_and_relations(annotations[str(i)])\n",
    "    \n",
    "    # Compute F1 score and Jaccard score\n",
    "    f1_scores = []\n",
    "    jaccard_scores = []\n",
    "    for gt, gen in zip(gt_texts, gen_texts):\n",
    "        gt_idx = s2idx[gt]\n",
    "        gen_idx = s2idx[gen]\n",
    "        f1_scores.append(f1_between_dicts(ent_rel_dicts[gt_idx], ent_rel_dicts[gen_idx]))\n",
    "        jaccard_scores.append(jaccard_between_dicts(ent_rel_dicts[gt_idx], ent_rel_dicts[gen_idx]))\n",
    "    \n",
    "    # Return\n",
    "    return f1_scores, jaccard_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "611470c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "f1_scores, jaccard_scores = eval_radgraph(gen_texts, gt_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cd1ac81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_scores: [1.0, 0.4444444444444444, 1, 0, 1.0, 0, 1]\n",
      "jaccard_scores: [1.0, 0.2857142857142857, 1, 0.0, 1.0, 0.0, 1]\n"
     ]
    }
   ],
   "source": [
    "print('f1_scores:', f1_scores)\n",
    "print('jaccard_scores:', jaccard_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b99d3",
   "metadata": {},
   "source": [
    "### RadCliQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4624c776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52fa896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.metrics.medical.radcliq import invoke_radcliq_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8e12ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RadCliQ over 7 reports ...\n",
      "\tCommand = cd /home/pamessina/CXR-Report-Metric/ && conda run -n radcliqenv python run_metric.py --gt_reports_path \"/mnt/data/pamessina/workspaces/medvqa-workspace/tmp/radcliq/radcliq-gt_20240624_192211.csv\" --gen_reports_path \"/mnt/data/pamessina/workspaces/medvqa-workspace/tmp/radcliq/radcliq-gen_20240624_192211.csv\" --output_path \"/mnt/data/pamessina/workspaces/medvqa-workspace/tmp/radcliq/radcliq-output_20240624_192211.csv\"\n",
      "RadCliq process done. Elapsed seconds = 104.33525276184082\n",
      "stdout: Number of shared study IDs: 7\n",
      "\n",
      "Tokenizing report impressions. All reports are cut off at 512 tokens.\n",
      "Using 1 GPUs!\n",
      "\n",
      "Begin report impression labeling. The progress bar counts the # of batches completed:\n",
      "The batch size is 18\n",
      "\n",
      "Tokenizing report impressions. All reports are cut off at 512 tokens.\n",
      "Using 1 GPUs!\n",
      "\n",
      "Begin report impression labeling. The progress bar counts the # of batches completed:\n",
      "The batch size is 18\n",
      "Preprocessing all the reports...\n",
      "1 reports done\n",
      "7 reports done\n",
      "Done with preprocessing.\n",
      "Running the inference now... This can take a bit of time\n",
      "Inference completed.\n",
      "Postprocessing output file...\n",
      "Done postprocessing.\n",
      "Saving results and performing final cleanup...\n",
      "Preprocessing all the reports...\n",
      "1 reports done\n",
      "7 reports done\n",
      "Done with preprocessing.\n",
      "Running the inference now... This can take a bit of time\n",
      "Inference completed.\n",
      "Postprocessing output file...\n",
      "Done postprocessing.\n",
      "Saving results and performing final cleanup...\n",
      "Average RadGraph entity F1 = 0.35714285714285715\n",
      "Average RadGraph relation F1 = 0.19999999999999998\n",
      "\n",
      "Ground truth average RadGraph entity counts = 2.0\n",
      "Ground truth average RadGraph relation counts = 1.0\n",
      "Average RadGraph entity counts = 1.4285714285714286\n",
      "Average RadGraph relation counts = 0.7142857142857143\n",
      "\n",
      "#Test reports (this is all test cases): 7\n",
      "\n",
      "\n",
      "100%|██████████| 7/7 [00:00<00:00, 1354.75it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 7/7 [00:00<00:00, 943.30it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "2024-06-24 19:23:01,442 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2024-06-24 19:23:01,557 - INFO - allennlp.models.archival - loading archive file /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz\n",
      "2024-06-24 19:23:01,558 - INFO - allennlp.models.archival - extracting archive file /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz to temp dir /tmp/tmpmb2c7jud\n",
      "2024-06-24 19:23:07,342 - INFO - allennlp.common.params - type = from_instances\n",
      "2024-06-24 19:23:07,342 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpmb2c7jud/vocabulary.\n",
      "2024-06-24 19:23:07,343 - INFO - allennlp.common.params - model.type = dygie\n",
      "2024-06-24 19:23:07,344 - INFO - allennlp.common.params - model.embedder.type = basic\n",
      "2024-06-24 19:23:07,344 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.type = pretrained_transformer_mismatched\n",
      "2024-06-24 19:23:07,345 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "2024-06-24 19:23:07,345 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.max_length = 512\n",
      "2024-06-24 19:23:07,345 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.train_parameters = True\n",
      "2024-06-24 19:23:07,345 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.last_layer_only = True\n",
      "2024-06-24 19:23:07,345 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.gradient_checkpointing = None\n",
      "2024-06-24 19:23:07,838 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2024-06-24 19:23:07,838 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2024-06-24 19:23:08,218 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/pytorch_model.bin from cache at /home/pamessina/.cache/torch/transformers/b37e65e3b849ec2eaf7af4c77b653dcdb3fa19df5fccc86ff0ffa16f126f5b75.a6a084e84faf3eb5eba47a0a81a786cbf5635e8b5fb67072ad3c918042935a30\n",
      "2024-06-24 19:23:12,242 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2024-06-24 19:23:12,242 - INFO - transformers.modeling_utils - All the weights of BertModel were initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2024-06-24 19:23:14,931 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2024-06-24 19:23:14,931 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2024-06-24 19:23:14,932 - INFO - transformers.tokenization_utils_base - Model name 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2024-06-24 19:23:17,292 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/vocab.txt from cache at /home/pamessina/.cache/torch/transformers/b5cd04b1472559c44d90fe1a1f3e8813538c0774130f99287bb3ef2f71574c34.3acf5667a44623ef633872cc290168316f7124c3bc23562e163aee8dcae4bb30\n",
      "2024-06-24 19:23:17,293 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/added_tokens.json from cache at None\n",
      "2024-06-24 19:23:17,293 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/special_tokens_map.json from cache at None\n",
      "2024-06-24 19:23:17,293 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer_config.json from cache at None\n",
      "2024-06-24 19:23:17,293 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer.json from cache at None\n",
      "2024-06-24 19:23:17,800 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2024-06-24 19:23:17,801 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2024-06-24 19:23:17,801 - INFO - transformers.tokenization_utils_base - Model name 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2024-06-24 19:23:20,157 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/vocab.txt from cache at /home/pamessina/.cache/torch/transformers/b5cd04b1472559c44d90fe1a1f3e8813538c0774130f99287bb3ef2f71574c34.3acf5667a44623ef633872cc290168316f7124c3bc23562e163aee8dcae4bb30\n",
      "2024-06-24 19:23:20,158 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/added_tokens.json from cache at None\n",
      "2024-06-24 19:23:20,158 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/special_tokens_map.json from cache at None\n",
      "2024-06-24 19:23:20,158 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer_config.json from cache at None\n",
      "2024-06-24 19:23:20,158 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer.json from cache at None\n",
      "2024-06-24 19:23:20,197 - INFO - allennlp.common.params - model.feature_size = 20\n",
      "2024-06-24 19:23:20,197 - INFO - allennlp.common.params - model.max_span_width = 8\n",
      "2024-06-24 19:23:20,197 - INFO - allennlp.common.params - model.target_task = relation\n",
      "2024-06-24 19:23:20,198 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = xavier_normal\n",
      "2024-06-24 19:23:20,199 - INFO - allennlp.common.params - model.initializer.regexes.0.1.gain = 1.0\n",
      "2024-06-24 19:23:20,199 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None\n",
      "2024-06-24 19:23:20,200 - INFO - allennlp.common.params - model.module_initializer.regexes.0.1.type = xavier_normal\n",
      "2024-06-24 19:23:20,200 - INFO - allennlp.common.params - model.module_initializer.regexes.0.1.gain = 1.0\n",
      "2024-06-24 19:23:20,200 - INFO - allennlp.common.params - model.module_initializer.regexes.1.1.type = xavier_normal\n",
      "2024-06-24 19:23:20,200 - INFO - allennlp.common.params - model.module_initializer.regexes.1.1.gain = 1.0\n",
      "2024-06-24 19:23:20,200 - INFO - allennlp.common.params - model.module_initializer.prevent_regexes = None\n",
      "2024-06-24 19:23:20,201 - INFO - allennlp.common.params - model.regularizer = None\n",
      "2024-06-24 19:23:20,201 - INFO - allennlp.common.params - model.display_metrics = None\n",
      "2024-06-24 19:23:20,202 - INFO - allennlp.common.params - ner.regularizer = None\n",
      "2024-06-24 19:23:20,208 - INFO - allennlp.common.params - coref.spans_per_word = 0.3\n",
      "2024-06-24 19:23:20,208 - INFO - allennlp.common.params - coref.max_antecedents = 100\n",
      "2024-06-24 19:23:20,209 - INFO - allennlp.common.params - coref.coref_prop = 0\n",
      "2024-06-24 19:23:20,209 - INFO - allennlp.common.params - coref.coref_prop_dropout_f = 0.0\n",
      "2024-06-24 19:23:20,209 - INFO - allennlp.common.params - coref.regularizer = None\n",
      "2024-06-24 19:23:20,331 - INFO - allennlp.common.params - relation.spans_per_word = 0.5\n",
      "2024-06-24 19:23:20,331 - INFO - allennlp.common.params - relation.positive_label_weight = 1.0\n",
      "2024-06-24 19:23:20,331 - INFO - allennlp.common.params - relation.regularizer = None\n",
      "2024-06-24 19:23:20,354 - INFO - allennlp.common.params - events.trigger_spans_per_word = 0.3\n",
      "2024-06-24 19:23:20,354 - INFO - allennlp.common.params - events.argument_spans_per_word = 0.8\n",
      "2024-06-24 19:23:20,354 - INFO - allennlp.common.params - events.regularizer = None\n",
      "2024-06-24 19:23:20,355 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:20,356 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,360 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.1._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,361 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.1._module.bias\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers - Initializing _distance_embedding.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,361 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,373 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,374 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,378 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,379 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.1._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,379 - INFO - allennlp.nn.initializers - Initializing _antecedent_scorer._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,379 - INFO - allennlp.nn.initializers - Initializing _f_network._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,459 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers -    _f_network._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.1._module.bias\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:20,459 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,464 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,464 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.1._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,465 - INFO - allennlp.nn.initializers - Initializing _relation_feedforwards.None__relation_labels._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,476 - INFO - allennlp.nn.initializers - Initializing _relation_feedforwards.None__relation_labels._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,477 - INFO - allennlp.nn.initializers - Initializing _relation_scorers.None__relation_labels.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,477 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:20,477 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:20,477 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,477 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,477 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.1._module.bias\n",
      "2024-06-24 19:23:20,477 - INFO - allennlp.nn.initializers -    _relation_feedforwards.None__relation_labels._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,477 - INFO - allennlp.nn.initializers -    _relation_feedforwards.None__relation_labels._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,478 - INFO - allennlp.nn.initializers -    _relation_scorers.None__relation_labels.bias\n",
      "2024-06-24 19:23:20,478 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:20,478 - INFO - allennlp.nn.initializers - Initializing _distance_embedding.weight using .*weight initializer\n",
      "2024-06-24 19:23:20,478 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:20,478 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:20,478 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:20,478 - INFO - allennlp.nn.initializers - Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._antecedent_scorer._module.bias\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._antecedent_scorer._module.weight\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._distance_embedding.weight\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._f_network._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._f_network._linear_layers.0.weight\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,482 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.1._module.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.1._module.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,483 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,484 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "2024-06-24 19:23:20,485 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,486 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "2024-06-24 19:23:20,487 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "2024-06-24 19:23:20,488 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "2024-06-24 19:23:20,489 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "2024-06-24 19:23:20,490 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,491 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,492 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.pooler.dense.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.pooler.dense.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _events._distance_embedding.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:20,493 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.1._module.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.1._module.weight\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.1._module.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.1._module.weight\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.0.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.0.weight\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.1.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.1.weight\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._relation_scorers.None__relation_labels.bias\n",
      "2024-06-24 19:23:20,494 - INFO - allennlp.nn.initializers -    _relation._relation_scorers.None__relation_labels.weight\n",
      "2024-06-24 19:23:22,096 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2024-06-24 19:23:22,098 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2024-06-24 19:23:22,098 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2024-06-24 19:23:22,536 - INFO - allennlp.common.params - dataset_reader.type = dygie\n",
      "2024-06-24 19:23:22,536 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
      "2024-06-24 19:23:22,536 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
      "2024-06-24 19:23:22,537 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2024-06-24 19:23:22,537 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2024-06-24 19:23:22,537 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
      "2024-06-24 19:23:22,537 - INFO - allennlp.common.params - dataset_reader.max_span_width = 8\n",
      "2024-06-24 19:23:22,537 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = pretrained_transformer_mismatched\n",
      "2024-06-24 19:23:22,538 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.token_min_padding_length = 0\n",
      "2024-06-24 19:23:22,538 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "2024-06-24 19:23:22,538 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.namespace = tags\n",
      "2024-06-24 19:23:22,538 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_length = 512\n",
      "reading instances: 7it [00:00, 1957.99it/s]\n",
      "2024-06-24 19:23:24,624 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpmb2c7jud\n",
      "2024-06-24 19:23:29,976 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2024-06-24 19:23:30,110 - INFO - allennlp.models.archival - loading archive file /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz\n",
      "2024-06-24 19:23:30,111 - INFO - allennlp.models.archival - extracting archive file /mnt/data/radgraph/data/models/model_checkpoint/model.tar.gz to temp dir /tmp/tmpktth_v5y\n",
      "2024-06-24 19:23:36,325 - INFO - allennlp.common.params - type = from_instances\n",
      "2024-06-24 19:23:36,325 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpktth_v5y/vocabulary.\n",
      "2024-06-24 19:23:36,326 - INFO - allennlp.common.params - model.type = dygie\n",
      "2024-06-24 19:23:36,327 - INFO - allennlp.common.params - model.embedder.type = basic\n",
      "2024-06-24 19:23:36,328 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.type = pretrained_transformer_mismatched\n",
      "2024-06-24 19:23:36,328 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "2024-06-24 19:23:36,328 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.max_length = 512\n",
      "2024-06-24 19:23:36,328 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.train_parameters = True\n",
      "2024-06-24 19:23:36,328 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.last_layer_only = True\n",
      "2024-06-24 19:23:36,328 - INFO - allennlp.common.params - model.embedder.token_embedders.bert.gradient_checkpointing = None\n",
      "2024-06-24 19:23:36,800 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2024-06-24 19:23:36,801 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2024-06-24 19:23:37,188 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/pytorch_model.bin from cache at /home/pamessina/.cache/torch/transformers/b37e65e3b849ec2eaf7af4c77b653dcdb3fa19df5fccc86ff0ffa16f126f5b75.a6a084e84faf3eb5eba47a0a81a786cbf5635e8b5fb67072ad3c918042935a30\n",
      "2024-06-24 19:23:41,161 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2024-06-24 19:23:41,161 - INFO - transformers.modeling_utils - All the weights of BertModel were initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2024-06-24 19:23:44,000 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2024-06-24 19:23:44,001 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2024-06-24 19:23:44,001 - INFO - transformers.tokenization_utils_base - Model name 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2024-06-24 19:23:46,339 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/vocab.txt from cache at /home/pamessina/.cache/torch/transformers/b5cd04b1472559c44d90fe1a1f3e8813538c0774130f99287bb3ef2f71574c34.3acf5667a44623ef633872cc290168316f7124c3bc23562e163aee8dcae4bb30\n",
      "2024-06-24 19:23:46,339 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/added_tokens.json from cache at None\n",
      "2024-06-24 19:23:46,339 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/special_tokens_map.json from cache at None\n",
      "2024-06-24 19:23:46,339 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer_config.json from cache at None\n",
      "2024-06-24 19:23:46,339 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer.json from cache at None\n",
      "2024-06-24 19:23:46,852 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/config.json from cache at /home/pamessina/.cache/torch/transformers/d6259b1a11dbf51cd20925d777803c76163bb3a51a1a2f51dc546051f2fafd89.dcaad90251a770a0bf072b529202dd8ed31e8d64f6198c8e9ae8de194e29d227\n",
      "2024-06-24 19:23:46,852 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2024-06-24 19:23:46,852 - INFO - transformers.tokenization_utils_base - Model name 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2024-06-24 19:23:49,156 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/vocab.txt from cache at /home/pamessina/.cache/torch/transformers/b5cd04b1472559c44d90fe1a1f3e8813538c0774130f99287bb3ef2f71574c34.3acf5667a44623ef633872cc290168316f7124c3bc23562e163aee8dcae4bb30\n",
      "2024-06-24 19:23:49,157 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/added_tokens.json from cache at None\n",
      "2024-06-24 19:23:49,157 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/special_tokens_map.json from cache at None\n",
      "2024-06-24 19:23:49,157 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer_config.json from cache at None\n",
      "2024-06-24 19:23:49,157 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tokenizer.json from cache at None\n",
      "2024-06-24 19:23:49,183 - INFO - allennlp.common.params - model.feature_size = 20\n",
      "2024-06-24 19:23:49,183 - INFO - allennlp.common.params - model.max_span_width = 8\n",
      "2024-06-24 19:23:49,183 - INFO - allennlp.common.params - model.target_task = relation\n",
      "2024-06-24 19:23:49,184 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = xavier_normal\n",
      "2024-06-24 19:23:49,184 - INFO - allennlp.common.params - model.initializer.regexes.0.1.gain = 1.0\n",
      "2024-06-24 19:23:49,185 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None\n",
      "2024-06-24 19:23:49,185 - INFO - allennlp.common.params - model.module_initializer.regexes.0.1.type = xavier_normal\n",
      "2024-06-24 19:23:49,185 - INFO - allennlp.common.params - model.module_initializer.regexes.0.1.gain = 1.0\n",
      "2024-06-24 19:23:49,185 - INFO - allennlp.common.params - model.module_initializer.regexes.1.1.type = xavier_normal\n",
      "2024-06-24 19:23:49,186 - INFO - allennlp.common.params - model.module_initializer.regexes.1.1.gain = 1.0\n",
      "2024-06-24 19:23:49,186 - INFO - allennlp.common.params - model.module_initializer.prevent_regexes = None\n",
      "2024-06-24 19:23:49,186 - INFO - allennlp.common.params - model.regularizer = None\n",
      "2024-06-24 19:23:49,186 - INFO - allennlp.common.params - model.display_metrics = None\n",
      "2024-06-24 19:23:49,187 - INFO - allennlp.common.params - ner.regularizer = None\n",
      "2024-06-24 19:23:49,192 - INFO - allennlp.common.params - coref.spans_per_word = 0.3\n",
      "2024-06-24 19:23:49,192 - INFO - allennlp.common.params - coref.max_antecedents = 100\n",
      "2024-06-24 19:23:49,192 - INFO - allennlp.common.params - coref.coref_prop = 0\n",
      "2024-06-24 19:23:49,192 - INFO - allennlp.common.params - coref.coref_prop_dropout_f = 0.0\n",
      "2024-06-24 19:23:49,192 - INFO - allennlp.common.params - coref.regularizer = None\n",
      "2024-06-24 19:23:49,294 - INFO - allennlp.common.params - relation.spans_per_word = 0.5\n",
      "2024-06-24 19:23:49,295 - INFO - allennlp.common.params - relation.positive_label_weight = 1.0\n",
      "2024-06-24 19:23:49,295 - INFO - allennlp.common.params - relation.regularizer = None\n",
      "2024-06-24 19:23:49,314 - INFO - allennlp.common.params - events.trigger_spans_per_word = 0.3\n",
      "2024-06-24 19:23:49,314 - INFO - allennlp.common.params - events.argument_spans_per_word = 0.8\n",
      "2024-06-24 19:23:49,314 - INFO - allennlp.common.params - events.regularizer = None\n",
      "2024-06-24 19:23:49,315 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:49,315 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,319 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers - Initializing _ner_scorers.None__ner_labels.1._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,320 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers -    _ner_scorers.None__ner_labels.1._module.bias\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers - Initializing _distance_embedding.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,320 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,331 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,331 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,335 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,336 - INFO - allennlp.nn.initializers - Initializing _mention_pruner._scorer.1._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,336 - INFO - allennlp.nn.initializers - Initializing _antecedent_scorer._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,336 - INFO - allennlp.nn.initializers - Initializing _f_network._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,406 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers -    _f_network._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers -    _mention_pruner._scorer.1._module.bias\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:49,407 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,411 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,411 - INFO - allennlp.nn.initializers - Initializing _mention_pruners.None__relation_labels._scorer.1._module.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,411 - INFO - allennlp.nn.initializers - Initializing _relation_feedforwards.None__relation_labels._linear_layers.0.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,422 - INFO - allennlp.nn.initializers - Initializing _relation_feedforwards.None__relation_labels._linear_layers.1.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,422 - INFO - allennlp.nn.initializers - Initializing _relation_scorers.None__relation_labels.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,422 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:49,422 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:49,422 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,422 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,422 - INFO - allennlp.nn.initializers -    _mention_pruners.None__relation_labels._scorer.1._module.bias\n",
      "2024-06-24 19:23:49,422 - INFO - allennlp.nn.initializers -    _relation_feedforwards.None__relation_labels._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,423 - INFO - allennlp.nn.initializers -    _relation_feedforwards.None__relation_labels._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,423 - INFO - allennlp.nn.initializers -    _relation_scorers.None__relation_labels.bias\n",
      "2024-06-24 19:23:49,423 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:49,423 - INFO - allennlp.nn.initializers - Initializing _distance_embedding.weight using .*weight initializer\n",
      "2024-06-24 19:23:49,423 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: .*weight_matrix\n",
      "2024-06-24 19:23:49,423 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:49,423 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2024-06-24 19:23:49,423 - INFO - allennlp.nn.initializers - Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._antecedent_feedforward._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._antecedent_scorer._module.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._antecedent_scorer._module.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._distance_embedding.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._f_network._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._f_network._linear_layers.0.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.0._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.1._module.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _coref._mention_pruner._scorer.1._module.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "2024-06-24 19:23:49,426 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "2024-06-24 19:23:49,427 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,428 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,429 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "2024-06-24 19:23:49,430 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "2024-06-24 19:23:49,431 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "2024-06-24 19:23:49,432 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "2024-06-24 19:23:49,433 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.pooler.dense.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _embedder.token_embedder_bert._matched_embedder.transformer_model.pooler.dense.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _events._distance_embedding.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.0._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.1._module.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _ner._ner_scorers.None__ner_labels.1._module.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.0.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.0._module._linear_layers.1.weight\n",
      "2024-06-24 19:23:49,434 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.1._module.bias\n",
      "2024-06-24 19:23:49,435 - INFO - allennlp.nn.initializers -    _relation._mention_pruners.None__relation_labels._scorer.1._module.weight\n",
      "2024-06-24 19:23:49,435 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.0.bias\n",
      "2024-06-24 19:23:49,435 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.0.weight\n",
      "2024-06-24 19:23:49,435 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.1.bias\n",
      "2024-06-24 19:23:49,435 - INFO - allennlp.nn.initializers -    _relation._relation_feedforwards.None__relation_labels._linear_layers.1.weight\n",
      "2024-06-24 19:23:49,435 - INFO - allennlp.nn.initializers -    _relation._relation_scorers.None__relation_labels.bias\n",
      "2024-06-24 19:23:49,435 - INFO - allennlp.nn.initializers -    _relation._relation_scorers.None__relation_labels.weight\n",
      "2024-06-24 19:23:51,048 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2024-06-24 19:23:51,050 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2024-06-24 19:23:51,050 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2024-06-24 19:23:51,443 - INFO - allennlp.common.params - dataset_reader.type = dygie\n",
      "2024-06-24 19:23:51,443 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
      "2024-06-24 19:23:51,443 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.max_span_width = 8\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = pretrained_transformer_mismatched\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.token_min_padding_length = 0\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "2024-06-24 19:23:51,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.namespace = tags\n",
      "2024-06-24 19:23:51,445 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_length = 512\n",
      "reading instances: 7it [00:00, 2242.77it/s]\n",
      "2024-06-24 19:23:53,345 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpktth_v5y\n",
      "100%|██████████| 7/7 [00:00<00:00, 35077.81it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 46603.38it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 54981.51it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 11108.64it/s]\n",
      "/home/pamessina/miniconda3/envs/radcliqenv/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/pamessina/miniconda3/envs/radcliqenv/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/pamessina/miniconda3/envs/radcliqenv/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = invoke_radcliq_process(gt_reports=gt_texts, gen_reports=gen_texts, device_id=\"0\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee6f13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu_score': array([1.        , 0.66666667, 0.16344984, 0.26726124, 1.        ,\n",
       "        0.63245553, 0.2236068 ]),\n",
       " 'bertscore': array([1.        , 0.8820058 , 0.5188881 , 0.69265145, 1.        ,\n",
       "        0.926922  , 0.5359159 ]),\n",
       " 'semb_score': array([1.        , 0.97885716, 0.77980483, 0.88076371, 1.00000012,\n",
       "        0.69918996, 0.95818394]),\n",
       " 'radgraph_combined': array([1.  , 0.45, 0.  , 0.  , 0.5 , 0.  , 0.  ]),\n",
       " 'RadCliQ-v0': array([-0.24213342,  0.78860565,  2.6102251 ,  1.97220876,  0.55238347,\n",
       "         1.86797675,  2.2677525 ]),\n",
       " 'RadCliQ-v1': array([-1.44069028, -0.53999885,  0.81781128,  0.43052385, -0.81808224,\n",
       "         0.2244038 ,  0.61225453])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
