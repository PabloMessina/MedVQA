{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 pamessina pamessina 1.3G Dec 28 12:05 '/mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/checkpoint_60_loss=0.9791.pt'\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/checkpoint_60_loss=0.9791.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 60\n",
      "   batches_per_epoch: 300\n",
      "   checkpoint_folder: None\n",
      "   iuxray_qa_adapted_reports_filename: None\n",
      "   mimiccxr_qa_adapted_reports_filename: None\n",
      "   vocab_min_freq: 10\n",
      "   embed_size: 256\n",
      "   question_encoding: one-hot\n",
      "   answer_decoding: transformer\n",
      "   question_hidden_size: 128\n",
      "   answer_hidden_size: 256\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: vitmodel-huggingface\n",
      "   image_local_feat_size: 768\n",
      "   image_encoder_pretrained_weights_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/checkpoint_60_loss=0.9791.pt\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: facebook/vit-mae-base\n",
      "   n_lstm_layers: 1\n",
      "   transf_dec_nhead: 2\n",
      "   transf_dec_dim_forward: 256\n",
      "   transf_dec_num_layers: 2\n",
      "   question_vec_size: 128\n",
      "   dropout_prob: 0\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,2e-4,55,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   n_val_examples_per_question: 10\n",
      "   min_train_examples_per_question: 100\n",
      "   batch_size: 55\n",
      "   iters_to_accumulate: 5\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   img_aug_mode: None\n",
      "   image_size: [224, 224]\n",
      "   mimiccxr_weight: 1\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   mimiccxr_weight_chexpert_mode: 0.2\n",
      "   iuxray_weight_chexpert_mode: 0.05\n",
      "   padchest_weight: 1.0\n",
      "   mimiccxr_include_chexpert_mode: False\n",
      "   iuxray_include_chexpert_mode: False\n",
      "   use_chexpert_mode_only: False\n",
      "   val_answer_decoding: greedy-search\n",
      "   beam_search_k: None\n",
      "   use_amp: True\n",
      "   medical_tokenization: False\n",
      "   medical_terms_frequency_filename: None\n",
      "   allowed_questions: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   balanced_split: False\n",
      "   balanced_dataloading: False\n",
      "   imbalance_reduction_coef: 0.5\n",
      "   n_healthy_per_question: 2\n",
      "   n_unhealthy_per_question: 3\n",
      "   n_positive_per_chexpert_label: 7\n",
      "   min_question_count: 100\n",
      "   iuxray_balanced_metadata_filename: None\n",
      "   mimiccxr_balanced_metadata_filename: None\n",
      "   one_question_per_batch: False\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: False\n",
      "   train_iuxray: False\n",
      "   iuxray_train_with_all: False\n",
      "   train_chexpert: False\n",
      "   chexpert_mode: None\n",
      "   train_cxr14: False\n",
      "   train_vinbig: False\n",
      "   vinbig_training_data: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: True\n",
      "   padchest_training_data_mode: all\n",
      "   padchest_use_validation: True\n",
      "   padchest_train_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\n",
      "   padchest_val_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\n",
      "   padchest_test_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\n",
      "   binary_loss_name: wbce-c\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: True\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_questions: False\n",
      "   n_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/checkpoint_60_loss=0.9791.pt\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.mask_token', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.5.attention.attention.query.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/checkpoint_60_loss=0.9791.pt\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 196\n",
      "  n_questions_aux_task = None\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,2e-4,55,1e-6\n",
      "1e-06 5 0.0002 55 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=True\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=7, one_hot_question_offset=0\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating PadChest vqa trainer ...\u001b[0m\n",
      "../train_vqa.py:1497: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug=debug)\n",
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of rows after filtering: 109821 (dropped rows with duplicate StudyID)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n",
      "Generating answers based on labels...\n",
      "100%|███████████████████████████████| 109821/109821 [00:01<00:00, 104858.17it/s]\n",
      "Done. Example answer: <s> normal </s>\n",
      "Generating answers based on localizations...\n",
      "100%|███████████████████████████████| 109821/109821 [00:01<00:00, 109606.22it/s]\n",
      "Done. Example answer: <s> loc bilateral , loc diffuse bilateral , loc perihilar </s>\n",
      "Number of non-empty answers: 63528\n",
      "Generating answers based on labels localizations by sentence...\n",
      "100%|████████████████████████████████| 109821/109821 [00:01<00:00, 61616.30it/s]\n",
      "Done. Example answer: <s> normal , unchanged </s>\n",
      "Creating dataset and dataloader...\n",
      "Example of positive localization answer: <s> loc middle lung field , loc right , loc cardiac , loc lobar , loc aortic , loc lung field </s>\n",
      "Example of negative localization answer: <s> </s>\n",
      "193it [00:30,  6.29it/s]\n",
      "Done!\n",
      "len(self.train_dataset) = 1000000000000000000\n",
      "Creating dataset and dataloader...\n",
      "Example of positive localization answer: <s> loc rib , loc right </s>\n",
      "Example of negative localization answer: <s> </s>\n",
      "193it [00:00, 216.43it/s]\n",
      "Done!\n",
      "len(self.val_dataset) = 4398\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 1\n",
      "len(_val_dataloaders) = 1\n",
      "_train_weights = [1.0]\n",
      "merged_dataset_name = padchest(vqa)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221228_131234_padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_orien_amp\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221228_131234_padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_orien_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221228_131234_padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_orien_amp/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m15) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 12.24913, a_loss 8.12948, ema 0.00000, oracc 0.60715, orien_loss 1.48750, gacc 0.51685, gloss 0.69276, b1 0.00098, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01245, padchxlmicf1 0.01139, padchxlzmacf1 0.02273, padchxlzmicf1 0.02276, padchxl_loss 0.84783, padchxlz_loss 0.89944, 101.04 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.00000, oracc 0.80559, gacc 0.52842, b1 0.00426, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01725, padchxlmicf1 0.01777, padchxlzmacf1 0.02879, padchxlzmicf1 0.02773, 22.93 secs\n",
      "Adjusting learning rate of group 0 to 2.8854e-06.\n",
      "\u001b[1m---- Epoch 2/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 11.78271, a_loss 7.85192, ema 0.00000, oracc 0.81158, orien_loss 0.91620, gacc 0.52776, gloss 0.68938, b1 0.01920, b2 0.00042, b3 0.00000, b4 0.00000, padchxlmacf1 0.01516, padchxlmicf1 0.01472, padchxlzmacf1 0.02384, padchxlzmicf1 0.02620, padchxl_loss 0.83432, padchxlz_loss 0.88339, 100.41 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.00000, oracc 0.80559, gacc 0.62119, b1 0.04943, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01952, padchxlmicf1 0.02219, padchxlzmacf1 0.03001, padchxlzmicf1 0.03727, 23.11 secs\n",
      "Adjusting learning rate of group 0 to 8.3255e-06.\n",
      "\u001b[1m---- Epoch 3/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 10.44176, a_loss 6.91446, ema 0.00321, oracc 0.88364, orien_loss 0.40029, gacc 0.71012, gloss 0.61820, b1 0.04152, b2 0.00109, b3 0.00028, b4 0.00000, padchxlmacf1 0.01734, padchxlmicf1 0.02133, padchxlzmacf1 0.02321, padchxlzmicf1 0.02432, padchxl_loss 0.78670, padchxlz_loss 0.83541, 98.18 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.00000, oracc 0.93065, gacc 0.82378, b1 0.00126, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01814, padchxlmicf1 0.02839, padchxlzmacf1 0.03098, padchxlzmicf1 0.03305, 23.95 secs\n",
      "Adjusting learning rate of group 0 to 2.4022e-05.\n",
      "\u001b[1m---- Epoch 4/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n",
      "loss 9.00364, a_loss 5.70124, ema 0.08061, oracc 0.93545, orien_loss 0.24000, gacc 0.83170, gloss 0.37847, b1 0.00001, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01941, padchxlmicf1 0.05499, padchxlzmacf1 0.03933, padchxlzmicf1 0.06816, padchxl_loss 0.65963, padchxlz_loss 0.72242, 95.67 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.07026, oracc 0.93611, gacc 0.88063, b1 0.00000, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.02123, padchxlmicf1 0.08059, padchxlzmacf1 0.04038, padchxlzmicf1 0.09519, 22.18 secs\n",
      "Adjusting learning rate of group 0 to 6.9314e-05.\n",
      "\u001b[1m---- Epoch 5/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000069) ...\n",
      "loss 6.84314, a_loss 4.51818, ema 0.05885, oracc 0.95188, orien_loss 0.16454, gacc 0.91194, gloss 0.21767, b1 0.00066, b2 0.00052, b3 0.00020, b4 0.00010, padchxlmacf1 0.02442, padchxlmicf1 0.09823, padchxlzmacf1 0.04179, padchxlzmicf1 0.09373, padchxl_loss 0.50559, padchxlz_loss 0.60427, 95.45 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.06344, oracc 0.96157, gacc 0.93088, b1 0.09529, b2 0.04719, b3 0.02298, b4 0.01311, padchxlmacf1 0.02830, padchxlmicf1 0.09177, padchxlzmacf1 0.04656, padchxlzmicf1 0.10221, 22.94 secs\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "\u001b[1m---- Epoch 6/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 4.99369, a_loss 3.26844, ema 0.16418, oracc 0.90115, orien_loss 0.33848, gacc 0.83970, gloss 0.34390, b1 0.07371, b2 0.05254, b3 0.03527, b4 0.02527, padchxlmacf1 0.02499, padchxlmicf1 0.08334, padchxlzmacf1 0.03983, padchxlzmicf1 0.09459, padchxl_loss 0.46671, padchxlz_loss 0.59173, 96.24 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema 0.23852, oracc 0.94156, gacc 0.91792, b1 0.00701, b2 0.00467, b3 0.00301, b4 0.00212, padchxlmacf1 0.02986, padchxlmicf1 0.09067, padchxlzmacf1 0.04693, padchxlzmicf1 0.09948, 22.04 secs\n",
      "Adjusting learning rate of group 0 to 1.8163e-04.\n",
      "\u001b[1m---- Epoch 7/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000182) ...\n",
      "loss 4.09434, a_loss 2.43407, ema 0.35618, oracc 0.95715, orien_loss 0.14213, gacc 0.93606, gloss 0.15728, b1 0.24824, b2 0.16287, b3 0.10369, b4 0.06921, padchxlmacf1 0.02909, padchxlmicf1 0.10482, padchxlzmacf1 0.04834, padchxlzmicf1 0.10252, padchxl_loss 0.44107, padchxlz_loss 0.56183, 96.48 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.33402, oracc 0.96976, gacc 0.94134, b1 0.18923, b2 0.12707, b3 0.07247, b4 0.04700, padchxlmacf1 0.03338, padchxlmicf1 0.09991, padchxlzmacf1 0.05155, padchxlzmicf1 0.14015, 22.70 secs\n",
      "Adjusting learning rate of group 0 to 1.6495e-04.\n",
      "\u001b[1m---- Epoch 8/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000165) ...\n",
      "loss 3.99530, a_loss 2.03939, ema 0.36994, oracc 0.96994, orien_loss 0.10641, gacc 0.94412, gloss 0.13868, b1 0.37245, b2 0.26237, b3 0.17970, b4 0.12726, padchxlmacf1 0.03324, padchxlmicf1 0.12185, padchxlzmacf1 0.05185, padchxlzmicf1 0.11655, padchxl_loss 0.42728, padchxlz_loss 0.55575, 95.96 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.32970, oracc 0.96498, gacc 0.93997, b1 0.17767, b2 0.12610, b3 0.07827, b4 0.05356, padchxlmacf1 0.03855, padchxlmicf1 0.12045, padchxlzmacf1 0.06307, padchxlzmicf1 0.15045, 22.17 secs\n",
      "Adjusting learning rate of group 0 to 1.4980e-04.\n",
      "\u001b[1m---- Epoch 9/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000150) ...\n",
      "loss 2.92753, a_loss 1.87956, ema 0.36927, oracc 0.97212, orien_loss 0.09838, gacc 0.95042, gloss 0.12392, b1 0.41577, b2 0.29742, b3 0.20662, b4 0.14756, padchxlmacf1 0.03870, padchxlmicf1 0.12940, padchxlzmacf1 0.05919, padchxlzmicf1 0.13875, padchxl_loss 0.40918, padchxlz_loss 0.52963, 96.53 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.32856, oracc 0.96430, gacc 0.94998, b1 0.22769, b2 0.16399, b3 0.11171, b4 0.08103, padchxlmacf1 0.04779, padchxlmicf1 0.11899, padchxlzmacf1 0.06852, padchxlzmicf1 0.14891, 22.40 secs\n",
      "Adjusting learning rate of group 0 to 1.3604e-04.\n",
      "\u001b[1m---- Epoch 10/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000136) ...\n",
      "loss 2.60248, a_loss 1.77774, ema 0.36776, oracc 0.97176, orien_loss 0.09548, gacc 0.95455, gloss 0.11036, b1 0.43968, b2 0.31770, b3 0.22363, b4 0.15993, padchxlmacf1 0.04307, padchxlmicf1 0.12824, padchxlzmacf1 0.06124, padchxlzmicf1 0.14144, padchxl_loss 0.40353, padchxlz_loss 0.53418, 95.94 secs\n",
      "(2) Validation stage ...\n",
      "ema 0.33106, oracc 0.97271, gacc 0.95975, b1 0.17320, b2 0.13258, b3 0.09702, b4 0.07409, padchxlmacf1 0.04866, padchxlmicf1 0.12355, padchxlzmacf1 0.06718, padchxlzmicf1 0.16632, 22.24 secs\n",
      "Adjusting learning rate of group 0 to 1.2355e-04.\n",
      "\u001b[1m---- Epoch 11/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000124) ...\n",
      "   iteration 3250\r"
     ]
    }
   ],
   "source": [
    "!python ../train_vqa.py \\\n",
    "        --epochs 60 \\\n",
    "        --batches-per-epoch 300 \\\n",
    "        --batch-size 55 \\\n",
    "        --iters-to-accumulate 5 \\\n",
    "        --num-workers 3 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,2e-4,55,1e-6\" \\\n",
    "        --use-padchest \\\n",
    "        --padchest-use-validation \\\n",
    "        --padchest-weight 1 \\\n",
    "        --padchest-train-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-val-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-test-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\" \\\n",
    "        --classify-orientation \\\n",
    "        --raw-image-encoding \"vitmodel-huggingface\" \\\n",
    "        --huggingface-model-name \"facebook/vit-mae-base\" \\\n",
    "        --image-size 224 224 \\\n",
    "        --image-local-feat-size 768 \\\n",
    "        --image-encoder-pretrained-weights-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20221228_105757_padchest_VitMAE/checkpoint_60_loss=0.9791.pt\" \\\n",
    "        --question-encoding \"one-hot\" \\\n",
    "        --answer-decoding \"transformer\" \\\n",
    "        --binary-loss-name \"wbce-c\" \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
