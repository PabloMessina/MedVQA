{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading iuxray reports from /mnt/workspace/iu-x-ray/dataset/reports/reports.min.json\n",
      "Number of sentences: 8617\n",
      "Shortest sentence: KUB.\n",
      "Longest sentence: The infrahilar pulmonary markings appear slightly prominent bilaterally, which XXXX represents XXXX appearance for the patient but difficult to completely exclude some reactive airway/bronchitic changes in the absence of comparison radiographs.. No airspace consolidation or lobar atelectasis.\n",
      "Loading CheXpert labeler\n",
      "Loading CheXbert labeler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8617it [00:00, 18598.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RadGraph labeler\n"
     ]
    }
   ],
   "source": [
    "from medvqa.datasets.iuxray import IUXRAY_REPORTS_MIN_JSON_PATH\n",
    "from medvqa.utils.files import (\n",
    "    get_checkpoint_folder_path,\n",
    "    get_results_folder_path,\n",
    "    load_json, load_pickle,\n",
    "    save_pickle,\n",
    ")\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def _load_iuxray_sentences():\n",
    "    print(f'Loading iuxray reports from {IUXRAY_REPORTS_MIN_JSON_PATH}')\n",
    "    reports = load_json(IUXRAY_REPORTS_MIN_JSON_PATH)\n",
    "    sentences = set()\n",
    "    for r in reports.values():\n",
    "        findings = r['findings']\n",
    "        impression = r['impression']\n",
    "        for x in (findings, impression):\n",
    "            if x:\n",
    "                for s in sent_tokenize(x):\n",
    "                    s = ' '.join(s.split()) # Remove extra spaces\n",
    "                    sentences.add(s)\n",
    "    sentences = list(sentences)\n",
    "    sentences.sort(key=lambda x: (len(x), x)) # Sort by length and then alphabetically\n",
    "    sentences = [s for s in sentences if any(c.isalpha() for c in s)] # Remove sentences without any alphabetic character\n",
    "    print(f'Number of sentences: {len(sentences)}')\n",
    "    print(f'Shortest sentence: {sentences[0]}')\n",
    "    print(f'Longest sentence: {sentences[-1]}')\n",
    "    return sentences\n",
    "\n",
    "sentences = _load_iuxray_sentences()\n",
    "\n",
    "print('Loading CheXpert labeler')\n",
    "from medvqa.metrics.medical.chexpert import ChexpertLabeler\n",
    "chexpert_labeler = ChexpertLabeler()\n",
    "chexpert_labels = chexpert_labeler.get_labels(sentences, update_cache_on_disk=True)\n",
    "\n",
    "print('Loading CheXbert labeler')\n",
    "from medvqa.metrics.medical.chexbert import CheXbertLabeler\n",
    "chexbert_labeler = CheXbertLabeler()\n",
    "chexbert_labels = chexbert_labeler.get_labels(sentences, update_cache_on_disk=True)\n",
    "\n",
    "print('Loading RadGraph labeler')\n",
    "from medvqa.metrics.medical.radgraph import RadGraphLabeler\n",
    "radgraph_labeler = RadGraphLabeler()\n",
    "radgraph_labels = radgraph_labeler.get_labels(sentences, update_cache_on_disk=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from medvqa.utils.metrics import jaccard_between_dicts\n",
    "\n",
    "i = 3001\n",
    "\n",
    "relevant_sentences = []\n",
    "if len(radgraph_labels[i]) > 0:\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            js = jaccard_between_dicts(radgraph_labels[i], radgraph_labels[j])\n",
    "            if js >= 0.4 or ((np.all(chexpert_labels[i] == chexpert_labels[j]) or \\\n",
    "                np.all(chexbert_labels[i] == chexbert_labels[j])) and js >= 0.2):\n",
    "                relevant_sentences.append(j)\n",
    "len(relevant_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal degenerative disease thoracic spine\n",
      "\n",
      "Thoracic spine.\n",
      "Degenerative spine.\n",
      "DISH of the thoracic spine.\n",
      "Degenerative changes spine.\n",
      "Minimal thoracic spondylosis.\n",
      "Degenerative changes the spine.\n",
      "Degenerative change of the spine.\n",
      "Degenerative changes in the spine.\n",
      "Degenerative changes of the spine.\n",
      "Scattered thoracic spine spurring.\n",
      "Degenerative changes of the spine..\n",
      "Degenerative changes thoracic spine.\n",
      "Levoscoliosis of the thoracic spine.\n",
      "Degenerative changes of the the spine.\n",
      "Dextroscoliosis of the thoracic spine.\n",
      "degenerative changes within the spine.\n",
      "Degenerative changes are present spine.\n",
      "Degenerative changes of thoracic spine.\n",
      "Degenerative disease of thoracic spine.\n",
      "Mild degenerative changes in the spine.\n",
      "Mild degenerative changes of the spine.\n",
      "Mild thoracic spine degenerative change\n",
      "Degenerative changes noted in the spine.\n",
      "Degenerative spurring of thoracic spine.\n",
      "Mild degenerative change thoracic spine.\n",
      "Mild thoracic spine degenerative change.\n",
      "No XXXX fractures of the thoracic spine.\n",
      "There degenerative changes of the spine.\n",
      "Mild degenerative changes thoracic spine.\n",
      "Mild thoracic spine degenerative changes.\n",
      "Stable degenerative changes in the spine.\n",
      "Stable degenerative changes of the spine.\n",
      "Degenerative changes in the thoracic spine\n",
      "Minimal osteophytes of the thoracic spine.\n",
      "Anterior osteophytes of the thoracic spine.\n",
      "Degenerative changes are seen in the spine.\n",
      "Degenerative changes in the thoracic spine.\n",
      "Degenerative changes of the thoracic spine.\n",
      "Degenerative spurring of the thoracic spine\n",
      "Degenerative this disease within the spine.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[i])\n",
    "print()\n",
    "for j in range(min(40, len(relevant_sentences))):\n",
    "    print(sentences[relevant_sentences[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8617, 14), (8617, 14))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chexpert_labels.shape, chexbert_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(11, 652562049788717786): 1}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radgraph_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.eval_fact_embedding_on_ranking import _load_mimiccxr_radiologist_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = _load_mimiccxr_radiologist_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(a, b):\n",
    "    for x, y in zip(a, b):\n",
    "        if x == -2:\n",
    "            continue\n",
    "        if x != y:\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2], dtype=int8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recurrent right lower lobe patchy airspace opacity , suggestive of waxing and waning atelectasis .\n",
      "\n",
      "band of atelectasis crossing the left hilus has been present for several days . 0.8571428571428571 1\n",
      "lungs are clear except for minimal patchy atelectasis in the right infrahilar region . 0.8571428571428571 1\n",
      "lungs are clear except for minimal lingular atelectasis or scar . 0.8571428571428571 1\n",
      "left lung is clear except for minor linear atelectasis in the left mid and lower lung regions . 0.8571428571428571 1\n",
      "lungs are clear except for linear atelectasis at the lung bases , left greater than right . 0.8571428571428571 1\n",
      "aside from mild atelectasis at the right base , lungs are clear . 0.8571428571428571 1\n",
      "aside from atelectasis in the right base the lungs are clear . 0.8571428571428571 1\n",
      "aside from atelectasis , left lung is grossly clear . 0.8571428571428571 1\n",
      "no substantial atelectasis except for the retrocardiac lung area . 0.8571428571428571 1\n",
      "left lung is clear aside from minimal left basal atelectasis . 0.8571428571428571 1\n",
      "the mid and upper zones are grossly clear except for some right mid zone atelectasis . 0.8571428571428571 1\n",
      "there is bilateral basal atelectasis , but overall improved since the prior study . 0.8571428571428571 1\n",
      "bilateral lower lobe atelectasis has improved . 0.8571428571428571 1\n",
      "there is increasing bibasilar atelectasis . 0.8571428571428571 1\n",
      "right basal linear atelectasis . 0.8571428571428571 1\n",
      "a mild increase in atelectatic lung regions at the right lung bases might be present . 0.8571428571428571 1\n",
      "left lower lobe opacities consistent with a large area of atelectasis and small effusion are unchanged . 0.8571428571428571 1\n",
      "otherwise , there has been little change in the appearance of the chest since the recent study of a few hours earlier , except for slight worsening of atelectasis in the left retrocardiac region . 0.8571428571428571 1\n",
      "worsening left lower lobe atelectasis . 0.8571428571428571 1\n",
      "overall appearance of the chest is unchanged except for development of minimal linear bibasilar atelectasis . 0.8571428571428571 1\n",
      "interval development of focal atelectasis in the retrocardiac region . 0.8571428571428571 1\n",
      "lungs are essentially clear with bibasal atelectasis . 0.8571428571428571 1\n",
      "lungs are clear except for linear atelectasis the left base . 0.8571428571428571 1\n",
      "new large left lower lobe atelectasis . 0.8571428571428571 1\n",
      "as compared to the previous radiograph , a minimal atelectasis at the right lung bases has cleared . 0.8571428571428571 1\n",
      "residual opacification at the left base is consistent with pleural fluid , atelectatic changes , and possible mild re expansion edema . 0.7142857142857143 1\n",
      "there is increasing opacification at the left base , worrisome for atelectasis and effusion . 0.8571428571428571 1\n",
      "in comparison with the study of xxxx , there is increased opacification at both bases with poor definition of the hemidiaphragms , consistent with bilateral pleural effusions and compressive basilar atelectasis . 0.8571428571428571 1\n",
      "bibasilar opacities likely reflect mild chf . 0.8571428571428571 1\n",
      "the lungs are essentially clear except for linear atelectasis in the left mid lower lung . 0.8571428571428571 1\n",
      "right lung grossly clear except for atelectasis at right lung base . 0.8571428571428571 1\n",
      "lungs are essentially clear except for minimal atelectasis at the left lower lung , unchanged since xxxx . 0.8571428571428571 1\n",
      "a platelike atelectasis and an opacity in the right lung , paralleling the minor fissure , has almost completely resolved . 1.0 1\n",
      "lungs are grossly clear aside from mild left basal atelectasis , not unexpected . 0.8571428571428571 1\n",
      "lungs are clear except for linear bibasilar atelectasis . 0.8571428571428571 1\n",
      "lungs overall clear except for minimal left basal atelectasis . 0.8571428571428571 1\n",
      "aside from linear atelectasis at the base , right lung is clear . 0.8571428571428571 1\n",
      "mild left basal atelectasis , otherwise normal . 0.8571428571428571 1\n",
      "aside from mild bibasilar atelectasis or linear scarring lungs are clear . 0.8571428571428571 1\n",
      "left lung is essentially clear except for left basal atelectasis . 0.8571428571428571 1\n",
      "aside from minimal atelectasis in the left lower lobe and left upper lobe , the lungs are clear . 0.8571428571428571 1\n",
      "aside from mild left lower lobe atelectasis , lungs are grossly clear . 0.8571428571428571 1\n",
      "aside from linear atelectasis at the base , the left lung is clear . 0.8571428571428571 1\n",
      "the lungs are clear aside from minimal atelectasis in the left base . 0.8571428571428571 1\n",
      "the left lung is essentially clear except for minimal atelectatic changes . 0.8571428571428571 1\n",
      "lungs are overall clear except for right basal atelectasis . 0.8571428571428571 1\n",
      "residual opacification at the left base with poor definition of the hemidiaphragm is consistent with layering effusion and atelectasis . 0.8571428571428571 1\n",
      "however , atelectasis at the left lung base has developed . 0.8571428571428571 1\n",
      "a linear focus of atelectasis in the mid lung region on the left is similar to the prior study . 0.8571428571428571 1\n",
      "atelectasis at both lung bases are present . 0.8571428571428571 1\n",
      "recurrent right lower lobe patchy airspace opacity , suggestive of waxing and waning atelectasis . 1.0 1\n",
      "increase of the pre existing retrocardiac atelectasis with substantial volume loss of the left hemithorax and leftward displacement of the mediastinum . 0.8571428571428571 1\n",
      "bibasilar opacities reflecting lower lung atelectasis have minimally worsened . 1.0 1\n",
      "continued bibasilar opacification with atelectatic changes , more prominent on the left , where there is also retrocardiac opacification consistent with volume loss in the lower lobe . 0.7142857142857143 1\n",
      "retrocardiac consolidation and patchy opacity at the right base most likely represent partial lower lobe compressive atelectasis . 0.8571428571428571 1\n",
      "there are atelectasis in the right lower lobe . 0.8571428571428571 1\n",
      "right lung nodules , masslike opacity in the left hemi thorax , left lower lobe atelectasis and small left effusion were better characterized in prior ct . 0.8571428571428571 1\n",
      "persistent opacification at the lung bases is probably a combination of residual edema and concurrent lower lobe atelectasis . 0.8571428571428571 1\n",
      "basal areas of atelectasis . 0.8571428571428571 1\n",
      "left retrocardiac consolidation is noted associated with minimal of vessel effusion . 0.5 0\n",
      "there is moderate pulmonary edema which has worsened slightly . 0.5714285714285714 0\n",
      "increased patchy opacities in the lung bases may reflect progression of underlying chronic interstitial lung disease , but superimposed atelectasis or infection cannot be completely excluded . 0.8571428571428571 0\n",
      "right upper lobe new consolidation is compatible with atelectasis with possibly superimposed aspiration . 0.6428571428571429 0\n",
      "whether this is atelectasis alone or concurrent pneumonia is radiographically indeterminate . 0.7142857142857143 0\n",
      "there is interval increase in retrocardiac density , likely reflecting a combination of layering pleural fluid and underlying collapse and / or consolidation . 0.5 0\n",
      "moderate pulmonary edema appears slightly improved since the xxxx examination . 0.5714285714285714 0\n",
      "same can be said for apparent consolidation at the base of the left lung , which , alternatively , could be due to atelectasis . 0.6428571428571429 0\n",
      "diffuse opacification and volume loss of the right lung is likely due to a combination of post surgical changes , the known hilar mass , and possible worsening atelectasis . 0.7857142857142857 0\n",
      "retrocardiac consolidation , may represent atelectasis or infection in the appropriate clinical setting . 0.5714285714285714 0\n",
      "a left lower lobe opacity is better delineated on the subsequent ct of the chest as an area of probable rounded atelectasis . 0.9285714285714286 0\n",
      "ap chest compared to xxxx left heart border is consistently obscured on prior chest radiographs , probably by atelectasis in the lingula which persists . 0.6428571428571429 0\n",
      "findings indicating early congestive heart failure . 0.5714285714285714 0\n",
      "severe cardiomegaly with small bilateral pleural effusions . 0.42857142857142855 0\n",
      "the effusions are somewhat improved since the prior chest xray . 0.5714285714285714 0\n",
      "a large mass like opacity sub cm in the right hilus traversed by air bronchograms could be atelectasis , particularly if the patient has had radiation . 0.8571428571428571 0\n",
      "subtle increase in opacity over the inferior spine on the lateral view , not substantiated on the frontal view , felt to unlikely represent consolidation , possibly atelectasis vs artifact . 0.6428571428571429 0\n",
      "right sided effusion is resolved . 0.5714285714285714 0\n",
      "there is again a large right pleural effusion with compressive basilar atelectasis as well as volume loss in the left lower lobe with smaller effusion . 0.42857142857142855 0\n",
      "although this most likely represents atelectasis and effusion , the possibility of supervening pneumonia would have to be considered in the appropriate clinical setting . 0.42857142857142855 0\n",
      "unchanged appearance of substantial left sided pleural effusion . 0.5714285714285714 0\n",
      "in addition to severe bibasilar atelectasis and small to moderate pleural effusions , there is now consolidation in the right lower lung , probably pneumonia . 0.5714285714285714 0\n",
      "there is again seen cardiomegaly , bilateral pleural effusions left greater than right and left retrocardiac opacity . 0.5 0\n",
      "pa and lateral chest compared to xxxx moderate right middle and lower lobe atelectasis and moderate right pleural effusion , partially fissural , not appreciably changed since xxxx . 0.5714285714285714 0\n",
      "there is a small left pleural effusion has increased compared to prior . 0.5714285714285714 0\n",
      "there are bilateral small pleural effusions with compressive atelectasis at the bases . 0.5714285714285714 0\n",
      "there are layering bilateral effusions with associated bibasilar airspace disease , left greater the right tube which is also unchanged . 0.5714285714285714 0\n",
      "small bilateral pleural effusions have significantly improved since xxxx . 0.5714285714285714 0\n",
      "improving patchy and linear opacities at both lung bases favor atelectasis . 0.9285714285714286 0\n",
      "increase in bilateral pleural effusion now small to moderate in size with bibasilar right greater than left opacities which are likely atelectatic however infection cannot be excluded given the clinical circumstance . 0.6428571428571429 0\n",
      "bibasilar opacities consistent with collapse and / or consolidation appear similar to xxxx at NUMBER p . 0.7142857142857143 0\n",
      "there is a left retrocardiac opacity which may be due to atelectasis or developing infiltrate . 0.8571428571428571 0\n",
      "patchy bilateral consolidation could represent either subsegmental atelectasis or the developing aspiration pneumonia . 0.5714285714285714 0\n",
      "left lower lobe opacification has been persistent since xxxx , presumably atelectasis . 0.8571428571428571 0\n",
      "differential considerations include aspiration , although findings are not definitely changed , versus atelectasis or pneumonia . 0.6428571428571429 0\n",
      "perihilar opacification in both lungs could be atelectasis or pneumonia , worsened on the right and improved on the left since xxxx . 0.7857142857142857 0\n",
      "left lower lobe opacity could reflect pneumonia or atelectasis depending on the clinical setting . 0.7857142857142857 0\n",
      "there is increasing airspace opacity in both lungs which may represent worsening consolidation related to atelectasis or pneumonia , although superimposed pulmonary edema should also be considered . 0.5714285714285714 0\n",
      "this could be due to atelectasis alone , but pneumonia cannot be excluded radiographically . 0.7142857142857143 0\n",
      "hazy right basilar opacity seen only on the frontal view may reflect overlying soft tissue as there is no correlate on the lateral view , though atelectasis or infection is not completely excluded . 0.7857142857142857 0\n",
      "new bibasilar opacities in the setting of very shallow inspiration , favor atelectasis , consider pneumonitis in the appropriate clinical setting . 0.8571428571428571 0\n"
     ]
    }
   ],
   "source": [
    "idx = 11\n",
    "print(sentences[idx])\n",
    "sorted_idxs = np.argsort([score(labels[idx], x) or score(x, labels[idx]) for x in labels])[::-1]\n",
    "print()\n",
    "for i in range(100):\n",
    "    j = sorted_idxs[i]\n",
    "    print(sentences[j],  np.mean(labels[j] == labels[idx]), score(labels[idx], labels[j]) or score(labels[j], labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import medvqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'medvqa.eval_fact_embedding_on_ranking' from '/home/pamessina/medvqa/medvqa/eval_fact_embedding_on_ranking.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(medvqa.eval_fact_embedding_on_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.eval_fact_embedding_on_ranking import SentenceRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = SentenceRanker('mimiccxr_rad_annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1165/1165 [00:00<00:00, 401905.26it/s]\n"
     ]
    }
   ],
   "source": [
    "query_idx = 11\n",
    "output = sr.rank_sentences(query_idx, model_name='microsoft/BiomedVLP-CXR-BERT-specialized',\n",
    "                           checkpoint_folder_path='/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_152825_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)',\n",
    "                           top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1165/1165 [00:00<00:00, 434111.95it/s]\n"
     ]
    }
   ],
   "source": [
    "query_idx = 6\n",
    "output = sr.rank_sentences(query_idx, model_name='emilyalsentzer/Bio_ClinicalBERT', average_token_embeddings=True,\n",
    "                           top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recurrent right lower lobe patchy airspace opacity , suggestive of waxing and waning atelectasis .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['recurrent right lower lobe patchy airspace opacity , suggestive of waxing and waning atelectasis .',\n",
       "  'pneumonia or atelectasis at the right lung base .',\n",
       "  'there is patchy opacity at the right base which could reflect compressive atelectasis .',\n",
       "  'right lower lobe collapse is persistent .',\n",
       "  'consolidation continues to progress in the right lower lobe .',\n",
       "  'patchy opacity at the right base may represent partial lower lobe atelectasis , although pneumonia or aspiration cannot be entirely excluded .',\n",
       "  'subtle opacification at the right lung base , which may represent atelectasis , however an early developing pneumonia is a consideration .',\n",
       "  'subtle opacity at the base of the right lung could represent atelectasis however infection should be considered in the appropriate clinical setting .',\n",
       "  'focal opacity at the right lung base has increased , either an atelectasis or consolidation .',\n",
       "  'subtle patchy right base opacity could be due to aspiration , infection or atelectasis .',\n",
       "  'increased opacities at the right lung base may reflect atelectasis or pneumonia in the proper clinical context .',\n",
       "  'there are atelectasis in the right lower lobe .',\n",
       "  'there is persistent atelectasis or consolidation in the right lower lobe .',\n",
       "  'a mild increase in atelectatic lung regions at the right lung bases might be present .',\n",
       "  'the only focal pulmonary abnormality today consolidation at the right lung base which could be either pneumonia or atelectasis , increased since xxxx .',\n",
       "  'would be difficult to entirely exclude right basilar consolidation although there is not evidence of such on the lateral view .',\n",
       "  'slight worsening of the right lower lobe atelectasis is noted on the current radiograph that might reflect interval increase in pleural effusion or additional compression of the right lower lobe airways .',\n",
       "  'minimal right lower lobe opacity , possibly atelectasis but infection cannot be excluded .',\n",
       "  'increased patchy opacification in the right lung base could reflect worsening atelectasis however infection is not excluded in the correct clinical setting .',\n",
       "  'there are minimal opacities in the right lower lobe , could be atelectasis , but superimposed infection / aspiration cannot be excluded and attention on followup studies is recommended .',\n",
       "  'linear density in the right infrahilar region may represent atelectasis or effusion in the right major fissure .',\n",
       "  'right basal consolidations appear to be increased and might potentially reflect increasing atelectasis , but infectious process in the area cannot be excluded .',\n",
       "  'underlying right base consolidation is not excluded .',\n",
       "  'subtle patchy right lower lobe opacity may relate to atelectasis and overlying vascular structures , but overlying consolidation due to pneumonia is not excluded in the appropriate clinical setting .',\n",
       "  'increased right pleural fluid obscuring a large right lower lobe consolidation .',\n",
       "  'right basilar opacity potentially due to layering effusion with atelectasis , infection not excluded .',\n",
       "  'moderate right sided pleural effusion with increased right basilar opacity , likely atelectasis , though infection is not excluded .',\n",
       "  'there is increasing opacification of the partially collapsed right lung .',\n",
       "  'persistent right basilar atelectasis and suspected small right pleural effusion .',\n",
       "  'right basal consolidation which persisted following clearing of right lower lobe collapse after xxxx has subsequently improved , though not cleared .',\n",
       "  'right basilar opacity silhouetting the hemidiaphragm suggestive of effusion with underlying atelectasis and possible consolidation .',\n",
       "  'interval increase in right pleural effusion with complete atelectasis of the right middle and lower lobes , raising concern for bronchial obstruction .',\n",
       "  'there is a right sided pleural effusion and some right basilar consolidation .',\n",
       "  'moderate right pleural effusion and right basal consolidation are new .',\n",
       "  'worsening right pleural effusion and increasing consolidation in the right lower lung raises concern for pneumonia , aspiration versus atelectasis .',\n",
       "  'relative opacity at the medial right lung base may be due to overlapping vascular structures , although consolidation is not excluded in the appropriate clinical setting .',\n",
       "  'increasing opacification at the right base medially with silhouetting of the hemidiaphragm is worrisome for a right middle lobe consolidation .',\n",
       "  'interval development of moderate right effusion with possible loculation and adjacent atelectasis and / or consolidation .',\n",
       "  'new small right sided pleural effusion with adjacent atelectasis .',\n",
       "  'large opacity projecting over the right mid to lower hemithorax may represent combination of pleural effusion and atelectasis , underlying consolidation is not excluded .',\n",
       "  'increased interstitial markings throughout the lungs with more confluent consolidation at the right lung base .',\n",
       "  'in comparison to xxxx chest radiograph , a moderate sized right pleural effusion has slightly increased in size with associated worsening right basilar atelectasis and or consolidation .',\n",
       "  'residual focal consolidation in right lower lung is worrisome for pneumonia .',\n",
       "  'consolidation involving the right lung base may be secondary to pneumonia .',\n",
       "  'more coalescent area medially on the right could well represent a developing consolidation .',\n",
       "  'hazy right basilar opacity seen only on the frontal view may reflect overlying soft tissue as there is no correlate on the lateral view , though atelectasis or infection is not completely excluded .',\n",
       "  'right lower lobe consolidation better seen on subsequent chest ct .',\n",
       "  'right lung base is partially obscured by the pleural fluid .',\n",
       "  'increased right mid and low lung opacity could represent consolidation or effusion .',\n",
       "  'new consolidation in the right middle lobe is consistent with pneumonia .',\n",
       "  'complete opacification of the right lower lung due to a combination of effusion and consolidation .',\n",
       "  'worsened right lung opacity , likely secondary to pleural effusion more or less segmental atelectasis .',\n",
       "  'right pleural effusion and right lower lobe opacity are noted and although the effusion appears to be grossly unchanged , the extent of the right basal consolidation potentially has increased and might reflect pleural effusion .',\n",
       "  'new consolidation developing in the right lung .',\n",
       "  'increased patchy opacities in the lung bases may reflect progression of underlying chronic interstitial lung disease , but superimposed atelectasis or infection cannot be completely excluded .',\n",
       "  'right pleural effusion is redemonstrated , associated with right lower lobe consolidation that appears to be slightly increased since the prior study and might reflect interval development of right lower lobe pneumonia .',\n",
       "  'small amount of pleural effusion cannot be excluded in particular on the right although it might represent atelectasis due to elevated right hemidiaphragm .',\n",
       "  'bibasilar opacities reflecting lower lung atelectasis have minimally worsened .',\n",
       "  'posterior opacity is noted , and might represent part of the atelectasis in combination with elevated right hemidiaphragm .',\n",
       "  'consolidation or collapse of the superior segment of the right lower lobe is probably still present , explaining the definition of the lowered major fissure medially .',\n",
       "  'right base opacity is concerning for consolidation possibly due to infection , underlying neoplastic process is not excluded either .',\n",
       "  'progressive opacification in the right lower lung could be fissural pleural fluid , but raises serious concern for pneumonia in a solitary aerated right lung .',\n",
       "  'right basal linear atelectasis .',\n",
       "  'low lung volumes with atelectasis at the right lung bases .',\n",
       "  'right basilar consolidation concerning for pneumonia and / or aspiration .',\n",
       "  'probable small right pleural effusion and right basal atelectasis .',\n",
       "  'a large mass like opacity sub cm in the right hilus traversed by air bronchograms could be atelectasis , particularly if the patient has had radiation .',\n",
       "  'there is increasing bibasilar atelectasis .',\n",
       "  'this may reflect atelectasis and dependent edema , but coexisting infection should be considered in the appropriate clinical setting .',\n",
       "  'as compared to the previous radiograph , there is a substantial increase in extent of the right pleural effusion with subsequent areas of atelectasis in the right lung .',\n",
       "  'right upper lobe new consolidation is compatible with atelectasis with possibly superimposed aspiration .',\n",
       "  'worsening consolidative opacity in the right infrahilar region may potentially represent an evolving aspiration pneumonia in the appropriate clinical setting .',\n",
       "  'this opacities could be atelectasis and or pneumonia .',\n",
       "  'bibasilar opacities likely reflect areas of atelectasis , although aspiration or pneumonia should also be considered .',\n",
       "  'superimposed atelectasis or pneumonia would be difficult to exclude .',\n",
       "  'right lower opacity is unchanged , is a combination of a pneumonic consolidation .',\n",
       "  'nodular opacities at the right lung base may be due to a combination of atelectasis and overlying rib shadows , but metastasis or even pneumonia are also possible .',\n",
       "  'this likely represents atelectasis and / or pneumonia in the appropriate clinical setting .',\n",
       "  'this could merely reflect atelectasis , though in the appropriate clinical setting superimposed pneumonia could be considered .',\n",
       "  'patchy bilateral consolidation could represent either subsegmental atelectasis or the developing aspiration pneumonia .',\n",
       "  'worsening superimposed opacities at the lung bases could be due to atelectasis , aspiration or infectious consolidation .',\n",
       "  'right pleural effusion and a adjacent consolidation is not excluded .',\n",
       "  'moderate pulmonary edema has increased .',\n",
       "  'bibasilar patchy airspace opacity could reflect atelectasis but infection or aspiration are not excluded .',\n",
       "  'vague increased density in the bilateral lower lung zones may represent micro atelectasis ( secondary to low lung volumes ) or early airspace consolidation .',\n",
       "  'more dense consolidation in the right lower lungs could be confluent edema or developing pneumonia .',\n",
       "  'although possibly representing bibasilar atelectasis , coexisting pneumonia should be considered in the appropriate clinical setting .',\n",
       "  'severe consolidation , right upper lobe , longstanding .',\n",
       "  'mild interstitial edema has increased , superimposed to known mild chronic interstitial lung disease .',\n",
       "  'increased bibasilar airspace opacities may be due to the edema , but pneumonia cannot be excluded in the appropriate clinical setting .',\n",
       "  'right lower lobe collapse has improved , but the lower lobe is still severely atelectatic and / or consolidated .',\n",
       "  'right upper lobe consolidation with cavitation .',\n",
       "  'lungs are overall clear except for right basal atelectasis .',\n",
       "  'minimally increased opacity at the lung bases which likely reflects atelectasis .',\n",
       "  'lungs are clear except for minimal patchy atelectasis in the right infrahilar region .',\n",
       "  'bibasilar atelectasis versus aspiration .',\n",
       "  'bibasilar opacities , likely atelectasis but pneumonia is not excluded .',\n",
       "  'interval worsening pulmonary edema .',\n",
       "  'possible mild increased pulmonary vascular prominence and possible tiny right pleural effusion .',\n",
       "  'right upper lobe and likely right middle lobe consolidation compatible with pneumonia in the proper clinical setting .'],\n",
       " [array([ 1, -2, -2, -2,  1, -2, -2,  1, -2, -2, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-1, -2, -2, -2, -2, -2, -2, -1, -2, -2, -2, -2, -2, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-2, -2, -2, -2, -2, -2,  1, -1, -2, -2, -2, -2, -2,  1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-1, -2, -1, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-1, -2, -2, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -1, -2, -2, -2, -2, -1, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-1, -2,  1, -2, -2, -2, -2, -1, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  0, -2, -2, -2, -2, -2, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2,  1, -2,  1, -2, -2, -2, -2, -1,  1], dtype=int8),\n",
       "  array([-1, -2, -2, -2,  1, -2, -1, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-1, -2, -2, -2,  1, -1, -2, -1, -2, -2, -2, -2, -1, -2], dtype=int8),\n",
       "  array([ 1, -2,  1, -2, -2, -2, -2, -1, -2,  1, -2, -2, -2, -1], dtype=int8),\n",
       "  array([-2, -2, -1, -2, -2, -2, -2, -2, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -1, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2,  1, -2, -2, -2, -1, -2, -2,  1, -2], dtype=int8),\n",
       "  array([-1, -2, -2, -2,  1, -1, -2, -1, -2, -2, -2,  1, -1, -2], dtype=int8),\n",
       "  array([ 1, -2, -1, -2, -2,  1, -2, -1, -2, -2, -2,  1,  1, -2], dtype=int8),\n",
       "  array([-2, -2, -2, -2,  1, -2,  1, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2,  1, -2,  1, -2, -2, -2, -2, -1, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2,  1, -2, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -1, -2,  1,  1, -2, -1, -2, -1, -2,  1, -1, -2], dtype=int8),\n",
       "  array([ 1, -2,  1, -2, -2,  1,  1,  1, -2, -2, -2, -2,  1,  1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2,  1, -2, -2, -2,  1, -2, -2,  1, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2,  1, -2, -2, -2,  1, -2, -2,  1, -2], dtype=int8),\n",
       "  array([-1, -2,  1, -2, -2,  1, -2, -1, -2,  1, -2, -2,  1, -2], dtype=int8),\n",
       "  array([-2, -2, -1, -2,  1, -2, -2, -2, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2,  1, -2, -2, -2, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-1, -2, -1, -2, -2,  1, -2, -1, -2, -1, -2, -2,  1, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2,  1, -2,  1, -2, -2, -2, -2,  1, -2], dtype=int8),\n",
       "  array([ 1, -2, -1, -2,  1,  1, -2, -1, -2, -1, -2,  1, -1, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2,  1, -2, -2, -2, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2,  1, -2, -2,  1, -2, -1, -2, -1, -2, -2,  1, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2,  1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -1, -2, -2], dtype=int8),\n",
       "  array([-2, -2, -1, -2,  1, -2, -2, -2, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2], dtype=int8),\n",
       "  array([-2, -2, -1, -2,  1, -1, -2, -2, -2, -1, -2,  1, -1, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2,  1, -2,  1], dtype=int8),\n",
       "  array([-2, -2,  1, -2,  1,  1, -2, -2, -2,  1, -2,  1,  1, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1,  1, -2, -1, -2, -2, -2,  1, -1, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2,  1,  1, -2, -2, -2, -2, -2,  1, -1, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2,  1, -2, -2, -2,  1, -2, -2,  1,  1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -1,  1, -1, -2, -2, -2, -2, -1, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2,  1, -2, -2, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2,  1, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -1, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2,  1, -2, -2, -2, -2,  1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-2, -2, -1, -2, -2,  1, -2, -2, -2, -2, -2,  1, -1, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2,  1,  1, -2, -2, -2, -2, -2,  1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2,  1, -2, -1, -2, -2, -2, -2, -1, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2,  1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -1,  1, -2, -2, -2, -1, -2, -2, -1, -2, -2, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2,  1, -2,  1, -2, -2, -2, -2,  1, -2], dtype=int8),\n",
       "  array([ 1, -2,  1, -2, -2, -2, -2,  1, -2,  1, -2, -2, -2, -1], dtype=int8),\n",
       "  array([-2, -2, -2, -2,  1, -2, -2, -2, -2,  1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-1, -2, -2, -2, -2, -2, -2, -1, -2, -2, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -1, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-1, -2, -1, -2, -2, -2, -2, -1, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2, -1, -2, -1, -2, -1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2, -1, -2, -1, -2, -1, -2, -2], dtype=int8),\n",
       "  array([-1, -2,  1, -2, -2, -2, -2, -1, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-1, -2, -1, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2,  1, -2, -2, -2,  1, -2, -2,  1, -2], dtype=int8),\n",
       "  array([-2, -2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -2, -2,  1, -2, -1], dtype=int8),\n",
       "  array([-1, -2, -1, -2,  1, -2, -2, -1, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -1, -2, -2, -1, -2, -2,  1, -1, -2, -2, -1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2, -1, -2, -1, -2, -1, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2, -2,  1,  1, -2, -2, -2, -2, -2,  1, -2, -2,  1], dtype=int8),\n",
       "  array([-2, -2, -1,  1,  1, -2, -2, -2, -2, -2, -1,  1, -2, -1], dtype=int8),\n",
       "  array([ 1, -2,  1, -2, -2, -2,  1, -1, -2, -1, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2,  1], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2, -2, -2, -2], dtype=int8),\n",
       "  array([-1, -2, -2, -2, -2, -2, -1, -1, -2, -2, -2, -2, -2, -2], dtype=int8),\n",
       "  array([ 1, -2, -2, -2,  1, -2, -2, -1, -2, -1, -2,  1, -2, -2], dtype=int8),\n",
       "  array([-2, -2, -2,  1, -2, -2, -2, -2, -2, -2,  1, -2, -2, -2], dtype=int8),\n",
       "  array([-2, -2, -2, -2, -2,  1,  1, -2, -2, -2, -2, -2, -1, -1], dtype=int8),\n",
       "  array([-2, -2,  1, -2, -2, -2, -2, -2, -2, -1, -2, -1, -2, -2], dtype=int8)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sr.sentences[query_idx])\n",
    "print()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-CXR Radiologist Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedVLP-CXR-BERT-specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=116,1396652957813721259).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 5315.63it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1145.26it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7021\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedVLP-BioViL-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-BioViL-T\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=104,2678825193822464594).pkl\n",
      "len(self.cache[\"hashes\"]) = 4451804\n",
      "self.cache[\"embeddings\"].shape = (4451804, 128)\n",
      "100%|██████████████████████████████████████| 1165/1165 [00:04<00:00, 236.70it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1161.96it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7097\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-BioViL-T\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### michiyasunaga/BioLinkBERT-large (cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: michiyasunaga/BioLinkBERT-large\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=107,40983794350539675).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 1024)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1517.76it/s]\n",
      "Embeddings shape: (1165, 1024)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1104.13it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.5205\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"michiyasunaga/BioLinkBERT-large\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### michiyasunaga/BioLinkBERT-large (tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: michiyasunaga/BioLinkBERT-large\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: True\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=132,1253761809285128054).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 1024)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1489.36it/s]\n",
      "Embeddings shape: (1165, 1024)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1112.89it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.6087\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"michiyasunaga/BioLinkBERT-large\" \\\n",
    "--batch_size 100 \\\n",
    "--average_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext (cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=137,1948119357925567498).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1819.72it/s]\n",
      "Embeddings shape: (1165, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1112.26it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.5853\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext (cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: True\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=162,1758876944325999998).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1819.70it/s]\n",
      "Embeddings shape: (1165, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1140.10it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.6065\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\" \\\n",
    "--batch_size 100 \\\n",
    "--average_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### emilyalsentzer/Bio_ClinicalBERT (cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: emilyalsentzer/Bio_ClinicalBERT\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=107,1542209846559636901).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1776.79it/s]\n",
      "Embeddings shape: (1165, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1137.14it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.5197\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"emilyalsentzer/Bio_ClinicalBERT\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### emilyalsentzer/Bio_ClinicalBERT (tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: emilyalsentzer/Bio_ClinicalBERT\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: True\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=132,412155249657523952).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 3205.03it/s]\n",
      "Embeddings shape: (1165, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1121.49it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.6310\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"emilyalsentzer/Bio_ClinicalBERT\" \\\n",
    "--batch_size 100 \\\n",
    "--average_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheXbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: CheXbert\n",
      "   device: cuda\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "100%|███████████████████████████████████████| 1165/1165 [00:13<00:00, 85.50it/s]\n",
      "Embeddings shape: (1165, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1125.10it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.8698\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"CheXbert\" \\\n",
    "--device \"cuda\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_194923_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_40_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9355.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=359,388447562913600091).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 4490.74it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1169.27it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7410\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_194923_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+EC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=366,4246634492375333681).pkl\n",
      "len(self.cache[\"hashes\"]) = 134854\n",
      "self.cache[\"embeddings\"].shape = (134854, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 5391.96it/s]\n",
      "Computing embeddings for 1165 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  5.44it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=366,4246634492375333681).pkl\n",
      "100%|███████████████████████████████████| 1165/1165 [00:00<00:00, 232794.86it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1153.45it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.6606\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_013825_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_64_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9474.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=387,2492157991032224976).pkl\n",
      "len(self.cache[\"hashes\"]) = 134854\n",
      "self.cache[\"embeddings\"].shape = (134854, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 4315.76it/s]\n",
      "Computing embeddings for 1165 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_64_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9474.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_013825_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_64_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9474.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 223 keys, intersection has 211 keys, union has 223 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  health_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  5.36it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=387,2492157991032224976).pkl\n",
      "100%|███████████████████████████████████| 1165/1165 [00:00<00:00, 262990.54it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1158.07it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7881\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_013825_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_155915_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_29_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9495.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=387,1308360022390228203).pkl\n",
      "len(self.cache[\"hashes\"]) = 8617\n",
      "self.cache[\"embeddings\"].shape = (8617, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 9423.09it/s]\n",
      "Computing embeddings for 1165 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_29_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9495.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_155915_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_29_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9495.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 223 keys, intersection has 211 keys, union has 223 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  health_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  4.92it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=387,1308360022390228203).pkl\n",
      "100%|███████████████████████████████████| 1165/1165 [00:00<00:00, 371388.93it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1167.89it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7875\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_155915_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+EC+NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_52_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9002.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=375,1493189425621868816).pkl\n",
      "len(self.cache[\"hashes\"]) = 134854\n",
      "self.cache[\"embeddings\"].shape = (134854, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 5449.10it/s]\n",
      "Computing embeddings for 1165 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_52_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9002.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_52_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9002.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 215 keys, intersection has 211 keys, union has 215 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  nli_classifier.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  nli_classifier.weight\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  5.42it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=375,1493189425621868816).pkl\n",
      "100%|███████████████████████████████████| 1165/1165 [00:00<00:00, 222826.58it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1159.77it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.6365\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_110825_MIMIC-CXR(triplets+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_93_spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9415.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=368,1866338409828607810).pkl\n",
      "len(self.cache[\"hashes\"]) = 134854\n",
      "self.cache[\"embeddings\"].shape = (134854, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 4499.87it/s]\n",
      "Computing embeddings for 1165 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_93_spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9415.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_110825_MIMIC-CXR(triplets+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_93_spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9415.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 216 keys, intersection has 211 keys, union has 216 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  spert_rel_classifier.bias\u001b[0m\n",
      "\u001b[93m  spert_rel_classifier.weight\u001b[0m\n",
      "\u001b[93m  spert_entity_classifier.weight\u001b[0m\n",
      "\u001b[93m  spert_size_embeddings.weight\u001b[0m\n",
      "\u001b[93m  spert_entity_classifier.bias\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  5.35it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=368,1866338409828607810).pkl\n",
      "100%|███████████████████████████████████| 1165/1165 [00:00<00:00, 220156.08it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1163.19it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7161\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_110825_MIMIC-CXR(triplets+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+C+EC+NLI+R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_152825_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_26_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9386.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,141429923173682938).pkl\n",
      "len(self.cache[\"hashes\"]) = 103629\n",
      "self.cache[\"embeddings\"].shape = (103629, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 6083.98it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:01<00:00, 1114.13it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7396\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_152825_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: mimiccxr_radiologist_annotations\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading mimiccxr sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr_sentences_and_relevant.pkl\n",
      "len(sentences): 1165\n",
      "len(relevant_sentences): 1165\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,3677907674814022869).pkl\n",
      "len(self.cache[\"hashes\"]) = 8617\n",
      "self.cache[\"embeddings\"].shape = (8617, 128)\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 9150.85it/s]\n",
      "Computing embeddings for 1165 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 232 keys, intersection has 211 keys, union has 232 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  spert_entity_classifier.weight\u001b[0m\n",
      "\u001b[93m  spert_rel_classifier.weight\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  5.00it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,3677907674814022869).pkl\n",
      "100%|███████████████████████████████████| 1165/1165 [00:00<00:00, 708271.37it/s]\n",
      "Embeddings shape: (1165, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|█████████████████████████████████████| 1165/1165 [00:00<00:00, 1167.26it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7529\u001b[0m\n",
      "mean_relevant: 57.8080\n",
      "count: 1156 / 1165 (99.23%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"mimiccxr_radiologist_annotations\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IU X-ray Automatic Labelers (chexpert + chexbert + radgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedVLP-CXR-BERT-specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray reports from /mnt/workspace/iu-x-ray/dataset/reports/reports.min.json\n",
      "Number of sentences: 8617\n",
      "Shortest sentence: KUB.\n",
      "Longest sentence: The infrahilar pulmonary markings appear slightly prominent bilaterally, which XXXX represents XXXX appearance for the patient but difficult to completely exclude some reactive airway/bronchitic changes in the absence of comparison radiographs.. No airspace consolidation or lobar atelectasis.\n",
      "Loading CheXpert labeler\n",
      "Loading CheXbert labeler\n",
      "8617it [00:00, 17841.73it/s]\n",
      "Loading RadGraph labeler\n",
      "100%|███████████████████████████████████████| 8617/8617 [03:07<00:00, 45.92it/s]\n",
      "Saving iuxray sentences and relevant sentences to /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=116,1396652957813721259).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 74038.46it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:53<00:00, 162.32it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.8515\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedVLP-BioViL-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-BioViL-T\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=104,2678825193822464594).pkl\n",
      "len(self.cache[\"hashes\"]) = 4451804\n",
      "self.cache[\"embeddings\"].shape = (4451804, 128)\n",
      "100%|█████████████████████████████████████| 8617/8617 [00:03<00:00, 2565.91it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:51<00:00, 168.34it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.8658\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-BioViL-T\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### michiyasunaga/BioLinkBERT-large (cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: michiyasunaga/BioLinkBERT-large\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=107,40983794350539675).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 1024)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 26704.83it/s]\n",
      "Embeddings shape: (8617, 1024)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:58<00:00, 147.38it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.6601\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"michiyasunaga/BioLinkBERT-large\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### michiyasunaga/BioLinkBERT-large (tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: michiyasunaga/BioLinkBERT-large\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: True\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=132,1253761809285128054).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 1024)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 26889.74it/s]\n",
      "Embeddings shape: (8617, 1024)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:58<00:00, 147.44it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.8621\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"michiyasunaga/BioLinkBERT-large\" \\\n",
    "--batch_size 100 \\\n",
    "--average_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext (cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=137,1948119357925567498).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 32819.15it/s]\n",
      "Embeddings shape: (8617, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [01:00<00:00, 143.52it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7768\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext (tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: True\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=162,1758876944325999998).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 33578.34it/s]\n",
      "Embeddings shape: (8617, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:59<00:00, 143.83it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9076\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\" \\\n",
    "--batch_size 100 \\\n",
    "--average_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### emilyalsentzer/Bio_ClinicalBERT (cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: emilyalsentzer/Bio_ClinicalBERT\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=107,1542209846559636901).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 33897.08it/s]\n",
      "Embeddings shape: (8617, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [01:00<00:00, 143.41it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.5998\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"emilyalsentzer/Bio_ClinicalBERT\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### emilyalsentzer/Bio_ClinicalBERT (tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: emilyalsentzer/Bio_ClinicalBERT\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: True\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=132,412155249657523952).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 768)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 32540.98it/s]\n",
      "Embeddings shape: (8617, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:54<00:00, 157.44it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9241\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"emilyalsentzer/Bio_ClinicalBERT\" \\\n",
    "--batch_size 100 \\\n",
    "--average_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheXbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: CheXbert\n",
      "   device: cuda\n",
      "   batch_size: 32\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: None\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "100%|███████████████████████████████████████| 8617/8617 [01:34<00:00, 91.56it/s]\n",
      "Embeddings shape: (8617, 768)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [01:00<00:00, 143.33it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9327\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"CheXbert\" \\\n",
    "--device \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_194923_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_40_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9355.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=359,388447562913600091).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 72143.81it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:56<00:00, 153.57it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9149\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_194923_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+EC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=366,4246634492375333681).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 70188.93it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [01:00<00:00, 142.96it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.8074\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_013825_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_64_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9474.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=387,2492157991032224976).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 76619.62it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:56<00:00, 153.43it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9435\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_013825_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_155915_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_29_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9495.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=387,1308360022390228203).pkl\n",
      "len(self.cache[\"hashes\"]) = 42198\n",
      "self.cache[\"embeddings\"].shape = (42198, 128)\n",
      "100%|███████████████████████████████████| 8617/8617 [00:00<00:00, 189559.27it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:56<00:00, 151.31it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9435\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_155915_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+EC+NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_52_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9002.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=375,1493189425621868816).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 75187.79it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:56<00:00, 153.14it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.7581\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_110825_MIMIC-CXR(triplets+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_93_spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9415.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=368,1866338409828607810).pkl\n",
      "len(self.cache[\"hashes\"]) = 144285\n",
      "self.cache[\"embeddings\"].shape = (144285, 128)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 73764.05it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [01:00<00:00, 141.36it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9038\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_110825_MIMIC-CXR(triplets+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CXR Fact Encoder (T -> T+C+EC+NLI+R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_152825_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_26_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9386.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,141429923173682938).pkl\n",
      "len(self.cache[\"hashes\"]) = 111895\n",
      "self.cache[\"embeddings\"].shape = (111895, 128)\n",
      "100%|████████████████████████████████████| 8617/8617 [00:00<00:00, 93461.52it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:56<00:00, 151.38it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.8816\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_152825_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   evaluation_mode: iuxray_with_automatic_labelers\n",
      "   model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   device: GPU\n",
      "   batch_size: 100\n",
      "   num_workers: 4\n",
      "   model_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   distance_metric: cosine\n",
      "   average_token_embeddings: False\n",
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(thr1=0.4,thr2=0.2).pkl\n",
      "len(sentences): 8617\n",
      "len(relevant_sentences): 8617\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "  0%|                                                  | 0/8617 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,3677907674814022869).pkl\n",
      "len(self.cache[\"hashes\"]) = 42198\n",
      "self.cache[\"embeddings\"].shape = (42198, 128)\n",
      "100%|███████████████████████████████████| 8617/8617 [00:00<00:00, 197126.29it/s]\n",
      "Embeddings shape: (8617, 128)\n",
      "Normalizing embeddings (for cosine similarity)\n",
      "\u001b[93m\u001b[1mEvaluating embeddings on ranking task with each sentence as query\u001b[0m\n",
      "100%|██████████████████████████████████████| 8617/8617 [00:55<00:00, 154.70it/s]\n",
      "\u001b[1m\u001b[35mmean_AUC: 0.9005\u001b[0m\n",
      "mean_relevant: 63.7905\n",
      "count: 7742 / 8617 (89.85%)\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_fact_embedding_on_ranking.py \\\n",
    "--evaluation_mode \"iuxray_with_automatic_labelers\" \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--batch_size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading iuxray sentences and relevant sentences from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray_sentences_and_relevant(rt=0.3).pkl\n"
     ]
    }
   ],
   "source": [
    "from medvqa.eval_fact_embedding_on_ranking import SentenceRanker\n",
    "\n",
    "sr = SentenceRanker('iuxray_with_automatic_labelers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pleural effusion or pneumothorax is identified.\n",
      "num_relevant: 282\n",
      "\n",
      "Negative for pneumothorax, pleural effusion or focal airspace consolidation.\n",
      "No evidence of pulmonary consolidation, large pleural effusion/pneumothorax.\n",
      "No definitive pneumothorax or pleural effusion.\n",
      "No consolidation, pleural effusion or pneumothorax.\n",
      "No definite pleural effusion seen, no pneumothorax.\n",
      "No focal air space consolidation, pneumothorax, pleural effusion, or pulmonary edema.\n",
      "There are no XXXX of a pleural effusion.\n",
      "Previously seen left pleural effusion has resolved.\n",
      "There is no acute pulmonary consolidation, pleural effusion or pneumothorax.\n",
      "No definite pleural effusion.\n",
      "There is no focal airspace disease, pneumothorax, or large pleural effusion.\n",
      "There is no focal airspace opacity, large pleural effusion, or pneumothorax.\n",
      "There is no pneumothorax, pleural effusion, or focal airspace consolidation.\n",
      "Specifically, no evidence of focal consolidation, pneumothorax, or pleural effusion..\n",
      "The lungs are clear without interval consolidation, pleural effusion or pneumothorax.\n",
      "The lungs are otherwise clear of focal infiltrate, pneumothorax, or pleural effusion.\n",
      "The lung XXXX are free of infiltrate and there is no pleural effusion.\n",
      "No pleural effusion is noted.\n",
      "No pleural effusion, no pneumothorax.\n",
      "Negative for pneumothorax, pleural effusion, or pneumoperitoneum.\n",
      "No pneumothorax, no pleural effusion.\n",
      "No focal area of consolidation, pleural effusion or pneumothorax.\n",
      "No focal infiltrate, pneumothorax or pleural effusion identified.\n",
      "No pneumothorax, pleural effusion or suspicious airspace opacity.\n",
      "There is no focal airspace disease, pneumothorax, or pleural effusion.\n",
      "There is no focal airspace opacity, pleural effusion, or pneumothorax.\n",
      "There is no pleural effusion, pneumothorax, or focal airspace disease.\n",
      "Negative for focal pulmonary consolidation, pleural effusion or pneumothorax.\n",
      "Negative for pneumothorax, pleural effusion, or focal airspace consolidation.\n",
      "No focal consolidation, large pleural effusion or pneumothorax is identified.\n"
     ]
    }
   ],
   "source": [
    "query_idx = 4006\n",
    "print(sr.sentences[query_idx])\n",
    "print('num_relevant:', len(sr.relevant_sentences[query_idx]))\n",
    "print()\n",
    "for i, idx in enumerate(sr.relevant_sentences[query_idx]):\n",
    "    if i == 30: break\n",
    "    print(sr.sentences[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_names = ['checkpoint_29_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9495.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8617/8617 [00:00<00:00, 384255.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=387,1308360022390228203).pkl\n",
      "len(self.cache[\"hashes\"]) = 9782\n",
      "self.cache[\"embeddings\"].shape = (9782, 128)\n",
      "No pleural effusion or pneumothorax is identified.\n",
      "num_relevant: 282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "No pleural effusion or pneumothorax is identified.\n",
      "0\n",
      "\n",
      "No pleural effusion or pneumothorax is noted.\n",
      "1\n",
      "\n",
      "No pleural effusion or pneumothorax is seen.\n",
      "1\n",
      "\n",
      "No pleural effusion or pneumothorax are seen.\n",
      "1\n",
      "\n",
      "No pneumothorax or pleural effusion is seen.\n",
      "1\n",
      "\n",
      "No pneumothorax or pleural effusion is noted.\n",
      "1\n",
      "\n",
      "No definitive pneumothorax or pleural effusion.\n",
      "1\n",
      "\n",
      "No radiographic evidence of pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No pneumothorax or pleural effusion demonstrated.\n",
      "1\n",
      "\n",
      "No appreciable pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No large pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No definite pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No evidence of pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No visible pneumothorax or pleural effusion.\n",
      "1\n",
      "\n",
      "No significant pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "There is no pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No visible pneumothorax or pleural effusion is seen.\n",
      "1\n",
      "\n",
      "No evidence of associated pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No pneumothorax or large pleural effusion.\n",
      "1\n",
      "\n",
      "No pleural effusion or visible pneumothorax.\n",
      "1\n",
      "\n",
      "No evidence for pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No pneumothorax or pleural effusion present.\n",
      "1\n",
      "\n",
      "No visible pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No large pleural effusion or pneumothorax is identified.\n",
      "1\n",
      "\n",
      "There is no visible pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "There is no pneumothorax or pleural effusion.\n",
      "1\n",
      "\n",
      "No evidence of large pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "There is no evidence of pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "There is no evidence for pleural effusion or pneumothorax.\n",
      "1\n",
      "\n",
      "No layering pleural effusion or pneumothorax seen on decubitus exam.\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = sr.rank_sentences(\n",
    "    query_idx,\n",
    "    model_name=\"microsoft/BiomedVLP-CXR-BERT-specialized\",\n",
    "#     checkpoint_folder_path=\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\",\n",
    "#     checkpoint_folder_path=\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_013825_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\",\n",
    "#     checkpoint_folder_path=\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_152825_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\",\n",
    "    checkpoint_folder_path=\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_155915_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\",\n",
    "    top_k=30,\n",
    ")\n",
    "print(sr.sentences[query_idx])\n",
    "print('num_relevant:', len(sr.relevant_sentences[query_idx]))\n",
    "print('-'*100)\n",
    "print()\n",
    "for i in range(len(output[0])):\n",
    "    print(output[0][i])\n",
    "    print(output[1][i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
