{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 500\n",
      "   batches_per_epoch: 500\n",
      "   batch_size: 40\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231226_233457_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   task_name: multitask\n",
      "   experiment_name: s2f+f2m+f2c+s2co+s2cal+nli\n",
      "   multitask_name_list: ['nli', 'sentence2facts', 'fact2metadata', 'fact2comparison', 'sentence2chestimagenome_observations', 'sentence2chestimagenome_anatomical_locations']\n",
      "   task2weight: {'sentence2facts': 1.0, 'fact2metadata': 1.0, 'fact2comparison': 0.3, 'sentence2chestimagenome_observations': 1.0, 'sentence2chestimagenome_anatomical_locations': 1.0, 'nli': 3.0}\n",
      "   val_size: 200\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_phrases2labels_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl']\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl']\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl']\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   use_sentence2facts_for_nli: True\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 9891 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mB\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mbilateral\u001b[0m\n",
      "\u001b[1m\u001b[35mboth sides\u001b[0m\n",
      "\u001b[1m\u001b[35mboth\u001b[0m\n",
      "\u001b[1m\u001b[35mon both sides\u001b[0m\n",
      "\u001b[1m\u001b[35msymmetrical\u001b[0m\n",
      "\u001b[1m\u001b[35mequally on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mpresent on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mseen on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35moccurring on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mappearing on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mfound on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mseen bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mobserved bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mdetected bilaterally\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9890 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilus\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the hilum on the left side\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 8511 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary vasculature\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral lung vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary blood vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mlung periphery vasculature\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature of the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels in the outer regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular network in the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary circulation\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels supplying the outer areas of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature in the peripheral regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular system in the outer regions of the lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 1864 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35ma\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mone\u001b[0m\n",
      "\u001b[1m\u001b[35msingle\u001b[0m\n",
      "\u001b[1m\u001b[35msole\u001b[0m\n",
      "\u001b[1m\u001b[35monly\u001b[0m\n",
      "\u001b[1m\u001b[35mindividual\u001b[0m\n",
      "\u001b[1m\u001b[35mlone\u001b[0m\n",
      "\u001b[1m\u001b[35msolitary\u001b[0m\n",
      "\u001b[1m\u001b[35munique\u001b[0m\n",
      "\u001b[1m\u001b[35mdistinct\u001b[0m\n",
      "\u001b[1m\u001b[35msingular\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 10000 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9999 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9997 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9994 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9996 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome not detected\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome is not observed\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant constriction\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of notable narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of substantial narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of notable constriction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19943 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of wheezing\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing detected\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing observed\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing present\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing found\u001b[0m\n",
      "--------\n",
      "\u001b[1mNumber of unique inputs: 190085\u001b[0m\n",
      "\u001b[1mNumber of total paraphrases: 2093173\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2facts dataset\u001b[0m\n",
      "Loaded 9999 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19971 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\n",
      "Loaded 14990 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\n",
      "Loaded 14991 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2F: No change in the volume or distribution of the residual small to moderate left pleural effusion, and attendant left lower lobe atelectasis following removal of the left pigtail pleural drainage catheter.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"No change in the volume of the residual small to moderate left pleural effusion\", \"No change in the distribution of the residual small to moderate left pleural effusion\", \"attendant left lower lobe atelectasis\", \"removal of the left pigtail pleural drainage catheter\"]\u001b[0m\n",
      "Number of train examples: 84735\n",
      "Number of val examples: 200\n",
      "Number of total examples: 84935\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing nli dataset\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "Number of sources: 5\n",
      "----\n",
      "\u001b[1mLoading sentence2facts input/output pairs for NLI...\u001b[0m\n",
      "Number of entailment samples added: 368966\n",
      "----\n",
      "\u001b[1mLoading general domain datasets...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset...\u001b[0m\n",
      "\u001b[1mSource: 0 | Label: contradiction -> 181976 (5334.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The aortic arch is noted to be mildly calcified, and the mediastinal and hilar contours are otherwise stable. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe aortic arch is noted to be severely calcified\u001b[0m\n",
      "\u001b[1mSource: 1 | Label: contradiction -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: In ED, attempts at subclavian and IJ's failed and a femoral line was placed. #Hypothesis:  The patient does not have central intravenous access. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: 2 | Label: contradiction -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Intervention was deferred at the time and the patient was sent to CCU for TEE. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient has no cardiac abnormality of symptoms, no indication for echo. \u001b[0m\n",
      "\u001b[1mSource: 3 | Label: contradiction -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Per report ECG with initial qtc of 410 now 475., QRS 82 initially, now 86 rate= 95. #Hypothesis:  Patient has normal EKG\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: 4 | Label: contradiction -> 212 (461.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No gross right effusion, though minimal pleural fluid could be present on the right. #Hypothesis: Small bilateral pleural effusions are new in the interval.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: 6 | Label: contradiction -> 88178 (4433.68)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The Arkansas Mountain AVA is an American Viticultural Area located in the Ozark Mountains of northwestern Arkansas. It is part of the larger Ozark Mountain AVA, which also includes regions in Missouri and Oklahoma. The smaller Altus AVA is entirely contained within the Arkansas Mountain AVA. The Arkansas Mountain AVA includes 2880000 acre , making it the ninth largest AVA as of 2008. #Hypothesis: The Arkansas Mountain AVA was the ninth largest AVA as of the year after 2006.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: 7 | Label: contradiction -> 274712 (5897.92)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: According to the publication, just months after the skiing accident death of her husband, Sonny, she tossed piles of irreplaceable mementos of the late singer-congressman into the dumpster, where they were retrieved by one of his former restaurant employees. #Hypothesis: Sonny's wife kept all of his belongings safely locked away.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: 8 | Label: contradiction -> 379404 (6365.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Two men hanging out together. #Hypothesis: A few guys oblivious of each other.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: 0 | Label: entailment -> 43334 (3654.55)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Fracture, posterior portion of the left middle rib is new since but appears healed. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe patient had a healed fracture in the left middle rib.\u001b[0m\n",
      "\u001b[1mSource: 1 | Label: entailment -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: HISTORY OF PRESENT ILLNESS:  Baby [**Name (NI) **] [**Known patient lastname 18393**] is a newborn infant admitted to the newborn ICU for evaluation of hypoglycemia and respiratory distress. #Hypothesis:  The baby has pulmonary problems. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: 2 | Label: entailment -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: HISTORY OF PRESENT ILLNESS:  This is a 63 year old primarily Spanish speaking male with a history of end-stage renal disease secondary to diabetes mellitus on dialysis. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient has an elevated creatinine. \u001b[0m\n",
      "\u001b[1mSource: 3 | Label: entailment -> 946 (966.10)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The patient also reports continued fever and an unusual amount of fatigue. #Hypothesis:  The patient has been feeling ill\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: 4 | Label: entailment -> 186 (428.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The cardiomediastinal and hilar contours are normal. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCardiac, mediastinal and hilar contours are normal.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSource: 5 | Label: entailment -> 368966 (6324.57)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Hazy bibasilar opacities concerning for aspiration or pneumonia. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mopacities concerning for pneumonia\u001b[0m\n",
      "\u001b[1mSource: 6 | Label: entailment -> 108502 (4680.39)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: ROSEVILLE (CBS13) — Thieves targeted the Apple Store in the Roseville Galleria, making off with merchandise on Tuesday. Roseville Police say four men entered the Apple Store around 8 p.m. on Tuesday and began stealing products on display. They made off with 20 iPhones and at least two Apple computers. The suspects are described as black male adults with hoodies and gloves. No arrests have been made. Nobody was injured during the robbery. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mOver 21 pieces of technology were stolen.\u001b[0m\n",
      "\u001b[1mSource: 7 | Label: entailment -> 275682 (5902.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: need to break out the bicycle too get that down and start to ride around #Hypothesis: The bicycle is still new but I will start riding it.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: 8 | Label: entailment -> 380226 (6369.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Young students are playing musical instruments in an orchestra. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mStudents in an orchestra are playing instruments.\u001b[0m\n",
      "\u001b[1mSource: 0 | Label: neutral -> 69708 (4164.76)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: There are multiple rib deformities in the left upper hemithorax, as seen on prior chest radiograph. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is no pleural effusion, pneumothorax, or free subdiaphragmatic gas.\u001b[0m\n",
      "\u001b[1mSource: 1 | Label: neutral -> 7486 (2131.74)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Partial hysterectomy secondary to fibroids 12. #Hypothesis:  Patient has pelvic pain\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: 2 | Label: neutral -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Recurrent UTIs: ----> Klebsiella (amp resistant) ----> Enterococcus  (Levo resistant) 6. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m the patient is female\u001b[0m\n",
      "\u001b[1mSource: 3 | Label: neutral -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: She has been on hemodialysis since [**3048-1-29**], after failure of her second kidney transplant. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m She is diabetic\u001b[0m\n",
      "\u001b[1mSource: 4 | Label: neutral -> 562 (762.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Lung volumes are low. #Hypothesis: The lungs are clear.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: 6 | Label: neutral -> 141850 (5012.51)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: The president Cristiani spoke today at the El Salvador military airport before he left for Costa Rica to attend the inauguration ceremony of president-elect Rafael Calderon Fournier. #Hypothesis: President Cristiani supports Rafael Calderon Fournier politically.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: 7 | Label: neutral -> 274304 (5895.82)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: hot air no hot i go hot air ballooning #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mI like going hot-air ballooning during weekends.\u001b[0m\n",
      "\u001b[1mSource: 8 | Label: neutral -> 378436 (6362.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The firefighter has a hose ready to tackle a car fire. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mA male firefighter checks his hose before using it.\u001b[0m\n",
      "----\n",
      "Number of RadNLI samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Moderate cardiomegaly and mediastinal vascular engorgement is stable. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCardiomediastinal and hilar silhouettes are normal.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Small bilateral pleural effusions are noted. #Hypothesis: The right atelectasis and pleural effusion has increased.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: pulmonary edema has improved. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary edema has worsened.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The heart size is normal. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe lungs are clear.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Severe cardiomegaly and cardiomediastinal hilar silhouettes are unchanged. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCardiomediastinal silhouette is stable.\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset...\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2metadata dataset\u001b[0m\n",
      "Loaded 19989 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\n",
      "Loaded 19948 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2M: hyperalimentation catheter passes into the duodenum\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"anatomical location\": \"duodenum\", \"detailed observation\": \"hyperalimentation catheter passes into the duodenum\", \"short observation\": \"catheter in duodenum\", \"category\": \"tubes and lines\", \"health status\": \"normal\", \"prev_study_comparison?\": \"no\", \"comparison status\": \"\"}\u001b[0m\n",
      "Number of train examples: 59721\n",
      "Number of val examples: 200\n",
      "Number of total examples: 59921\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2comparison dataset\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Added 341589 paraphrased inputs (total 443344)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mpossible superimposed non-typical pneumonia\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2C: The fluid collection with compartmentalization in the right pleural space remains unchanged\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "Counter:\n",
      "Counter({'no comparison': 276665, 'stable/unchanged': 46121, 'worsened': 27557, 'resolved': 26871, 'improved': 18519, 'progressed': 10316, 'new finding': 8625, 'larger': 5687, 'increase': 5642, 'decrease': 5399, 'smaller': 4471, 'unclear comparison': 3230, 'position changed': 2691, 'reappeared': 1522, 'other': 28})\n",
      "Output: no comparison, train size: 276650, val size: 15, weight: 326.80332399852057\n",
      "Output: worsened, train size: 27542, val size: 15, weight: 217.54319875804714\n",
      "Output: resolved, train size: 26857, val size: 14, weight: 216.47267916050666\n",
      "Output: larger, train size: 5673, val size: 14, weight: 155.49830995863778\n",
      "Output: improved, train size: 18505, val size: 14, weight: 200.94841544934332\n",
      "Output: smaller, train size: 4457, val size: 14, weight: 146.939423083012\n",
      "Output: progressed, train size: 10302, val size: 14, weight: 177.70587815266865\n",
      "Output: increase, train size: 5628, val size: 14, weight: 155.2118953407358\n",
      "Output: decrease, train size: 5385, val size: 14, weight: 153.6293473761691\n",
      "Output: unclear comparison, train size: 3216, val size: 14, weight: 135.74700551052337\n",
      "Output: new finding, train size: 8611, val size: 14, weight: 170.87627086853004\n",
      "Output: stable/unchanged, train size: 46107, val size: 14, weight: 240.0236968314092\n",
      "Output: reappeared, train size: 1508, val size: 14, weight: 111.48024795838076\n",
      "Output: other, train size: 26, val size: 2, weight: 22.094133543878307\n",
      "Output: position changed, train size: 2677, val size: 14, weight: 129.6501373802887\n",
      "Number of train examples: 443144\n",
      "Number of val examples: 200\n",
      "Number of total examples: 443344\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_observations dataset\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 556111/556111 [00:19<00:00, 27873.92it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is a moderate right pleural effusion with adjacent atelectasis, not significantly changed since prior given differences in the technique.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"atelectasis\", \"lung opacity\", \"pleural effusion\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mPatchy bibasilar airspace opacities, as seen previously, findings which may reflect atelectasis though aspiration cannot be excluded.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"airspace opacity\", \"atelectasis\", \"lung opacity\", \"aspiration\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m2. Improving bibasilar opacities, most consistent with resolving edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\", \"pulmonary edema/hazy opacity\"]\u001b[0m\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\n",
      "Loaded 14859 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\n",
      "Loaded 19952 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\n",
      "Loaded 19959 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\n",
      "Loaded 9977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\n",
      "Loaded 9974 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\n",
      "Loaded 9987 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\n",
      "Added 103692 paraphrased inputs (total 749511)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mpossible fluid retention within the alveoli\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"pulmonary edema/hazy opacity\"]\u001b[0m\n",
      "100%|███████████████████████████████| 749511/749511 [00:03<00:00, 210574.95it/s]\n",
      "Number of total examples: 749511\n",
      "Number of examples without labels: 160864\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 4498\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  rales\u001b[0m\n",
      "\u001b[93m\u001b[1m  radiodensity\u001b[0m\n",
      "\u001b[93m\u001b[1m  surgical clips/fiducial markers\u001b[0m\n",
      "\u001b[93m\u001b[1m  vascular catheter misplacement\u001b[0m\n",
      "\u001b[93m\u001b[1m  CVC\u001b[0m\n",
      "\u001b[93m\u001b[1m  fixation device\u001b[0m\n",
      "\u001b[93m\u001b[1m  abnormality\u001b[0m\n",
      "\u001b[93m\u001b[1m  anesthesia machine\u001b[0m\n",
      "\u001b[93m\u001b[1m  empyema\u001b[0m\n",
      "\u001b[93m\u001b[1m  benign finding\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: lung opacity, train size: 306077, val size: 3, weight: 6051.9855683435435\n",
      "Label: None, train size: 160861, val size: 3, weight: 5173.637314058976\n",
      "Label: atelectasis, train size: 94016, val size: 3, weight: 4508.9863600443605\n",
      "Label: pleural effusion, train size: 93005, val size: 3, weight: 4496.226870241478\n",
      "Label: pneumonia, train size: 46684, val size: 2, weight: 3731.5494493019914\n",
      "Label: pulmonary edema/hazy opacity, train size: 43447, val size: 3, weight: 3657.224507266033\n",
      "Label: enlarged cardiac silhouette, train size: 35272, val size: 3, weight: 3447.218209738445\n",
      "Label: enteric tube, train size: 27592, val size: 2, weight: 3210.327888152985\n",
      "Label: vascular congestion, train size: 25945, val size: 3, weight: 3152.7062857605365\n",
      "Label: consolidation, train size: 25253, val size: 3, weight: 3127.6160152511925\n",
      "Label: lung lesion, train size: 23554, val size: 3, weight: 3063.5883893845053\n",
      "Label: endotracheal tube, train size: 21902, val size: 2, weight: 2997.6787843574043\n",
      "Label: pleural/parenchymal scarring, train size: 18674, val size: 3, weight: 2856.484034513788\n",
      "Label: low lung volumes, train size: 18582, val size: 2, weight: 2852.182850402761\n",
      "Label: pneumothorax, train size: 16788, val size: 3, weight: 2764.715849693273\n",
      "Label: mass/nodule (not otherwise specified), train size: 14461, val size: 3, weight: 2639.44603934545\n",
      "Label: picc, train size: 14281, val size: 2, weight: 2629.105873064592\n",
      "Label: cardiac pacer and wires, train size: 13386, val size: 3, weight: 2576.1067163563926\n",
      "Label: ij line, train size: 12403, val size: 2, weight: 2514.5691269420468\n",
      "Label: lobar/segmental collapse, train size: 12364, val size: 3, weight: 2512.0494320584767\n",
      "Label: aspiration, train size: 12232, val size: 3, weight: 2503.474512677858\n",
      "Label: linear/patchy atelectasis, train size: 12097, val size: 3, weight: 2494.629000078744\n",
      "Label: chest tube, train size: 11744, val size: 2, weight: 2471.1267449016627\n",
      "Label: airspace opacity, train size: 11712, val size: 3, weight: 2468.9688730909206\n",
      "Label: enlarged hilum, train size: 11702, val size: 3, weight: 2468.2935871944405\n",
      "Label: rib fracture, train size: 9953, val size: 3, weight: 2342.524074481851\n",
      "Label: hyperaeration, train size: 8174, val size: 3, weight: 2195.391442440566\n",
      "Label: mediastinal widening, train size: 7456, val size: 3, weight: 2128.8592409272346\n",
      "Label: chest port, train size: 7007, val size: 2, weight: 2084.682936477713\n",
      "Label: copd/emphysema, train size: 6978, val size: 3, weight: 2081.755066062073\n",
      "Label: costophrenic angle blunting, train size: 6948, val size: 3, weight: 2078.7163066961857\n",
      "Label: vascular calcification, train size: 6941, val size: 3, weight: 2078.0058021141735\n",
      "Label: tortuous aorta, train size: 6433, val size: 3, weight: 2024.897240123363\n",
      "Label: fluid overload/heart failure, train size: 6399, val size: 2, weight: 2021.2284948571937\n",
      "Label: elevated hemidiaphragm, train size: 6129, val size: 3, weight: 1991.5473533897875\n",
      "Label: infiltration, train size: 5732, val size: 3, weight: 1946.0193661096143\n",
      "Label: mediastinal displacement, train size: 5213, val size: 3, weight: 1882.6912979474973\n",
      "Label: subcutaneous air, train size: 5134, val size: 3, weight: 1872.6322270652297\n",
      "Label: increased reticular markings/ild pattern, train size: 5105, val size: 3, weight: 1868.9099009277677\n",
      "Label: multiple masses/nodules, train size: 4862, val size: 3, weight: 1837.0657664201624\n",
      "Label: spinal fracture, train size: 4600, val size: 3, weight: 1801.3383696149233\n",
      "Label: subclavian line, train size: 4438, val size: 2, weight: 1778.4631991838367\n",
      "Label: interstitial lung disease, train size: 4262, val size: 2, weight: 1752.878475308976\n",
      "Label: pigtail catheter, train size: 4255, val size: 2, weight: 1751.844397866601\n",
      "Label: spinal degenerative changes, train size: 4159, val size: 3, weight: 1737.5305267974525\n",
      "Label: superior mediastinal mass/enlargement, train size: 4151, val size: 3, weight: 1736.3264075484174\n",
      "Label: hernia, train size: 3758, val size: 3, weight: 1674.8775776563707\n",
      "Label: vascular redistribution, train size: 3619, val size: 3, weight: 1651.9771131903597\n",
      "Label: calcified nodule, train size: 3462, val size: 3, weight: 1625.2971499895343\n",
      "Label: sub-diaphragmatic air, train size: 3419, val size: 3, weight: 1617.8308750381188\n",
      "Label: tracheostomy tube, train size: 3389, val size: 2, weight: 1612.57979558949\n",
      "Label: bone lesion, train size: 3355, val size: 3, weight: 1606.5860342268288\n",
      "Label: scoliosis, train size: 3058, val size: 3, weight: 1552.1817553012304\n",
      "Label: granulomatous disease, train size: 2939, val size: 2, weight: 1529.2656343138838\n",
      "Label: swan-ganz catheter, train size: 2890, val size: 2, weight: 1519.627077653289\n",
      "Label: prosthetic valve, train size: 2569, val size: 3, weight: 1453.2612259803755\n",
      "Label: lung cancer, train size: 2552, val size: 2, weight: 1449.577533684502\n",
      "Label: alveolar hemorrhage, train size: 2527, val size: 3, weight: 1444.1269844732285\n",
      "Label: rotated, train size: 2256, val size: 2, weight: 1382.3025197632915\n",
      "Label: shoulder osteoarthritis, train size: 2193, val size: 3, weight: 1367.1468775689086\n",
      "Label: bronchiectasis, train size: 2120, val size: 3, weight: 1349.1771482882289\n",
      "Label: cabg grafts, train size: 1955, val size: 3, weight: 1306.8099438527813\n",
      "Label: pericardial effusion, train size: 1812, val size: 2, weight: 1267.9063687003427\n",
      "Label: hydropneumothorax, train size: 1626, val size: 3, weight: 1213.7814867690233\n",
      "Label: artifact, train size: 1620, val size: 2, weight: 1211.9617627361386\n",
      "Label: breast/nipple shadows, train size: 1555, val size: 2, weight: 1191.9259650670556\n",
      "Label: clavicle fracture, train size: 1553, val size: 3, weight: 1191.29988246945\n",
      "Label: cyst/bullae, train size: 1470, val size: 3, weight: 1164.7840527553165\n",
      "Label: pneumomediastinum, train size: 1353, val size: 3, weight: 1125.4955964821115\n",
      "Label: intra-aortic balloon pump, train size: 929, val size: 2, weight: 958.4495787563336\n",
      "Label: goiter, train size: 904, val size: 2, weight: 947.0179422691563\n",
      "Label: mediastinal drain, train size: 902, val size: 2, weight: 946.0938066688462\n",
      "Label: aortic graft/repair, train size: 772, val size: 3, weight: 882.6521598477713\n",
      "Label: diaphragmatic eventration (benign), train size: 403, val size: 3, weight: 648.2558205703779\n",
      "Label: skin fold, train size: 309, val size: 2, weight: 565.9095182711955\n",
      "--------\n",
      "Number of train examples: 1269632\n",
      "Number of val examples: 201\n",
      "Number of total examples: 1269833\n",
      "Number of train datasets: 75\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: FINDINGS: NG tube passes beyond the GE junction into the stomach with the tip not visualized.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"enteric tube\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: The lungs are otherwise free of focal consolidations, effusions or pneumothorax.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: The wall and mural nodule of a left apical mycetoma has become more radiopaque compared to prior exams in ___ and ___.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung lesion\", \"lung opacity\", \"mass/nodule (not otherwise specified)\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: As compared to the previous examination, the patient has received a dual chamber pacemaker.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac pacer and wires\"]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_anatomical_locations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:12<00:00, 45947.02it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is blunting of the bilateral costophrenic angles potentially due to effusions or scarring, unchanged.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left costophrenic angle\", \"left lung\", \"right costophrenic angle\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mSince the prior study, the patient has been intubated with the ET tube tip being approximately 5 cm above the carina.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"carina\", \"neck\", \"trachea\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mIt is very similar to radiographs from ___ and is interstitial in character, although the more diffuse interstitial abnormality seems decreased since that time on more recent radiographs from ___ when the vague interstitial process in the right lower lobe was absent.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lower lung zone\", \"left lung\", \"left mid lung zone\", \"right lower lung zone\", \"right lung\", \"right mid lung zone\"]\u001b[0m\n",
      "Loaded 24929 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\n",
      "Loaded 24951 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\n",
      "Loaded 24988 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\n",
      "Added 352702 paraphrased inputs (total 983681)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe lung is partially deflated\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "100%|███████████████████████████████| 983681/983681 [00:05<00:00, 191029.71it/s]\n",
      "Number of total examples: 983681\n",
      "Number of examples without labels: 96958\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 24482\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  lower chest\u001b[0m\n",
      "\u001b[93m\u001b[1m  lower thoracic vertebral body\u001b[0m\n",
      "\u001b[93m\u001b[1m  left hemithyroid\u001b[0m\n",
      "\u001b[93m\u001b[1m  appropriate contrast\u001b[0m\n",
      "\u001b[93m\u001b[1m  left neck\u001b[0m\n",
      "\u001b[93m\u001b[1m  left\u001b[0m\n",
      "\u001b[93m\u001b[1m  papillary muscles\u001b[0m\n",
      "\u001b[93m\u001b[1m  right brachiocephalic vein\u001b[0m\n",
      "\u001b[93m\u001b[1m  infracarinal area\u001b[0m\n",
      "\u001b[93m\u001b[1m  cervical-thoracic junction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: right lung, train size: 349084, val size: 5, weight: 6242.9355564343105\n",
      "Label: left lung, train size: 347225, val size: 5, weight: 6235.103393177052\n",
      "Label: mediastinum, train size: 143984, val size: 5, weight: 5031.460504014974\n",
      "Label: left lower lung zone, train size: 139376, val size: 5, weight: 4990.237069279952\n",
      "Label: right lower lung zone, train size: 133945, val size: 5, weight: 4940.17073747044\n",
      "Label: right hilar structures, train size: 104617, val size: 5, weight: 4636.375497405217\n",
      "Label: left hilar structures, train size: 99487, val size: 5, weight: 4576.131805864965\n",
      "Label: cardiac silhouette, train size: 97575, val size: 6, weight: 4553.0205146796025\n",
      "Label: None, train size: 96952, val size: 6, weight: 4545.4092000523615\n",
      "Label: left costophrenic angle, train size: 87765, val size: 5, weight: 4428.19987568603\n",
      "Label: right costophrenic angle, train size: 85851, val size: 5, weight: 4402.515314740781\n",
      "Label: abdomen, train size: 69673, val size: 6, weight: 4164.198707011264\n",
      "Label: neck, train size: 68256, val size: 5, weight: 4141.222632448245\n",
      "Label: right mid lung zone, train size: 42045, val size: 5, weight: 3623.628546632869\n",
      "Label: right chest wall, train size: 39925, val size: 5, weight: 3571.05662943441\n",
      "Label: left chest wall, train size: 39694, val size: 5, weight: 3565.192337979823\n",
      "Label: left mid lung zone, train size: 39058, val size: 5, weight: 3548.902250131568\n",
      "Label: trachea, train size: 37839, val size: 5, weight: 3517.0688698772865\n",
      "Label: spine, train size: 36956, val size: 5, weight: 3493.4868233148586\n",
      "Label: right upper lung zone, train size: 31919, val size: 5, weight: 3349.50077662811\n",
      "Label: upper mediastinum, train size: 31116, val size: 5, weight: 3324.8742940677407\n",
      "Label: left hemidiaphragm, train size: 26171, val size: 5, weight: 3160.7840671222925\n",
      "Label: left upper lung zone, train size: 24839, val size: 5, weight: 3112.3403006987855\n",
      "Label: right hemidiaphragm, train size: 22851, val size: 5, weight: 3036.0083211058554\n",
      "Label: svc, train size: 22432, val size: 5, weight: 3019.2453384259757\n",
      "Label: aortic arch, train size: 21161, val size: 6, weight: 2966.8154495839394\n",
      "Label: right apical zone, train size: 20025, val size: 5, weight: 2917.7792506791234\n",
      "Label: left apical zone, train size: 17357, val size: 5, weight: 2793.230755651543\n",
      "Label: carina, train size: 17240, val size: 6, weight: 2787.4287457636287\n",
      "Label: right atrium, train size: 15519, val size: 5, weight: 2698.244513187888\n",
      "Label: right shoulder, train size: 13642, val size: 5, weight: 2591.545258544152\n",
      "Label: right clavicle, train size: 12044, val size: 5, weight: 2491.135064610795\n",
      "Label: left clavicle, train size: 11920, val size: 5, weight: 2482.9130455893564\n",
      "Label: left shoulder, train size: 10446, val size: 5, weight: 2379.624870939936\n",
      "Label: cavoatrial junction, train size: 7289, val size: 6, weight: 2112.6755747892425\n",
      "Label: left arm, train size: 6537, val size: 5, weight: 2036.027136668286\n",
      "Label: right arm, train size: 6391, val size: 5, weight: 2020.3630749061633\n",
      "Label: left breast, train size: 2992, val size: 5, weight: 1539.5563487032764\n",
      "Label: right breast, train size: 2321, val size: 5, weight: 1397.6140821020338\n",
      "--------\n",
      "Number of train examples: 2383519\n",
      "Number of val examples: 201\n",
      "Number of total examples: 2383720\n",
      "Number of train datasets: 39\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Increasing bibasilar and perihilar opacifications since ___, concerning for aspiration or infection.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left hilar structures\", \"left lower lung zone\", \"left lung\", \"right hilar structures\", \"right lower lung zone\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: The gastric tube has been removed.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"abdomen\", \"mediastinum\", \"neck\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: issues in obtaining a clear picture of the side poor\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: IMPRESSION: Displaced left mid clavicular fracture.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left clavicle\"]\u001b[0m\n",
      "seq2seq_trainer.name =  multitask(s2f+f2m+f2c+s2co+s2cal+nli)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231227_013137_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231227_013137_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_59_s2s_loss=0.4444.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231226_233457_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)/checkpoint_59_s2s_loss=0.4444.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231227_013137_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.63996, s2s_loss 0.69219, 50.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.64987, 2.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.3987.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.59591, s2s_loss 0.67421, 52.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.50831, 2.02 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.4185.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.78796, s2s_loss 0.62159, 52.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.13830, 2.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.4826.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.48681, s2s_loss 0.55372, 52.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.08711, 2.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_s2s_loss=0.4956.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.63344, s2s_loss 0.54090, 52.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.07515, 2.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_s2s_loss=0.4986.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.50604, s2s_loss 0.53629, 52.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.06877, 2.02 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.5001.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.62624, s2s_loss 0.53369, 52.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.06392, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.5013.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.60783, s2s_loss 0.52487, 52.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.06160, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_s2s_loss=0.5021.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.51055, s2s_loss 0.52991, 43.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.06053, 2.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.5021.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.46407, s2s_loss 0.52493, 53.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.06011, 2.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_s2s_loss=0.5024.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.53794, s2s_loss 0.52191, 52.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.05968, 2.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_s2s_loss=0.5027.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.60083, s2s_loss 0.52474, 52.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.05949, 2.03 secs\n",
      "\u001b[1m---- Epoch 13/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.46839, s2s_loss 0.52174, 52.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.04189, 2.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_s2s_loss=0.5065.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.48690, s2s_loss 0.51139, 52.37 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.03261, 2.05 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_s2s_loss=0.5089.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.50324, s2s_loss 0.51100, 52.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.02994, 2.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_s2s_loss=0.5095.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48355, s2s_loss 0.50449, 52.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.02804, 2.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_s2s_loss=0.5102.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.50140, s2s_loss 0.49995, 53.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.02680, 2.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_s2s_loss=0.5107.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.44990, s2s_loss 0.50150, 52.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.02617, 2.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_s2s_loss=0.5108.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44345, s2s_loss 0.49934, 52.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.02579, 2.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_s2s_loss=0.5110.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40103, s2s_loss 0.50851, 52.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.02559, 2.06 secs\n",
      "\u001b[1m---- Epoch 21/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.46668, s2s_loss 0.49804, 52.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.01389, 2.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_s2s_loss=0.5137.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.59847, s2s_loss 0.48655, 52.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.00882, 2.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_s2s_loss=0.5153.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40427, s2s_loss 0.48588, 43.37 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.00529, 2.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_s2s_loss=0.5161.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.49340, s2s_loss 0.48976, 43.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.00180, 2.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_s2s_loss=0.5167.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.59929, s2s_loss 0.48469, 52.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 1.00023, 2.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_s2s_loss=0.5173.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.53530, s2s_loss 0.48051, 52.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.99973, 2.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_s2s_loss=0.5176.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44413, s2s_loss 0.49069, 52.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.99933, 2.06 secs\n",
      "\u001b[1m---- Epoch 28/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43846, s2s_loss 0.48586, 52.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.99921, 2.03 secs\n",
      "\u001b[1m---- Epoch 29/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.45139, s2s_loss 0.49076, 52.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.98731, 2.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_29_s2s_loss=0.5200.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 30/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.41500, s2s_loss 0.47620, 52.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.98163, 2.12 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_s2s_loss=0.5219.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.50884, s2s_loss 0.46554, 52.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.97897, 2.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_s2s_loss=0.5230.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41812, s2s_loss 0.47030, 52.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.97778, 2.05 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_s2s_loss=0.5231.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.43927, s2s_loss 0.47483, 52.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.97612, 2.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_s2s_loss=0.5232.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.65035, s2s_loss 0.47393, 48.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.97566, 2.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_34_s2s_loss=0.5234.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 35/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.36840, s2s_loss 0.46778, 53.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.97532, 2.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_35_s2s_loss=0.5238.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 36/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55597, s2s_loss 0.46730, 52.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.97520, 2.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_36_s2s_loss=0.5238.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 37/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.80258, s2s_loss 0.46376, 53.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.96926, 2.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_37_s2s_loss=0.5253.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 38/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.42791, s2s_loss 0.46503, 52.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.96272, 2.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_38_s2s_loss=0.5268.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 39/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.60806, s2s_loss 0.45293, 45.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.96195, 2.18 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_39_s2s_loss=0.5276.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 40/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.28430, s2s_loss 0.45449, 53.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.96005, 2.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_s2s_loss=0.5279.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.43124, s2s_loss 0.45910, 53.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.95892, 2.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_41_s2s_loss=0.5280.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 42/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.54485, s2s_loss 0.45720, 52.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.95836, 2.53 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_42_s2s_loss=0.5282.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 43/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.48772, s2s_loss 0.45932, 52.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.95815, 2.18 secs\n",
      "\u001b[1m---- Epoch 44/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.49437, s2s_loss 0.45324, 53.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.95804, 2.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_44_s2s_loss=0.5285.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 45/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42517, s2s_loss 0.45773, 52.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94966, 2.14 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_45_s2s_loss=0.5302.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 46/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.35283, s2s_loss 0.44868, 53.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94678, 2.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_46_s2s_loss=0.5313.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 47/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.39276, s2s_loss 0.45156, 53.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94340, 2.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_s2s_loss=0.5320.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.42057, s2s_loss 0.44879, 51.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94220, 2.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_s2s_loss=0.5324.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.46173, s2s_loss 0.44529, 53.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94162, 2.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_s2s_loss=0.5327.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.56844, s2s_loss 0.44314, 53.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94116, 2.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_50_s2s_loss=0.5329.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 51/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.49175, s2s_loss 0.44445, 53.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94108, 2.74 secs\n",
      "\u001b[1m---- Epoch 52/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46348, s2s_loss 0.44655, 54.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.94106, 2.63 secs\n",
      "\u001b[1m---- Epoch 53/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.51533, s2s_loss 0.44471, 53.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93852, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_53_s2s_loss=0.5335.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 54/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.50378, s2s_loss 0.44063, 53.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93585, 2.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_54_s2s_loss=0.5343.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 55/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.43415, s2s_loss 0.44220, 53.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93244, 2.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_55_s2s_loss=0.5351.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 56/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.39516, s2s_loss 0.43561, 54.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93138, 2.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_56_s2s_loss=0.5356.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 57/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.36563, s2s_loss 0.43578, 53.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93132, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_57_s2s_loss=0.5357.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 58/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42618, s2s_loss 0.43969, 53.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93093, 2.61 secs\n",
      "\u001b[1m---- Epoch 59/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.42191, s2s_loss 0.44236, 53.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93074, 2.46 secs\n",
      "\u001b[1m---- Epoch 60/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43328, s2s_loss 0.43194, 53.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.93064, 2.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_60_s2s_loss=0.5360.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 61/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42993, s2s_loss 0.43608, 53.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.92482, 2.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_61_s2s_loss=0.5372.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 62/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33183, s2s_loss 0.43338, 54.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91945, 2.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_62_s2s_loss=0.5386.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 63/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.56672, s2s_loss 0.43246, 52.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91595, 2.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_63_s2s_loss=0.5396.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 64/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41159, s2s_loss 0.42917, 53.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91532, 2.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_s2s_loss=0.5399.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47924, s2s_loss 0.42209, 52.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91485, 2.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_65_s2s_loss=0.5403.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 66/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.56793, s2s_loss 0.41849, 52.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91461, 2.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_66_s2s_loss=0.5406.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 67/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44919, s2s_loss 0.42995, 53.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91417, 2.18 secs\n",
      "\u001b[1m---- Epoch 68/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31808, s2s_loss 0.42947, 52.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91410, 2.22 secs\n",
      "\u001b[1m---- Epoch 69/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38504, s2s_loss 0.42651, 52.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.91006, 2.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_69_s2s_loss=0.5413.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 70/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.35755, s2s_loss 0.42174, 52.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.90785, 2.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_70_s2s_loss=0.5421.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 71/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36476, s2s_loss 0.42091, 53.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.90578, 2.14 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_71_s2s_loss=0.5426.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 72/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.71957, s2s_loss 0.42023, 53.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.90581, 2.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_72_s2s_loss=0.5427.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 73/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47072, s2s_loss 0.42366, 53.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.90479, 2.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_73_s2s_loss=0.5427.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 74/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.37329, s2s_loss 0.41473, 53.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.90444, 2.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_74_s2s_loss=0.5433.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 75/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31249, s2s_loss 0.41510, 53.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.90439, 2.23 secs\n",
      "\u001b[1m---- Epoch 76/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42043, s2s_loss 0.41931, 53.13 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.90433, 2.45 secs\n",
      "\u001b[1m---- Epoch 77/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41743, s2s_loss 0.42242, 53.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89911, 2.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_77_s2s_loss=0.5442.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 78/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.39000, s2s_loss 0.41266, 52.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89707, 2.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_78_s2s_loss=0.5452.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 79/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.39607, s2s_loss 0.41473, 53.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89541, 2.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_79_s2s_loss=0.5455.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 80/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.27959, s2s_loss 0.42289, 52.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89481, 2.20 secs\n",
      "\u001b[1m---- Epoch 81/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.49510, s2s_loss 0.41269, 52.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89398, 2.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_81_s2s_loss=0.5460.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 82/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.36996, s2s_loss 0.41475, 52.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89373, 2.35 secs\n",
      "\u001b[1m---- Epoch 83/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63759, s2s_loss 0.41177, 52.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89360, 2.18 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_83_s2s_loss=0.5461.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 84/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40368, s2s_loss 0.41191, 52.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.89354, 2.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_84_s2s_loss=0.5461.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 85/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.46434, s2s_loss 0.41045, 53.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88953, 2.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_85_s2s_loss=0.5472.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 86/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.41569, s2s_loss 0.40884, 53.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88515, 2.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_86_s2s_loss=0.5484.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 87/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.38299, s2s_loss 0.40451, 52.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88268, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_87_s2s_loss=0.5492.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 88/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.59774, s2s_loss 0.40448, 52.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88207, 2.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_88_s2s_loss=0.5494.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 89/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41081, s2s_loss 0.41079, 53.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88153, 2.20 secs\n",
      "\u001b[1m---- Epoch 90/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.43094, s2s_loss 0.41555, 49.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88153, 2.13 secs\n",
      "\u001b[1m---- Epoch 91/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40273, s2s_loss 0.40443, 52.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88140, 2.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_91_s2s_loss=0.5496.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 92/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38923, s2s_loss 0.40937, 52.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.88129, 2.10 secs\n",
      "\u001b[1m---- Epoch 93/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37999, s2s_loss 0.40492, 52.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87351, 2.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_93_s2s_loss=0.5516.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 94/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38210, s2s_loss 0.40617, 53.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87431, 2.45 secs\n",
      "\u001b[1m---- Epoch 95/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.47483, s2s_loss 0.40481, 53.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87174, 2.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_95_s2s_loss=0.5520.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 96/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35311, s2s_loss 0.40499, 52.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87155, 2.05 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_96_s2s_loss=0.5521.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 97/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.34913, s2s_loss 0.40112, 53.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87100, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_97_s2s_loss=0.5524.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 98/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.36646, s2s_loss 0.40627, 53.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87072, 2.10 secs\n",
      "\u001b[1m---- Epoch 99/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.60139, s2s_loss 0.40224, 48.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87059, 2.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_99_s2s_loss=0.5524.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 100/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.51528, s2s_loss 0.39865, 53.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.87064, 2.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_100_s2s_loss=0.5526.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 101/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.49669, s2s_loss 0.40056, 53.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86637, 2.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_101_s2s_loss=0.5536.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 102/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33299, s2s_loss 0.39992, 51.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86580, 2.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_102_s2s_loss=0.5538.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 103/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.28473, s2s_loss 0.40270, 53.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86421, 2.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_103_s2s_loss=0.5541.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 104/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.37110, s2s_loss 0.39732, 52.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86404, 2.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_104_s2s_loss=0.5544.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 105/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.35315, s2s_loss 0.39689, 53.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86429, 2.56 secs\n",
      "\u001b[1m---- Epoch 106/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35025, s2s_loss 0.39641, 53.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86434, 2.45 secs\n",
      "\u001b[1m---- Epoch 107/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.37331, s2s_loss 0.39477, 53.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86415, 2.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_107_s2s_loss=0.5545.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 108/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.33016, s2s_loss 0.39338, 52.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.86403, 2.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_108_s2s_loss=0.5546.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 109/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41129, s2s_loss 0.39798, 53.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85931, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_109_s2s_loss=0.5556.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 110/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.54790, s2s_loss 0.39912, 53.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85606, 2.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_110_s2s_loss=0.5564.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 111/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41474, s2s_loss 0.39598, 53.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85459, 2.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_111_s2s_loss=0.5569.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 112/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.40007, s2s_loss 0.40003, 52.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85397, 2.55 secs\n",
      "\u001b[1m---- Epoch 113/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39929, s2s_loss 0.39095, 52.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85327, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_113_s2s_loss=0.5575.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 114/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.46497, s2s_loss 0.39145, 53.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85298, 2.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_114_s2s_loss=0.5576.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 115/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44109, s2s_loss 0.39553, 52.87 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.85284, 2.13 secs\n",
      "\u001b[1m---- Epoch 116/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.27115, s2s_loss 0.39418, 53.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85278, 2.32 secs\n",
      "\u001b[1m---- Epoch 117/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35950, s2s_loss 0.39627, 53.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.85020, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_117_s2s_loss=0.5581.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 118/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.55754, s2s_loss 0.38847, 53.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84696, 2.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_118_s2s_loss=0.5593.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 119/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.30997, s2s_loss 0.39177, 53.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84526, 2.57 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_119_s2s_loss=0.5596.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 120/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.36679, s2s_loss 0.38815, 53.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84489, 2.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_120_s2s_loss=0.5599.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 121/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54376, s2s_loss 0.39032, 53.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84503, 2.54 secs\n",
      "\u001b[1m---- Epoch 122/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.36251, s2s_loss 0.38737, 53.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84469, 2.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_122_s2s_loss=0.5600.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 123/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.39531, s2s_loss 0.39004, 52.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84467, 2.40 secs\n",
      "\u001b[1m---- Epoch 124/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.33960, s2s_loss 0.38341, 53.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84460, 2.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_124_s2s_loss=0.5602.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 125/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.30714, s2s_loss 0.38965, 53.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.84033, 2.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_125_s2s_loss=0.5610.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 126/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.39690, s2s_loss 0.39127, 52.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83728, 2.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_126_s2s_loss=0.5617.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 127/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.31129, s2s_loss 0.38623, 53.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83712, 2.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_127_s2s_loss=0.5620.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 128/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29372, s2s_loss 0.38320, 53.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83612, 2.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_128_s2s_loss=0.5625.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 129/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41388, s2s_loss 0.38249, 53.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83637, 2.54 secs\n",
      "\u001b[1m---- Epoch 130/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.30423, s2s_loss 0.38653, 53.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83624, 2.24 secs\n",
      "\u001b[1m---- Epoch 131/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46415, s2s_loss 0.38570, 53.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83616, 2.28 secs\n",
      "\u001b[1m---- Epoch 132/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37232, s2s_loss 0.38774, 53.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83610, 2.51 secs\n",
      "\u001b[1m---- Epoch 133/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37956, s2s_loss 0.38273, 53.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.83140, 2.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_133_s2s_loss=0.5637.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 134/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.35747, s2s_loss 0.38826, 53.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82796, 2.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_134_s2s_loss=0.5644.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 135/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.39815, s2s_loss 0.37798, 53.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82818, 2.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_135_s2s_loss=0.5649.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 136/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.38138, s2s_loss 0.37994, 53.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82718, 2.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_136_s2s_loss=0.5650.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 137/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.42309, s2s_loss 0.38223, 53.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82711, 2.48 secs\n",
      "\u001b[1m---- Epoch 138/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29440, s2s_loss 0.38439, 53.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82708, 2.43 secs\n",
      "\u001b[1m---- Epoch 139/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.35973, s2s_loss 0.37859, 53.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82698, 2.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_139_s2s_loss=0.5652.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 140/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.34369, s2s_loss 0.37624, 53.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82692, 2.73 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_140_s2s_loss=0.5653.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 141/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41579, s2s_loss 0.37860, 53.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82206, 2.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_141_s2s_loss=0.5665.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 142/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.33129, s2s_loss 0.37537, 53.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.82113, 2.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_142_s2s_loss=0.5669.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 143/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.37684, s2s_loss 0.38467, 53.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81937, 2.34 secs\n",
      "\u001b[1m---- Epoch 144/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.30810, s2s_loss 0.37848, 53.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81886, 2.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_144_s2s_loss=0.5674.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 145/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41753, s2s_loss 0.37358, 53.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81899, 2.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_145_s2s_loss=0.5676.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 146/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.43035, s2s_loss 0.37499, 50.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81879, 2.54 secs\n",
      "\u001b[1m---- Epoch 147/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.42998, s2s_loss 0.38058, 53.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81858, 2.25 secs\n",
      "\u001b[1m---- Epoch 148/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32821, s2s_loss 0.37542, 53.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81856, 2.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_148_s2s_loss=0.5676.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 149/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40407, s2s_loss 0.37240, 52.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81770, 2.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_149_s2s_loss=0.5680.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 150/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29522, s2s_loss 0.37155, 53.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81273, 2.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_150_s2s_loss=0.5694.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 151/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.33905, s2s_loss 0.37669, 53.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81255, 2.15 secs\n",
      "\u001b[1m---- Epoch 152/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.30524, s2s_loss 0.37466, 53.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81265, 2.08 secs\n",
      "\u001b[1m---- Epoch 153/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38660, s2s_loss 0.37175, 52.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81230, 2.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_153_s2s_loss=0.5695.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 154/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.48105, s2s_loss 0.37366, 50.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81221, 2.62 secs\n",
      "\u001b[1m---- Epoch 155/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.36736, s2s_loss 0.37403, 54.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81213, 2.22 secs\n",
      "\u001b[1m---- Epoch 156/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.32469, s2s_loss 0.37297, 52.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81213, 2.25 secs\n",
      "\u001b[1m---- Epoch 157/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.46543, s2s_loss 0.37450, 53.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81110, 2.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_157_s2s_loss=0.5697.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 158/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.32162, s2s_loss 0.37319, 53.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.81018, 2.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_158_s2s_loss=0.5700.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 159/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.43104, s2s_loss 0.37460, 49.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80756, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_159_s2s_loss=0.5707.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 160/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.50448, s2s_loss 0.37058, 53.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80694, 2.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_160_s2s_loss=0.5710.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 161/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41657, s2s_loss 0.37398, 53.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80676, 2.31 secs\n",
      "\u001b[1m---- Epoch 162/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.36830, s2s_loss 0.36942, 52.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80662, 2.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_162_s2s_loss=0.5712.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 163/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41902, s2s_loss 0.37115, 53.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80656, 2.20 secs\n",
      "\u001b[1m---- Epoch 164/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42944, s2s_loss 0.36933, 53.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80648, 2.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_164_s2s_loss=0.5712.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 165/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36562, s2s_loss 0.37232, 52.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80203, 2.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_165_s2s_loss=0.5723.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 166/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38414, s2s_loss 0.37321, 53.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80336, 2.39 secs\n",
      "\u001b[1m---- Epoch 167/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.32487, s2s_loss 0.36953, 53.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80211, 2.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_167_s2s_loss=0.5724.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 168/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.32887, s2s_loss 0.36978, 52.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80038, 2.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_168_s2s_loss=0.5729.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 169/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.24891, s2s_loss 0.36503, 52.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.80017, 2.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_169_s2s_loss=0.5732.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 170/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32390, s2s_loss 0.37496, 53.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79990, 2.36 secs\n",
      "\u001b[1m---- Epoch 171/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40916, s2s_loss 0.36819, 53.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79974, 2.45 secs\n",
      "\u001b[1m---- Epoch 172/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.24843, s2s_loss 0.36483, 52.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79978, 2.57 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_172_s2s_loss=0.5733.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 173/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44106, s2s_loss 0.37080, 52.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79882, 2.51 secs\n",
      "\u001b[1m---- Epoch 174/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31016, s2s_loss 0.37064, 52.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79419, 2.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_174_s2s_loss=0.5746.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 175/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.38423, s2s_loss 0.36448, 53.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79448, 2.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_175_s2s_loss=0.5748.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 176/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.37906, s2s_loss 0.36457, 51.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79394, 2.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_176_s2s_loss=0.5750.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 177/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.43161, s2s_loss 0.36542, 52.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79356, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_177_s2s_loss=0.5750.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 178/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.37027, s2s_loss 0.36516, 49.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79325, 2.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_178_s2s_loss=0.5751.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 179/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28259, s2s_loss 0.36631, 53.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79316, 2.09 secs\n",
      "\u001b[1m---- Epoch 180/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41350, s2s_loss 0.35755, 53.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79315, 2.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_180_s2s_loss=0.5756.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 181/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.29722, s2s_loss 0.36801, 53.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.79087, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_181_s2s_loss=0.5756.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 182/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31915, s2s_loss 0.36436, 52.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78955, 2.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_182_s2s_loss=0.5762.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 183/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.27792, s2s_loss 0.36788, 53.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78799, 2.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_183_s2s_loss=0.5765.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 184/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.38019, s2s_loss 0.36289, 53.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78718, 2.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_184_s2s_loss=0.5770.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 185/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.30451, s2s_loss 0.36001, 53.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78764, 2.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_185_s2s_loss=0.5770.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 186/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35715, s2s_loss 0.36163, 53.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78744, 2.31 secs\n",
      "\u001b[1m---- Epoch 187/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44968, s2s_loss 0.36331, 52.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78739, 2.35 secs\n",
      "\u001b[1m---- Epoch 188/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31263, s2s_loss 0.36105, 53.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78737, 2.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_188_s2s_loss=0.5770.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 189/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40408, s2s_loss 0.36265, 53.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78637, 2.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_189_s2s_loss=0.5772.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 190/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.48427, s2s_loss 0.36260, 53.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78255, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_190_s2s_loss=0.5783.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 191/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.35637, s2s_loss 0.36157, 53.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78118, 2.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_191_s2s_loss=0.5787.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 192/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.30262, s2s_loss 0.36333, 53.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78163, 2.20 secs\n",
      "\u001b[1m---- Epoch 193/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.37379, s2s_loss 0.35239, 48.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78124, 2.79 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_193_s2s_loss=0.5792.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 194/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.30603, s2s_loss 0.36015, 53.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78110, 2.10 secs\n",
      "\u001b[1m---- Epoch 195/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41464, s2s_loss 0.35448, 53.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78095, 2.18 secs\n",
      "\u001b[1m---- Epoch 196/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.39410, s2s_loss 0.36256, 53.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78092, 2.37 secs\n",
      "\u001b[1m---- Epoch 197/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35708, s2s_loss 0.35671, 53.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.78055, 2.36 secs\n",
      "\u001b[1m---- Epoch 198/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.45159, s2s_loss 0.36235, 53.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77799, 2.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_198_s2s_loss=0.5796.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 199/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.35282, s2s_loss 0.35649, 53.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77832, 2.57 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_199_s2s_loss=0.5798.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 200/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.34914, s2s_loss 0.35363, 49.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77717, 2.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_200_s2s_loss=0.5803.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 201/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39603, s2s_loss 0.35776, 53.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77682, 2.61 secs\n",
      "\u001b[1m---- Epoch 202/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.21849, s2s_loss 0.35564, 53.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77653, 2.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_202_s2s_loss=0.5804.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 203/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43214, s2s_loss 0.36035, 53.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77642, 2.66 secs\n",
      "\u001b[1m---- Epoch 204/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38899, s2s_loss 0.35841, 52.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77647, 2.35 secs\n",
      "\u001b[1m---- Epoch 205/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33333, s2s_loss 0.35521, 53.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77483, 2.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_205_s2s_loss=0.5809.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 206/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29973, s2s_loss 0.35305, 52.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77344, 2.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_206_s2s_loss=0.5814.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 207/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.29864, s2s_loss 0.35750, 53.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77124, 2.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_207_s2s_loss=0.5818.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 208/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35442, s2s_loss 0.35861, 53.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77062, 2.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_208_s2s_loss=0.5819.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 209/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.33728, s2s_loss 0.35139, 53.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77041, 2.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_209_s2s_loss=0.5824.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 210/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.33809, s2s_loss 0.35604, 53.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77029, 2.31 secs\n",
      "\u001b[1m---- Epoch 211/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.37032, s2s_loss 0.35643, 52.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77014, 2.49 secs\n",
      "\u001b[1m---- Epoch 212/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32689, s2s_loss 0.36105, 53.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.77004, 2.14 secs\n",
      "\u001b[1m---- Epoch 213/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.39736, s2s_loss 0.35985, 52.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76947, 2.11 secs\n",
      "\u001b[1m---- Epoch 214/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.39964, s2s_loss 0.35449, 52.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76554, 2.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_214_s2s_loss=0.5836.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 215/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41890, s2s_loss 0.35594, 52.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76519, 2.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_215_s2s_loss=0.5836.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 216/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.26041, s2s_loss 0.35508, 53.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76533, 2.12 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_216_s2s_loss=0.5836.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 217/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.29823, s2s_loss 0.34947, 52.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76491, 2.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_217_s2s_loss=0.5840.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 218/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.30474, s2s_loss 0.35495, 52.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76486, 2.18 secs\n",
      "\u001b[1m---- Epoch 219/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46709, s2s_loss 0.35073, 52.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76492, 2.16 secs\n",
      "\u001b[1m---- Epoch 220/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.29553, s2s_loss 0.35448, 52.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76485, 2.06 secs\n",
      "\u001b[1m---- Epoch 221/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.39426, s2s_loss 0.34897, 52.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76729, 2.09 secs\n",
      "\u001b[1m---- Epoch 222/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30122, s2s_loss 0.35122, 52.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76387, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_222_s2s_loss=0.5842.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 223/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40207, s2s_loss 0.35219, 47.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76265, 2.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_223_s2s_loss=0.5845.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 224/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.39093, s2s_loss 0.35305, 52.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76146, 2.05 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_224_s2s_loss=0.5848.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 225/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.30744, s2s_loss 0.35140, 52.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76090, 2.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_225_s2s_loss=0.5851.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 226/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.39435, s2s_loss 0.34906, 52.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76075, 2.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_226_s2s_loss=0.5853.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 227/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28680, s2s_loss 0.34697, 52.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.76070, 2.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_227_s2s_loss=0.5854.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 228/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "   iteration 113625\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.30575, s2s_loss 0.32670, 53.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.71384, 2.42 secs\n",
      "\u001b[1m---- Epoch 329/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.42343, s2s_loss 0.32901, 53.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.71367, 2.57 secs\n",
      "\u001b[1m---- Epoch 330/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.26228, s2s_loss 0.32245, 53.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.71354, 2.57 secs\n",
      "\u001b[1m---- Epoch 331/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40125, s2s_loss 0.32989, 51.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.71362, 2.22 secs\n",
      "\u001b[1m---- Epoch 332/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.33300, s2s_loss 0.32571, 53.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.71352, 2.52 secs\n",
      "\u001b[1m---- Epoch 333/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.34543, s2s_loss 0.32679, 53.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.71270, 2.16 secs\n",
      "\u001b[1m---- Epoch 334/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.25232, s2s_loss 0.33030, 53.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70964, 2.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_334_s2s_loss=0.6016.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 335/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.29813, s2s_loss 0.32580, 54.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.71057, 2.68 secs\n",
      "\u001b[1m---- Epoch 336/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.43011, s2s_loss 0.32483, 54.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70949, 2.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_336_s2s_loss=0.6020.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 337/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.28387, s2s_loss 0.32919, 53.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70907, 2.31 secs\n",
      "\u001b[1m---- Epoch 338/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.38698, s2s_loss 0.32148, 53.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70912, 2.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_338_s2s_loss=0.6023.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 339/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.25680, s2s_loss 0.32697, 53.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70924, 2.22 secs\n",
      "\u001b[1m---- Epoch 340/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.50890, s2s_loss 0.32429, 53.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70918, 2.44 secs\n",
      "\u001b[1m---- Epoch 341/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41497, s2s_loss 0.33079, 53.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70812, 2.48 secs\n",
      "\u001b[1m---- Epoch 342/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.27124, s2s_loss 0.33111, 53.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70774, 2.35 secs\n",
      "\u001b[1m---- Epoch 343/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.31275, s2s_loss 0.32746, 53.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70789, 2.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_343_s2s_loss=0.6023.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 344/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.25512, s2s_loss 0.33289, 53.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70750, 2.65 secs\n",
      "\u001b[1m---- Epoch 345/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.30925, s2s_loss 0.32644, 53.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70742, 2.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_345_s2s_loss=0.6025.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 346/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31791, s2s_loss 0.32304, 53.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70745, 2.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_346_s2s_loss=0.6027.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 347/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.18677, s2s_loss 0.32154, 52.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70736, 2.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_347_s2s_loss=0.6028.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 348/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.25621, s2s_loss 0.32505, 52.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70735, 2.37 secs\n",
      "\u001b[1m---- Epoch 349/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.28747, s2s_loss 0.32454, 53.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70732, 2.37 secs\n",
      "\u001b[1m---- Epoch 350/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.23009, s2s_loss 0.33051, 53.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70601, 2.24 secs\n",
      "\u001b[1m---- Epoch 351/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.31710, s2s_loss 0.32200, 53.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70578, 2.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_351_s2s_loss=0.6033.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 352/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.28887, s2s_loss 0.32672, 53.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70554, 2.29 secs\n",
      "\u001b[1m---- Epoch 353/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38226, s2s_loss 0.32181, 53.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70538, 2.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_353_s2s_loss=0.6034.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 354/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.37059, s2s_loss 0.32453, 53.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70519, 2.30 secs\n",
      "\u001b[1m---- Epoch 355/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28867, s2s_loss 0.32392, 53.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70500, 2.75 secs\n",
      "\u001b[1m---- Epoch 356/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.24581, s2s_loss 0.32477, 53.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70496, 2.35 secs\n",
      "\u001b[1m---- Epoch 357/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.25338, s2s_loss 0.32909, 53.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70699, 2.16 secs\n",
      "\u001b[1m---- Epoch 358/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.46849, s2s_loss 0.32342, 53.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70540, 2.40 secs\n",
      "\u001b[1m---- Epoch 359/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40081, s2s_loss 0.32118, 53.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70679, 2.77 secs\n",
      "\u001b[1m---- Epoch 360/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.33709, s2s_loss 0.32558, 52.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70523, 2.73 secs\n",
      "\u001b[1m---- Epoch 361/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.31551, s2s_loss 0.32363, 54.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70518, 2.37 secs\n",
      "\u001b[1m---- Epoch 362/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29961, s2s_loss 0.32298, 53.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70500, 2.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_362_s2s_loss=0.6034.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 363/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.27090, s2s_loss 0.32635, 54.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70488, 2.46 secs\n",
      "\u001b[1m---- Epoch 364/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.26366, s2s_loss 0.32597, 53.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70485, 2.41 secs\n",
      "\u001b[1m---- Epoch 365/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.27496, s2s_loss 0.32751, 53.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70351, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_365_s2s_loss=0.6037.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 366/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30811, s2s_loss 0.32113, 54.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70147, 2.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_366_s2s_loss=0.6046.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 367/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.34939, s2s_loss 0.32472, 53.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70112, 2.21 secs\n",
      "\u001b[1m---- Epoch 368/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.27408, s2s_loss 0.32664, 53.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70040, 2.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_368_s2s_loss=0.6047.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 369/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.27502, s2s_loss 0.32123, 53.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70081, 2.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_369_s2s_loss=0.6048.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 370/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.34825, s2s_loss 0.32973, 54.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70045, 2.64 secs\n",
      "\u001b[1m---- Epoch 371/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.23775, s2s_loss 0.32234, 54.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.70041, 2.82 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_371_s2s_loss=0.6049.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 372/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.34464, s2s_loss 0.31903, 54.37 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.70044, 2.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_372_s2s_loss=0.6051.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 373/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.30237, s2s_loss 0.32015, 55.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69524, 2.97 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_373_s2s_loss=0.6066.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 374/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31643, s2s_loss 0.32037, 56.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69558, 2.86 secs\n",
      "\u001b[1m---- Epoch 375/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.22989, s2s_loss 0.32193, 55.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69501, 2.86 secs\n",
      "\u001b[1m---- Epoch 376/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.42547, s2s_loss 0.32155, 55.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69542, 2.76 secs\n",
      "\u001b[1m---- Epoch 377/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39586, s2s_loss 0.32355, 54.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69531, 2.67 secs\n",
      "\u001b[1m---- Epoch 378/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45222, s2s_loss 0.31802, 54.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69501, 2.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_378_s2s_loss=0.6068.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 379/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.18187, s2s_loss 0.32230, 53.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69513, 2.38 secs\n",
      "\u001b[1m---- Epoch 380/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.45294, s2s_loss 0.32592, 53.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69509, 2.68 secs\n",
      "\u001b[1m---- Epoch 381/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.24208, s2s_loss 0.32260, 53.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69645, 2.55 secs\n",
      "\u001b[1m---- Epoch 382/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40207, s2s_loss 0.32015, 54.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69516, 2.41 secs\n",
      "\u001b[1m---- Epoch 383/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.42397, s2s_loss 0.32032, 54.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69342, 2.87 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_383_s2s_loss=0.6072.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 384/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.20259, s2s_loss 0.31708, 54.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69437, 2.58 secs\n",
      "\u001b[1m---- Epoch 385/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.30958, s2s_loss 0.32155, 54.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69387, 2.79 secs\n",
      "\u001b[1m---- Epoch 386/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.39710, s2s_loss 0.32300, 55.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69369, 2.94 secs\n",
      "\u001b[1m---- Epoch 387/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40393, s2s_loss 0.32054, 54.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69356, 2.65 secs\n",
      "\u001b[1m---- Epoch 388/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31823, s2s_loss 0.31525, 54.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69358, 2.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_388_s2s_loss=0.6074.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 389/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41514, s2s_loss 0.32156, 53.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69448, 2.27 secs\n",
      "\u001b[1m---- Epoch 390/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.20115, s2s_loss 0.31917, 53.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69428, 2.19 secs\n",
      "\u001b[1m---- Epoch 391/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.34614, s2s_loss 0.31909, 50.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69210, 2.79 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_391_s2s_loss=0.6077.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 392/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.32121, s2s_loss 0.31765, 53.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69136, 2.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_392_s2s_loss=0.6080.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 393/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.26926, s2s_loss 0.32159, 53.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69121, 2.65 secs\n",
      "\u001b[1m---- Epoch 394/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.27382, s2s_loss 0.31767, 54.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69123, 2.89 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_394_s2s_loss=0.6080.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 395/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.45101, s2s_loss 0.31966, 53.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69132, 2.66 secs\n",
      "\u001b[1m---- Epoch 396/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31336, s2s_loss 0.31613, 54.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.69128, 2.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_396_s2s_loss=0.6081.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 397/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.30724, s2s_loss 0.32200, 53.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68868, 2.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_397_s2s_loss=0.6086.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 398/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.25265, s2s_loss 0.31716, 54.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68691, 2.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_398_s2s_loss=0.6094.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 399/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.27709, s2s_loss 0.32175, 54.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68738, 2.72 secs\n",
      "\u001b[1m---- Epoch 400/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.25391, s2s_loss 0.31694, 54.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68778, 2.50 secs\n",
      "\u001b[1m---- Epoch 401/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39348, s2s_loss 0.31704, 54.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68760, 2.31 secs\n",
      "\u001b[1m---- Epoch 402/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35567, s2s_loss 0.31887, 54.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68763, 2.77 secs\n",
      "\u001b[1m---- Epoch 403/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40197, s2s_loss 0.31353, 55.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68790, 2.53 secs\n",
      "\u001b[1m---- Epoch 404/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.33188, s2s_loss 0.31402, 53.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68793, 2.47 secs\n",
      "\u001b[1m---- Epoch 405/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.30800, s2s_loss 0.31734, 53.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68571, 2.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_405_s2s_loss=0.6098.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 406/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38943, s2s_loss 0.32060, 54.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68734, 2.70 secs\n",
      "\u001b[1m---- Epoch 407/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41495, s2s_loss 0.32153, 54.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68609, 2.70 secs\n",
      "\u001b[1m---- Epoch 408/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.23163, s2s_loss 0.31534, 54.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68572, 2.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_408_s2s_loss=0.6099.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 409/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.26302, s2s_loss 0.32224, 54.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68544, 2.62 secs\n",
      "\u001b[1m---- Epoch 410/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29568, s2s_loss 0.31829, 55.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68538, 2.77 secs\n",
      "\u001b[1m---- Epoch 411/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32688, s2s_loss 0.31384, 55.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68545, 2.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_411_s2s_loss=0.6101.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 412/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.30362, s2s_loss 0.31494, 55.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68547, 2.56 secs\n",
      "\u001b[1m---- Epoch 413/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.27228, s2s_loss 0.31040, 55.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68482, 2.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_413_s2s_loss=0.6105.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 414/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29064, s2s_loss 0.32090, 52.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68352, 2.76 secs\n",
      "\u001b[1m---- Epoch 415/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.25867, s2s_loss 0.31795, 53.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68274, 3.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_415_s2s_loss=0.6107.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 416/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41322, s2s_loss 0.31224, 55.61 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.68374, 2.77 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_416_s2s_loss=0.6107.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 417/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.40782, s2s_loss 0.31604, 54.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68375, 2.66 secs\n",
      "\u001b[1m---- Epoch 418/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35263, s2s_loss 0.31477, 53.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68351, 2.21 secs\n",
      "\u001b[1m---- Epoch 419/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.19174, s2s_loss 0.31487, 55.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68349, 2.63 secs\n",
      "\u001b[1m---- Epoch 420/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.29077, s2s_loss 0.31981, 56.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68346, 2.51 secs\n",
      "\u001b[1m---- Epoch 421/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.27497, s2s_loss 0.31648, 57.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68182, 3.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_421_s2s_loss=0.6111.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 422/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31801, s2s_loss 0.31760, 58.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68091, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_422_s2s_loss=0.6113.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 423/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.42981, s2s_loss 0.32163, 57.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68060, 2.68 secs\n",
      "\u001b[1m---- Epoch 424/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.27674, s2s_loss 0.31331, 56.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68059, 2.78 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_424_s2s_loss=0.6117.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 425/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.29912, s2s_loss 0.31892, 54.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68132, 2.44 secs\n",
      "\u001b[1m---- Epoch 426/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.37786, s2s_loss 0.31091, 53.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68154, 2.60 secs\n",
      "\u001b[1m---- Epoch 427/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28251, s2s_loss 0.31391, 53.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68156, 2.20 secs\n",
      "\u001b[1m---- Epoch 428/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.27079, s2s_loss 0.31156, 52.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68160, 2.80 secs\n",
      "\u001b[1m---- Epoch 429/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.62743, s2s_loss 0.31295, 53.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68259, 2.65 secs\n",
      "\u001b[1m---- Epoch 430/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.32838, s2s_loss 0.31958, 53.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68050, 2.51 secs\n",
      "\u001b[1m---- Epoch 431/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.23158, s2s_loss 0.31737, 54.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68076, 2.64 secs\n",
      "\u001b[1m---- Epoch 432/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.26345, s2s_loss 0.31517, 55.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68009, 2.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_432_s2s_loss=0.6117.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 433/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.28723, s2s_loss 0.31887, 54.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67981, 2.70 secs\n",
      "\u001b[1m---- Epoch 434/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.30865, s2s_loss 0.31880, 54.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67963, 2.31 secs\n",
      "\u001b[1m---- Epoch 435/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40447, s2s_loss 0.30934, 54.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67968, 2.71 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_435_s2s_loss=0.6122.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 436/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.35967, s2s_loss 0.31256, 54.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67970, 2.83 secs\n",
      "\u001b[1m---- Epoch 437/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36050, s2s_loss 0.31720, 54.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.68309, 2.80 secs\n",
      "\u001b[1m---- Epoch 438/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37528, s2s_loss 0.31475, 55.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67952, 2.73 secs\n",
      "\u001b[1m---- Epoch 439/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.18416, s2s_loss 0.31148, 55.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67990, 2.88 secs\n",
      "\u001b[1m---- Epoch 440/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.55209, s2s_loss 0.31513, 55.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67899, 2.81 secs\n",
      "\u001b[1m---- Epoch 441/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.28685, s2s_loss 0.31352, 55.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67896, 2.82 secs\n",
      "\u001b[1m---- Epoch 442/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.24472, s2s_loss 0.31071, 55.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67891, 2.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_442_s2s_loss=0.6124.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 443/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32468, s2s_loss 0.31140, 56.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67884, 3.13 secs\n",
      "\u001b[1m---- Epoch 444/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.27962, s2s_loss 0.31229, 55.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67891, 4.13 secs\n",
      "\u001b[1m---- Epoch 445/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.22556, s2s_loss 0.31728, 58.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67865, 3.30 secs\n",
      "\u001b[1m---- Epoch 446/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.39006, s2s_loss 0.31371, 57.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67639, 3.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_446_s2s_loss=0.6130.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 447/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.29315, s2s_loss 0.30403, 59.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67690, 2.90 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_447_s2s_loss=0.6134.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 448/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.37359, s2s_loss 0.31403, 59.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67656, 2.97 secs\n",
      "\u001b[1m---- Epoch 449/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.19456, s2s_loss 0.31523, 57.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67614, 2.83 secs\n",
      "\u001b[1m---- Epoch 450/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.34015, s2s_loss 0.31507, 55.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67614, 2.62 secs\n",
      "\u001b[1m---- Epoch 451/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.25207, s2s_loss 0.31753, 54.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67599, 2.71 secs\n",
      "\u001b[1m---- Epoch 452/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.35183, s2s_loss 0.31637, 53.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67598, 2.53 secs\n",
      "\u001b[1m---- Epoch 453/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.29088, s2s_loss 0.31458, 54.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67688, 2.67 secs\n",
      "\u001b[1m---- Epoch 454/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.25458, s2s_loss 0.31065, 53.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67940, 2.52 secs\n",
      "\u001b[1m---- Epoch 455/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.36098, s2s_loss 0.30953, 54.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67678, 2.22 secs\n",
      "\u001b[1m---- Epoch 456/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.40292, s2s_loss 0.30564, 53.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67595, 2.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_456_s2s_loss=0.6136.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 457/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.24753, s2s_loss 0.30685, 54.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67596, 2.67 secs\n",
      "\u001b[1m---- Epoch 458/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.27616, s2s_loss 0.30990, 55.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67602, 3.16 secs\n",
      "\u001b[1m---- Epoch 459/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32079, s2s_loss 0.31052, 57.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67569, 2.94 secs\n",
      "\u001b[1m---- Epoch 460/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.25336, s2s_loss 0.30987, 57.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67558, 2.96 secs\n",
      "\u001b[1m---- Epoch 461/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36294, s2s_loss 0.31364, 61.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67714, 4.69 secs\n",
      "\u001b[1m---- Epoch 462/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40181, s2s_loss 0.31756, 62.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67436, 3.24 secs\n",
      "\u001b[1m---- Epoch 463/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.41101, s2s_loss 0.31319, 64.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67391, 4.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_463_s2s_loss=0.6138.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 464/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.26682, s2s_loss 0.30770, 61.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67372, 3.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_464_s2s_loss=0.6142.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 465/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.34524, s2s_loss 0.31703, 54.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67351, 2.91 secs\n",
      "\u001b[1m---- Epoch 466/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.27664, s2s_loss 0.30410, 53.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67330, 2.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_466_s2s_loss=0.6145.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 467/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.40672, s2s_loss 0.31139, 53.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67334, 2.14 secs\n",
      "\u001b[1m---- Epoch 468/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.20448, s2s_loss 0.30752, 53.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67333, 2.71 secs\n",
      "\u001b[1m---- Epoch 469/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32198, s2s_loss 0.31766, 53.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67285, 2.29 secs\n",
      "\u001b[1m---- Epoch 470/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30565, s2s_loss 0.30746, 53.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67088, 2.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_470_s2s_loss=0.6151.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 471/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.20521, s2s_loss 0.30471, 53.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67163, 2.24 secs\n",
      "\u001b[1m---- Epoch 472/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35157, s2s_loss 0.31707, 53.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67098, 2.95 secs\n",
      "\u001b[1m---- Epoch 473/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.36064, s2s_loss 0.31336, 54.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67150, 2.23 secs\n",
      "\u001b[1m---- Epoch 474/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.39527, s2s_loss 0.31147, 54.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67156, 2.75 secs\n",
      "\u001b[1m---- Epoch 475/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.21906, s2s_loss 0.30539, 54.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67151, 2.79 secs\n",
      "\u001b[1m---- Epoch 476/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32715, s2s_loss 0.31163, 56.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67156, 3.20 secs\n",
      "\u001b[1m---- Epoch 477/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33087, s2s_loss 0.30736, 56.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67245, 3.25 secs\n",
      "\u001b[1m---- Epoch 478/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38528, s2s_loss 0.30978, 57.37 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67195, 2.91 secs\n",
      "\u001b[1m---- Epoch 479/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.35058, s2s_loss 0.31140, 58.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66936, 2.87 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_479_s2s_loss=0.6154.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 480/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.34028, s2s_loss 0.30456, 56.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66913, 2.87 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_480_s2s_loss=0.6159.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 481/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.26778, s2s_loss 0.31343, 54.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66883, 2.71 secs\n",
      "\u001b[1m---- Epoch 482/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31481, s2s_loss 0.30665, 54.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66899, 2.54 secs\n",
      "\u001b[1m---- Epoch 483/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31710, s2s_loss 0.30973, 53.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66907, 2.48 secs\n",
      "\u001b[1m---- Epoch 484/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42395, s2s_loss 0.30584, 53.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66902, 2.32 secs\n",
      "\u001b[1m---- Epoch 485/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.21592, s2s_loss 0.30053, 53.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.67081, 2.88 secs\n",
      "\u001b[1m---- Epoch 486/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.35318, s2s_loss 0.30963, 54.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66822, 2.20 secs\n",
      "\u001b[1m---- Epoch 487/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.35893, s2s_loss 0.30980, 55.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66747, 2.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_487_s2s_loss=0.6161.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 488/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.31267, s2s_loss 0.30841, 55.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66757, 2.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_488_s2s_loss=0.6161.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 489/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.31642, s2s_loss 0.30999, 55.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66740, 2.64 secs\n",
      "\u001b[1m---- Epoch 490/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29070, s2s_loss 0.30414, 56.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66737, 3.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_490_s2s_loss=0.6165.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 491/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.35467, s2s_loss 0.30654, 56.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66751, 2.83 secs\n",
      "\u001b[1m---- Epoch 492/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.18796, s2s_loss 0.31101, 55.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66771, 2.85 secs\n",
      "\u001b[1m---- Epoch 493/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.25759, s2s_loss 0.31229, 55.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66652, 2.86 secs\n",
      "\u001b[1m---- Epoch 494/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.32356, s2s_loss 0.30954, 55.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66857, 2.82 secs\n",
      "\u001b[1m---- Epoch 495/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.29199, s2s_loss 0.30613, 54.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66711, 2.54 secs\n",
      "\u001b[1m---- Epoch 496/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.22777, s2s_loss 0.31379, 53.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66735, 2.34 secs\n",
      "\u001b[1m---- Epoch 497/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.21839, s2s_loss 0.30491, 53.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66672, 2.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_497_s2s_loss=0.6166.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 498/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29366, s2s_loss 0.31525, 53.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66687, 2.62 secs\n",
      "\u001b[1m---- Epoch 499/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43820, s2s_loss 0.30519, 53.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66678, 2.37 secs\n",
      "\u001b[1m---- Epoch 500/500\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41155, s2s_loss 0.30783, 53.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.66680, 2.38 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231226_233457_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)\" \\\n",
    "--epochs 500 \\\n",
    "--batches_per_epoch 500 \\\n",
    "--batch_size 40 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--task_name \"multitask\" \\\n",
    "--experiment_name \"s2f+f2m+f2c+s2co+s2cal+nli\" \\\n",
    "--multitask_name_list \\\n",
    "\"nli\" \\\n",
    "\"sentence2facts\" \\\n",
    "\"fact2metadata\" \\\n",
    "\"fact2comparison\" \\\n",
    "\"sentence2chestimagenome_observations\" \\\n",
    "\"sentence2chestimagenome_anatomical_locations\" \\\n",
    "--task2weight '{\"sentence2facts\": 1.0, \"fact2metadata\": 1.0, \"fact2comparison\": 0.3, \"sentence2chestimagenome_observations\": 1.0, \"sentence2chestimagenome_anatomical_locations\": 1.0, \"nli\": 3.0}' \\\n",
    "--sentence_to_facts_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\"\\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\" \\\n",
    "--fact_to_metadata_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--fact_to_comparison_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "--chest_imagenome_phrases2labels_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\" \\\n",
    "--chest_imagenome_obs_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--chest_imagenome_anatloc_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--use_sentence2facts_for_nli \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--paraphrased_inputs_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--seq2seq_model_name \"t5\" \\\n",
    "--t5_model_name \"t5-small\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 250\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 23\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231231_134624_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   task_name: multitask\n",
      "   experiment_name: s2f+f2m+f2c+s2co+s2cal+nli\n",
      "   multitask_name_list: ['nli', 'sentence2facts', 'fact2metadata', 'fact2comparison', 'sentence2chestimagenome_observations', 'sentence2chestimagenome_anatomical_locations']\n",
      "   task2weight: {'sentence2facts': 1.0, 'fact2metadata': 1.0, 'fact2comparison': 0.3, 'sentence2chestimagenome_observations': 1.0, 'sentence2chestimagenome_anatomical_locations': 1.0, 'nli': 5.0}\n",
      "   val_size: 200\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_phrases2labels_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl']\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl']\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl']\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl']\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   use_sentence2facts_for_nli: True\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   num_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 9891 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mB\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mbilateral\u001b[0m\n",
      "\u001b[1m\u001b[35mboth sides\u001b[0m\n",
      "\u001b[1m\u001b[35mboth\u001b[0m\n",
      "\u001b[1m\u001b[35mon both sides\u001b[0m\n",
      "\u001b[1m\u001b[35msymmetrical\u001b[0m\n",
      "\u001b[1m\u001b[35mequally on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mpresent on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mseen on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35moccurring on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mappearing on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mfound on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted on both sides\u001b[0m\n",
      "\u001b[1m\u001b[35mnoted bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mseen bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mobserved bilaterally\u001b[0m\n",
      "\u001b[1m\u001b[35mdetected bilaterally\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9890 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilus\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the left hilum\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleft to the hilum on the left side\u001b[0m\n",
      "\u001b[1m\u001b[35mleft side of the pulmonary hilum on the left\u001b[0m\n",
      "\u001b[1m\u001b[35mleftward to the hilum on the left side\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 8511 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary vasculature\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral lung vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary blood vessels\u001b[0m\n",
      "\u001b[1m\u001b[35mlung periphery vasculature\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature of the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels in the outer regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular network in the peripheral lung\u001b[0m\n",
      "\u001b[1m\u001b[35mperipheral pulmonary circulation\u001b[0m\n",
      "\u001b[1m\u001b[35mblood vessels supplying the outer areas of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mvasculature in the peripheral regions of the lung\u001b[0m\n",
      "\u001b[1m\u001b[35mpulmonary vessels in the lung periphery\u001b[0m\n",
      "\u001b[1m\u001b[35mvascular system in the outer regions of the lung\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 1864 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35ma\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mone\u001b[0m\n",
      "\u001b[1m\u001b[35msingle\u001b[0m\n",
      "\u001b[1m\u001b[35msole\u001b[0m\n",
      "\u001b[1m\u001b[35monly\u001b[0m\n",
      "\u001b[1m\u001b[35mindividual\u001b[0m\n",
      "\u001b[1m\u001b[35mlone\u001b[0m\n",
      "\u001b[1m\u001b[35msolitary\u001b[0m\n",
      "\u001b[1m\u001b[35munique\u001b[0m\n",
      "\u001b[1m\u001b[35mdistinct\u001b[0m\n",
      "\u001b[1m\u001b[35msingular\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 10000 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mRib number five\u001b[0m\n",
      "\u001b[1m\u001b[35mRib 5\u001b[0m\n",
      "\u001b[1m\u001b[35m5th rib bone\u001b[0m\n",
      "\u001b[1m\u001b[35mBone of the fifth rib\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth rib structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th rib\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costal bone\u001b[0m\n",
      "\u001b[1m\u001b[35mCostal bone number five\u001b[0m\n",
      "\u001b[1m\u001b[35m5th costae\u001b[0m\n",
      "\u001b[1m\u001b[35mCostae number five\u001b[0m\n",
      "\u001b[1m\u001b[35mFifth costae structure\u001b[0m\n",
      "\u001b[1m\u001b[35mStructure of the 5th costae\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9999 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT 2\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography exam\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan test\u001b[0m\n",
      "\u001b[1m\u001b[35mCT diagnostic imaging\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging technique\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan investigation\u001b[0m\n",
      "\u001b[1m\u001b[35mCT radiographic study\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography imaging procedure\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan evaluation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9997 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mline location\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is located\u001b[0m\n",
      "\u001b[1m\u001b[35mThe position of the line is\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line can be seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is situated\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is found\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line appears\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is detected\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is identified\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is visible\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is present\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is observed\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is noted\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is seen\u001b[0m\n",
      "\u001b[1m\u001b[35mThe line is evident\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9994 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence of\u001b[0m\n",
      "\u001b[1m\u001b[35mNot visible\u001b[0m\n",
      "\u001b[1m\u001b[35mNot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mNot detected\u001b[0m\n",
      "\u001b[1m\u001b[35mNot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mNot present\u001b[0m\n",
      "\u001b[1m\u001b[35mAbsence of\u001b[0m\n",
      "\u001b[1m\u001b[35mLack of\u001b[0m\n",
      "\u001b[1m\u001b[35mNegative for\u001b[0m\n",
      "\u001b[1m\u001b[35mNo signs of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo findings of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo indications of\u001b[0m\n",
      "\u001b[1m\u001b[35mNo evidence suggesting\u001b[0m\n",
      "\u001b[1m\u001b[35mNo abnormalities detected\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 9996 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mCT on\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan done\u001b[0m\n",
      "\u001b[1m\u001b[35mImaging study using computed tomography\u001b[0m\n",
      "\u001b[1m\u001b[35mRadiological examination using CT\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan taken\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging performed\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan carried out\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging study performed\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan conducted\u001b[0m\n",
      "\u001b[1m\u001b[35mComputed tomography study done\u001b[0m\n",
      "\u001b[1m\u001b[35mCT imaging examination\u001b[0m\n",
      "\u001b[1m\u001b[35mCT scan performed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of all-trans retinoic acid syndrome\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome not detected\u001b[0m\n",
      "\u001b[1m\u001b[35mall-trans retinoic acid syndrome is not observed\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19937 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant constriction\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of notable narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of substantial narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of significant narrowing\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of significant constriction\u001b[0m\n",
      "\u001b[1m\u001b[35mno presence of notable constriction\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 19943 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of wheezing\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of wheezing\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing detected\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing observed\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing present\u001b[0m\n",
      "\u001b[1m\u001b[35mno wheezing found\u001b[0m\n",
      "--------\n",
      "\u001b[1mNumber of unique inputs: 190085\u001b[0m\n",
      "\u001b[1mNumber of total paraphrases: 2093173\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2facts dataset\u001b[0m\n",
      "Loaded 9999 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19971 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\n",
      "Loaded 14990 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\n",
      "Loaded 14991 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2F: There is persistent mild elevation of the left hemidiaphragm and bandlike atlectasis in the left lower lung.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"persistent mild elevation of the left hemidiaphragm\", \"bandlike atelectasis in the left lower lung\"]\u001b[0m\n",
      "Number of train examples: 84735\n",
      "Number of val examples: 200\n",
      "Number of total examples: 84935\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing nli dataset\u001b[0m\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "Number of sources: 5\n",
      "----\n",
      "\u001b[1mLoading sentence2facts input/output pairs for NLI...\u001b[0m\n",
      "Number of entailment samples added: 368966\n",
      "----\n",
      "\u001b[1mLoading general domain datasets...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset...\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: contradiction -> 181976 (5334.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Right pleural catheter is again seen with evidence of compressive atelectasis and small residual pleural effusion. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is no right pleural catheter present in the chest X-ray.\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: contradiction -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Dual chamber ICD implant (left pectoral) on [**3082-5-10**], with a pacesetter atrial lead and a CPI ventricular lead. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient has normal heart.\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: contradiction -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The pain appeared localized mostly to his back without any radiation. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m the patient denies back pain\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: contradiction -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Nasolaryngoscopy exam by ED resident showed arytenoid, vocal chord, and supraglottic edema. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m Patient has normal endoscopy findings\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: contradiction -> 212 (461.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The left pleural effusion has decreased and is now small. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is no focal consolidation, pleural effusion, or pneumothorax.\u001b[0m\n",
      "\u001b[1mSource: anli | Label: contradiction -> 88178 (4433.68)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: No More Freebies<br>Tom moved to a new neighborhood. Tom noticed his new neighbor didn't have a password on his wi-fi. Tom began surfing off his neighbor's wi-fi service for free. One day Tom noticed his neighbor did secure his wi-fi with passwords. Tom then had to call the cable company and begin paying for service. #Hypothesis: Tom doesn't use the internet\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: contradiction -> 274712 (5897.92)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Rest assured. #Hypothesis: Please worry about it. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: snli | Label: contradiction -> 379404 (6365.96)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: An adult and a child walk along the beach during the day. #Hypothesis: two people are on the beach.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: entailment -> 43334 (3654.55)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Stable position of the right internal jugular vein catheter. #Hypothesis: The right internal jugular vein catheter has not moved from its previous position.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: entailment -> 7488 (2131.93)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Sleep apnea 14. #Hypothesis:  The patient stops breathing while sleeping. \u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: entailment -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Last night, he began having chest pain at rest. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m patient has chest pain at rest\u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: entailment -> 946 (966.10)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: He exercised for roughly three minutes and had diffuse ST-T wave abnormalities that were nondiagnostic due to left bundle branch block. #Hypothesis:  He has EKG abnormalities\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: entailment -> 186 (428.52)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: There are patchy bibasilar airspace opacities, likely reflective of atelectasis though infection cannot be completely excluded. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mBibasilar patchy opacities may reflect atelectasis though infection is not excluded.\u001b[0m\n",
      "\u001b[1mSource: s2f | Label: entailment -> 368966 (6324.57)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: The cardiomediastinal silhouette is unchanged, atherosclerotic calcifications again noted at the aortic arch. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35matherosclerotic calcifications at the aortic arch\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSource: anli | Label: entailment -> 108502 (4680.39)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: MEXICO CITY, Oct 29 - Mexico's central bank on Thursday said it sold $200 million of $200 million offered in an auction after the peso weakened sharply from its fix rate in the previous session. The central bank sold the dollars at an average weighted price of 16.6276 pesos per dollar. An auction is triggerd when the currency is trading 1 percent weaker than its fix rate in the previous session. The peso slumped from the fix rate seen on Wednesday after the U.S. Federal Reserve suggested it could raise interest rates at its next meeting in December. (Reporting by Michael O'Boyle) #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe fixed rate was higher on Wednesday than the peso was at the time of the reporting\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: entailment -> 275682 (5902.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Nothing says Las Vegas quite like this; the glittering costumes and chorus line of showgirls is what the myth was based on. #Generate entailment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mVegas has a lot of glittering costumes and showgirls.\u001b[0m\n",
      "\u001b[1mSource: snli | Label: entailment -> 380226 (6369.17)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A male adolescent or young teenager is wearing camouflaged shorts or long swim trunks and is upside down over a body of water like a lake or reservoir whose cement walls and graveled top appear in the corner. #Hypothesis: A young teenager is wearing camouflaged shorts.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: entailment\u001b[0m\n",
      "\u001b[1mSource: gpt-4-0613 | Label: neutral -> 69708 (4164.76)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: A large cavitary component within the central part of left upper lobe consolidation appears unchanged. #Hypothesis: There is no evidence of pleural effusion in the chest X-ray.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: mednli_train | Label: neutral -> 7486 (2131.74)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: DM HTN Chronic low back pain PTSD #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient is on an ace inhibitor.\u001b[0m\n",
      "\u001b[1mSource: mednli_dev | Label: neutral -> 930 (958.90)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: She progressed to a vaginal delivery. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m The patient had an epidural. \u001b[0m\n",
      "\u001b[1mSource: mednli_test | Label: neutral -> 948 (966.99)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Presently, the patient's pneumonia is significantly improved and patient presents for long-term airway management #Hypothesis:  Patient has trouble breathing\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mSource: radnli_dev | Label: neutral -> 562 (762.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: There is mild cardiomegaly. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere are no acute osseous abnormalities.\u001b[0m\n",
      "\u001b[1mSource: anli | Label: neutral -> 141850 (5012.51)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Poland is an unincorporated community in eastern Cass Township, Clay County, Indiana, United States. It lies along State Road 42 southeast of the city of Brazil, the county seat of Clay County. Its elevation is 696 feet (212 m). Although Poland is unincorporated, it has a post office, with the ZIP code of 47868. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mUnincorporated communities rarely have a post office.\u001b[0m\n",
      "\u001b[1mSource: multinli | Label: neutral -> 274304 (5895.82)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: and are among the three largest recipients of U.S. outbound mail, the third being . U.S. outbound mail to and exceeds the inbound mail from each of those countries by about a -to-one ratio. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThey wanted to see it balanced more.\u001b[0m\n",
      "\u001b[1mSource: snli | Label: neutral -> 378436 (6362.16)\u001b[0m\n",
      "Example:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: An older woman is holding a large baby with mittens to protect her from scratching herself. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mA woman holding a baby that is wearing pink gloves.\u001b[0m\n",
      "----\n",
      "Number of RadNLI test samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: There are mediastinal drains. #Hypothesis: The mediastinal silhouette is unremarkable.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: neutral\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI1: Slight increase in right base atelectasis. #Hypothesis: Slight improvement in right base atelectasis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mMost likely: contradiction\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Moderate right pneumothorax has enlarged since the ___ study. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNo change in moderate right pneumothorax.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: slight decrease in small right pleural effusion. #Generate contradiction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mslight increase in small right pleural effusion.\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mNLI2: Bibasilar atelectasis has worsened, particularly in the left retrocardiac region. #Generate neutral\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is mild bibasilar atelectasis.\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset...\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2metadata dataset\u001b[0m\n",
      "Loaded 19989 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\n",
      "Loaded 19948 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\n",
      "Loaded 19984 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2M: persistent LUL opacification\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"anatomical location\": \"left upper lobe\", \"detailed observation\": \"persistent opacification in the left upper lobe\", \"short observation\": \"opacification in left upper lobe\", \"category\": \"anatomical finding\", \"health status\": \"abnormal\", \"prev_study_comparison?\": \"no\", \"comparison status\": \"\"}\u001b[0m\n",
      "Number of train examples: 59721\n",
      "Number of val examples: 200\n",
      "Number of total examples: 59921\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing fact2comparison dataset\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Added 341589 paraphrased inputs (total 443344)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mSuperior air flow compared to 2:24am\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mimproved\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mF2C: subtotally collapsed right lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "Counter:\n",
      "Counter({'no comparison': 276665, 'stable/unchanged': 46121, 'worsened': 27557, 'resolved': 26871, 'improved': 18519, 'progressed': 10316, 'new finding': 8625, 'larger': 5687, 'increase': 5642, 'decrease': 5399, 'smaller': 4471, 'unclear comparison': 3230, 'position changed': 2691, 'reappeared': 1522, 'other': 28})\n",
      "Output: no comparison, train size: 276650, val size: 15, weight: 326.80332399852057\n",
      "Output: worsened, train size: 27542, val size: 15, weight: 217.54319875804714\n",
      "Output: resolved, train size: 26857, val size: 14, weight: 216.47267916050666\n",
      "Output: larger, train size: 5673, val size: 14, weight: 155.49830995863778\n",
      "Output: improved, train size: 18505, val size: 14, weight: 200.94841544934332\n",
      "Output: smaller, train size: 4457, val size: 14, weight: 146.939423083012\n",
      "Output: progressed, train size: 10302, val size: 14, weight: 177.70587815266865\n",
      "Output: increase, train size: 5628, val size: 14, weight: 155.2118953407358\n",
      "Output: decrease, train size: 5385, val size: 14, weight: 153.6293473761691\n",
      "Output: unclear comparison, train size: 3216, val size: 14, weight: 135.74700551052337\n",
      "Output: new finding, train size: 8611, val size: 14, weight: 170.87627086853004\n",
      "Output: stable/unchanged, train size: 46107, val size: 14, weight: 240.0236968314092\n",
      "Output: reappeared, train size: 1508, val size: 14, weight: 111.48024795838076\n",
      "Output: other, train size: 26, val size: 2, weight: 22.094133543878307\n",
      "Output: position changed, train size: 2677, val size: 14, weight: 129.6501373802887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 443144\n",
      "Number of val examples: 200\n",
      "Number of total examples: 443344\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_observations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:19<00:00, 27846.75it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mLeft chest wall dual-lead pacing device is seen, unchanged in position.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"cardiac pacer and wires\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mFINDINGS: No evidence of rib fractures.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mBilateral pigtail catheters are in unchanged position.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"pigtail catheter\"]\u001b[0m\n",
      "Loaded 5000 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\n",
      "Loaded 14859 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\n",
      "Loaded 19952 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\n",
      "Loaded 19959 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\n",
      "Loaded 9977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\n",
      "Loaded 9974 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\n",
      "Loaded 9987 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\n",
      "Added 103692 paraphrased inputs (total 749511)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mradiographic finding suggestive of effusion\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"pleural effusion\"]\u001b[0m\n",
      "100%|███████████████████████████████| 749511/749511 [00:03<00:00, 213250.67it/s]\n",
      "Number of total examples: 749511\n",
      "Number of examples without labels: 160864\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 4498\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  contrast material extravasation\u001b[0m\n",
      "\u001b[93m\u001b[1m  retrocardiac opacity\u001b[0m\n",
      "\u001b[93m\u001b[1m  hip dislocation\u001b[0m\n",
      "\u001b[93m\u001b[1m  switch\u001b[0m\n",
      "\u001b[93m\u001b[1m  microscope\u001b[0m\n",
      "\u001b[93m\u001b[1m  air pollution\u001b[0m\n",
      "\u001b[93m\u001b[1m  vascular interruption\u001b[0m\n",
      "\u001b[93m\u001b[1m  normal lung inflation\u001b[0m\n",
      "\u001b[93m\u001b[1m  abdominal\u001b[0m\n",
      "\u001b[93m\u001b[1m  pleural cavity\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: lung opacity, train size: 306066, val size: 3, weight: 6051.933911292828\n",
      "Label: None, train size: 160861, val size: 3, weight: 5173.637314058976\n",
      "Label: atelectasis, train size: 94005, val size: 3, weight: 4508.8481434163405\n",
      "Label: pleural effusion, train size: 93000, val size: 3, weight: 4496.163483083717\n",
      "Label: pneumonia, train size: 46683, val size: 2, weight: 3731.5271448792964\n",
      "Label: pulmonary edema/hazy opacity, train size: 43440, val size: 3, weight: 3657.0589695764065\n",
      "Label: enlarged cardiac silhouette, train size: 35271, val size: 3, weight: 3447.1902081153794\n",
      "Label: enteric tube, train size: 27592, val size: 2, weight: 3210.327888152985\n",
      "Label: vascular congestion, train size: 25946, val size: 3, weight: 3152.742152431716\n",
      "Label: consolidation, train size: 25253, val size: 3, weight: 3127.6160152511925\n",
      "Label: lung lesion, train size: 23555, val size: 3, weight: 3063.627148811275\n",
      "Label: endotracheal tube, train size: 21900, val size: 2, weight: 2997.59661410789\n",
      "Label: pleural/parenchymal scarring, train size: 18674, val size: 3, weight: 2856.484034513788\n",
      "Label: low lung volumes, train size: 18580, val size: 2, weight: 2852.0891581832807\n",
      "Label: pneumothorax, train size: 16788, val size: 3, weight: 2764.715849693273\n",
      "Label: mass/nodule (not otherwise specified), train size: 14465, val size: 3, weight: 2639.674661580133\n",
      "Label: picc, train size: 14278, val size: 2, weight: 2628.9326656169824\n",
      "Label: cardiac pacer and wires, train size: 13384, val size: 3, weight: 2575.9851883187807\n",
      "Label: ij line, train size: 12401, val size: 2, weight: 2514.4400637647846\n",
      "Label: lobar/segmental collapse, train size: 12364, val size: 3, weight: 2512.0494320584767\n",
      "Label: aspiration, train size: 12232, val size: 3, weight: 2503.474512677858\n",
      "Label: linear/patchy atelectasis, train size: 12098, val size: 3, weight: 2494.6948074626707\n",
      "Label: chest tube, train size: 11744, val size: 2, weight: 2471.1267449016627\n",
      "Label: airspace opacity, train size: 11712, val size: 3, weight: 2468.9688730909206\n",
      "Label: enlarged hilum, train size: 11707, val size: 3, weight: 2468.6312868519553\n",
      "Label: rib fracture, train size: 9953, val size: 3, weight: 2342.524074481851\n",
      "Label: hyperaeration, train size: 8176, val size: 3, weight: 2195.5703070707195\n",
      "Label: mediastinal widening, train size: 7456, val size: 3, weight: 2128.8592409272346\n",
      "Label: chest port, train size: 7006, val size: 2, weight: 2084.5821317249197\n",
      "Label: copd/emphysema, train size: 6975, val size: 3, weight: 2081.4516456626866\n",
      "Label: costophrenic angle blunting, train size: 6947, val size: 3, weight: 2078.6148399685108\n",
      "Label: vascular calcification, train size: 6940, val size: 3, weight: 2077.9042561972665\n",
      "Label: tortuous aorta, train size: 6434, val size: 3, weight: 2025.0049175942086\n",
      "Label: fluid overload/heart failure, train size: 6397, val size: 2, weight: 2021.012218187064\n",
      "Label: elevated hemidiaphragm, train size: 6130, val size: 3, weight: 1991.6591270041336\n",
      "Label: infiltration, train size: 5732, val size: 3, weight: 1946.0193661096143\n",
      "Label: mediastinal displacement, train size: 5214, val size: 3, weight: 1882.8178771125665\n",
      "Label: subcutaneous air, train size: 5135, val size: 3, weight: 1872.760295627013\n",
      "Label: increased reticular markings/ild pattern, train size: 5105, val size: 3, weight: 1868.9099009277677\n",
      "Label: multiple masses/nodules, train size: 4863, val size: 3, weight: 1837.1992813518787\n",
      "Label: spinal fracture, train size: 4600, val size: 3, weight: 1801.3383696149233\n",
      "Label: subclavian line, train size: 4436, val size: 2, weight: 1778.176840969267\n",
      "Label: interstitial lung disease, train size: 4258, val size: 2, weight: 1752.2877322477812\n",
      "Label: pigtail catheter, train size: 4253, val size: 2, weight: 1751.5487095068083\n",
      "Label: spinal degenerative changes, train size: 4159, val size: 3, weight: 1737.5305267974525\n",
      "Label: superior mediastinal mass/enlargement, train size: 4151, val size: 3, weight: 1736.3264075484174\n",
      "Label: hernia, train size: 3759, val size: 3, weight: 1675.0399893609738\n",
      "Label: vascular redistribution, train size: 3617, val size: 3, weight: 1651.6427919602718\n",
      "Label: calcified nodule, train size: 3463, val size: 3, weight: 1625.4699497970707\n",
      "Label: sub-diaphragmatic air, train size: 3420, val size: 3, weight: 1618.0053116099564\n",
      "Label: tracheostomy tube, train size: 3387, val size: 2, weight: 1612.228479261125\n",
      "Label: bone lesion, train size: 3354, val size: 3, weight: 1606.409055373111\n",
      "Label: scoliosis, train size: 3058, val size: 3, weight: 1552.1817553012304\n",
      "Label: granulomatous disease, train size: 2937, val size: 2, weight: 1528.874590326155\n",
      "Label: swan-ganz catheter, train size: 2891, val size: 2, weight: 1519.8250022159373\n",
      "Label: prosthetic valve, train size: 2569, val size: 3, weight: 1453.2612259803755\n",
      "Label: lung cancer, train size: 2550, val size: 2, weight: 1449.1429571746244\n",
      "Label: alveolar hemorrhage, train size: 2526, val size: 3, weight: 1443.908128630521\n",
      "Label: rotated, train size: 2255, val size: 2, weight: 1382.0644175504171\n",
      "Label: shoulder osteoarthritis, train size: 2192, val size: 3, weight: 1366.9037277779044\n",
      "Label: bronchiectasis, train size: 2121, val size: 3, weight: 1349.426376336198\n",
      "Label: cabg grafts, train size: 1955, val size: 3, weight: 1306.8099438527813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: pericardial effusion, train size: 1811, val size: 2, weight: 1267.6265025986982\n",
      "Label: hydropneumothorax, train size: 1625, val size: 3, weight: 1213.478539967914\n",
      "Label: artifact, train size: 1619, val size: 2, weight: 1211.6579976564915\n",
      "Label: breast/nipple shadows, train size: 1554, val size: 2, weight: 1191.6129970770162\n",
      "Label: clavicle fracture, train size: 1553, val size: 3, weight: 1191.29988246945\n",
      "Label: cyst/bullae, train size: 1471, val size: 3, weight: 1165.1099156323796\n",
      "Label: pneumomediastinum, train size: 1354, val size: 3, weight: 1125.8416244922646\n",
      "Label: intra-aortic balloon pump, train size: 928, val size: 2, weight: 957.9965160328238\n",
      "Label: goiter, train size: 903, val size: 2, weight: 946.5560551333667\n",
      "Label: mediastinal drain, train size: 900, val size: 2, weight: 945.1682231149956\n",
      "Label: aortic graft/repair, train size: 771, val size: 3, weight: 882.1360585110112\n",
      "Label: diaphragmatic eventration (benign), train size: 403, val size: 3, weight: 648.2558205703779\n",
      "Label: skin fold, train size: 308, val size: 2, weight: 564.9502057601105\n",
      "--------\n",
      "Number of train examples: 1269573\n",
      "Number of val examples: 201\n",
      "Number of total examples: 1269774\n",
      "Number of train datasets: 75\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Prominence of the azygos vein is noted.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: IMPRESSION: Decreased amount of free air under the diaphragm consistent with the recent surgery.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"sub-diaphragmatic air\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: The currently is no evidence of acute lung disease such as pneumonia or pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CO: Assessment of the lung apices is obscured by the patient's neck and chin projecting over and obscuring this region.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"lung opacity\"]\u001b[0m\n",
      "\u001b[1m----------------------------------------\u001b[0m\n",
      "\u001b[1mPreparing sentence2chestimagenome_anatomical_locations dataset\u001b[0m\n",
      "100%|████████████████████████████████| 556111/556111 [00:12<00:00, 46184.82it/s]\n",
      "Loaded 556111 input/output pairs from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\n",
      "Examples:\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mDiscrete areas of nodular consolidation are more pronounced, concerning for concurrent abnormality such as septic emboli, pulmonary infarcts, or multifocal pneumonia.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lung\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mDistribution suggests massive aspiration.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lower lung zone\", \"left lung\", \"right lower lung zone\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is marked calcification of the aorta.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"aortic arch\", \"mediastinum\"]\u001b[0m\n",
      "Loaded 24929 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\n",
      "Loaded 24951 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\n",
      "Loaded 24988 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\n",
      "Added 352702 paraphrased inputs (total 983681)\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mprominent radiolucency in the middle of the sternum\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"mediastinum\"]\u001b[0m\n",
      "100%|███████████████████████████████| 983681/983681 [00:05<00:00, 193791.13it/s]\n",
      "Number of total examples: 983681\n",
      "Number of examples without labels: 96958\n",
      "\u001b[93m\u001b[1mWARNING: Number of unknown labels: 24482\u001b[0m\n",
      "\u001b[93m\u001b[1mSome unknown labels:\u001b[0m\n",
      "\u001b[93m\u001b[1m  pleural effusions\u001b[0m\n",
      "\u001b[93m\u001b[1m  lower midline\u001b[0m\n",
      "\u001b[93m\u001b[1m  sternal wire\u001b[0m\n",
      "\u001b[93m\u001b[1m  mid chest\u001b[0m\n",
      "\u001b[93m\u001b[1m  interventricular septum\u001b[0m\n",
      "\u001b[93m\u001b[1m  bilateral pulmonary opacities\u001b[0m\n",
      "\u001b[93m\u001b[1m  bilateral pulmonary edema\u001b[0m\n",
      "\u001b[93m\u001b[1m  low noise\u001b[0m\n",
      "\u001b[93m\u001b[1m  left ventricular outflow\u001b[0m\n",
      "\u001b[93m\u001b[1m  thoracic aorta\u001b[0m\n",
      "--------\n",
      "\u001b[1mLabel stats:\u001b[0m\n",
      "Label: right lung, train size: 349085, val size: 5, weight: 6242.939760065056\n",
      "Label: left lung, train size: 347230, val size: 5, weight: 6235.1245060798265\n",
      "Label: mediastinum, train size: 143981, val size: 5, weight: 5031.434024951432\n",
      "Label: left lower lung zone, train size: 139366, val size: 5, weight: 4990.146384403511\n",
      "Label: right lower lung zone, train size: 133936, val size: 5, weight: 4940.086380969972\n",
      "Label: right hilar structures, train size: 104615, val size: 5, weight: 4636.352491144966\n",
      "Label: left hilar structures, train size: 99485, val size: 5, weight: 4576.107823312077\n",
      "Label: cardiac silhouette, train size: 97577, val size: 6, weight: 4553.04488436699\n",
      "Label: None, train size: 96952, val size: 6, weight: 4545.4092000523615\n",
      "Label: left costophrenic angle, train size: 87765, val size: 5, weight: 4428.19987568603\n",
      "Label: right costophrenic angle, train size: 85852, val size: 5, weight: 4402.528856723907\n",
      "Label: abdomen, train size: 69663, val size: 6, weight: 4164.037910054699\n",
      "Label: neck, train size: 68254, val size: 5, weight: 4141.189927819187\n",
      "Label: right mid lung zone, train size: 42045, val size: 5, weight: 3623.628546632869\n",
      "Label: right chest wall, train size: 39924, val size: 5, weight: 3571.0313022776\n",
      "Label: left chest wall, train size: 39692, val size: 5, weight: 3565.1414441529187\n",
      "Label: left mid lung zone, train size: 39060, val size: 5, weight: 3548.953812888611\n",
      "Label: trachea, train size: 37843, val size: 5, weight: 3517.174677782791\n",
      "Label: spine, train size: 36958, val size: 5, weight: 3493.5407500408123\n",
      "Label: right upper lung zone, train size: 31920, val size: 5, weight: 3349.5311314541104\n",
      "Label: upper mediastinum, train size: 31113, val size: 5, weight: 3324.781333165901\n",
      "Label: left hemidiaphragm, train size: 26171, val size: 5, weight: 3160.7840671222925\n",
      "Label: left upper lung zone, train size: 24841, val size: 5, weight: 3112.4145859114196\n",
      "Label: right hemidiaphragm, train size: 22848, val size: 5, weight: 3035.8891777107024\n",
      "Label: svc, train size: 22434, val size: 5, weight: 3019.3259457618706\n",
      "Label: aortic arch, train size: 21159, val size: 6, weight: 2966.7309866118158\n",
      "Label: right apical zone, train size: 20026, val size: 5, weight: 2917.823382268566\n",
      "Label: left apical zone, train size: 17355, val size: 5, weight: 2793.1318375972596\n",
      "Label: carina, train size: 17239, val size: 6, weight: 2787.379021190687\n",
      "Label: right atrium, train size: 15512, val size: 5, weight: 2697.866074702561\n",
      "Label: right shoulder, train size: 13642, val size: 5, weight: 2591.545258544152\n",
      "Label: right clavicle, train size: 12043, val size: 5, weight: 2491.069025063256\n",
      "Label: left clavicle, train size: 11921, val size: 5, weight: 2482.9796208502485\n",
      "Label: left shoulder, train size: 10448, val size: 5, weight: 2379.7725601768875\n",
      "Label: cavoatrial junction, train size: 7287, val size: 6, weight: 2112.480024151944\n",
      "Label: left arm, train size: 6534, val size: 5, weight: 2035.7080040077433\n",
      "Label: right arm, train size: 6386, val size: 5, weight: 2019.821762793147\n",
      "Label: left breast, train size: 2993, val size: 5, weight: 1539.749194493932\n",
      "Label: right breast, train size: 2321, val size: 5, weight: 1397.6140821020338\n",
      "--------\n",
      "Number of train examples: 2383476\n",
      "Number of val examples: 201\n",
      "Number of total examples: 2383677\n",
      "Number of train datasets: 39\n",
      "--------\n",
      "\u001b[1mExamples:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Port-A-Cath projects over the right chest wall with catheter extending to the level of the low SVC.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"mediastinum\", \"right chest wall\", \"svc\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Pleural parenchymal scarring within the lung apices with calcified nodules is similar compared to the previous exam, and likely reflective of prior granulomatous disease.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left apical zone\", \"left lung\", \"right apical zone\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: The pre-existing predominantly peripheral and bilateral basal parenchymal opacities.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lower lung zone\", \"left lung\", \"right lower lung zone\", \"right lung\"]\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mS2CA: Improvement of the bibasilar atelectasis.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m[\"left lower lung zone\", \"left lung\", \"right lower lung zone\", \"right lung\"]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_trainer.name =  multitask(s2f+f2m+f2c+s2co+s2cal+nli)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231231_145653_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231231_145653_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_49_s2s_loss=0.6201.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231231_134624_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)/checkpoint_49_s2s_loss=0.6201.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231231_145653_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.49444, s2s_loss 0.39268, 62.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64772, 3.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.6180.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.45279, s2s_loss 0.38625, 54.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64780, 3.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.6183.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.31082, s2s_loss 0.39203, 60.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64824, 3.21 secs\n",
      "\u001b[1m---- Epoch 4/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.28639, s2s_loss 0.38704, 60.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64819, 3.25 secs\n",
      "\u001b[1m---- Epoch 5/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.50577, s2s_loss 0.38605, 61.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64687, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_s2s_loss=0.6186.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.53928, s2s_loss 0.39330, 60.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64821, 3.19 secs\n",
      "\u001b[1m---- Epoch 7/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.40056, s2s_loss 0.38579, 60.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64693, 3.31 secs\n",
      "\u001b[1m---- Epoch 8/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.32004, s2s_loss 0.39323, 60.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64670, 3.24 secs\n",
      "\u001b[1m---- Epoch 9/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.20929, s2s_loss 0.38860, 54.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64622, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.6187.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42464, s2s_loss 0.39317, 60.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64600, 3.21 secs\n",
      "\u001b[1m---- Epoch 11/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.52776, s2s_loss 0.39452, 58.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64590, 3.21 secs\n",
      "\u001b[1m---- Epoch 12/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40218, s2s_loss 0.38750, 60.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64590, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_s2s_loss=0.6189.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37277, s2s_loss 0.39083, 60.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64116, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_s2s_loss=0.6203.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29066, s2s_loss 0.39242, 60.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64619, 3.21 secs\n",
      "\u001b[1m---- Epoch 15/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.51594, s2s_loss 0.38692, 59.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64572, 3.20 secs\n",
      "\u001b[1m---- Epoch 16/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.56040, s2s_loss 0.38744, 60.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64438, 3.21 secs\n",
      "\u001b[1m---- Epoch 17/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.37849, s2s_loss 0.39243, 60.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64389, 3.20 secs\n",
      "\u001b[1m---- Epoch 18/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.20963, s2s_loss 0.38686, 60.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64368, 3.22 secs\n",
      "\u001b[1m---- Epoch 19/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28647, s2s_loss 0.39631, 60.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64358, 3.20 secs\n",
      "\u001b[1m---- Epoch 20/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.27117, s2s_loss 0.38415, 60.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64351, 3.20 secs\n",
      "\u001b[1m---- Epoch 21/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37151, s2s_loss 0.38636, 60.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64723, 3.23 secs\n",
      "\u001b[1m---- Epoch 22/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.46900, s2s_loss 0.38800, 60.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64194, 3.18 secs\n",
      "\u001b[1m---- Epoch 23/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.47309, s2s_loss 0.38647, 60.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64147, 3.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_s2s_loss=0.6204.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.26324, s2s_loss 0.39388, 60.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64082, 3.22 secs\n",
      "\u001b[1m---- Epoch 25/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.36674, s2s_loss 0.39081, 60.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64053, 3.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_s2s_loss=0.6205.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.20245, s2s_loss 0.38907, 60.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64051, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_s2s_loss=0.6206.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.34641, s2s_loss 0.38974, 60.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64069, 3.25 secs\n",
      "\u001b[1m---- Epoch 28/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.24821, s2s_loss 0.37994, 60.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64056, 3.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_28_s2s_loss=0.6211.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 29/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41568, s2s_loss 0.38493, 60.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64114, 3.22 secs\n",
      "\u001b[1m---- Epoch 30/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29379, s2s_loss 0.38485, 58.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64054, 3.24 secs\n",
      "\u001b[1m---- Epoch 31/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41917, s2s_loss 0.38978, 60.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.64009, 3.27 secs\n",
      "\u001b[1m---- Epoch 32/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.42368, s2s_loss 0.39355, 61.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63921, 3.25 secs\n",
      "\u001b[1m---- Epoch 33/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41381, s2s_loss 0.38334, 60.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63928, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_s2s_loss=0.6213.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63614, s2s_loss 0.38432, 60.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63883, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_34_s2s_loss=0.6214.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 35/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.24128, s2s_loss 0.38966, 60.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63896, 3.23 secs\n",
      "\u001b[1m---- Epoch 36/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.28533, s2s_loss 0.38649, 60.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63891, 3.22 secs\n",
      "\u001b[1m---- Epoch 37/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35470, s2s_loss 0.39585, 60.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63779, 3.23 secs\n",
      "\u001b[1m---- Epoch 38/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40047, s2s_loss 0.38896, 60.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63922, 3.21 secs\n",
      "\u001b[1m---- Epoch 39/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.32940, s2s_loss 0.38727, 60.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63678, 3.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_39_s2s_loss=0.6219.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 40/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.39756, s2s_loss 0.37938, 60.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63585, 3.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_s2s_loss=0.6227.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.33246, s2s_loss 0.37975, 60.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63669, 3.25 secs\n",
      "\u001b[1m---- Epoch 42/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.38908, s2s_loss 0.38392, 59.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63683, 3.23 secs\n",
      "\u001b[1m---- Epoch 43/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.51442, s2s_loss 0.38210, 60.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63663, 3.15 secs\n",
      "\u001b[1m---- Epoch 44/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63062, s2s_loss 0.38763, 60.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63660, 3.21 secs\n",
      "\u001b[1m---- Epoch 45/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33836, s2s_loss 0.38175, 60.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63559, 3.18 secs\n",
      "\u001b[1m---- Epoch 46/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.51675, s2s_loss 0.38998, 60.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63837, 3.18 secs\n",
      "\u001b[1m---- Epoch 47/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.46593, s2s_loss 0.37762, 60.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63572, 3.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_s2s_loss=0.6228.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.28382, s2s_loss 0.39100, 60.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63432, 3.17 secs\n",
      "\u001b[1m---- Epoch 49/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54265, s2s_loss 0.38854, 60.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63487, 3.21 secs\n",
      "\u001b[1m---- Epoch 50/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.37359, s2s_loss 0.39631, 60.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63511, 3.21 secs\n",
      "\u001b[1m---- Epoch 51/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.49220, s2s_loss 0.38544, 60.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63485, 3.23 secs\n",
      "\u001b[1m---- Epoch 52/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42711, s2s_loss 0.39060, 61.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63495, 3.21 secs\n",
      "\u001b[1m---- Epoch 53/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.24509, s2s_loss 0.38187, 60.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62869, 3.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_53_s2s_loss=0.6250.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 54/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.30973, s2s_loss 0.39009, 49.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63204, 3.20 secs\n",
      "\u001b[1m---- Epoch 55/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.28749, s2s_loss 0.38545, 60.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63191, 3.22 secs\n",
      "\u001b[1m---- Epoch 56/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29169, s2s_loss 0.38941, 60.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63119, 3.21 secs\n",
      "\u001b[1m---- Epoch 57/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.16402, s2s_loss 0.38300, 57.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63118, 3.21 secs\n",
      "\u001b[1m---- Epoch 58/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.45086, s2s_loss 0.38881, 60.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63073, 3.28 secs\n",
      "\u001b[1m---- Epoch 59/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.23313, s2s_loss 0.39251, 61.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63064, 3.24 secs\n",
      "\u001b[1m---- Epoch 60/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.37844, s2s_loss 0.38665, 60.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63074, 3.23 secs\n",
      "\u001b[1m---- Epoch 61/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.22875, s2s_loss 0.38150, 50.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63586, 3.20 secs\n",
      "\u001b[1m---- Epoch 62/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.31343, s2s_loss 0.38896, 55.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63340, 3.25 secs\n",
      "\u001b[1m---- Epoch 63/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.54385, s2s_loss 0.38900, 60.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63128, 3.22 secs\n",
      "\u001b[1m---- Epoch 64/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.37063, s2s_loss 0.38337, 60.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63092, 3.21 secs\n",
      "\u001b[1m---- Epoch 65/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.44993, s2s_loss 0.37931, 60.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63058, 3.19 secs\n",
      "\u001b[1m---- Epoch 66/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31195, s2s_loss 0.38612, 60.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63082, 3.19 secs\n",
      "\u001b[1m---- Epoch 67/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.27844, s2s_loss 0.38469, 60.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63065, 3.20 secs\n",
      "\u001b[1m---- Epoch 68/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.36705, s2s_loss 0.38342, 60.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63073, 3.20 secs\n",
      "\u001b[1m---- Epoch 69/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44042, s2s_loss 0.38299, 60.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63261, 3.25 secs\n",
      "\u001b[1m---- Epoch 70/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40554, s2s_loss 0.38309, 60.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63108, 3.25 secs\n",
      "\u001b[1m---- Epoch 71/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.29207, s2s_loss 0.37409, 60.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63061, 3.29 secs\n",
      "\u001b[1m---- Epoch 72/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35547, s2s_loss 0.38037, 60.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62926, 3.21 secs\n",
      "\u001b[1m---- Epoch 73/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39657, s2s_loss 0.38568, 60.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62921, 3.34 secs\n",
      "\u001b[1m---- Epoch 74/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62169, s2s_loss 0.38127, 60.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62914, 3.33 secs\n",
      "\u001b[1m---- Epoch 75/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.19894, s2s_loss 0.38215, 61.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62928, 3.20 secs\n",
      "\u001b[1m---- Epoch 76/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43206, s2s_loss 0.38564, 60.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62930, 3.21 secs\n",
      "\u001b[1m---- Epoch 77/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.25988, s2s_loss 0.39164, 60.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.63041, 3.24 secs\n",
      "\u001b[1m---- Epoch 78/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37416, s2s_loss 0.38421, 60.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62650, 3.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_78_s2s_loss=0.6256.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 79/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.59849, s2s_loss 0.38517, 60.42 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62652, 3.21 secs\n",
      "\u001b[1m---- Epoch 80/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.38662, s2s_loss 0.39087, 60.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62597, 3.24 secs\n",
      "\u001b[1m---- Epoch 81/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38332, s2s_loss 0.38575, 60.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62681, 3.23 secs\n",
      "\u001b[1m---- Epoch 82/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.33715, s2s_loss 0.38398, 60.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62696, 3.21 secs\n",
      "\u001b[1m---- Epoch 83/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.45967, s2s_loss 0.37956, 61.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62686, 3.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_83_s2s_loss=0.6257.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 84/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.18823, s2s_loss 0.38586, 60.81 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.62692, 3.24 secs\n",
      "\u001b[1m---- Epoch 85/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.47707, s2s_loss 0.38919, 60.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62787, 3.24 secs\n",
      "\u001b[1m---- Epoch 86/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37532, s2s_loss 0.38392, 60.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62889, 3.22 secs\n",
      "\u001b[1m---- Epoch 87/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "loss 0.31108, s2s_loss 0.39002, 60.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62751, 3.24 secs\n",
      "\u001b[1m---- Epoch 88/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.38570, s2s_loss 0.38713, 61.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62717, 3.20 secs\n",
      "\u001b[1m---- Epoch 89/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41858, s2s_loss 0.39036, 55.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62732, 3.21 secs\n",
      "\u001b[1m---- Epoch 90/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.53659, s2s_loss 0.38411, 60.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62706, 3.22 secs\n",
      "\u001b[1m---- Epoch 91/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.53214, s2s_loss 0.38198, 60.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62695, 3.25 secs\n",
      "\u001b[1m---- Epoch 92/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38379, s2s_loss 0.38224, 60.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62706, 3.21 secs\n",
      "\u001b[1m---- Epoch 93/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.31838, s2s_loss 0.38320, 60.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62679, 3.21 secs\n",
      "\u001b[1m---- Epoch 94/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.37657, s2s_loss 0.38361, 60.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62450, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_94_s2s_loss=0.6263.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 95/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.34212, s2s_loss 0.38707, 60.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62539, 3.21 secs\n",
      "\u001b[1m---- Epoch 96/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.47251, s2s_loss 0.38721, 60.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62571, 3.20 secs\n",
      "\u001b[1m---- Epoch 97/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.38926, s2s_loss 0.38617, 60.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62516, 3.20 secs\n",
      "\u001b[1m---- Epoch 98/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29218, s2s_loss 0.37957, 60.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62481, 3.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_98_s2s_loss=0.6264.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 99/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41205, s2s_loss 0.38508, 60.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62485, 3.24 secs\n",
      "\u001b[1m---- Epoch 100/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.52190, s2s_loss 0.38258, 60.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62469, 3.23 secs\n",
      "\u001b[1m---- Epoch 101/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42057, s2s_loss 0.38855, 60.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62555, 3.20 secs\n",
      "\u001b[1m---- Epoch 102/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.28930, s2s_loss 0.38146, 60.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62182, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_102_s2s_loss=0.6273.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 103/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.32613, s2s_loss 0.37872, 60.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62332, 3.23 secs\n",
      "\u001b[1m---- Epoch 104/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.33378, s2s_loss 0.39051, 61.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62194, 3.22 secs\n",
      "\u001b[1m---- Epoch 105/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.28509, s2s_loss 0.37702, 60.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62222, 3.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_105_s2s_loss=0.6274.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 106/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31589, s2s_loss 0.38815, 60.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62253, 3.31 secs\n",
      "\u001b[1m---- Epoch 107/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.50512, s2s_loss 0.38202, 54.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62271, 3.22 secs\n",
      "\u001b[1m---- Epoch 108/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.45595, s2s_loss 0.36863, 58.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62269, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_108_s2s_loss=0.6277.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 109/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.37544, s2s_loss 0.38201, 61.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62625, 3.22 secs\n",
      "\u001b[1m---- Epoch 110/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.36988, s2s_loss 0.37725, 60.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62758, 3.26 secs\n",
      "\u001b[1m---- Epoch 111/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.64902, s2s_loss 0.38298, 60.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62536, 3.23 secs\n",
      "\u001b[1m---- Epoch 112/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.17436, s2s_loss 0.38077, 60.73 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62376, 3.21 secs\n",
      "\u001b[1m---- Epoch 113/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.47113, s2s_loss 0.37146, 60.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62429, 3.21 secs\n",
      "\u001b[1m---- Epoch 114/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41162, s2s_loss 0.37932, 60.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62414, 3.25 secs\n",
      "\u001b[1m---- Epoch 115/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.49345, s2s_loss 0.38085, 56.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62400, 3.23 secs\n",
      "\u001b[1m---- Epoch 116/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.24711, s2s_loss 0.38111, 60.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62394, 3.21 secs\n",
      "\u001b[1m---- Epoch 117/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35285, s2s_loss 0.38017, 61.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61817, 3.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_117_s2s_loss=0.6286.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 118/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.53718, s2s_loss 0.38171, 61.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62066, 3.21 secs\n",
      "\u001b[1m---- Epoch 119/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41375, s2s_loss 0.38139, 60.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62148, 3.21 secs\n",
      "\u001b[1m---- Epoch 120/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.39612, s2s_loss 0.37415, 60.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62020, 3.23 secs\n",
      "\u001b[1m---- Epoch 121/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.22612, s2s_loss 0.37752, 60.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62029, 3.22 secs\n",
      "\u001b[1m---- Epoch 122/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.38201, s2s_loss 0.38084, 60.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61999, 3.20 secs\n",
      "\u001b[1m---- Epoch 123/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.50917, s2s_loss 0.38610, 60.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61996, 3.21 secs\n",
      "\u001b[1m---- Epoch 124/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.18086, s2s_loss 0.38155, 60.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62001, 3.25 secs\n",
      "\u001b[1m---- Epoch 125/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.40938, s2s_loss 0.37562, 59.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62254, 3.22 secs\n",
      "\u001b[1m---- Epoch 126/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.20609, s2s_loss 0.37244, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62185, 3.24 secs\n",
      "\u001b[1m---- Epoch 127/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.25116, s2s_loss 0.38399, 60.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62285, 3.23 secs\n",
      "\u001b[1m---- Epoch 128/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.35443, s2s_loss 0.38153, 60.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62167, 3.21 secs\n",
      "\u001b[1m---- Epoch 129/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.22378, s2s_loss 0.38124, 61.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62128, 3.55 secs\n",
      "\u001b[1m---- Epoch 130/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42269, s2s_loss 0.38019, 57.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62106, 3.47 secs\n",
      "\u001b[1m---- Epoch 131/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.54970, s2s_loss 0.38883, 56.21 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.62096, 3.66 secs\n",
      "\u001b[1m---- Epoch 132/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.75617, s2s_loss 0.38145, 57.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62089, 3.72 secs\n",
      "\u001b[1m---- Epoch 133/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.39605, s2s_loss 0.38356, 58.46 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61883, 3.40 secs\n",
      "\u001b[1m---- Epoch 134/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.21079, s2s_loss 0.37877, 57.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62004, 3.65 secs\n",
      "\u001b[1m---- Epoch 135/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.27942, s2s_loss 0.37716, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61945, 6.07 secs\n",
      "\u001b[1m---- Epoch 136/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.40804, s2s_loss 0.36951, 57.97 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61949, 3.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_136_s2s_loss=0.6287.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 137/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54057, s2s_loss 0.37769, 59.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61911, 4.10 secs\n",
      "\u001b[1m---- Epoch 138/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.46912, s2s_loss 0.37742, 60.69 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61915, 4.59 secs\n",
      "\u001b[1m---- Epoch 139/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.22306, s2s_loss 0.37599, 61.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61924, 4.33 secs\n",
      "\u001b[1m---- Epoch 140/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.32280, s2s_loss 0.37515, 61.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61929, 4.18 secs\n",
      "\u001b[1m---- Epoch 141/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.28422, s2s_loss 0.37808, 62.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.62095, 4.39 secs\n",
      "\u001b[1m---- Epoch 142/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.38028, s2s_loss 0.36789, 60.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61743, 4.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_142_s2s_loss=0.6295.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 143/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.45795, s2s_loss 0.38338, 61.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61697, 4.16 secs\n",
      "\u001b[1m---- Epoch 144/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.43193, s2s_loss 0.38054, 62.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61589, 4.61 secs\n",
      "\u001b[1m---- Epoch 145/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.49354, s2s_loss 0.37892, 63.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61590, 4.39 secs\n",
      "\u001b[1m---- Epoch 146/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.15537, s2s_loss 0.38000, 64.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61570, 4.50 secs\n",
      "\u001b[1m---- Epoch 147/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46113, s2s_loss 0.37897, 65.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61573, 4.20 secs\n",
      "\u001b[1m---- Epoch 148/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43071, s2s_loss 0.37557, 64.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61565, 4.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_148_s2s_loss=0.6297.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 149/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.35835, s2s_loss 0.38147, 62.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61992, 4.12 secs\n",
      "\u001b[1m---- Epoch 150/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.28195, s2s_loss 0.38072, 63.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61661, 4.14 secs\n",
      "\u001b[1m---- Epoch 151/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.49777, s2s_loss 0.38298, 67.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61841, 4.33 secs\n",
      "\u001b[1m---- Epoch 152/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.54304, s2s_loss 0.38271, 68.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61751, 4.77 secs\n",
      "\u001b[1m---- Epoch 153/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.32310, s2s_loss 0.37556, 65.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61789, 4.05 secs\n",
      "\u001b[1m---- Epoch 154/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29965, s2s_loss 0.38034, 63.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61744, 4.20 secs\n",
      "\u001b[1m---- Epoch 155/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43923, s2s_loss 0.38018, 62.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61768, 4.37 secs\n",
      "\u001b[1m---- Epoch 156/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.48650, s2s_loss 0.37588, 63.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61760, 4.50 secs\n",
      "\u001b[1m---- Epoch 157/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.73097, s2s_loss 0.38269, 62.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61168, 4.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_157_s2s_loss=0.6307.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 158/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.34042, s2s_loss 0.37555, 65.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61305, 4.40 secs\n",
      "\u001b[1m---- Epoch 159/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.48447, s2s_loss 0.37798, 66.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61372, 4.88 secs\n",
      "\u001b[1m---- Epoch 160/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.48730, s2s_loss 0.37808, 70.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61428, 3.89 secs\n",
      "\u001b[1m---- Epoch 161/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.60263, s2s_loss 0.37906, 67.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61380, 4.41 secs\n",
      "\u001b[1m---- Epoch 162/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35463, s2s_loss 0.37889, 66.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61356, 4.29 secs\n",
      "\u001b[1m---- Epoch 163/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.47185, s2s_loss 0.37731, 63.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61360, 4.44 secs\n",
      "\u001b[1m---- Epoch 164/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31998, s2s_loss 0.37961, 64.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61374, 4.35 secs\n",
      "\u001b[1m---- Epoch 165/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36112, s2s_loss 0.38457, 63.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61718, 4.15 secs\n",
      "\u001b[1m---- Epoch 166/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.55046, s2s_loss 0.37215, 63.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61679, 4.33 secs\n",
      "\u001b[1m---- Epoch 167/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.12184, s2s_loss 0.37126, 63.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61702, 4.22 secs\n",
      "\u001b[1m---- Epoch 168/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.26963, s2s_loss 0.37479, 63.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61654, 4.05 secs\n",
      "\u001b[1m---- Epoch 169/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.37651, s2s_loss 0.37623, 62.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61560, 4.18 secs\n",
      "\u001b[1m---- Epoch 170/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41505, s2s_loss 0.38077, 67.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61529, 4.53 secs\n",
      "\u001b[1m---- Epoch 171/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.50743, s2s_loss 0.37169, 62.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61541, 4.66 secs\n",
      "\u001b[1m---- Epoch 172/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.29345, s2s_loss 0.38169, 65.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61537, 4.31 secs\n",
      "\u001b[1m---- Epoch 173/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.34302, s2s_loss 0.37549, 64.79 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61274, 4.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_173_s2s_loss=0.6308.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 174/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.36585, s2s_loss 0.36940, 64.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61413, 4.43 secs\n",
      "\u001b[1m---- Epoch 175/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.34523, s2s_loss 0.37631, 61.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61269, 5.31 secs\n",
      "\u001b[1m---- Epoch 176/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.33263, s2s_loss 0.37311, 64.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61289, 4.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_176_s2s_loss=0.6308.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 177/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.40342, s2s_loss 0.37692, 66.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61247, 5.12 secs\n",
      "\u001b[1m---- Epoch 178/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.34514, s2s_loss 0.37774, 68.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61251, 5.01 secs\n",
      "\u001b[1m---- Epoch 179/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32545, s2s_loss 0.38074, 70.76 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.61249, 5.20 secs\n",
      "\u001b[1m---- Epoch 180/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38914, s2s_loss 0.38092, 68.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61248, 4.26 secs\n",
      "\u001b[1m---- Epoch 181/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.38594, s2s_loss 0.37727, 63.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61008, 4.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_181_s2s_loss=0.6316.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 182/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.34580, s2s_loss 0.37412, 62.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61342, 4.48 secs\n",
      "\u001b[1m---- Epoch 183/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.31278, s2s_loss 0.37587, 61.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61153, 4.19 secs\n",
      "\u001b[1m---- Epoch 184/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29615, s2s_loss 0.37161, 64.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61185, 3.98 secs\n",
      "\u001b[1m---- Epoch 185/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.33980, s2s_loss 0.37158, 64.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61185, 5.45 secs\n",
      "\u001b[1m---- Epoch 186/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.22050, s2s_loss 0.37982, 63.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61175, 4.52 secs\n",
      "\u001b[1m---- Epoch 187/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.55831, s2s_loss 0.37020, 63.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61156, 4.79 secs\n",
      "\u001b[1m---- Epoch 188/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42512, s2s_loss 0.36758, 64.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61149, 3.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_188_s2s_loss=0.6316.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 189/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.43179, s2s_loss 0.38040, 65.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60992, 4.11 secs\n",
      "\u001b[1m---- Epoch 190/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.36317, s2s_loss 0.37424, 67.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61108, 4.52 secs\n",
      "\u001b[1m---- Epoch 191/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.46411, s2s_loss 0.37021, 65.20 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61142, 4.21 secs\n",
      "\u001b[1m---- Epoch 192/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.21435, s2s_loss 0.37521, 65.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61109, 3.96 secs\n",
      "\u001b[1m---- Epoch 193/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.39941, s2s_loss 0.36916, 66.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61226, 3.98 secs\n",
      "\u001b[1m---- Epoch 194/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.58737, s2s_loss 0.37777, 66.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61205, 5.42 secs\n",
      "\u001b[1m---- Epoch 195/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32256, s2s_loss 0.37433, 69.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61185, 5.60 secs\n",
      "\u001b[1m---- Epoch 196/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.39785, s2s_loss 0.37745, 68.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61188, 4.43 secs\n",
      "\u001b[1m---- Epoch 197/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41000, s2s_loss 0.37932, 65.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60579, 4.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_197_s2s_loss=0.6330.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 198/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.26943, s2s_loss 0.37714, 61.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60999, 4.92 secs\n",
      "\u001b[1m---- Epoch 199/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.34661, s2s_loss 0.37942, 63.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60846, 4.50 secs\n",
      "\u001b[1m---- Epoch 200/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.49141, s2s_loss 0.37463, 63.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60897, 4.41 secs\n",
      "\u001b[1m---- Epoch 201/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.22005, s2s_loss 0.37492, 62.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60862, 4.35 secs\n",
      "\u001b[1m---- Epoch 202/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.44511, s2s_loss 0.38291, 63.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60847, 4.80 secs\n",
      "\u001b[1m---- Epoch 203/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.26476, s2s_loss 0.37589, 65.06 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60837, 4.32 secs\n",
      "\u001b[1m---- Epoch 204/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.74249, s2s_loss 0.37937, 64.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60841, 4.30 secs\n",
      "\u001b[1m---- Epoch 205/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.36987, s2s_loss 0.37959, 65.28 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60749, 4.91 secs\n",
      "\u001b[1m---- Epoch 206/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.29020, s2s_loss 0.37299, 69.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.61009, 4.99 secs\n",
      "\u001b[1m---- Epoch 207/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.25195, s2s_loss 0.37369, 68.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60812, 4.77 secs\n",
      "\u001b[1m---- Epoch 208/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.29918, s2s_loss 0.37569, 66.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60553, 4.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_208_s2s_loss=0.6333.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 209/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.56423, s2s_loss 0.37420, 63.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60617, 4.01 secs\n",
      "\u001b[1m---- Epoch 210/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.50248, s2s_loss 0.37365, 62.88 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60611, 4.46 secs\n",
      "\u001b[1m---- Epoch 211/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.38380, s2s_loss 0.37894, 68.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60618, 4.48 secs\n",
      "\u001b[1m---- Epoch 212/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40816, s2s_loss 0.37674, 68.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60606, 4.63 secs\n",
      "\u001b[1m---- Epoch 213/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.20240, s2s_loss 0.38396, 68.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60638, 4.34 secs\n",
      "\u001b[1m---- Epoch 214/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.42006, s2s_loss 0.36614, 65.50 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60761, 4.31 secs\n",
      "\u001b[1m---- Epoch 215/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.39656, s2s_loss 0.37243, 64.57 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60635, 4.18 secs\n",
      "\u001b[1m---- Epoch 216/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41003, s2s_loss 0.37678, 63.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60630, 4.47 secs\n",
      "\u001b[1m---- Epoch 217/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.62900, s2s_loss 0.37584, 65.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60599, 4.54 secs\n",
      "\u001b[1m---- Epoch 218/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32636, s2s_loss 0.38038, 68.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60577, 5.16 secs\n",
      "\u001b[1m---- Epoch 219/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31580, s2s_loss 0.37202, 68.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60562, 4.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_219_s2s_loss=0.6334.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 220/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46108, s2s_loss 0.38346, 66.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60556, 4.27 secs\n",
      "\u001b[1m---- Epoch 221/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.23731, s2s_loss 0.37341, 64.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60642, 4.38 secs\n",
      "\u001b[1m---- Epoch 222/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.69936, s2s_loss 0.37887, 64.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60645, 4.64 secs\n",
      "\u001b[1m---- Epoch 223/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.40694, s2s_loss 0.37422, 64.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60698, 4.32 secs\n",
      "\u001b[1m---- Epoch 224/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.20300, s2s_loss 0.37601, 64.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60605, 4.32 secs\n",
      "\u001b[1m---- Epoch 225/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.58359, s2s_loss 0.37096, 63.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60581, 4.33 secs\n",
      "\u001b[1m---- Epoch 226/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35117, s2s_loss 0.38275, 65.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60537, 5.52 secs\n",
      "\u001b[1m---- Epoch 227/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32410, s2s_loss 0.36819, 64.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60508, 4.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_227_s2s_loss=0.6338.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 228/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.40325, s2s_loss 0.38044, 64.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60509, 4.34 secs\n",
      "\u001b[1m---- Epoch 229/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.29488, s2s_loss 0.36713, 64.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60491, 4.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_229_s2s_loss=0.6339.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 230/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.40831, s2s_loss 0.37191, 65.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60619, 4.30 secs\n",
      "\u001b[1m---- Epoch 231/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.19697, s2s_loss 0.37599, 64.43 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60650, 4.44 secs\n",
      "\u001b[1m---- Epoch 232/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.31431, s2s_loss 0.37013, 64.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60564, 4.05 secs\n",
      "\u001b[1m---- Epoch 233/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.60129, s2s_loss 0.36427, 66.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.60503, 4.97 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_233_s2s_loss=0.6340.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 234/250\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20231231_134624_multitask(s2f+f2m+f2c+s2co+s2cal+nli)_Seq2Seq(t5-small)\" \\\n",
    "--epochs 250 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 23 \\\n",
    "--num_workers 2 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--task_name \"multitask\" \\\n",
    "--experiment_name \"s2f+f2m+f2c+s2co+s2cal+nli\" \\\n",
    "--multitask_name_list \\\n",
    "\"nli\" \\\n",
    "\"sentence2facts\" \\\n",
    "\"fact2metadata\" \\\n",
    "\"fact2comparison\" \\\n",
    "\"sentence2chestimagenome_observations\" \\\n",
    "\"sentence2chestimagenome_anatomical_locations\" \\\n",
    "--task2weight '{\"sentence2facts\": 1.0, \"fact2metadata\": 1.0, \"fact2comparison\": 0.3, \"sentence2chestimagenome_observations\": 1.0, \"sentence2chestimagenome_anatomical_locations\": 1.0, \"nli\": 5.0}' \\\n",
    "--sentence_to_facts_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_facts_from_sentences(cluster-balanced,hardest).jsonl\"\\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_facts_from_sentences(cluster-balanced,hardest)_part2.jsonl\" \\\n",
    "--fact_to_metadata_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "--fact_to_comparison_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "--chest_imagenome_phrases2labels_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/chest_imagenome/phrases2labels(num_obs=76,num_anat=39,num_phrases=556111).pkl\" \\\n",
    "--chest_imagenome_obs_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_chest_imagenome_labels_from_sentences__top5000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__top20000_most_difficult__offset=5000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__1of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_labels_from_sentences__skip_top20000__40000_uniform__2of2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_observations_from_sentences(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--chest_imagenome_anatloc_input_output_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top25000_most_difficult.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__top50000_most_difficult__offset=25000.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_chest_imagenome_anatomies_from_sentences__skip_top50000_most_difficult__uniform.jsonl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--use_sentence2facts_for_nli \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--paraphrased_inputs_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--seq2seq_model_name \"t5\" \\\n",
    "--t5_model_name \"t5-small\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
