{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25c052a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batch_size: 40\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 512\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: True\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  freeze_huggingface_model: True\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Freezing huggingface model\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 14049\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Here she was noted to follow simple comands but was combative prompting sedation and intubation.\n",
      "Hypothesis:  She was alert and oriented x 3. \n",
      "Label: contradiction\n",
      "Number of val samples: 480\n",
      "\u001b[1mExample val text:\u001b[0m\n",
      "Premise: No acute focal pneumonia or pleural effusion.\n",
      "Hypothesis: Again noted are diffuse infiltrative parenchymal opacities, right worse than left; this is largely due to pulmonary edema and the right-sided pleural effusion, but underlying pneumonia cannot be excluded.\n",
      "Label: contradiction\n",
      "\u001b[1mIncluding half of the RadNLI dev samples in the train set...\u001b[0m\n",
      "Number of train samples: 14288\n",
      "Number of val samples: 241\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: Improved aeration at the lung bases with residual subsegmental atelectasis at the left lung base.\n",
      "Hypothesis: There are likely patchy opacities in the lung bases reflective of atelectasis.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding val triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test triplet ranking dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_81_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9542.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_81_cacc+chf1+chf1+cscc+hscc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9542.pt\n",
      "\u001b[93mWarning: model state dict has 215 keys, loaded state dict has 223 keys, intersection has 211 keys, union has 227 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in model but not in loaded state dict:\u001b[0m\n",
      "\u001b[93m  nli_classifier.weight\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  nli_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_201528_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.09662, nli_loss 1.09662, nli_acc 0.39040, 20.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41494, 0.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.4125.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.09193, nli_loss 1.09193, nli_acc 0.46046, 21.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.40664, 0.57 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 1.07166, nli_loss 1.07166, nli_acc 0.51190, 20.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34025, 0.61 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.99213, nli_loss 0.99213, nli_acc 0.54129, 20.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.31535, 0.56 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.93346, nli_loss 0.93346, nli_acc 0.55571, 20.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.31535, 0.56 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.92253, nli_loss 0.92253, nli_acc 0.56019, 20.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.32780, 0.56 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.91924, nli_loss 0.91924, nli_acc 0.56033, 20.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34855, 0.55 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.91784, nli_loss 0.91784, nli_acc 0.56033, 20.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.34855, 0.52 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.91559, nli_loss 0.91559, nli_acc 0.56096, 20.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.61 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.91767, nli_loss 0.91767, nli_acc 0.55725, 19.69 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.57 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.91472, nli_loss 0.91472, nli_acc 0.55802, 20.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.53 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.91299, nli_loss 0.91299, nli_acc 0.56215, 20.22 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.58 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.90900, nli_loss 0.90900, nli_acc 0.56110, 20.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.39834, 0.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc=0.4146.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.90211, nli_loss 0.90211, nli_acc 0.56698, 20.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.40249, 0.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc=0.4189.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.89535, nli_loss 0.89535, nli_acc 0.57335, 20.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.42324, 0.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_nli_acc=0.4382.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.89399, nli_loss 0.89399, nli_acc 0.57370, 19.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.57 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.89253, nli_loss 0.89253, nli_acc 0.57328, 20.33 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.61 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.89141, nli_loss 0.89141, nli_acc 0.57524, 20.63 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.61 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.89427, nli_loss 0.89427, nli_acc 0.57517, 20.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.57 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.89197, nli_loss 0.89197, nli_acc 0.57440, 20.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.41909, 0.59 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.88483, nli_loss 0.88483, nli_acc 0.57727, 20.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.61 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.87790, nli_loss 0.87790, nli_acc 0.57993, 20.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.35685, 0.59 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.87681, nli_loss 0.87681, nli_acc 0.57874, 20.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.59 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.87343, nli_loss 0.87343, nli_acc 0.58182, 20.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.62 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.87219, nli_loss 0.87219, nli_acc 0.58455, 20.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36100, 0.56 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.87034, nli_loss 0.87034, nli_acc 0.58490, 20.24 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.54 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.87080, nli_loss 0.87080, nli_acc 0.58532, 20.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.57 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.87304, nli_loss 0.87304, nli_acc 0.58042, 20.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.36515, 0.64 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "^C iteration 10025\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 343, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 263, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 155, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 60, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 39, in step_fn\n",
      "    logits = model(premises_encoding, hypotheses_encoding)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 46, in forward\n",
      "    h_vectors = self._text_to_embedding_func(hypotheses_encoding)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/nli.py\", line 30, in <lambda>\n",
      "    self._text_to_embedding_func = lambda x: self.model.get_projected_text_embeddings(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1358, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1020, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 610, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 495, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 425, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 323, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230916_194059_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 100 \\\n",
    "--batch_size 40 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--freeze_huggingface_model \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbc9c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 50\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 512\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 14049\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: Following her cardiac catheterization, the patient continued to be hypoxic and hypotensive requiring Dopamine and five liters nasal cannula oxygen.\n",
      "Hypothesis:  The patient had a heart attack\n",
      "Label: neutral\n",
      "Number of val samples: 480\n",
      "\u001b[1mExample val text:\u001b[0m\n",
      "Premise: Small area of parenchymal sparing in the left upper lobe is unchanged.\n",
      "Hypothesis: No newly appeared parenchymal opacities.\n",
      "Label: neutral\n",
      "\u001b[1mIncluding half of the RadNLI dev samples in the train set...\u001b[0m\n",
      "Number of train samples: 14431\n",
      "Number of val samples: 98\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: Low lung volumes with little overall change in the appearance of the heart and lungs.\n",
      "Hypothesis: Low lung volumes accentuate the pulmonary vascular structures.\n",
      "Label: neutral\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding val triplet ranking dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test triplet ranking dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_13_nli_acc=0.6636.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/checkpoint_13_nli_acc=0.6636.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_211627_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,512)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.46760, nli_loss 0.46760, nli_acc 0.80757, 93.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68367, 0.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.6961.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.45626, nli_loss 0.45626, nli_acc 0.81256, 94.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_nli_acc=0.7057.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.43799, nli_loss 0.43799, nli_acc 0.82046, 93.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71429, 0.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nli_acc=0.7249.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.46279, nli_loss 0.46279, nli_acc 0.80556, 92.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.51 secs\n",
      "\u001b[1m---- Epoch 5/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.39927, nli_loss 0.39927, nli_acc 0.83549, 91.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72449, 0.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nli_acc=0.7356.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.34970, nli_loss 0.34970, nli_acc 0.85961, 93.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73469, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_nli_acc=0.7472.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.31957, nli_loss 0.31957, nli_acc 0.87208, 93.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71429, 0.44 secs\n",
      "\u001b[1m---- Epoch 8/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.30354, nli_loss 0.30354, nli_acc 0.87603, 90.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72449, 0.43 secs\n",
      "\u001b[1m---- Epoch 9/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.29936, nli_loss 0.29936, nli_acc 0.88053, 89.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.44 secs\n",
      "\u001b[1m---- Epoch 10/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.29454, nli_loss 0.29454, nli_acc 0.88137, 91.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.45 secs\n",
      "\u001b[1m---- Epoch 11/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.28951, nli_loss 0.28951, nli_acc 0.88719, 93.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.43 secs\n",
      "\u001b[1m---- Epoch 12/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.28311, nli_loss 0.28311, nli_acc 0.88837, 91.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.45 secs\n",
      "\u001b[1m---- Epoch 13/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.35399, nli_loss 0.35399, nli_acc 0.85226, 95.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.46 secs\n",
      "\u001b[1m---- Epoch 14/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.30348, nli_loss 0.30348, nli_acc 0.87659, 94.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74490, 0.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc=0.7581.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.25795, nli_loss 0.25795, nli_acc 0.89848, 95.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68367, 0.41 secs\n",
      "\u001b[1m---- Epoch 16/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.23823, nli_loss 0.23823, nli_acc 0.90708, 93.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.42 secs\n",
      "\u001b[1m---- Epoch 17/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.22737, nli_loss 0.22737, nli_acc 0.91165, 94.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.48 secs\n",
      "\u001b[1m---- Epoch 18/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.22223, nli_loss 0.22223, nli_acc 0.91352, 95.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.41 secs\n",
      "\u001b[1m---- Epoch 19/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.21676, nli_loss 0.21676, nli_acc 0.91546, 92.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.60 secs\n",
      "\u001b[1m---- Epoch 20/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.21354, nli_loss 0.21354, nli_acc 0.91775, 90.48 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.44 secs\n",
      "\u001b[1m---- Epoch 21/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.29496, nli_loss 0.29496, nli_acc 0.87887, 95.02 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69388, 0.47 secs\n",
      "\u001b[1m---- Epoch 22/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.24369, nli_loss 0.24369, nli_acc 0.90229, 94.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70408, 0.45 secs\n",
      "\u001b[1m---- Epoch 23/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.20478, nli_loss 0.20478, nli_acc 0.91920, 93.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66327, 0.41 secs\n",
      "\u001b[1m---- Epoch 24/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.18193, nli_loss 0.18193, nli_acc 0.93112, 93.19 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.46 secs\n",
      "\u001b[1m---- Epoch 25/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.17825, nli_loss 0.17825, nli_acc 0.93140, 95.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.44 secs\n",
      "\u001b[1m---- Epoch 26/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.16974, nli_loss 0.16974, nli_acc 0.93743, 93.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65306, 0.47 secs\n",
      "\u001b[1m---- Epoch 27/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "^C iteration 19100\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 343, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 263, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_NLI.py\", line 155, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 60, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/nli.py\", line 47, in step_fn\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230920_202753_NLI(MedNLI+RadNLI)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128)/\" \\\n",
    "--epochs 50 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ab52bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 150\n",
      "   batches_per_epoch: 800\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   nli_hidden_size: 512\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   merged_input: True\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(96629,13236055).jsonl\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  merged_input: True\n",
      "  hidden_size: 512\n",
      "  freeze_huggingface_model: False\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of train samples: 96629\n",
      "\u001b[1mExample train text:\u001b[0m\n",
      "Premise: The heart, mediastinal and hilar contours are normal without any lymphadenopathy.\n",
      "Hypothesis: The patient's heart and mediastinal structures show no abnormalities.\n",
      "Label: entailment\n",
      "Number of test samples: 480\n",
      "\u001b[1mExample test text:\u001b[0m\n",
      "Premise: There has been improvement in the pulmonary edema, now moderate.\n",
      "Hypothesis: There is no pulmonary edema.\n",
      "Label: contradiction\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "\u001b[1mBuilding test NLI dataset and dataloader...\u001b[0m\n",
      "nli_trainer.name =  NLI(MedNLI+RadNLI+GPT-4)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_190901_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,merged)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_190901_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,merged)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_94_cacc+chf1+chf1+cscc+hscc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9266.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_94_cacc+chf1+chf1+cscc+hscc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9266.pt\n",
      "\u001b[93mWarning: model state dict has 213 keys, loaded state dict has 227 keys, intersection has 213 keys, union has 227 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_190901_NLI(MedNLI+RadNLI+GPT-4)_BerBasedNLI(microsoft-BiomedVLP-CXR-BERT-specialized,128,merged)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.08139, nli_loss 1.08139, nli_acc 0.40175, 57.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.53958, 0.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc=0.5258.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.96889, nli_loss 0.96889, nli_acc 0.54094, 56.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.61875, 0.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_nli_acc=0.6110.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.72841, nli_loss 0.72841, nli_acc 0.69394, 55.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.57500, 0.66 secs\n",
      "\u001b[1m---- Epoch 4/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.61002, nli_loss 0.61002, nli_acc 0.74525, 59.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.57708, 0.63 secs\n",
      "\u001b[1m---- Epoch 5/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.52000, nli_loss 0.52000, nli_acc 0.78431, 55.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.59167, 0.64 secs\n",
      "\u001b[1m---- Epoch 6/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.48727, nli_loss 0.48727, nli_acc 0.79675, 58.85 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.63750, 0.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_nli_acc=0.6534.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.45744, nli_loss 0.45744, nli_acc 0.81250, 55.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65625, 0.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_nli_acc=0.6719.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.43166, nli_loss 0.43166, nli_acc 0.82488, 56.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65833, 0.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_nli_acc=0.6750.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.41901, nli_loss 0.41901, nli_acc 0.82962, 46.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.67917, 0.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_nli_acc=0.6942.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 10/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.35661, nli_loss 0.35661, nli_acc 0.86413, 54.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68125, 0.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_nli_acc=0.6995.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.35773, nli_loss 0.35773, nli_acc 0.86319, 56.20 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68125, 0.67 secs\n",
      "\u001b[1m---- Epoch 12/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.38316, nli_loss 0.38316, nli_acc 0.84913, 57.29 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68125, 0.65 secs\n",
      "\u001b[1m---- Epoch 13/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.45798, nli_loss 0.45798, nli_acc 0.81281, 57.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.68750, 0.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc=0.7000.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.43020, nli_loss 0.43020, nli_acc 0.82300, 58.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72083, 0.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc=0.7311.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.40709, nli_loss 0.40709, nli_acc 0.83456, 54.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_nli_acc=0.7641.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.35082, nli_loss 0.35082, nli_acc 0.86087, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72083, 0.64 secs\n",
      "\u001b[1m---- Epoch 17/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.35205, nli_loss 0.35205, nli_acc 0.86162, 55.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71458, 0.65 secs\n",
      "\u001b[1m---- Epoch 18/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.36892, nli_loss 0.36892, nli_acc 0.85431, 55.51 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.64 secs\n",
      "\u001b[1m---- Epoch 19/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.29465, nli_loss 0.29465, nli_acc 0.89356, 55.47 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.67 secs\n",
      "\u001b[1m---- Epoch 20/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.31498, nli_loss 0.31498, nli_acc 0.88050, 55.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73125, 0.65 secs\n",
      "\u001b[1m---- Epoch 21/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.40767, nli_loss 0.40767, nli_acc 0.83262, 56.12 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73333, 0.65 secs\n",
      "\u001b[1m---- Epoch 22/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.36535, nli_loss 0.36535, nli_acc 0.85700, 56.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.61458, 0.65 secs\n",
      "\u001b[1m---- Epoch 23/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.35502, nli_loss 0.35502, nli_acc 0.85919, 53.66 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70000, 0.68 secs\n",
      "\u001b[1m---- Epoch 24/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.36825, nli_loss 0.36825, nli_acc 0.85106, 55.92 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72708, 0.64 secs\n",
      "\u001b[1m---- Epoch 25/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.30581, nli_loss 0.30581, nli_acc 0.88500, 54.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72083, 0.67 secs\n",
      "\u001b[1m---- Epoch 26/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.32455, nli_loss 0.32455, nli_acc 0.87256, 57.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72083, 0.65 secs\n",
      "\u001b[1m---- Epoch 27/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.25645, nli_loss 0.25645, nli_acc 0.90506, 58.76 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.67 secs\n",
      "\u001b[1m---- Epoch 28/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.25975, nli_loss 0.25975, nli_acc 0.90738, 54.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.66 secs\n",
      "\u001b[1m---- Epoch 29/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.35712, nli_loss 0.35712, nli_acc 0.85819, 58.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72500, 0.65 secs\n",
      "\u001b[1m---- Epoch 30/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.37634, nli_loss 0.37634, nli_acc 0.84944, 56.94 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71667, 0.65 secs\n",
      "\u001b[1m---- Epoch 31/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.32181, nli_loss 0.32181, nli_acc 0.87450, 56.29 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.66 secs\n",
      "\u001b[1m---- Epoch 32/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.32547, nli_loss 0.32547, nli_acc 0.87250, 54.10 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_nli_acc=0.7679.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.26955, nli_loss 0.26955, nli_acc 0.89706, 57.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_nli_acc=0.7741.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.27372, nli_loss 0.27372, nli_acc 0.89469, 59.32 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_34_nli_acc=0.7776.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 35/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.22336, nli_loss 0.22336, nli_acc 0.91981, 54.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.64 secs\n",
      "\u001b[1m---- Epoch 36/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.26183, nli_loss 0.26183, nli_acc 0.90169, 56.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.64 secs\n",
      "\u001b[1m---- Epoch 37/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.32969, nli_loss 0.32969, nli_acc 0.87175, 55.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72708, 0.66 secs\n",
      "\u001b[1m---- Epoch 38/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.34439, nli_loss 0.34439, nli_acc 0.86275, 52.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71458, 0.66 secs\n",
      "\u001b[1m---- Epoch 39/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.28738, nli_loss 0.28738, nli_acc 0.88594, 57.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_39_nli_acc=0.7786.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 40/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.28350, nli_loss 0.28350, nli_acc 0.89331, 57.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.67 secs\n",
      "\u001b[1m---- Epoch 41/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.24273, nli_loss 0.24273, nli_acc 0.90744, 56.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_41_nli_acc=0.7882.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 42/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.27803, nli_loss 0.27803, nli_acc 0.89250, 59.67 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_42_nli_acc=0.7905.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 43/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.20186, nli_loss 0.20186, nli_acc 0.93144, 55.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_43_nli_acc=0.7925.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 44/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.22934, nli_loss 0.22934, nli_acc 0.91506, 55.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78333, 0.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_44_nli_acc=0.7965.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 45/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.29266, nli_loss 0.29266, nli_acc 0.88513, 56.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73333, 0.66 secs\n",
      "\u001b[1m---- Epoch 46/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.30029, nli_loss 0.30029, nli_acc 0.88244, 55.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.65 secs\n",
      "\u001b[1m---- Epoch 47/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.26525, nli_loss 0.26525, nli_acc 0.89687, 56.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.64 secs\n",
      "\u001b[1m---- Epoch 48/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.29464, nli_loss 0.29464, nli_acc 0.88300, 55.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.67 secs\n",
      "\u001b[1m---- Epoch 49/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.22730, nli_loss 0.22730, nli_acc 0.91844, 54.89 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.66 secs\n",
      "\u001b[1m---- Epoch 50/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.25441, nli_loss 0.25441, nli_acc 0.90225, 53.89 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.66 secs\n",
      "\u001b[1m---- Epoch 51/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.18366, nli_loss 0.18366, nli_acc 0.93637, 55.86 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nli_acc 0.77708, 0.64 secs\n",
      "\u001b[1m---- Epoch 52/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.20110, nli_loss 0.20110, nli_acc 0.92725, 58.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.65 secs\n",
      "\u001b[1m---- Epoch 53/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.26099, nli_loss 0.26099, nli_acc 0.89950, 53.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72708, 0.64 secs\n",
      "\u001b[1m---- Epoch 54/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.31370, nli_loss 0.31370, nli_acc 0.87731, 57.09 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.66 secs\n",
      "\u001b[1m---- Epoch 55/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.24473, nli_loss 0.24473, nli_acc 0.90687, 54.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77708, 0.65 secs\n",
      "\u001b[1m---- Epoch 56/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.27149, nli_loss 0.27149, nli_acc 0.89256, 54.78 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.64 secs\n",
      "\u001b[1m---- Epoch 57/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.20423, nli_loss 0.20423, nli_acc 0.92356, 55.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.66 secs\n",
      "\u001b[1m---- Epoch 58/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.22947, nli_loss 0.22947, nli_acc 0.91094, 56.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.68 secs\n",
      "\u001b[1m---- Epoch 59/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.15913, nli_loss 0.15913, nli_acc 0.94600, 55.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.64 secs\n",
      "\u001b[1m---- Epoch 60/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.20514, nli_loss 0.20514, nli_acc 0.92512, 57.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.59 secs\n",
      "\u001b[1m---- Epoch 61/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.25877, nli_loss 0.25877, nli_acc 0.89912, 55.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72708, 0.66 secs\n",
      "\u001b[1m---- Epoch 62/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.29657, nli_loss 0.29657, nli_acc 0.88350, 57.85 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73750, 0.67 secs\n",
      "\u001b[1m---- Epoch 63/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.22716, nli_loss 0.22716, nli_acc 0.91263, 52.29 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.65 secs\n",
      "\u001b[1m---- Epoch 64/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.24103, nli_loss 0.24103, nli_acc 0.90700, 52.05 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.66 secs\n",
      "\u001b[1m---- Epoch 65/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.18246, nli_loss 0.18246, nli_acc 0.93300, 50.85 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.65 secs\n",
      "\u001b[1m---- Epoch 66/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.22734, nli_loss 0.22734, nli_acc 0.91331, 57.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.65 secs\n",
      "\u001b[1m---- Epoch 67/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.15030, nli_loss 0.15030, nli_acc 0.94881, 53.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76250, 0.65 secs\n",
      "\u001b[1m---- Epoch 68/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.18737, nli_loss 0.18737, nli_acc 0.93244, 58.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.67 secs\n",
      "\u001b[1m---- Epoch 69/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.25149, nli_loss 0.25149, nli_acc 0.90144, 59.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.65 secs\n",
      "\u001b[1m---- Epoch 70/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.26389, nli_loss 0.26389, nli_acc 0.89725, 57.01 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.65 secs\n",
      "\u001b[1m---- Epoch 71/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.21156, nli_loss 0.21156, nli_acc 0.91881, 56.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73125, 0.67 secs\n",
      "\u001b[1m---- Epoch 72/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.25245, nli_loss 0.25245, nli_acc 0.89881, 58.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.65 secs\n",
      "\u001b[1m---- Epoch 73/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.18274, nli_loss 0.18274, nli_acc 0.93269, 59.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.66 secs\n",
      "\u001b[1m---- Epoch 74/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.21680, nli_loss 0.21680, nli_acc 0.91550, 60.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.65 secs\n",
      "\u001b[1m---- Epoch 75/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.14207, nli_loss 0.14207, nli_acc 0.95131, 56.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.65 secs\n",
      "\u001b[1m---- Epoch 76/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.16533, nli_loss 0.16533, nli_acc 0.94088, 57.62 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.65 secs\n",
      "\u001b[1m---- Epoch 77/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.22628, nli_loss 0.22628, nli_acc 0.91450, 55.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73333, 0.66 secs\n",
      "\u001b[1m---- Epoch 78/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.27117, nli_loss 0.27117, nli_acc 0.89212, 57.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.64 secs\n",
      "\u001b[1m---- Epoch 79/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.21045, nli_loss 0.21045, nli_acc 0.91950, 55.54 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.65 secs\n",
      "\u001b[1m---- Epoch 80/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.23192, nli_loss 0.23192, nli_acc 0.91156, 51.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.65 secs\n",
      "\u001b[1m---- Epoch 81/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.16639, nli_loss 0.16639, nli_acc 0.94006, 54.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.66 secs\n",
      "\u001b[1m---- Epoch 82/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.19322, nli_loss 0.19322, nli_acc 0.92631, 53.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.65 secs\n",
      "\u001b[1m---- Epoch 83/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.13163, nli_loss 0.13163, nli_acc 0.95637, 56.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.66 secs\n",
      "\u001b[1m---- Epoch 84/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.17021, nli_loss 0.17021, nli_acc 0.93775, 56.83 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.63 secs\n",
      "\u001b[1m---- Epoch 85/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.22256, nli_loss 0.22256, nli_acc 0.91431, 57.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73750, 0.65 secs\n",
      "\u001b[1m---- Epoch 86/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.25282, nli_loss 0.25282, nli_acc 0.90187, 56.11 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.64 secs\n",
      "\u001b[1m---- Epoch 87/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.18579, nli_loss 0.18579, nli_acc 0.92919, 56.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.65 secs\n",
      "\u001b[1m---- Epoch 88/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.20371, nli_loss 0.20371, nli_acc 0.92350, 57.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.67 secs\n",
      "\u001b[1m---- Epoch 89/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.14842, nli_loss 0.14842, nli_acc 0.94631, 55.69 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73542, 0.65 secs\n",
      "\u001b[1m---- Epoch 90/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.18540, nli_loss 0.18540, nli_acc 0.93037, 53.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.65 secs\n",
      "\u001b[1m---- Epoch 91/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.12456, nli_loss 0.12456, nli_acc 0.95856, 55.58 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.66 secs\n",
      "\u001b[1m---- Epoch 92/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.15071, nli_loss 0.15071, nli_acc 0.94725, 57.02 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.63 secs\n",
      "\u001b[1m---- Epoch 93/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.21058, nli_loss 0.21058, nli_acc 0.92075, 55.92 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.66 secs\n",
      "\u001b[1m---- Epoch 94/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.22576, nli_loss 0.22576, nli_acc 0.91175, 56.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.66 secs\n",
      "\u001b[1m---- Epoch 95/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.16591, nli_loss 0.16591, nli_acc 0.93800, 56.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69583, 0.67 secs\n",
      "\u001b[1m---- Epoch 96/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.20532, nli_loss 0.20532, nli_acc 0.92237, 57.23 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.66 secs\n",
      "\u001b[1m---- Epoch 97/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.15217, nli_loss 0.15217, nli_acc 0.94663, 56.15 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.66 secs\n",
      "\u001b[1m---- Epoch 98/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.17592, nli_loss 0.17592, nli_acc 0.93325, 55.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.65 secs\n",
      "\u001b[1m---- Epoch 99/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11785, nli_loss 0.11785, nli_acc 0.95956, 57.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.65 secs\n",
      "\u001b[1m---- Epoch 100/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.13814, nli_loss 0.13814, nli_acc 0.94975, 57.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77500, 0.67 secs\n",
      "\u001b[1m---- Epoch 101/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.19378, nli_loss 0.19378, nli_acc 0.92763, 55.43 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.66458, 0.66 secs\n",
      "\u001b[1m---- Epoch 102/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.23918, nli_loss 0.23918, nli_acc 0.90769, 56.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.64 secs\n",
      "\u001b[1m---- Epoch 103/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.17214, nli_loss 0.17214, nli_acc 0.93606, 57.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76875, 0.65 secs\n",
      "\u001b[1m---- Epoch 104/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.19074, nli_loss 0.19074, nli_acc 0.92863, 57.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.64 secs\n",
      "\u001b[1m---- Epoch 105/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.14022, nli_loss 0.14022, nli_acc 0.94837, 56.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.65 secs\n",
      "\u001b[1m---- Epoch 106/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.16768, nli_loss 0.16768, nli_acc 0.93812, 54.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.67 secs\n",
      "\u001b[1m---- Epoch 107/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.10726, nli_loss 0.10726, nli_acc 0.96525, 53.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.66 secs\n",
      "\u001b[1m---- Epoch 108/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.13938, nli_loss 0.13938, nli_acc 0.94956, 53.20 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.64 secs\n",
      "\u001b[1m---- Epoch 109/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.19123, nli_loss 0.19123, nli_acc 0.92744, 44.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.65 secs\n",
      "\u001b[1m---- Epoch 110/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.21771, nli_loss 0.21771, nli_acc 0.91812, 58.81 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.65 secs\n",
      "\u001b[1m---- Epoch 111/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.16548, nli_loss 0.16548, nli_acc 0.93819, 59.95 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_111_nli_acc=0.7969.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 112/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.17545, nli_loss 0.17545, nli_acc 0.93244, 53.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.78125, 0.65 secs\n",
      "\u001b[1m---- Epoch 113/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.13069, nli_loss 0.13069, nli_acc 0.95312, 55.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.65 secs\n",
      "\u001b[1m---- Epoch 114/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.15582, nli_loss 0.15582, nli_acc 0.94112, 58.11 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.66 secs\n",
      "\u001b[1m---- Epoch 115/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.10453, nli_loss 0.10453, nli_acc 0.96325, 57.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.69 secs\n",
      "\u001b[1m---- Epoch 116/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.12668, nli_loss 0.12668, nli_acc 0.95700, 57.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.65 secs\n",
      "\u001b[1m---- Epoch 117/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.17422, nli_loss 0.17422, nli_acc 0.93381, 55.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.66 secs\n",
      "\u001b[1m---- Epoch 118/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.19360, nli_loss 0.19360, nli_acc 0.92637, 59.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77292, 0.66 secs\n",
      "\u001b[1m---- Epoch 119/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.14712, nli_loss 0.14712, nli_acc 0.94844, 57.90 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.66 secs\n",
      "\u001b[1m---- Epoch 120/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.16997, nli_loss 0.16997, nli_acc 0.93719, 58.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74167, 0.66 secs\n",
      "\u001b[1m---- Epoch 121/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.12633, nli_loss 0.12633, nli_acc 0.95594, 55.41 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.71 secs\n",
      "\u001b[1m---- Epoch 122/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.15072, nli_loss 0.15072, nli_acc 0.94469, 59.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.65 secs\n",
      "\u001b[1m---- Epoch 123/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09415, nli_loss 0.09415, nli_acc 0.96937, 56.49 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75000, 0.65 secs\n",
      "\u001b[1m---- Epoch 124/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.11229, nli_loss 0.11229, nli_acc 0.96044, 53.38 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.66 secs\n",
      "\u001b[1m---- Epoch 125/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.15924, nli_loss 0.15924, nli_acc 0.94163, 55.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65000, 0.64 secs\n",
      "\u001b[1m---- Epoch 126/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.19333, nli_loss 0.19333, nli_acc 0.92650, 60.42 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72500, 0.67 secs\n",
      "\u001b[1m---- Epoch 127/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.14829, nli_loss 0.14829, nli_acc 0.94600, 58.30 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74375, 0.65 secs\n",
      "\u001b[1m---- Epoch 128/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.16522, nli_loss 0.16522, nli_acc 0.93769, 57.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.66 secs\n",
      "\u001b[1m---- Epoch 129/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11868, nli_loss 0.11868, nli_acc 0.95850, 56.06 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73958, 0.66 secs\n",
      "\u001b[1m---- Epoch 130/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.13666, nli_loss 0.13666, nli_acc 0.94831, 57.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75208, 0.66 secs\n",
      "\u001b[1m---- Epoch 131/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.08880, nli_loss 0.08880, nli_acc 0.97275, 57.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.65 secs\n",
      "\u001b[1m---- Epoch 132/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.10655, nli_loss 0.10655, nli_acc 0.96213, 57.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.64 secs\n",
      "\u001b[1m---- Epoch 133/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.17407, nli_loss 0.17407, nli_acc 0.93269, 57.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.65833, 0.66 secs\n",
      "\u001b[1m---- Epoch 134/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.19538, nli_loss 0.19538, nli_acc 0.92563, 51.17 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.64 secs\n",
      "\u001b[1m---- Epoch 135/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.13860, nli_loss 0.13860, nli_acc 0.94875, 57.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.64 secs\n",
      "\u001b[1m---- Epoch 136/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.14880, nli_loss 0.14880, nli_acc 0.94369, 55.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.65 secs\n",
      "\u001b[1m---- Epoch 137/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11174, nli_loss 0.11174, nli_acc 0.96137, 56.13 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.64 secs\n",
      "\u001b[1m---- Epoch 138/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.12818, nli_loss 0.12818, nli_acc 0.95444, 55.65 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75417, 0.65 secs\n",
      "\u001b[1m---- Epoch 139/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.08662, nli_loss 0.08662, nli_acc 0.97113, 54.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.65 secs\n",
      "\u001b[1m---- Epoch 140/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.10311, nli_loss 0.10311, nli_acc 0.96413, 54.14 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.66 secs\n",
      "\u001b[1m---- Epoch 141/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.15959, nli_loss 0.15959, nli_acc 0.94150, 52.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77917, 0.65 secs\n",
      "\u001b[1m---- Epoch 142/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.17033, nli_loss 0.17033, nli_acc 0.93494, 54.16 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.77083, 0.66 secs\n",
      "\u001b[1m---- Epoch 143/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.12904, nli_loss 0.12904, nli_acc 0.95125, 51.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74583, 0.65 secs\n",
      "\u001b[1m---- Epoch 144/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.14781, nli_loss 0.14781, nli_acc 0.94525, 54.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74792, 0.66 secs\n",
      "\u001b[1m---- Epoch 145/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11015, nli_loss 0.11015, nli_acc 0.95981, 56.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76458, 0.65 secs\n",
      "\u001b[1m---- Epoch 146/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.12538, nli_loss 0.12538, nli_acc 0.95562, 55.20 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75625, 0.66 secs\n",
      "\u001b[1m---- Epoch 147/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07899, nli_loss 0.07899, nli_acc 0.97337, 55.46 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76042, 0.63 secs\n",
      "\u001b[1m---- Epoch 148/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09379, nli_loss 0.09379, nli_acc 0.96613, 55.82 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.76667, 0.64 secs\n",
      "\u001b[1m---- Epoch 149/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.14775, nli_loss 0.14775, nli_acc 0.94650, 53.55 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72917, 0.64 secs\n",
      "\u001b[1m---- Epoch 150/150\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.17331, nli_loss 0.17331, nli_acc 0.93400, 52.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75833, 0.63 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_NLI.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230923_132315_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(96629,13236055).jsonl\" \\\n",
    "--epochs 150 \\\n",
    "--batches_per_epoch 800 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--merged_input \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
