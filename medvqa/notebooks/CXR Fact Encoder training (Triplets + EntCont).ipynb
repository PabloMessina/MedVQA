{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df2a794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 500\n",
      "   batch_size: 22\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 8\n",
      "   override_lr: False\n",
      "   triplet_loss_weight: 2.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 0.8, 2.5, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   gpt4_radnli_labels_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\n",
      "   dataset_name: MIMIC-CXR(triplets+entcont)\n",
      "   triplets_weight: 2.0\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 0.0\n",
      "   entcon_weight: 1.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 8\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 0.8\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 2.5\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "----\n",
      "\u001b[1mBuilding train entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 26442\n",
      "Number of contradiction samples: 95777\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'There may be a trace left pleural effusion but no large pleural effusion is seen.', 'ent_h': 'There may be a trace left pleural effusion, but no large pleural effusion is seen.', 'con_p': \"There are no leads present in the patient's chest X-ray.\", 'con_h': 'ICD with leads in standard placement.'}\n",
      "{'ent_p': 'There is dense right basilar opacity compatible with pneumonia.', 'ent_h': 'There is a right lower lung opacity which may represent pneumonia.', 'con_p': 'Retrocardiac opacification is consistent with volume loss in the lower lobe and small effusion.', 'con_h': \"The patient's chest X-ray shows no signs of volume loss or effusion.\"}\n",
      "{'ent_p': 'Minimal retrocardiac atelectasis but no evidence of pneumonia.', 'ent_h': \"The patient's chest X-ray shows signs of slight lung collapse but no signs of pneumonia.\", 'con_p': 'There is a small right pleural effusion.', 'con_h': 'There is a large amount of fluid in the right lung.'}\n",
      "{'ent_p': 'Basilar interstitial opacities of uncertain etiology, though likely atypical infection or chronic interstitial lung disease.', 'ent_h': \"The patient's chest X-ray shows basilar interstitial opacities.\", 'con_p': 'The tip of the endotracheal tube is 2 cm above the carina.', 'con_h': 'The tip of the endotracheal tube is exactly at the level of the carina.'}\n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RadNLI samples: 480\n",
      "Number of GPT-4 RadNLI labels: 960\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 926\n",
      "----\n",
      "\u001b[1mBuilding val entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 290\n",
      "Number of contradiction samples: 335\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'No large pleural effusion or pneumothorax is seen.', 'ent_h': 'There is no pneumothorax.', 'con_p': 'Heart size cannot be assessed.', 'con_h': 'Heart size is normal.'}\n",
      "{'ent_p': 'There is no focal consolidation, pleural effusion, pneumothorax, or pulmonary edema.', 'ent_h': 'No pleural effusions.', 'con_p': 'Heart size is normal.', 'con_h': 'Heart size cannot be assessed.'}\n",
      "{'ent_p': 'The lungs are unremarkable.', 'ent_h': 'Lungs are clear.', 'con_p': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.', 'con_h': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.'}\n",
      "{'ent_p': 'As compared to the previous radiograph, the pre-existing pulmonary edema has clearly decreased in extent and severity but is still visible.', 'ent_h': 'As compared to the previous radiograph, the pre-existing pulmonary edema has decreased in extent and severity.', 'con_p': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.', 'con_h': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.'}\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n",
      "len(_train_dataloaders) = 2\n",
      "len(_train_weights) = 2\n",
      "_train_weights = [2.0, 1.0]\n",
      "len(_val_dataloaders) = 2\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets+entcont)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_234549_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_234549_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_20_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9437.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_20_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9437.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_234549_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63770, triplet_loss 0.81428, entcon_loss 0.28239, entcon_acc 0.95153, 80.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92600, tacc(al2) 0.85600, tacc(ob0) 0.99300, tacc(ob1) 0.96300, tacc(ob2) 0.93600, tacc(ob3) 0.96700, tacc(ob4) 0.95000, tacc(ob5) 0.75600, tacc(ob6) 0.97000, tacc(ob7) 0.92800, entcon_acc 0.97612, 22.14 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9429.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.63891, triplet_loss 0.81750, entcon_loss 0.28279, entcon_acc 0.94447, 83.31 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92400, tacc(al2) 0.85700, tacc(ob0) 0.99300, tacc(ob1) 0.96500, tacc(ob2) 0.93900, tacc(ob3) 0.96600, tacc(ob4) 0.95000, tacc(ob5) 0.76200, tacc(ob6) 0.97100, tacc(ob7) 0.93000, entcon_acc 0.96418, 22.98 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.64546, triplet_loss 0.82998, entcon_loss 0.27753, entcon_acc 0.95700, 83.55 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.93100, tacc(al2) 0.86000, tacc(ob0) 0.99300, tacc(ob1) 0.96400, tacc(ob2) 0.92600, tacc(ob3) 0.96600, tacc(ob4) 0.95300, tacc(ob5) 0.74200, tacc(ob6) 0.97100, tacc(ob7) 0.93200, entcon_acc 0.96716, 23.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9430.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64415, triplet_loss 0.82067, entcon_loss 0.28899, entcon_acc 0.94797, 83.42 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92200, tacc(al2) 0.88400, tacc(ob0) 0.99000, tacc(ob1) 0.96400, tacc(ob2) 0.93300, tacc(ob3) 0.96900, tacc(ob4) 0.94400, tacc(ob5) 0.74300, tacc(ob6) 0.95400, tacc(ob7) 0.93100, entcon_acc 0.96418, 23.32 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.65039, triplet_loss 0.83254, entcon_loss 0.28719, entcon_acc 0.94230, 83.46 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.91900, tacc(al2) 0.86400, tacc(ob0) 0.99200, tacc(ob1) 0.96500, tacc(ob2) 0.93200, tacc(ob3) 0.96800, tacc(ob4) 0.95600, tacc(ob5) 0.75900, tacc(ob6) 0.95700, tacc(ob7) 0.92000, entcon_acc 0.98507, 23.72 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.64095, triplet_loss 0.82608, entcon_loss 0.27180, entcon_acc 0.96053, 89.83 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.92600, tacc(al2) 0.85800, tacc(ob0) 0.99400, tacc(ob1) 0.96500, tacc(ob2) 0.94200, tacc(ob3) 0.96700, tacc(ob4) 0.95100, tacc(ob5) 0.75100, tacc(ob6) 0.96800, tacc(ob7) 0.92900, entcon_acc 0.96119, 23.25 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.63826, triplet_loss 0.81873, entcon_loss 0.27514, entcon_acc 0.95619, 83.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.92200, tacc(al2) 0.86600, tacc(ob0) 0.99400, tacc(ob1) 0.96800, tacc(ob2) 0.93800, tacc(ob3) 0.96900, tacc(ob4) 0.95100, tacc(ob5) 0.75400, tacc(ob6) 0.96400, tacc(ob7) 0.92300, entcon_acc 0.95522, 23.53 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.63678, triplet_loss 0.81939, entcon_loss 0.27264, entcon_acc 0.94937, 77.53 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92000, tacc(al2) 0.86800, tacc(ob0) 0.99400, tacc(ob1) 0.96300, tacc(ob2) 0.94000, tacc(ob3) 0.96800, tacc(ob4) 0.95200, tacc(ob5) 0.73600, tacc(ob6) 0.96000, tacc(ob7) 0.92500, entcon_acc 0.97612, 23.25 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.63944, triplet_loss 0.82375, entcon_loss 0.27190, entcon_acc 0.95563, 83.66 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92100, tacc(al2) 0.86600, tacc(ob0) 0.99400, tacc(ob1) 0.96200, tacc(ob2) 0.94000, tacc(ob3) 0.96900, tacc(ob4) 0.95200, tacc(ob5) 0.74800, tacc(ob6) 0.96400, tacc(ob7) 0.92600, entcon_acc 0.97313, 23.41 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63797, triplet_loss 0.81979, entcon_loss 0.27213, entcon_acc 0.96084, 85.48 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92100, tacc(al2) 0.86900, tacc(ob0) 0.99400, tacc(ob1) 0.96300, tacc(ob2) 0.94100, tacc(ob3) 0.96800, tacc(ob4) 0.95300, tacc(ob5) 0.74800, tacc(ob6) 0.96500, tacc(ob7) 0.92500, entcon_acc 0.96716, 23.13 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63299, triplet_loss 0.81718, entcon_loss 0.26572, entcon_acc 0.95972, 81.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92100, tacc(al2) 0.87000, tacc(ob0) 0.99400, tacc(ob1) 0.96200, tacc(ob2) 0.94200, tacc(ob3) 0.96900, tacc(ob4) 0.95200, tacc(ob5) 0.74900, tacc(ob6) 0.96500, tacc(ob7) 0.92600, entcon_acc 0.97313, 23.13 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63489, triplet_loss 0.81534, entcon_loss 0.27508, entcon_acc 0.95264, 80.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92100, tacc(al2) 0.87000, tacc(ob0) 0.99300, tacc(ob1) 0.96200, tacc(ob2) 0.93900, tacc(ob3) 0.96900, tacc(ob4) 0.95100, tacc(ob5) 0.75000, tacc(ob6) 0.96500, tacc(ob7) 0.92400, entcon_acc 0.95821, 23.10 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.65060, triplet_loss 0.83101, entcon_loss 0.28761, entcon_acc 0.94496, 80.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93000, tacc(al2) 0.87600, tacc(ob0) 0.98900, tacc(ob1) 0.96900, tacc(ob2) 0.92200, tacc(ob3) 0.97100, tacc(ob4) 0.94700, tacc(ob5) 0.73100, tacc(ob6) 0.94800, tacc(ob7) 0.92700, entcon_acc 0.95821, 23.14 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.64299, triplet_loss 0.82545, entcon_loss 0.27915, entcon_acc 0.95237, 80.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92300, tacc(al2) 0.87300, tacc(ob0) 0.99100, tacc(ob1) 0.96300, tacc(ob2) 0.92800, tacc(ob3) 0.97100, tacc(ob4) 0.95600, tacc(ob5) 0.75500, tacc(ob6) 0.96800, tacc(ob7) 0.91500, entcon_acc 0.96418, 23.24 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.63448, triplet_loss 0.81767, entcon_loss 0.26918, entcon_acc 0.95781, 104.03 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92000, tacc(al2) 0.86200, tacc(ob0) 0.99200, tacc(ob1) 0.96600, tacc(ob2) 0.93100, tacc(ob3) 0.97100, tacc(ob4) 0.94900, tacc(ob5) 0.73900, tacc(ob6) 0.96400, tacc(ob7) 0.92500, entcon_acc 0.97910, 23.08 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63519, triplet_loss 0.81817, entcon_loss 0.26703, entcon_acc 0.95865, 68.61 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92000, tacc(al2) 0.85900, tacc(ob0) 0.99200, tacc(ob1) 0.96600, tacc(ob2) 0.93200, tacc(ob3) 0.97100, tacc(ob4) 0.95200, tacc(ob5) 0.74400, tacc(ob6) 0.96300, tacc(ob7) 0.92600, entcon_acc 0.97313, 22.70 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63602, triplet_loss 0.81911, entcon_loss 0.27093, entcon_acc 0.95863, 65.55 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92400, tacc(al2) 0.85700, tacc(ob0) 0.99000, tacc(ob1) 0.96800, tacc(ob2) 0.93200, tacc(ob3) 0.97000, tacc(ob4) 0.95400, tacc(ob5) 0.74900, tacc(ob6) 0.96200, tacc(ob7) 0.92600, entcon_acc 0.98209, 23.01 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63481, triplet_loss 0.81995, entcon_loss 0.26565, entcon_acc 0.95999, 79.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92500, tacc(al2) 0.85400, tacc(ob0) 0.99100, tacc(ob1) 0.96700, tacc(ob2) 0.93100, tacc(ob3) 0.96900, tacc(ob4) 0.95300, tacc(ob5) 0.75100, tacc(ob6) 0.96500, tacc(ob7) 0.92900, entcon_acc 0.97313, 23.02 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63657, triplet_loss 0.81948, entcon_loss 0.26856, entcon_acc 0.95920, 65.55 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92700, tacc(al2) 0.85900, tacc(ob0) 0.99100, tacc(ob1) 0.96700, tacc(ob2) 0.93300, tacc(ob3) 0.96900, tacc(ob4) 0.95300, tacc(ob5) 0.75100, tacc(ob6) 0.96500, tacc(ob7) 0.92900, entcon_acc 0.97015, 22.92 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63472, triplet_loss 0.82341, entcon_loss 0.25847, entcon_acc 0.95836, 80.68 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92700, tacc(al2) 0.85800, tacc(ob0) 0.99100, tacc(ob1) 0.96800, tacc(ob2) 0.93500, tacc(ob3) 0.96900, tacc(ob4) 0.95300, tacc(ob5) 0.75100, tacc(ob6) 0.96700, tacc(ob7) 0.92900, entcon_acc 0.95522, 23.11 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64255, triplet_loss 0.82626, entcon_loss 0.27623, entcon_acc 0.94910, 82.22 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.92200, tacc(al2) 0.84000, tacc(ob0) 0.98900, tacc(ob1) 0.96700, tacc(ob2) 0.92600, tacc(ob3) 0.96600, tacc(ob4) 0.94000, tacc(ob5) 0.75500, tacc(ob6) 0.96500, tacc(ob7) 0.92300, entcon_acc 0.96418, 23.34 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.64338, triplet_loss 0.82802, entcon_loss 0.27186, entcon_acc 0.95619, 84.31 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93200, tacc(al2) 0.86700, tacc(ob0) 0.98900, tacc(ob1) 0.96400, tacc(ob2) 0.94100, tacc(ob3) 0.96700, tacc(ob4) 0.94500, tacc(ob5) 0.75200, tacc(ob6) 0.97200, tacc(ob7) 0.92000, entcon_acc 0.97015, 23.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9430.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.63878, triplet_loss 0.82327, entcon_loss 0.27090, entcon_acc 0.95400, 82.90 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93200, tacc(al2) 0.86400, tacc(ob0) 0.99100, tacc(ob1) 0.96200, tacc(ob2) 0.92600, tacc(ob3) 0.96700, tacc(ob4) 0.93700, tacc(ob5) 0.73800, tacc(ob6) 0.96300, tacc(ob7) 0.92400, entcon_acc 0.95821, 23.42 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.64012, triplet_loss 0.82832, entcon_loss 0.26483, entcon_acc 0.95917, 83.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92700, tacc(al2) 0.86400, tacc(ob0) 0.98900, tacc(ob1) 0.96000, tacc(ob2) 0.93900, tacc(ob3) 0.97100, tacc(ob4) 0.94000, tacc(ob5) 0.74500, tacc(ob6) 0.96900, tacc(ob7) 0.92500, entcon_acc 0.96716, 23.55 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63022, triplet_loss 0.81522, entcon_loss 0.25799, entcon_acc 0.96386, 82.82 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86200, tacc(ob0) 0.99000, tacc(ob1) 0.96500, tacc(ob2) 0.93700, tacc(ob3) 0.97100, tacc(ob4) 0.94100, tacc(ob5) 0.74800, tacc(ob6) 0.97200, tacc(ob7) 0.92700, entcon_acc 0.96119, 23.57 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62850, triplet_loss 0.81288, entcon_loss 0.26084, entcon_acc 0.95482, 77.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86200, tacc(ob0) 0.99000, tacc(ob1) 0.96500, tacc(ob2) 0.94000, tacc(ob3) 0.97300, tacc(ob4) 0.94200, tacc(ob5) 0.74800, tacc(ob6) 0.96600, tacc(ob7) 0.92500, entcon_acc 0.98209, 22.84 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9432.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62836, triplet_loss 0.81124, entcon_loss 0.26369, entcon_acc 0.96217, 81.35 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92900, tacc(al2) 0.86400, tacc(ob0) 0.99000, tacc(ob1) 0.96500, tacc(ob2) 0.94000, tacc(ob3) 0.97200, tacc(ob4) 0.93800, tacc(ob5) 0.74400, tacc(ob6) 0.96500, tacc(ob7) 0.92400, entcon_acc 0.96716, 23.19 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63687, triplet_loss 0.82310, entcon_loss 0.26218, entcon_acc 0.95865, 79.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92900, tacc(al2) 0.86200, tacc(ob0) 0.99000, tacc(ob1) 0.96600, tacc(ob2) 0.94000, tacc(ob3) 0.97200, tacc(ob4) 0.93900, tacc(ob5) 0.75000, tacc(ob6) 0.96700, tacc(ob7) 0.92600, entcon_acc 0.94925, 23.03 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64337, triplet_loss 0.83002, entcon_loss 0.27118, entcon_acc 0.95482, 67.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.93100, tacc(al2) 0.87500, tacc(ob0) 0.99100, tacc(ob1) 0.96600, tacc(ob2) 0.93400, tacc(ob3) 0.96200, tacc(ob4) 0.95300, tacc(ob5) 0.75500, tacc(ob6) 0.96400, tacc(ob7) 0.92000, entcon_acc 0.95522, 23.04 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63780, triplet_loss 0.82130, entcon_loss 0.27190, entcon_acc 0.95400, 80.03 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92700, tacc(al2) 0.86500, tacc(ob0) 0.99100, tacc(ob1) 0.96200, tacc(ob2) 0.94500, tacc(ob3) 0.96500, tacc(ob4) 0.95200, tacc(ob5) 0.77700, tacc(ob6) 0.97500, tacc(ob7) 0.92100, entcon_acc 0.94925, 22.96 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.64151, triplet_loss 0.82632, entcon_loss 0.26965, entcon_acc 0.95318, 81.40 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93200, tacc(al2) 0.86300, tacc(ob0) 0.99200, tacc(ob1) 0.96500, tacc(ob2) 0.95100, tacc(ob3) 0.96600, tacc(ob4) 0.95300, tacc(ob5) 0.76600, tacc(ob6) 0.97300, tacc(ob7) 0.92100, entcon_acc 0.96716, 22.97 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9441.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.62569, triplet_loss 0.81143, entcon_loss 0.25532, entcon_acc 0.96244, 80.42 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92700, tacc(al2) 0.86500, tacc(ob0) 0.99200, tacc(ob1) 0.96500, tacc(ob2) 0.93100, tacc(ob3) 0.96800, tacc(ob4) 0.94300, tacc(ob5) 0.74800, tacc(ob6) 0.96600, tacc(ob7) 0.92500, entcon_acc 0.95224, 23.22 secs\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63251, triplet_loss 0.81991, entcon_loss 0.25883, entcon_acc 0.95700, 80.28 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92900, tacc(al2) 0.86700, tacc(ob0) 0.99200, tacc(ob1) 0.96700, tacc(ob2) 0.93600, tacc(ob3) 0.96700, tacc(ob4) 0.94600, tacc(ob5) 0.75600, tacc(ob6) 0.96800, tacc(ob7) 0.92500, entcon_acc 0.95821, 22.70 secs\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63240, triplet_loss 0.81614, entcon_loss 0.26273, entcon_acc 0.96276, 80.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92900, tacc(al2) 0.86400, tacc(ob0) 0.99200, tacc(ob1) 0.96600, tacc(ob2) 0.93900, tacc(ob3) 0.97000, tacc(ob4) 0.94700, tacc(ob5) 0.76100, tacc(ob6) 0.96900, tacc(ob7) 0.92700, entcon_acc 0.97015, 22.84 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_34_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63266, triplet_loss 0.81743, entcon_loss 0.26424, entcon_acc 0.95591, 79.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92900, tacc(al2) 0.86700, tacc(ob0) 0.99200, tacc(ob1) 0.96500, tacc(ob2) 0.94000, tacc(ob3) 0.97000, tacc(ob4) 0.94600, tacc(ob5) 0.76300, tacc(ob6) 0.97000, tacc(ob7) 0.92700, entcon_acc 0.95224, 22.95 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.62985, triplet_loss 0.81598, entcon_loss 0.25870, entcon_acc 0.96217, 79.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92900, tacc(al2) 0.86600, tacc(ob0) 0.99200, tacc(ob1) 0.96600, tacc(ob2) 0.93800, tacc(ob3) 0.97000, tacc(ob4) 0.94700, tacc(ob5) 0.76100, tacc(ob6) 0.96900, tacc(ob7) 0.92600, entcon_acc 0.97313, 22.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_36_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9446.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64067, triplet_loss 0.82740, entcon_loss 0.26494, entcon_acc 0.95728, 79.46 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.93000, tacc(al2) 0.87300, tacc(ob0) 0.99200, tacc(ob1) 0.96200, tacc(ob2) 0.94200, tacc(ob3) 0.96300, tacc(ob4) 0.95600, tacc(ob5) 0.77900, tacc(ob6) 0.96700, tacc(ob7) 0.91500, entcon_acc 0.96418, 23.22 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63909, triplet_loss 0.82510, entcon_loss 0.26819, entcon_acc 0.95427, 81.40 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92600, tacc(al2) 0.84800, tacc(ob0) 0.99000, tacc(ob1) 0.97000, tacc(ob2) 0.92600, tacc(ob3) 0.97000, tacc(ob4) 0.95300, tacc(ob5) 0.74200, tacc(ob6) 0.96500, tacc(ob7) 0.92600, entcon_acc 0.94627, 23.16 secs\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.63388, triplet_loss 0.82108, entcon_loss 0.26059, entcon_acc 0.96053, 82.28 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92500, tacc(al2) 0.85800, tacc(ob0) 0.99200, tacc(ob1) 0.96900, tacc(ob2) 0.93100, tacc(ob3) 0.96700, tacc(ob4) 0.96100, tacc(ob5) 0.75500, tacc(ob6) 0.97000, tacc(ob7) 0.92100, entcon_acc 0.97612, 23.11 secs\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63059, triplet_loss 0.81629, entcon_loss 0.25694, entcon_acc 0.96030, 79.75 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92700, tacc(al2) 0.85400, tacc(ob0) 0.99200, tacc(ob1) 0.96700, tacc(ob2) 0.92900, tacc(ob3) 0.97100, tacc(ob4) 0.95000, tacc(ob5) 0.73900, tacc(ob6) 0.96900, tacc(ob7) 0.92600, entcon_acc 0.97612, 22.87 secs\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63104, triplet_loss 0.81876, entcon_loss 0.25671, entcon_acc 0.95863, 79.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92800, tacc(al2) 0.85400, tacc(ob0) 0.99200, tacc(ob1) 0.96600, tacc(ob2) 0.93100, tacc(ob3) 0.97200, tacc(ob4) 0.95200, tacc(ob5) 0.74700, tacc(ob6) 0.96900, tacc(ob7) 0.92400, entcon_acc 0.95821, 22.96 secs\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62640, triplet_loss 0.81358, entcon_loss 0.25317, entcon_acc 0.96407, 80.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92600, tacc(al2) 0.85000, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.93100, tacc(ob3) 0.97400, tacc(ob4) 0.95100, tacc(ob5) 0.75000, tacc(ob6) 0.96900, tacc(ob7) 0.92600, entcon_acc 0.97612, 23.16 secs\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63142, triplet_loss 0.81766, entcon_loss 0.25670, entcon_acc 0.95947, 80.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92600, tacc(al2) 0.85300, tacc(ob0) 0.99200, tacc(ob1) 0.96500, tacc(ob2) 0.92800, tacc(ob3) 0.97400, tacc(ob4) 0.95000, tacc(ob5) 0.75000, tacc(ob6) 0.97000, tacc(ob7) 0.92600, entcon_acc 0.96716, 23.07 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.62776, triplet_loss 0.81479, entcon_loss 0.25480, entcon_acc 0.96326, 80.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92700, tacc(al2) 0.85200, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.92900, tacc(ob3) 0.97400, tacc(ob4) 0.95000, tacc(ob5) 0.74900, tacc(ob6) 0.97000, tacc(ob7) 0.92600, entcon_acc 0.96119, 23.10 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.63399, triplet_loss 0.81902, entcon_loss 0.26504, entcon_acc 0.96108, 80.08 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.92700, tacc(al2) 0.86200, tacc(ob0) 0.99200, tacc(ob1) 0.96200, tacc(ob2) 0.93500, tacc(ob3) 0.96900, tacc(ob4) 0.95000, tacc(ob5) 0.75700, tacc(ob6) 0.96900, tacc(ob7) 0.92600, entcon_acc 0.95224, 23.09 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.63514, triplet_loss 0.82404, entcon_loss 0.25505, entcon_acc 0.96386, 80.75 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92400, tacc(al2) 0.85700, tacc(ob0) 0.99400, tacc(ob1) 0.96400, tacc(ob2) 0.92800, tacc(ob3) 0.96800, tacc(ob4) 0.95000, tacc(ob5) 0.74900, tacc(ob6) 0.96600, tacc(ob7) 0.91800, entcon_acc 0.96119, 23.03 secs\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.63309, triplet_loss 0.82364, entcon_loss 0.25312, entcon_acc 0.96434, 79.38 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92900, tacc(al2) 0.87000, tacc(ob0) 0.99500, tacc(ob1) 0.96100, tacc(ob2) 0.93500, tacc(ob3) 0.96900, tacc(ob4) 0.94700, tacc(ob5) 0.76100, tacc(ob6) 0.97500, tacc(ob7) 0.92900, entcon_acc 0.96716, 23.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9451.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63300, triplet_loss 0.82226, entcon_loss 0.25563, entcon_acc 0.96326, 80.53 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92700, tacc(al2) 0.85900, tacc(ob0) 0.99300, tacc(ob1) 0.96200, tacc(ob2) 0.92900, tacc(ob3) 0.97000, tacc(ob4) 0.95400, tacc(ob5) 0.74600, tacc(ob6) 0.97200, tacc(ob7) 0.92700, entcon_acc 0.95522, 22.80 secs\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.62835, triplet_loss 0.81293, entcon_loss 0.25695, entcon_acc 0.96303, 81.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92800, tacc(al2) 0.85000, tacc(ob0) 0.99300, tacc(ob1) 0.96500, tacc(ob2) 0.92800, tacc(ob3) 0.96900, tacc(ob4) 0.95300, tacc(ob5) 0.74400, tacc(ob6) 0.97300, tacc(ob7) 0.92600, entcon_acc 0.97015, 22.95 secs\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62912, triplet_loss 0.81829, entcon_loss 0.25192, entcon_acc 0.96407, 81.45 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92800, tacc(al2) 0.85000, tacc(ob0) 0.99400, tacc(ob1) 0.96500, tacc(ob2) 0.92900, tacc(ob3) 0.96700, tacc(ob4) 0.95200, tacc(ob5) 0.74300, tacc(ob6) 0.97200, tacc(ob7) 0.92400, entcon_acc 0.97313, 23.24 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62468, triplet_loss 0.81282, entcon_loss 0.24954, entcon_acc 0.96625, 81.02 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92700, tacc(al2) 0.85300, tacc(ob0) 0.99400, tacc(ob1) 0.96400, tacc(ob2) 0.93200, tacc(ob3) 0.96800, tacc(ob4) 0.95100, tacc(ob5) 0.74700, tacc(ob6) 0.97200, tacc(ob7) 0.92700, entcon_acc 0.95224, 22.89 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.62893, triplet_loss 0.81706, entcon_loss 0.25041, entcon_acc 0.96495, 80.02 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92700, tacc(al2) 0.85400, tacc(ob0) 0.99400, tacc(ob1) 0.96400, tacc(ob2) 0.93000, tacc(ob3) 0.96700, tacc(ob4) 0.95200, tacc(ob5) 0.74700, tacc(ob6) 0.97300, tacc(ob7) 0.92600, entcon_acc 0.95821, 22.22 secs\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.63452, triplet_loss 0.82106, entcon_loss 0.26256, entcon_acc 0.95563, 82.51 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92700, tacc(al2) 0.87200, tacc(ob0) 0.99100, tacc(ob1) 0.95900, tacc(ob2) 0.93100, tacc(ob3) 0.97200, tacc(ob4) 0.95000, tacc(ob5) 0.74800, tacc(ob6) 0.97100, tacc(ob7) 0.92400, entcon_acc 0.94925, 23.06 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "^C iteration 26675\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 676, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 587, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 364, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 302, in step_fn_wrapper\n",
      "    output = step_fn__triplet_ranking(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 72, in step_fn__triplet_ranking\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 500 \\\n",
    "--batch_size 22 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 8 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 0.8, 2.5, 2.]}\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--gpt4_radnli_labels_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "--triplets_weight 2.0 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 0 \\\n",
    "--entcon_weight 1.0 \\\n",
    "--triplet_loss_weight 2.0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets+entcont)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b52d615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 60\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 20\n",
      "   val_batch_size: 80\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210401_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   freeze_huggingface_model: False\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 2.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   integrated_sentence_facts_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\n",
      "   gpt4_radnli_labels_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\n",
      "   dataset_name: MIMIC-CXR(triplets+entcont)\n",
      "   triplets_weight: 2.0\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 0.0\n",
      "   entcon_weight: 1.0\n",
      "   radgraph_ner_re_weight: 0.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210401_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 1.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.0\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "Loading integrated sentence facts from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl...\n",
      "Number of integrated sentence facts: 677694\n",
      "Number of entailment samples added: 1323679\n",
      "----\n",
      "\u001b[1mBuilding train entailment/contradiction dataset and dataloader...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 21667\n",
      "Number of contradiction samples: 95777\n",
      "Source: 0 -> (len(ent_premises), len(cont_premises)): (21667, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 3744\n",
      "Number of contradiction samples: 95777\n",
      "Source: 1 -> (len(ent_premises), len(cont_premises)): (3744, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 465\n",
      "Number of contradiction samples: 95777\n",
      "Source: 2 -> (len(ent_premises), len(cont_premises)): (465, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 473\n",
      "Number of contradiction samples: 95777\n",
      "Source: 3 -> (len(ent_premises), len(cont_premises)): (473, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 93\n",
      "Number of contradiction samples: 95777\n",
      "Source: 4 -> (len(ent_premises), len(cont_premises)): (93, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 1323679\n",
      "Number of contradiction samples: 95777\n",
      "Source: 5 -> (len(ent_premises), len(cont_premises)): (1323679, 95777)\n",
      "Number of datasets: 6\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'There has been interval placement of a left basilar chest tube.', 'ent_h': 'There is a left-sided basilar chest tube.', 'con_p': 'There are persistent unilateral pleural effusions and a left retrocardiac opacity.', 'con_h': 'There are persistent bilateral pleural effusions and a left retrocardiac opacity.'}\n",
      "{'ent_p': 'Pt recent admission was for bloody stools and significantly elevated INR.', 'ent_h': ' The patient had a GI bleed. ', 'con_p': \"The patient's right heart border area is completely clear with no signs of infection.\", 'con_h': 'Subtle opacity over the right heart border is similar appearing to and may represent a focal infection.'}\n",
      "{'ent_p': 'She notes some swelling throughout but denies joint swelling or redness.', 'ent_h': ' Denies joint effusion', 'con_p': 'New right middle lobe opacity most consistent with atelectasis.', 'con_h': 'The right middle lobe opacity is most consistent with pneumonia.'}\n",
      "{'ent_p': 'There is no pericardial effusion.', 'ent_h': ' No abnormal collection of fluid in the pericardial cavity', 'con_p': 'The left portion of the chest is not visualized.', 'con_h': 'The left portion of the chest is clearly visualized.'}\n",
      "----\n",
      "Number of RadNLI samples: 480\n",
      "Number of GPT-4 RadNLI labels: 960\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 926\n",
      "----\n",
      "\u001b[1mBuilding val entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 290\n",
      "Number of contradiction samples: 335\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'There is no evidence of pneumothorax.', 'ent_h': 'No pleural effusion or pneumothorax is seen.', 'con_p': 'Heart size cannot be assessed.', 'con_h': 'Heart size is normal.'}\n",
      "{'ent_p': 'There is no pneumothorax or focal airspace consolidation.', 'ent_h': 'There is no pleural effusion or pneumothorax.', 'con_p': 'Heart size is normal.', 'con_h': 'Heart size cannot be assessed.'}\n",
      "{'ent_p': 'mild interstitial edema elsewhere is stable.', 'ent_h': 'mild interstitial edema elsewhere is unchanged.', 'con_p': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.', 'con_h': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.'}\n",
      "{'ent_p': 'Stable appearance of moderate pulmonary edema since ___.', 'ent_h': 'Stable, moderate pulmonary edema.', 'con_p': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.', 'con_h': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.'}\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n",
      "len(_train_dataloaders) = 2\n",
      "len(_train_weights) = 2\n",
      "_train_weights = [2.0, 1.0]\n",
      "len(_val_dataloaders) = 2\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets+entcont)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_24_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9390.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210401_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_24_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9390.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63725, triplet_loss 0.83116, entcon_loss 0.24943, entcon_acc 0.97075, 86.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92800, tacc(al2) 0.87600, tacc(ob0) 0.99000, tacc(ob1) 0.96300, tacc(ob2) 0.93600, tacc(ob3) 0.97200, tacc(ob4) 0.96200, tacc(ob5) 0.81400, tacc(ob6) 0.96500, tacc(ob7) 0.92400, entcon_acc 0.97313, 7.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.63615, triplet_loss 0.83194, entcon_loss 0.24457, entcon_acc 0.97525, 66.88 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93200, tacc(al2) 0.87600, tacc(ob0) 0.98900, tacc(ob1) 0.96500, tacc(ob2) 0.92900, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.80000, tacc(ob6) 0.96600, tacc(ob7) 0.92400, entcon_acc 0.95821, 9.40 secs\n",
      "\u001b[1m---- Epoch 3/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.63618, triplet_loss 0.83135, entcon_loss 0.24585, entcon_acc 0.97400, 87.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93400, tacc(al2) 0.87200, tacc(ob0) 0.98700, tacc(ob1) 0.96500, tacc(ob2) 0.92500, tacc(ob3) 0.97100, tacc(ob4) 0.95600, tacc(ob5) 0.78800, tacc(ob6) 0.96200, tacc(ob7) 0.92600, entcon_acc 0.96716, 9.26 secs\n",
      "\u001b[1m---- Epoch 4/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64259, triplet_loss 0.83728, entcon_loss 0.25320, entcon_acc 0.97350, 98.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92100, tacc(al2) 0.86500, tacc(ob0) 0.99100, tacc(ob1) 0.96700, tacc(ob2) 0.94200, tacc(ob3) 0.96800, tacc(ob4) 0.95800, tacc(ob5) 0.79500, tacc(ob6) 0.96600, tacc(ob7) 0.92200, entcon_acc 0.97612, 9.07 secs\n",
      "\u001b[1m---- Epoch 5/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.63971, triplet_loss 0.83465, entcon_loss 0.24982, entcon_acc 0.97100, 98.38 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92500, tacc(al2) 0.85800, tacc(ob0) 0.99000, tacc(ob1) 0.97000, tacc(ob2) 0.92700, tacc(ob3) 0.96700, tacc(ob4) 0.95200, tacc(ob5) 0.77700, tacc(ob6) 0.96700, tacc(ob7) 0.91900, entcon_acc 0.95821, 8.56 secs\n",
      "\u001b[1m---- Epoch 6/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.63207, triplet_loss 0.82438, entcon_loss 0.24745, entcon_acc 0.96975, 99.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93200, tacc(al2) 0.86100, tacc(ob0) 0.99000, tacc(ob1) 0.96900, tacc(ob2) 0.93000, tacc(ob3) 0.96700, tacc(ob4) 0.95600, tacc(ob5) 0.78000, tacc(ob6) 0.96400, tacc(ob7) 0.92300, entcon_acc 0.97015, 8.83 secs\n",
      "\u001b[1m---- Epoch 7/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.64039, triplet_loss 0.83691, entcon_loss 0.24733, entcon_acc 0.97175, 101.45 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.86100, tacc(ob0) 0.98700, tacc(ob1) 0.97200, tacc(ob2) 0.92700, tacc(ob3) 0.96700, tacc(ob4) 0.95500, tacc(ob5) 0.77900, tacc(ob6) 0.96400, tacc(ob7) 0.92500, entcon_acc 0.96716, 8.85 secs\n",
      "\u001b[1m---- Epoch 8/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.63736, triplet_loss 0.83304, entcon_loss 0.24601, entcon_acc 0.97275, 79.52 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93100, tacc(al2) 0.86100, tacc(ob0) 0.98900, tacc(ob1) 0.96900, tacc(ob2) 0.92800, tacc(ob3) 0.96800, tacc(ob4) 0.95400, tacc(ob5) 0.77400, tacc(ob6) 0.96700, tacc(ob7) 0.92900, entcon_acc 0.97015, 9.65 secs\n",
      "\u001b[1m---- Epoch 9/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.63074, triplet_loss 0.82078, entcon_loss 0.25067, entcon_acc 0.96500, 79.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93500, tacc(al2) 0.86200, tacc(ob0) 0.98900, tacc(ob1) 0.96800, tacc(ob2) 0.92400, tacc(ob3) 0.96900, tacc(ob4) 0.95600, tacc(ob5) 0.77400, tacc(ob6) 0.96500, tacc(ob7) 0.92700, entcon_acc 0.95522, 10.11 secs\n",
      "\u001b[1m---- Epoch 10/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63208, triplet_loss 0.82406, entcon_loss 0.24810, entcon_acc 0.97000, 78.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93400, tacc(al2) 0.86100, tacc(ob0) 0.98900, tacc(ob1) 0.96900, tacc(ob2) 0.92900, tacc(ob3) 0.96900, tacc(ob4) 0.95400, tacc(ob5) 0.76900, tacc(ob6) 0.96700, tacc(ob7) 0.92800, entcon_acc 0.98209, 10.14 secs\n",
      "\u001b[1m---- Epoch 11/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63392, triplet_loss 0.82888, entcon_loss 0.24402, entcon_acc 0.97000, 96.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93300, tacc(al2) 0.86200, tacc(ob0) 0.98900, tacc(ob1) 0.96900, tacc(ob2) 0.92900, tacc(ob3) 0.96800, tacc(ob4) 0.95700, tacc(ob5) 0.77500, tacc(ob6) 0.96600, tacc(ob7) 0.92800, entcon_acc 0.96418, 9.53 secs\n",
      "\u001b[1m---- Epoch 12/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63170, triplet_loss 0.82478, entcon_loss 0.24555, entcon_acc 0.97425, 97.07 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93300, tacc(al2) 0.86200, tacc(ob0) 0.98900, tacc(ob1) 0.97000, tacc(ob2) 0.92900, tacc(ob3) 0.96900, tacc(ob4) 0.95400, tacc(ob5) 0.77500, tacc(ob6) 0.96500, tacc(ob7) 0.92700, entcon_acc 0.97015, 9.21 secs\n",
      "\u001b[1m---- Epoch 13/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.63966, triplet_loss 0.83453, entcon_loss 0.24990, entcon_acc 0.96750, 96.80 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.93600, tacc(al2) 0.85700, tacc(ob0) 0.98400, tacc(ob1) 0.95800, tacc(ob2) 0.91800, tacc(ob3) 0.96500, tacc(ob4) 0.95000, tacc(ob5) 0.76200, tacc(ob6) 0.96700, tacc(ob7) 0.92700, entcon_acc 0.97015, 9.17 secs\n",
      "\u001b[1m---- Epoch 14/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63708, triplet_loss 0.83195, entcon_loss 0.24734, entcon_acc 0.97200, 96.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93600, tacc(al2) 0.87600, tacc(ob0) 0.98300, tacc(ob1) 0.96000, tacc(ob2) 0.93000, tacc(ob3) 0.96500, tacc(ob4) 0.95800, tacc(ob5) 0.77900, tacc(ob6) 0.96400, tacc(ob7) 0.92600, entcon_acc 0.94925, 9.05 secs\n",
      "\u001b[1m---- Epoch 15/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.63316, triplet_loss 0.82744, entcon_loss 0.24461, entcon_acc 0.97250, 97.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93600, tacc(al2) 0.86500, tacc(ob0) 0.98400, tacc(ob1) 0.96100, tacc(ob2) 0.93200, tacc(ob3) 0.96600, tacc(ob4) 0.95700, tacc(ob5) 0.77600, tacc(ob6) 0.97100, tacc(ob7) 0.93300, entcon_acc 0.95224, 9.10 secs\n",
      "\u001b[1m---- Epoch 16/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.62861, triplet_loss 0.82182, entcon_loss 0.24219, entcon_acc 0.97625, 97.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.93300, tacc(al2) 0.86600, tacc(ob0) 0.98300, tacc(ob1) 0.96000, tacc(ob2) 0.92800, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.76700, tacc(ob6) 0.96400, tacc(ob7) 0.93600, entcon_acc 0.97612, 8.96 secs\n",
      "\u001b[1m---- Epoch 17/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63462, triplet_loss 0.82988, entcon_loss 0.24410, entcon_acc 0.97750, 96.58 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.86600, tacc(ob0) 0.98400, tacc(ob1) 0.95900, tacc(ob2) 0.93000, tacc(ob3) 0.97000, tacc(ob4) 0.95700, tacc(ob5) 0.77000, tacc(ob6) 0.96700, tacc(ob7) 0.93700, entcon_acc 0.97612, 8.87 secs\n",
      "\u001b[1m---- Epoch 18/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62899, triplet_loss 0.82214, entcon_loss 0.24269, entcon_acc 0.97225, 97.08 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.86900, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.93000, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.76800, tacc(ob6) 0.96700, tacc(ob7) 0.93400, entcon_acc 0.96119, 8.91 secs\n",
      "\u001b[1m---- Epoch 19/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62712, triplet_loss 0.82141, entcon_loss 0.23854, entcon_acc 0.97775, 96.19 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.86800, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.93000, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.76900, tacc(ob6) 0.96600, tacc(ob7) 0.93600, entcon_acc 0.95821, 8.77 secs\n",
      "\u001b[1m---- Epoch 20/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63588, triplet_loss 0.83119, entcon_loss 0.24525, entcon_acc 0.97375, 96.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93200, tacc(al2) 0.86800, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.93100, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.76800, tacc(ob6) 0.96600, tacc(ob7) 0.93400, entcon_acc 0.96119, 8.89 secs\n",
      "\u001b[1m---- Epoch 21/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.63676, triplet_loss 0.82932, entcon_loss 0.25166, entcon_acc 0.96575, 96.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92600, tacc(al2) 0.87000, tacc(ob0) 0.98300, tacc(ob1) 0.96400, tacc(ob2) 0.92300, tacc(ob3) 0.97100, tacc(ob4) 0.95400, tacc(ob5) 0.75300, tacc(ob6) 0.95200, tacc(ob7) 0.93000, entcon_acc 0.94030, 8.77 secs\n",
      "\u001b[1m---- Epoch 22/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.64256, triplet_loss 0.83969, entcon_loss 0.24831, entcon_acc 0.97275, 97.55 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93300, tacc(al2) 0.86000, tacc(ob0) 0.98400, tacc(ob1) 0.96400, tacc(ob2) 0.92200, tacc(ob3) 0.97200, tacc(ob4) 0.95300, tacc(ob5) 0.73700, tacc(ob6) 0.96000, tacc(ob7) 0.93200, entcon_acc 0.94328, 8.94 secs\n",
      "\u001b[1m---- Epoch 23/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.63564, triplet_loss 0.83171, entcon_loss 0.24350, entcon_acc 0.97625, 97.69 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92700, tacc(al2) 0.85900, tacc(ob0) 0.98500, tacc(ob1) 0.96200, tacc(ob2) 0.93200, tacc(ob3) 0.97100, tacc(ob4) 0.95900, tacc(ob5) 0.76000, tacc(ob6) 0.96900, tacc(ob7) 0.92800, entcon_acc 0.95821, 8.74 secs\n",
      "\u001b[1m---- Epoch 24/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63086, triplet_loss 0.82290, entcon_loss 0.24679, entcon_acc 0.96900, 96.95 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92900, tacc(al2) 0.85500, tacc(ob0) 0.98500, tacc(ob1) 0.96200, tacc(ob2) 0.93100, tacc(ob3) 0.96700, tacc(ob4) 0.95200, tacc(ob5) 0.75200, tacc(ob6) 0.96000, tacc(ob7) 0.92800, entcon_acc 0.95522, 8.85 secs\n",
      "\u001b[1m---- Epoch 25/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63357, triplet_loss 0.83023, entcon_loss 0.24026, entcon_acc 0.97150, 96.92 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.85400, tacc(ob0) 0.98500, tacc(ob1) 0.96400, tacc(ob2) 0.93500, tacc(ob3) 0.96800, tacc(ob4) 0.95600, tacc(ob5) 0.76300, tacc(ob6) 0.96100, tacc(ob7) 0.93100, entcon_acc 0.94627, 8.61 secs\n",
      "\u001b[1m---- Epoch 26/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63219, triplet_loss 0.82830, entcon_loss 0.23996, entcon_acc 0.97600, 98.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92700, tacc(al2) 0.85600, tacc(ob0) 0.98600, tacc(ob1) 0.96400, tacc(ob2) 0.93200, tacc(ob3) 0.96700, tacc(ob4) 0.95400, tacc(ob5) 0.76400, tacc(ob6) 0.96100, tacc(ob7) 0.93000, entcon_acc 0.95522, 8.74 secs\n",
      "\u001b[1m---- Epoch 27/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62636, triplet_loss 0.82129, entcon_loss 0.23649, entcon_acc 0.97500, 97.28 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92800, tacc(al2) 0.85900, tacc(ob0) 0.98600, tacc(ob1) 0.96400, tacc(ob2) 0.93500, tacc(ob3) 0.96800, tacc(ob4) 0.95600, tacc(ob5) 0.76400, tacc(ob6) 0.96100, tacc(ob7) 0.93000, entcon_acc 0.95821, 8.77 secs\n",
      "\u001b[1m---- Epoch 28/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.62829, triplet_loss 0.82214, entcon_loss 0.24060, entcon_acc 0.97550, 95.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92700, tacc(al2) 0.85500, tacc(ob0) 0.98600, tacc(ob1) 0.96400, tacc(ob2) 0.93400, tacc(ob3) 0.96800, tacc(ob4) 0.95600, tacc(ob5) 0.75800, tacc(ob6) 0.96100, tacc(ob7) 0.93000, entcon_acc 0.95821, 8.79 secs\n",
      "\u001b[1m---- Epoch 29/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.63892, triplet_loss 0.83533, entcon_loss 0.24610, entcon_acc 0.97325, 95.49 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93000, tacc(al2) 0.83700, tacc(ob0) 0.98500, tacc(ob1) 0.96100, tacc(ob2) 0.93000, tacc(ob3) 0.97400, tacc(ob4) 0.95200, tacc(ob5) 0.76400, tacc(ob6) 0.95600, tacc(ob7) 0.92600, entcon_acc 0.96119, 8.86 secs\n",
      "\u001b[1m---- Epoch 30/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63856, triplet_loss 0.83346, entcon_loss 0.24876, entcon_acc 0.96975, 95.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.93300, tacc(al2) 0.83300, tacc(ob0) 0.98400, tacc(ob1) 0.96300, tacc(ob2) 0.90800, tacc(ob3) 0.96900, tacc(ob4) 0.94400, tacc(ob5) 0.73500, tacc(ob6) 0.95000, tacc(ob7) 0.92600, entcon_acc 0.95224, 8.77 secs\n",
      "\u001b[1m---- Epoch 31/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.63743, triplet_loss 0.83351, entcon_loss 0.24529, entcon_acc 0.97000, 96.06 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93500, tacc(al2) 0.85100, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.92700, tacc(ob3) 0.97400, tacc(ob4) 0.95500, tacc(ob5) 0.77300, tacc(ob6) 0.96200, tacc(ob7) 0.93200, entcon_acc 0.96418, 8.69 secs\n",
      "\u001b[1m---- Epoch 32/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63197, triplet_loss 0.82594, entcon_loss 0.24404, entcon_acc 0.97200, 97.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93400, tacc(al2) 0.85600, tacc(ob0) 0.98500, tacc(ob1) 0.95900, tacc(ob2) 0.92600, tacc(ob3) 0.97100, tacc(ob4) 0.95200, tacc(ob5) 0.77100, tacc(ob6) 0.96000, tacc(ob7) 0.93200, entcon_acc 0.96418, 8.67 secs\n",
      "\u001b[1m---- Epoch 33/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "^C\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 741, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 652, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 402, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 129, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 354, in step_fn_wrapper\n",
      "    output = step_fn__entcon(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 291, in step_fn__entcon\n",
      "    gradient_accumulator.step(batch_loss, model)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 28, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210401_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 60 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 20 \\\n",
    "--val_batch_size 80 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 1., 1., 2.]}\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--integrated_sentence_facts_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\" \\\n",
    "--gpt4_radnli_labels_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "--triplets_weight 2.0 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 0 \\\n",
    "--entcon_weight 1.0 \\\n",
    "--radgraph_ner_re_weight 0 \\\n",
    "--triplet_loss_weight 2.0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets+entcont)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95401e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 21\n",
      "   val_batch_size: 80\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   pretrained_checkpoint_folder_paths: None\n",
      "   freeze_huggingface_model: False\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   fact_decoder_embed_size: 256\n",
      "   fact_decoder_hidden_size: 256\n",
      "   fact_decoder_nhead: 1\n",
      "   fact_decoder_dim_feedforward: 256\n",
      "   fact_decoder_num_layers: 1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,2e-6,8e-5,8,2e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   sentence_autoencoder_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.5, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl\n",
      "   integrated_sentence_facts_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\n",
      "   sentences_and_cluster_ids_filepath: None\n",
      "   dataset_name: MIMIC-CXR(triplets+entcon)\n",
      "   triplets_weight: 1.0\n",
      "   metadata_classification_weight: 0\n",
      "   chest_imagenome_observations_classification_weight: 0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0\n",
      "   nli_weight: 0\n",
      "   entcon_weight: 1.0\n",
      "   radgraph_ner_re_weight: 0\n",
      "   sentence_autoencoder_weight: 0\n",
      "   use_nli_val_in_train: False\n",
      "   use_anli: False\n",
      "   use_multinli: False\n",
      "   use_snli: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 2.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.5\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "Number of RadNLI samples: 480\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 841\n",
      "----\n",
      "\u001b[1mBuilding val entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 243\n",
      "Number of contradiction samples: 318\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'mild pulmonary edema is stable.', 'ent_h': 'mild pulmonary edema is unchanged.', 'con_p': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.', 'con_h': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.'}\n",
      "{'ent_p': 'No discrete consolidation, pleural effusion, pneumothorax, or pulmonary edema is identified.', 'ent_h': 'No pleural effusions.', 'con_p': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.', 'con_h': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.'}\n",
      "{'ent_p': 'Increasing right effusion.', 'ent_h': 'Increasing effusion on the right.', 'con_p': 'Known small left pneumothorax is not well seen.', 'con_h': 'No pneumothorax.'}\n",
      "{'ent_p': 'small bilateral effusions are unchanged.', 'ent_h': 'small bilateral effusions are stable.', 'con_p': 'No pneumothorax.', 'con_h': 'Known small left pneumothorax is not well seen.'}\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl...\n",
      "Number of samples: 169025\n",
      "Loading integrated sentence facts from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl...\n",
      "Number of integrated sentence facts: 677694\n",
      "Number of entailment samples added: 1323679\n",
      "----\n",
      "\u001b[1mBuilding train entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 13185\n",
      "Number of contradiction samples: 99121\n",
      "Source: 0 -> (len(ent_premises), len(cont_premises)): (13185, 99121)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 5445\n",
      "Number of contradiction samples: 99121\n",
      "Source: 1 -> (len(ent_premises), len(cont_premises)): (5445, 99121)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 1770\n",
      "Number of contradiction samples: 99121\n",
      "Source: 2 -> (len(ent_premises), len(cont_premises)): (1770, 99121)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 3744\n",
      "Number of contradiction samples: 99121\n",
      "Source: 3 -> (len(ent_premises), len(cont_premises)): (3744, 99121)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 465\n",
      "Number of contradiction samples: 99121\n",
      "Source: 4 -> (len(ent_premises), len(cont_premises)): (465, 99121)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 473\n",
      "Number of contradiction samples: 99121\n",
      "Source: 5 -> (len(ent_premises), len(cont_premises)): (473, 99121)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 93\n",
      "Number of contradiction samples: 99121\n",
      "Source: 6 -> (len(ent_premises), len(cont_premises)): (93, 99121)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 1323679\n",
      "Number of contradiction samples: 99121\n",
      "Source: 7 -> (len(ent_premises), len(cont_premises)): (1323679, 99121)\n",
      "Number of datasets: 8\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'Bibasilar atelectasis is redemonstrated, increased in the left lower lobe.', 'ent_h': \"The patient's left lower lobe shows an increase in bibasilar atelectasis.\", 'con_p': 'Small to moderate right pleural effusion with atelectasis.', 'con_h': 'No new consolidation, pneumothorax, or pleural effusion is seen.'}\n",
      "{'ent_p': 'Some linear atelectasis at the left lung base is noted.', 'ent_h': 'Linear atelectasis is also noted of the left lung base.', 'con_p': 'No clips are seen projecting over the upper mediastinum.', 'con_h': 'Multiple clips are seen projecting over the upper mediastinum.'}\n",
      "{'ent_p': 'There is no pleural effusion or a pneumothorax.', 'ent_h': 'No pneumothorax or gross effusion detected.', 'con_p': 'New consolidation in the right upper lobe could be pneumonia or pulmonary hemorrhage.', 'con_h': 'The right upper lobe shows signs of emphysema, not pneumonia or pulmonary hemorrhage.'}\n",
      "{'ent_p': 'Aortic valve disease.', 'ent_h': ' Patient has aortic stenosis', 'con_p': 'New moderate right pneumothorax without mediastinal shift is seen.', 'con_h': 'The moderate right pneumothorax is unchanged since the prior exam.'}\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(_train_dataloaders) = 2\n",
      "len(_train_weights) = 2\n",
      "_train_weights = [1.0, 1.0]\n",
      "len(_val_dataloaders) = 2\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets+entcon)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "  use_fact_decoder: False\n",
      "  fact_decoder_embed_size: 256\n",
      "  fact_decoder_hidden_size: 256\n",
      "  fact_decoder_nhead: 1\n",
      "  fact_decoder_dim_feedforward: 256\n",
      "  fact_decoder_num_layers: 1\n",
      "  fact_decoder_start_idx: None\n",
      "  fact_decoder_vocab_size: None\n",
      "  fact_decoder_dropout_prob: 0\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)', 'pretrained_checkpoint_folder_paths': None}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,2e-6,8e-5,8,2e-6\n",
      "1e-06 3 8e-05 8 2e-06 8e-05 8 2e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240129_080646_MIMIC-CXR(triplets+entcon)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240129_080646_MIMIC-CXR(triplets+entcon)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9444.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20240129_080646_MIMIC-CXR(triplets+entcon)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.33207, triplet_loss 0.42079, entcon_loss 0.24334, entcon_acc 0.97143, 107.28 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.87500, tacc(ob0) 0.99000, tacc(ob1) 0.96400, tacc(ob2) 0.93400, tacc(ob3) 0.97000, tacc(ob4) 0.96300, tacc(ob5) 0.81300, tacc(ob6) 0.96600, tacc(ob7) 0.92300, entcon_acc 0.96226, 8.81 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9367.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.33191, triplet_loss 0.42078, entcon_loss 0.24303, entcon_acc 0.97206, 105.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93000, tacc(al2) 0.87900, tacc(ob0) 0.99000, tacc(ob1) 0.96300, tacc(ob2) 0.93100, tacc(ob3) 0.96900, tacc(ob4) 0.96100, tacc(ob5) 0.80900, tacc(ob6) 0.96500, tacc(ob7) 0.92300, entcon_acc 0.97799, 9.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9372.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.33085, triplet_loss 0.42011, entcon_loss 0.24159, entcon_acc 0.97476, 100.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93000, tacc(al2) 0.87200, tacc(ob0) 0.98800, tacc(ob1) 0.95900, tacc(ob2) 0.93300, tacc(ob3) 0.96800, tacc(ob4) 0.96000, tacc(ob5) 0.80500, tacc(ob6) 0.96200, tacc(ob7) 0.92100, entcon_acc 0.98428, 9.07 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33621, triplet_loss 0.42749, entcon_loss 0.24494, entcon_acc 0.97540, 104.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92700, tacc(al2) 0.86500, tacc(ob0) 0.98900, tacc(ob1) 0.96300, tacc(ob2) 0.93700, tacc(ob3) 0.96800, tacc(ob4) 0.95600, tacc(ob5) 0.81600, tacc(ob6) 0.96000, tacc(ob7) 0.92200, entcon_acc 0.97484, 9.08 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000050) ...\n",
      "loss 0.33059, triplet_loss 0.42373, entcon_loss 0.23745, entcon_acc 0.97587, 103.83 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93200, tacc(al2) 0.87100, tacc(ob0) 0.98800, tacc(ob1) 0.96500, tacc(ob2) 0.94000, tacc(ob3) 0.97200, tacc(ob4) 0.95900, tacc(ob5) 0.81400, tacc(ob6) 0.96200, tacc(ob7) 0.92500, entcon_acc 0.98428, 9.05 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9390.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.33040, triplet_loss 0.42209, entcon_loss 0.23870, entcon_acc 0.97683, 104.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93000, tacc(al2) 0.86800, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.93700, tacc(ob3) 0.97000, tacc(ob4) 0.96200, tacc(ob5) 0.79900, tacc(ob6) 0.95200, tacc(ob7) 0.92900, entcon_acc 0.98742, 8.94 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 0.32698, triplet_loss 0.42278, entcon_loss 0.23117, entcon_acc 0.97667, 105.67 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93300, tacc(al2) 0.86500, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.94200, tacc(ob3) 0.97100, tacc(ob4) 0.96000, tacc(ob5) 0.80200, tacc(ob6) 0.96100, tacc(ob7) 0.92600, entcon_acc 0.98742, 9.14 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.32742, triplet_loss 0.42514, entcon_loss 0.22971, entcon_acc 0.97857, 104.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93300, tacc(al2) 0.86600, tacc(ob0) 0.98700, tacc(ob1) 0.96300, tacc(ob2) 0.94200, tacc(ob3) 0.97000, tacc(ob4) 0.95800, tacc(ob5) 0.81900, tacc(ob6) 0.96200, tacc(ob7) 0.92200, entcon_acc 0.98742, 9.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9394.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.32441, triplet_loss 0.42139, entcon_loss 0.22743, entcon_acc 0.98032, 105.59 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93500, tacc(al2) 0.87000, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.94500, tacc(ob3) 0.96900, tacc(ob4) 0.96000, tacc(ob5) 0.80900, tacc(ob6) 0.96300, tacc(ob7) 0.92400, entcon_acc 0.98113, 9.14 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.32612, triplet_loss 0.42412, entcon_loss 0.22812, entcon_acc 0.97683, 106.04 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93200, tacc(al2) 0.87000, tacc(ob0) 0.98700, tacc(ob1) 0.96200, tacc(ob2) 0.94400, tacc(ob3) 0.96700, tacc(ob4) 0.96100, tacc(ob5) 0.80600, tacc(ob6) 0.96200, tacc(ob7) 0.92300, entcon_acc 0.97799, 9.27 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32313, triplet_loss 0.41798, entcon_loss 0.22827, entcon_acc 0.98063, 95.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93300, tacc(al2) 0.86700, tacc(ob0) 0.98700, tacc(ob1) 0.96200, tacc(ob2) 0.94600, tacc(ob3) 0.96700, tacc(ob4) 0.96200, tacc(ob5) 0.81000, tacc(ob6) 0.96100, tacc(ob7) 0.92700, entcon_acc 0.98742, 9.83 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32607, triplet_loss 0.42305, entcon_loss 0.22910, entcon_acc 0.97698, 87.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93300, tacc(al2) 0.86800, tacc(ob0) 0.98700, tacc(ob1) 0.96200, tacc(ob2) 0.94500, tacc(ob3) 0.96800, tacc(ob4) 0.96200, tacc(ob5) 0.80900, tacc(ob6) 0.96200, tacc(ob7) 0.92700, entcon_acc 0.97799, 10.37 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32512, triplet_loss 0.41676, entcon_loss 0.23348, entcon_acc 0.97730, 106.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92800, tacc(al2) 0.85700, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.93500, tacc(ob3) 0.96600, tacc(ob4) 0.95300, tacc(ob5) 0.79500, tacc(ob6) 0.95900, tacc(ob7) 0.92200, entcon_acc 0.98113, 9.19 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32643, triplet_loss 0.41865, entcon_loss 0.23422, entcon_acc 0.97460, 97.51 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93100, tacc(al2) 0.86800, tacc(ob0) 0.99000, tacc(ob1) 0.95900, tacc(ob2) 0.95100, tacc(ob3) 0.96600, tacc(ob4) 0.95000, tacc(ob5) 0.82200, tacc(ob6) 0.96800, tacc(ob7) 0.92200, entcon_acc 0.98742, 9.67 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.32737, triplet_loss 0.42215, entcon_loss 0.23258, entcon_acc 0.97905, 105.90 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.87800, tacc(ob0) 0.98800, tacc(ob1) 0.95400, tacc(ob2) 0.95400, tacc(ob3) 0.96600, tacc(ob4) 0.95500, tacc(ob5) 0.82000, tacc(ob6) 0.96700, tacc(ob7) 0.92100, entcon_acc 0.98742, 9.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9398.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.32451, triplet_loss 0.42365, entcon_loss 0.22537, entcon_acc 0.97778, 96.30 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93500, tacc(al2) 0.87500, tacc(ob0) 0.98900, tacc(ob1) 0.95700, tacc(ob2) 0.94700, tacc(ob3) 0.97100, tacc(ob4) 0.95500, tacc(ob5) 0.82000, tacc(ob6) 0.96500, tacc(ob7) 0.92000, entcon_acc 0.98742, 14.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9398.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.32487, triplet_loss 0.42193, entcon_loss 0.22780, entcon_acc 0.97937, 102.75 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93100, tacc(al2) 0.87300, tacc(ob0) 0.98700, tacc(ob1) 0.95800, tacc(ob2) 0.94800, tacc(ob3) 0.96800, tacc(ob4) 0.96000, tacc(ob5) 0.81500, tacc(ob6) 0.96700, tacc(ob7) 0.92200, entcon_acc 0.98428, 9.65 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.32335, triplet_loss 0.42100, entcon_loss 0.22571, entcon_acc 0.98238, 103.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93000, tacc(al2) 0.87300, tacc(ob0) 0.98800, tacc(ob1) 0.95600, tacc(ob2) 0.94200, tacc(ob3) 0.96600, tacc(ob4) 0.95800, tacc(ob5) 0.81200, tacc(ob6) 0.96500, tacc(ob7) 0.92100, entcon_acc 0.97799, 9.58 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32122, triplet_loss 0.41709, entcon_loss 0.22536, entcon_acc 0.97778, 105.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93100, tacc(al2) 0.87100, tacc(ob0) 0.98900, tacc(ob1) 0.96000, tacc(ob2) 0.94700, tacc(ob3) 0.96600, tacc(ob4) 0.95900, tacc(ob5) 0.81500, tacc(ob6) 0.96600, tacc(ob7) 0.92300, entcon_acc 0.97484, 9.49 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32512, triplet_loss 0.42295, entcon_loss 0.22730, entcon_acc 0.97889, 104.53 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93300, tacc(al2) 0.87100, tacc(ob0) 0.98900, tacc(ob1) 0.96100, tacc(ob2) 0.94700, tacc(ob3) 0.96700, tacc(ob4) 0.95900, tacc(ob5) 0.81400, tacc(ob6) 0.96400, tacc(ob7) 0.92200, entcon_acc 0.98742, 9.44 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.33118, triplet_loss 0.42439, entcon_loss 0.23797, entcon_acc 0.97556, 104.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93000, tacc(al2) 0.86500, tacc(ob0) 0.98700, tacc(ob1) 0.95500, tacc(ob2) 0.93800, tacc(ob3) 0.96900, tacc(ob4) 0.95900, tacc(ob5) 0.80600, tacc(ob6) 0.95900, tacc(ob7) 0.91400, entcon_acc 0.96541, 9.37 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32698, triplet_loss 0.42466, entcon_loss 0.22930, entcon_acc 0.97778, 107.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93400, tacc(al2) 0.85900, tacc(ob0) 0.98500, tacc(ob1) 0.96200, tacc(ob2) 0.94700, tacc(ob3) 0.96600, tacc(ob4) 0.96100, tacc(ob5) 0.81500, tacc(ob6) 0.96800, tacc(ob7) 0.92000, entcon_acc 0.99057, 9.38 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.32634, triplet_loss 0.42514, entcon_loss 0.22754, entcon_acc 0.98032, 103.51 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86300, tacc(ob0) 0.98500, tacc(ob1) 0.95900, tacc(ob2) 0.94000, tacc(ob3) 0.97100, tacc(ob4) 0.95900, tacc(ob5) 0.81300, tacc(ob6) 0.96100, tacc(ob7) 0.91700, entcon_acc 0.96541, 9.57 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.32461, triplet_loss 0.42634, entcon_loss 0.22288, entcon_acc 0.98429, 101.57 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92500, tacc(al2) 0.86800, tacc(ob0) 0.98800, tacc(ob1) 0.96300, tacc(ob2) 0.94400, tacc(ob3) 0.97200, tacc(ob4) 0.95600, tacc(ob5) 0.81600, tacc(ob6) 0.96300, tacc(ob7) 0.92500, entcon_acc 0.98428, 9.31 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.32092, triplet_loss 0.41995, entcon_loss 0.22189, entcon_acc 0.98206, 104.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92800, tacc(al2) 0.86500, tacc(ob0) 0.98900, tacc(ob1) 0.96300, tacc(ob2) 0.94100, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.81800, tacc(ob6) 0.96100, tacc(ob7) 0.92800, entcon_acc 0.98742, 9.39 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.32165, triplet_loss 0.42106, entcon_loss 0.22224, entcon_acc 0.98111, 102.01 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92900, tacc(al2) 0.86200, tacc(ob0) 0.98800, tacc(ob1) 0.96400, tacc(ob2) 0.94300, tacc(ob3) 0.97200, tacc(ob4) 0.95900, tacc(ob5) 0.82200, tacc(ob6) 0.96300, tacc(ob7) 0.92700, entcon_acc 0.97799, 9.76 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.32159, triplet_loss 0.41995, entcon_loss 0.22323, entcon_acc 0.98079, 103.73 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92800, tacc(al2) 0.85900, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.94100, tacc(ob3) 0.97100, tacc(ob4) 0.95700, tacc(ob5) 0.81700, tacc(ob6) 0.95900, tacc(ob7) 0.92600, entcon_acc 0.98742, 9.49 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.32131, triplet_loss 0.41803, entcon_loss 0.22459, entcon_acc 0.97810, 105.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86300, tacc(ob0) 0.98900, tacc(ob1) 0.96300, tacc(ob2) 0.94500, tacc(ob3) 0.97000, tacc(ob4) 0.95800, tacc(ob5) 0.82100, tacc(ob6) 0.96300, tacc(ob7) 0.92600, entcon_acc 0.98742, 9.51 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32582, triplet_loss 0.42227, entcon_loss 0.22938, entcon_acc 0.97556, 102.04 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92700, tacc(al2) 0.86100, tacc(ob0) 0.98400, tacc(ob1) 0.96100, tacc(ob2) 0.93600, tacc(ob3) 0.96700, tacc(ob4) 0.96400, tacc(ob5) 0.79900, tacc(ob6) 0.96100, tacc(ob7) 0.91200, entcon_acc 0.97799, 9.54 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32571, triplet_loss 0.42310, entcon_loss 0.22833, entcon_acc 0.97714, 104.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93500, tacc(al2) 0.87400, tacc(ob0) 0.98700, tacc(ob1) 0.95800, tacc(ob2) 0.94200, tacc(ob3) 0.96600, tacc(ob4) 0.96400, tacc(ob5) 0.81600, tacc(ob6) 0.96200, tacc(ob7) 0.92100, entcon_acc 0.99371, 9.48 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.32454, triplet_loss 0.41977, entcon_loss 0.22930, entcon_acc 0.97778, 104.23 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93400, tacc(al2) 0.87900, tacc(ob0) 0.98700, tacc(ob1) 0.95800, tacc(ob2) 0.94100, tacc(ob3) 0.96900, tacc(ob4) 0.95900, tacc(ob5) 0.81800, tacc(ob6) 0.96100, tacc(ob7) 0.92100, entcon_acc 0.98113, 9.37 secs\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.32173, triplet_loss 0.42077, entcon_loss 0.22269, entcon_acc 0.97937, 105.90 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93200, tacc(al2) 0.87700, tacc(ob0) 0.98700, tacc(ob1) 0.96000, tacc(ob2) 0.94100, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.82200, tacc(ob6) 0.96000, tacc(ob7) 0.92400, entcon_acc 0.98113, 9.55 secs\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.31968, triplet_loss 0.41858, entcon_loss 0.22077, entcon_acc 0.97825, 100.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.93200, tacc(al2) 0.87200, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.94100, tacc(ob3) 0.96800, tacc(ob4) 0.95500, tacc(ob5) 0.81700, tacc(ob6) 0.96100, tacc(ob7) 0.92200, entcon_acc 0.99371, 9.52 secs\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31997, triplet_loss 0.42080, entcon_loss 0.21915, entcon_acc 0.98413, 104.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93100, tacc(al2) 0.87400, tacc(ob0) 0.98900, tacc(ob1) 0.96200, tacc(ob2) 0.94400, tacc(ob3) 0.96900, tacc(ob4) 0.95600, tacc(ob5) 0.81800, tacc(ob6) 0.95900, tacc(ob7) 0.92300, entcon_acc 0.98428, 9.52 secs\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.32116, triplet_loss 0.41906, entcon_loss 0.22326, entcon_acc 0.97683, 104.70 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93200, tacc(al2) 0.87500, tacc(ob0) 0.99000, tacc(ob1) 0.96100, tacc(ob2) 0.94600, tacc(ob3) 0.96900, tacc(ob4) 0.95600, tacc(ob5) 0.81900, tacc(ob6) 0.96300, tacc(ob7) 0.92300, entcon_acc 0.98428, 9.73 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31867, triplet_loss 0.41655, entcon_loss 0.22080, entcon_acc 0.97937, 104.35 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93200, tacc(al2) 0.87400, tacc(ob0) 0.99000, tacc(ob1) 0.96000, tacc(ob2) 0.94600, tacc(ob3) 0.96900, tacc(ob4) 0.95600, tacc(ob5) 0.82100, tacc(ob6) 0.96300, tacc(ob7) 0.92300, entcon_acc 0.98428, 9.71 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_36_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9400.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32481, triplet_loss 0.42263, entcon_loss 0.22699, entcon_acc 0.97984, 103.64 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93200, tacc(al2) 0.87400, tacc(ob0) 0.98900, tacc(ob1) 0.96100, tacc(ob2) 0.93300, tacc(ob3) 0.96400, tacc(ob4) 0.95700, tacc(ob5) 0.81000, tacc(ob6) 0.96500, tacc(ob7) 0.92100, entcon_acc 0.98428, 9.61 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32308, triplet_loss 0.41945, entcon_loss 0.22670, entcon_acc 0.97968, 105.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.93100, tacc(al2) 0.85700, tacc(ob0) 0.98600, tacc(ob1) 0.96300, tacc(ob2) 0.93400, tacc(ob3) 0.96900, tacc(ob4) 0.96000, tacc(ob5) 0.80000, tacc(ob6) 0.96900, tacc(ob7) 0.92600, entcon_acc 0.98113, 9.37 secs\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.32338, triplet_loss 0.42465, entcon_loss 0.22211, entcon_acc 0.97762, 103.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93300, tacc(al2) 0.86400, tacc(ob0) 0.98500, tacc(ob1) 0.96600, tacc(ob2) 0.93700, tacc(ob3) 0.96600, tacc(ob4) 0.95700, tacc(ob5) 0.80600, tacc(ob6) 0.96100, tacc(ob7) 0.92900, entcon_acc 0.99057, 9.54 secs\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.32111, triplet_loss 0.42114, entcon_loss 0.22108, entcon_acc 0.97794, 105.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93200, tacc(al2) 0.87100, tacc(ob0) 0.98500, tacc(ob1) 0.96400, tacc(ob2) 0.94100, tacc(ob3) 0.97200, tacc(ob4) 0.96000, tacc(ob5) 0.82000, tacc(ob6) 0.96600, tacc(ob7) 0.92600, entcon_acc 0.97170, 9.55 secs\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.32127, triplet_loss 0.42205, entcon_loss 0.22049, entcon_acc 0.98127, 96.16 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93000, tacc(al2) 0.87300, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.93700, tacc(ob3) 0.97000, tacc(ob4) 0.95900, tacc(ob5) 0.81300, tacc(ob6) 0.96100, tacc(ob7) 0.92100, entcon_acc 0.98428, 10.22 secs\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31836, triplet_loss 0.41721, entcon_loss 0.21951, entcon_acc 0.98127, 88.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93100, tacc(al2) 0.86800, tacc(ob0) 0.98400, tacc(ob1) 0.96400, tacc(ob2) 0.93300, tacc(ob3) 0.97000, tacc(ob4) 0.95600, tacc(ob5) 0.81500, tacc(ob6) 0.95900, tacc(ob7) 0.92400, entcon_acc 0.98428, 10.83 secs\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31853, triplet_loss 0.42009, entcon_loss 0.21697, entcon_acc 0.98254, 105.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93100, tacc(al2) 0.87100, tacc(ob0) 0.98500, tacc(ob1) 0.96600, tacc(ob2) 0.93300, tacc(ob3) 0.96800, tacc(ob4) 0.95800, tacc(ob5) 0.81400, tacc(ob6) 0.96100, tacc(ob7) 0.92300, entcon_acc 0.97484, 9.67 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31895, triplet_loss 0.41996, entcon_loss 0.21794, entcon_acc 0.98095, 105.35 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93000, tacc(al2) 0.86900, tacc(ob0) 0.98600, tacc(ob1) 0.96600, tacc(ob2) 0.93900, tacc(ob3) 0.96700, tacc(ob4) 0.95800, tacc(ob5) 0.81500, tacc(ob6) 0.96100, tacc(ob7) 0.92200, entcon_acc 0.98742, 9.70 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32242, triplet_loss 0.42040, entcon_loss 0.22444, entcon_acc 0.97968, 105.27 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93700, tacc(al2) 0.85700, tacc(ob0) 0.97900, tacc(ob1) 0.96700, tacc(ob2) 0.93400, tacc(ob3) 0.97200, tacc(ob4) 0.95000, tacc(ob5) 0.79600, tacc(ob6) 0.95500, tacc(ob7) 0.91900, entcon_acc 0.98113, 9.86 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32438, triplet_loss 0.42232, entcon_loss 0.22644, entcon_acc 0.97333, 105.25 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.94400, tacc(al2) 0.86600, tacc(ob0) 0.98200, tacc(ob1) 0.96400, tacc(ob2) 0.94800, tacc(ob3) 0.97900, tacc(ob4) 0.95600, tacc(ob5) 0.81100, tacc(ob6) 0.96300, tacc(ob7) 0.92400, entcon_acc 0.99057, 9.69 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_46_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9402.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.32235, triplet_loss 0.42387, entcon_loss 0.22082, entcon_acc 0.97952, 93.24 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.94400, tacc(al2) 0.86600, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.94200, tacc(ob3) 0.97700, tacc(ob4) 0.96300, tacc(ob5) 0.81800, tacc(ob6) 0.96100, tacc(ob7) 0.92800, entcon_acc 0.97170, 10.79 secs\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.31850, triplet_loss 0.41765, entcon_loss 0.21935, entcon_acc 0.98159, 104.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.94000, tacc(al2) 0.87000, tacc(ob0) 0.98800, tacc(ob1) 0.96400, tacc(ob2) 0.94300, tacc(ob3) 0.97700, tacc(ob4) 0.96300, tacc(ob5) 0.81100, tacc(ob6) 0.95900, tacc(ob7) 0.92500, entcon_acc 0.98742, 9.90 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9409.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.32037, triplet_loss 0.42011, entcon_loss 0.22064, entcon_acc 0.98143, 105.35 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.94100, tacc(al2) 0.86900, tacc(ob0) 0.98700, tacc(ob1) 0.96400, tacc(ob2) 0.94400, tacc(ob3) 0.97600, tacc(ob4) 0.96200, tacc(ob5) 0.82000, tacc(ob6) 0.96400, tacc(ob7) 0.92200, entcon_acc 0.97799, 9.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9412.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31625, triplet_loss 0.42022, entcon_loss 0.21227, entcon_acc 0.98524, 106.01 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.94000, tacc(al2) 0.87000, tacc(ob0) 0.98700, tacc(ob1) 0.96200, tacc(ob2) 0.94400, tacc(ob3) 0.97600, tacc(ob4) 0.96700, tacc(ob5) 0.82500, tacc(ob6) 0.96700, tacc(ob7) 0.92300, entcon_acc 0.97799, 9.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_50_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9423.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31908, triplet_loss 0.41901, entcon_loss 0.21914, entcon_acc 0.97841, 104.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93800, tacc(al2) 0.87100, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.94500, tacc(ob3) 0.97300, tacc(ob4) 0.96400, tacc(ob5) 0.81800, tacc(ob6) 0.96500, tacc(ob7) 0.92300, entcon_acc 0.97484, 10.10 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31611, triplet_loss 0.41908, entcon_loss 0.21314, entcon_acc 0.98714, 104.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93800, tacc(al2) 0.87200, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.94500, tacc(ob3) 0.97300, tacc(ob4) 0.96700, tacc(ob5) 0.82000, tacc(ob6) 0.96600, tacc(ob7) 0.92300, entcon_acc 0.97484, 10.24 secs\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32169, triplet_loss 0.42348, entcon_loss 0.21989, entcon_acc 0.97984, 100.06 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98600, tacc(al1) 0.92900, tacc(al2) 0.88400, tacc(ob0) 0.98800, tacc(ob1) 0.96500, tacc(ob2) 0.94900, tacc(ob3) 0.96800, tacc(ob4) 0.96600, tacc(ob5) 0.82400, tacc(ob6) 0.96200, tacc(ob7) 0.92700, entcon_acc 0.96541, 10.36 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32202, triplet_loss 0.42201, entcon_loss 0.22202, entcon_acc 0.98063, 105.11 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98400, tacc(al1) 0.93500, tacc(al2) 0.87600, tacc(ob0) 0.98500, tacc(ob1) 0.95900, tacc(ob2) 0.93500, tacc(ob3) 0.96500, tacc(ob4) 0.96000, tacc(ob5) 0.78800, tacc(ob6) 0.95700, tacc(ob7) 0.92400, entcon_acc 0.97799, 9.97 secs\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.31797, triplet_loss 0.41819, entcon_loss 0.21775, entcon_acc 0.98349, 99.23 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.93200, tacc(al2) 0.87100, tacc(ob0) 0.98900, tacc(ob1) 0.96600, tacc(ob2) 0.93700, tacc(ob3) 0.97200, tacc(ob4) 0.96000, tacc(ob5) 0.82000, tacc(ob6) 0.96200, tacc(ob7) 0.93200, entcon_acc 0.96541, 10.29 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.31749, triplet_loss 0.41654, entcon_loss 0.21844, entcon_acc 0.98016, 105.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.93000, tacc(al2) 0.87200, tacc(ob0) 0.98600, tacc(ob1) 0.95700, tacc(ob2) 0.94300, tacc(ob3) 0.97000, tacc(ob4) 0.96300, tacc(ob5) 0.81800, tacc(ob6) 0.96200, tacc(ob7) 0.92200, entcon_acc 0.95597, 10.11 secs\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.31776, triplet_loss 0.42010, entcon_loss 0.21541, entcon_acc 0.98286, 105.05 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.93100, tacc(al2) 0.87700, tacc(ob0) 0.99000, tacc(ob1) 0.96100, tacc(ob2) 0.93700, tacc(ob3) 0.97000, tacc(ob4) 0.96200, tacc(ob5) 0.81800, tacc(ob6) 0.96200, tacc(ob7) 0.92500, entcon_acc 0.97799, 10.48 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31694, triplet_loss 0.41978, entcon_loss 0.21410, entcon_acc 0.98206, 96.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.93000, tacc(al2) 0.87200, tacc(ob0) 0.99000, tacc(ob1) 0.96800, tacc(ob2) 0.94200, tacc(ob3) 0.97400, tacc(ob4) 0.96400, tacc(ob5) 0.81800, tacc(ob6) 0.96200, tacc(ob7) 0.92400, entcon_acc 0.98742, 11.01 secs\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31547, triplet_loss 0.41758, entcon_loss 0.21335, entcon_acc 0.98302, 101.42 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.93200, tacc(al2) 0.87400, tacc(ob0) 0.98900, tacc(ob1) 0.96800, tacc(ob2) 0.94100, tacc(ob3) 0.97300, tacc(ob4) 0.96400, tacc(ob5) 0.82000, tacc(ob6) 0.96300, tacc(ob7) 0.92300, entcon_acc 0.98113, 10.90 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31577, triplet_loss 0.41923, entcon_loss 0.21231, entcon_acc 0.98460, 99.81 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93500, tacc(al2) 0.87300, tacc(ob0) 0.98900, tacc(ob1) 0.96500, tacc(ob2) 0.94300, tacc(ob3) 0.97300, tacc(ob4) 0.96500, tacc(ob5) 0.82600, tacc(ob6) 0.96200, tacc(ob7) 0.92400, entcon_acc 0.98113, 10.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_60_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9423.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32324, triplet_loss 0.42358, entcon_loss 0.22289, entcon_acc 0.97810, 105.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93600, tacc(al2) 0.86400, tacc(ob0) 0.98800, tacc(ob1) 0.96500, tacc(ob2) 0.94400, tacc(ob3) 0.96600, tacc(ob4) 0.95500, tacc(ob5) 0.81700, tacc(ob6) 0.96700, tacc(ob7) 0.92700, entcon_acc 0.97484, 10.21 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32048, triplet_loss 0.42218, entcon_loss 0.21877, entcon_acc 0.98048, 103.90 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93200, tacc(al2) 0.87900, tacc(ob0) 0.98800, tacc(ob1) 0.96000, tacc(ob2) 0.94400, tacc(ob3) 0.96700, tacc(ob4) 0.95500, tacc(ob5) 0.83000, tacc(ob6) 0.96600, tacc(ob7) 0.94000, entcon_acc 0.97484, 10.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_62_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9425.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.31988, triplet_loss 0.42372, entcon_loss 0.21605, entcon_acc 0.98143, 105.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93500, tacc(al2) 0.86500, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.94300, tacc(ob3) 0.97000, tacc(ob4) 0.95900, tacc(ob5) 0.82500, tacc(ob6) 0.96700, tacc(ob7) 0.93100, entcon_acc 0.97484, 10.28 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.31896, triplet_loss 0.42170, entcon_loss 0.21621, entcon_acc 0.97952, 105.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93500, tacc(al2) 0.85700, tacc(ob0) 0.98700, tacc(ob1) 0.96500, tacc(ob2) 0.93700, tacc(ob3) 0.97000, tacc(ob4) 0.95600, tacc(ob5) 0.82300, tacc(ob6) 0.96400, tacc(ob7) 0.92600, entcon_acc 0.97170, 9.92 secs\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.31461, triplet_loss 0.41650, entcon_loss 0.21272, entcon_acc 0.98222, 105.52 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93500, tacc(al2) 0.85900, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.94700, tacc(ob3) 0.97000, tacc(ob4) 0.95900, tacc(ob5) 0.83100, tacc(ob6) 0.96600, tacc(ob7) 0.92600, entcon_acc 0.98113, 10.38 secs\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31728, triplet_loss 0.42168, entcon_loss 0.21287, entcon_acc 0.98476, 104.08 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93700, tacc(al2) 0.85900, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.94500, tacc(ob3) 0.97100, tacc(ob4) 0.95900, tacc(ob5) 0.83200, tacc(ob6) 0.96700, tacc(ob7) 0.92600, entcon_acc 0.97484, 10.71 secs\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31891, triplet_loss 0.42312, entcon_loss 0.21470, entcon_acc 0.98048, 103.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93500, tacc(al2) 0.86100, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.94400, tacc(ob3) 0.97000, tacc(ob4) 0.96000, tacc(ob5) 0.83300, tacc(ob6) 0.96600, tacc(ob7) 0.92900, entcon_acc 0.98113, 10.78 secs\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31698, triplet_loss 0.42031, entcon_loss 0.21365, entcon_acc 0.98159, 105.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93500, tacc(al2) 0.86100, tacc(ob0) 0.98700, tacc(ob1) 0.96200, tacc(ob2) 0.94300, tacc(ob3) 0.96900, tacc(ob4) 0.96000, tacc(ob5) 0.83100, tacc(ob6) 0.96500, tacc(ob7) 0.92800, entcon_acc 0.97170, 10.64 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32249, triplet_loss 0.42332, entcon_loss 0.22165, entcon_acc 0.98095, 104.24 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93000, tacc(al2) 0.87500, tacc(ob0) 0.98800, tacc(ob1) 0.96300, tacc(ob2) 0.93300, tacc(ob3) 0.97000, tacc(ob4) 0.96000, tacc(ob5) 0.80800, tacc(ob6) 0.96300, tacc(ob7) 0.92800, entcon_acc 0.99057, 10.47 secs\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.31897, triplet_loss 0.42226, entcon_loss 0.21569, entcon_acc 0.98127, 104.24 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93200, tacc(al2) 0.85900, tacc(ob0) 0.98500, tacc(ob1) 0.96700, tacc(ob2) 0.93000, tacc(ob3) 0.97000, tacc(ob4) 0.95100, tacc(ob5) 0.79900, tacc(ob6) 0.96100, tacc(ob7) 0.92900, entcon_acc 0.98742, 10.31 secs\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.32039, triplet_loss 0.42638, entcon_loss 0.21440, entcon_acc 0.98365, 104.20 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93100, tacc(al2) 0.85500, tacc(ob0) 0.98900, tacc(ob1) 0.96700, tacc(ob2) 0.93600, tacc(ob3) 0.97000, tacc(ob4) 0.95400, tacc(ob5) 0.80700, tacc(ob6) 0.96700, tacc(ob7) 0.93200, entcon_acc 0.97799, 10.55 secs\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.31941, triplet_loss 0.42200, entcon_loss 0.21682, entcon_acc 0.98143, 105.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93100, tacc(al2) 0.85600, tacc(ob0) 0.98800, tacc(ob1) 0.96400, tacc(ob2) 0.93800, tacc(ob3) 0.97000, tacc(ob4) 0.95700, tacc(ob5) 0.81100, tacc(ob6) 0.96300, tacc(ob7) 0.92500, entcon_acc 0.98742, 10.20 secs\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.31559, triplet_loss 0.42048, entcon_loss 0.21070, entcon_acc 0.98524, 105.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93400, tacc(al2) 0.86000, tacc(ob0) 0.98800, tacc(ob1) 0.96600, tacc(ob2) 0.94000, tacc(ob3) 0.97300, tacc(ob4) 0.95800, tacc(ob5) 0.81600, tacc(ob6) 0.96400, tacc(ob7) 0.92800, entcon_acc 0.97170, 10.37 secs\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31396, triplet_loss 0.41671, entcon_loss 0.21121, entcon_acc 0.98413, 104.57 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93600, tacc(al2) 0.85900, tacc(ob0) 0.98800, tacc(ob1) 0.96500, tacc(ob2) 0.93700, tacc(ob3) 0.97200, tacc(ob4) 0.95900, tacc(ob5) 0.81300, tacc(ob6) 0.96400, tacc(ob7) 0.92700, entcon_acc 0.99057, 10.55 secs\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31655, triplet_loss 0.41912, entcon_loss 0.21399, entcon_acc 0.98222, 96.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93600, tacc(al2) 0.86200, tacc(ob0) 0.98900, tacc(ob1) 0.96600, tacc(ob2) 0.93800, tacc(ob3) 0.97200, tacc(ob4) 0.96100, tacc(ob5) 0.81200, tacc(ob6) 0.96400, tacc(ob7) 0.92800, entcon_acc 0.98113, 11.18 secs\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31434, triplet_loss 0.42004, entcon_loss 0.20864, entcon_acc 0.98476, 104.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93600, tacc(al2) 0.86200, tacc(ob0) 0.98900, tacc(ob1) 0.96600, tacc(ob2) 0.93900, tacc(ob3) 0.97200, tacc(ob4) 0.96000, tacc(ob5) 0.81500, tacc(ob6) 0.96500, tacc(ob7) 0.92800, entcon_acc 0.98113, 10.80 secs\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.32000, triplet_loss 0.41966, entcon_loss 0.22035, entcon_acc 0.98238, 104.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92700, tacc(al2) 0.85900, tacc(ob0) 0.98300, tacc(ob1) 0.96200, tacc(ob2) 0.92800, tacc(ob3) 0.97400, tacc(ob4) 0.95600, tacc(ob5) 0.80500, tacc(ob6) 0.96500, tacc(ob7) 0.92000, entcon_acc 0.97484, 10.63 secs\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32268, triplet_loss 0.42177, entcon_loss 0.22358, entcon_acc 0.97778, 106.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93400, tacc(al2) 0.86300, tacc(ob0) 0.98700, tacc(ob1) 0.96200, tacc(ob2) 0.94300, tacc(ob3) 0.97600, tacc(ob4) 0.95800, tacc(ob5) 0.83400, tacc(ob6) 0.96800, tacc(ob7) 0.92100, entcon_acc 0.97484, 10.64 secs\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.31786, triplet_loss 0.42154, entcon_loss 0.21419, entcon_acc 0.98302, 104.88 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93500, tacc(al2) 0.87000, tacc(ob0) 0.98900, tacc(ob1) 0.96600, tacc(ob2) 0.94700, tacc(ob3) 0.97400, tacc(ob4) 0.96000, tacc(ob5) 0.83000, tacc(ob6) 0.96500, tacc(ob7) 0.92100, entcon_acc 0.96855, 10.67 secs\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.31602, triplet_loss 0.42174, entcon_loss 0.21031, entcon_acc 0.98286, 100.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93700, tacc(al2) 0.86800, tacc(ob0) 0.98900, tacc(ob1) 0.96000, tacc(ob2) 0.94000, tacc(ob3) 0.97100, tacc(ob4) 0.96200, tacc(ob5) 0.82300, tacc(ob6) 0.96800, tacc(ob7) 0.92300, entcon_acc 0.97170, 11.09 secs\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.31468, triplet_loss 0.41863, entcon_loss 0.21073, entcon_acc 0.98508, 104.11 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93600, tacc(al2) 0.86500, tacc(ob0) 0.98700, tacc(ob1) 0.95900, tacc(ob2) 0.93700, tacc(ob3) 0.97400, tacc(ob4) 0.96100, tacc(ob5) 0.81700, tacc(ob6) 0.96400, tacc(ob7) 0.92200, entcon_acc 0.98742, 10.76 secs\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31444, triplet_loss 0.41935, entcon_loss 0.20953, entcon_acc 0.98460, 105.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93700, tacc(al2) 0.86500, tacc(ob0) 0.98700, tacc(ob1) 0.96100, tacc(ob2) 0.93800, tacc(ob3) 0.97300, tacc(ob4) 0.96200, tacc(ob5) 0.82100, tacc(ob6) 0.96900, tacc(ob7) 0.92400, entcon_acc 0.97799, 11.01 secs\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31477, triplet_loss 0.41816, entcon_loss 0.21137, entcon_acc 0.98127, 106.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93800, tacc(al2) 0.86600, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.94100, tacc(ob3) 0.97200, tacc(ob4) 0.96300, tacc(ob5) 0.82500, tacc(ob6) 0.96800, tacc(ob7) 0.92600, entcon_acc 0.96226, 10.79 secs\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31481, triplet_loss 0.42066, entcon_loss 0.20895, entcon_acc 0.98349, 106.11 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93700, tacc(al2) 0.86400, tacc(ob0) 0.98800, tacc(ob1) 0.96400, tacc(ob2) 0.93900, tacc(ob3) 0.97300, tacc(ob4) 0.96300, tacc(ob5) 0.82700, tacc(ob6) 0.96900, tacc(ob7) 0.92600, entcon_acc 0.95597, 10.31 secs\n",
      "\u001b[1m---- Epoch 85/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.31802, triplet_loss 0.42107, entcon_loss 0.21498, entcon_acc 0.98206, 105.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92700, tacc(al2) 0.86900, tacc(ob0) 0.98700, tacc(ob1) 0.94900, tacc(ob2) 0.93600, tacc(ob3) 0.96800, tacc(ob4) 0.96200, tacc(ob5) 0.81200, tacc(ob6) 0.96800, tacc(ob7) 0.92400, entcon_acc 0.98113, 10.54 secs\n",
      "\u001b[1m---- Epoch 86/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.32080, triplet_loss 0.42617, entcon_loss 0.21542, entcon_acc 0.98190, 106.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.87700, tacc(ob0) 0.98900, tacc(ob1) 0.95800, tacc(ob2) 0.94500, tacc(ob3) 0.97400, tacc(ob4) 0.96400, tacc(ob5) 0.83800, tacc(ob6) 0.96800, tacc(ob7) 0.92200, entcon_acc 0.98428, 10.78 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_86_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9427.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 87/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.31642, triplet_loss 0.41689, entcon_loss 0.21596, entcon_acc 0.98365, 106.51 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.93000, tacc(al2) 0.87000, tacc(ob0) 0.98500, tacc(ob1) 0.95800, tacc(ob2) 0.94300, tacc(ob3) 0.97200, tacc(ob4) 0.96300, tacc(ob5) 0.83000, tacc(ob6) 0.96600, tacc(ob7) 0.92300, entcon_acc 0.98113, 10.54 secs\n",
      "\u001b[1m---- Epoch 88/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.31702, triplet_loss 0.42006, entcon_loss 0.21398, entcon_acc 0.98556, 104.01 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93300, tacc(al2) 0.86900, tacc(ob0) 0.98600, tacc(ob1) 0.96000, tacc(ob2) 0.94500, tacc(ob3) 0.97200, tacc(ob4) 0.96200, tacc(ob5) 0.83300, tacc(ob6) 0.96600, tacc(ob7) 0.92500, entcon_acc 0.97170, 10.79 secs\n",
      "\u001b[1m---- Epoch 89/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.31602, triplet_loss 0.42020, entcon_loss 0.21184, entcon_acc 0.98254, 103.48 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92700, tacc(al2) 0.86200, tacc(ob0) 0.98600, tacc(ob1) 0.96000, tacc(ob2) 0.94200, tacc(ob3) 0.97000, tacc(ob4) 0.96400, tacc(ob5) 0.82700, tacc(ob6) 0.96500, tacc(ob7) 0.92600, entcon_acc 0.98113, 10.83 secs\n",
      "\u001b[1m---- Epoch 90/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31790, triplet_loss 0.42579, entcon_loss 0.21001, entcon_acc 0.98333, 102.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92700, tacc(al2) 0.86300, tacc(ob0) 0.98700, tacc(ob1) 0.96300, tacc(ob2) 0.94300, tacc(ob3) 0.97000, tacc(ob4) 0.96500, tacc(ob5) 0.82600, tacc(ob6) 0.96300, tacc(ob7) 0.92500, entcon_acc 0.96226, 10.77 secs\n",
      "\u001b[1m---- Epoch 91/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31172, triplet_loss 0.41477, entcon_loss 0.20866, entcon_acc 0.98317, 106.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92700, tacc(al2) 0.86600, tacc(ob0) 0.98700, tacc(ob1) 0.96400, tacc(ob2) 0.93700, tacc(ob3) 0.97100, tacc(ob4) 0.96500, tacc(ob5) 0.82500, tacc(ob6) 0.96000, tacc(ob7) 0.92400, entcon_acc 0.98113, 10.93 secs\n",
      "\u001b[1m---- Epoch 92/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31501, triplet_loss 0.41979, entcon_loss 0.21023, entcon_acc 0.98333, 106.82 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92700, tacc(al2) 0.86500, tacc(ob0) 0.98700, tacc(ob1) 0.96400, tacc(ob2) 0.94100, tacc(ob3) 0.97100, tacc(ob4) 0.96600, tacc(ob5) 0.82600, tacc(ob6) 0.96300, tacc(ob7) 0.92400, entcon_acc 0.97170, 10.92 secs\n",
      "\u001b[1m---- Epoch 93/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.31847, triplet_loss 0.42128, entcon_loss 0.21567, entcon_acc 0.98238, 90.19 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.93400, tacc(al2) 0.83600, tacc(ob0) 0.98600, tacc(ob1) 0.95800, tacc(ob2) 0.92800, tacc(ob3) 0.97300, tacc(ob4) 0.95200, tacc(ob5) 0.81000, tacc(ob6) 0.95800, tacc(ob7) 0.92100, entcon_acc 0.98428, 10.98 secs\n",
      "\u001b[1m---- Epoch 94/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.31934, triplet_loss 0.42362, entcon_loss 0.21506, entcon_acc 0.98317, 102.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93700, tacc(al2) 0.86200, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.93800, tacc(ob3) 0.97300, tacc(ob4) 0.96600, tacc(ob5) 0.85300, tacc(ob6) 0.96800, tacc(ob7) 0.91700, entcon_acc 0.97170, 10.96 secs\n",
      "\u001b[1m---- Epoch 95/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.31827, triplet_loss 0.42223, entcon_loss 0.21431, entcon_acc 0.98063, 105.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.93400, tacc(al2) 0.86200, tacc(ob0) 0.98500, tacc(ob1) 0.95900, tacc(ob2) 0.93700, tacc(ob3) 0.97200, tacc(ob4) 0.96300, tacc(ob5) 0.83000, tacc(ob6) 0.96800, tacc(ob7) 0.92300, entcon_acc 0.98113, 10.92 secs\n",
      "\u001b[1m---- Epoch 96/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.31572, triplet_loss 0.41818, entcon_loss 0.21325, entcon_acc 0.98238, 97.95 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93700, tacc(al2) 0.86400, tacc(ob0) 0.98500, tacc(ob1) 0.95900, tacc(ob2) 0.94100, tacc(ob3) 0.97100, tacc(ob4) 0.96100, tacc(ob5) 0.82700, tacc(ob6) 0.96800, tacc(ob7) 0.92700, entcon_acc 0.98113, 10.94 secs\n",
      "\u001b[1m---- Epoch 97/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.31366, triplet_loss 0.41841, entcon_loss 0.20892, entcon_acc 0.98444, 105.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93700, tacc(al2) 0.86800, tacc(ob0) 0.98600, tacc(ob1) 0.95800, tacc(ob2) 0.94000, tacc(ob3) 0.96900, tacc(ob4) 0.96300, tacc(ob5) 0.82700, tacc(ob6) 0.96900, tacc(ob7) 0.92600, entcon_acc 0.97170, 10.80 secs\n",
      "\u001b[1m---- Epoch 98/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.31503, triplet_loss 0.42303, entcon_loss 0.20703, entcon_acc 0.98397, 106.35 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.94000, tacc(al2) 0.86800, tacc(ob0) 0.98600, tacc(ob1) 0.96100, tacc(ob2) 0.94200, tacc(ob3) 0.96700, tacc(ob4) 0.96600, tacc(ob5) 0.84100, tacc(ob6) 0.97100, tacc(ob7) 0.92700, entcon_acc 0.98113, 10.51 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_98_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9440.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 99/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.31489, triplet_loss 0.42116, entcon_loss 0.20863, entcon_acc 0.98365, 105.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.94000, tacc(al2) 0.86700, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.94100, tacc(ob3) 0.96900, tacc(ob4) 0.96400, tacc(ob5) 0.84000, tacc(ob6) 0.97000, tacc(ob7) 0.92800, entcon_acc 0.97799, 10.91 secs\n",
      "\u001b[1m---- Epoch 100/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.31267, triplet_loss 0.41785, entcon_loss 0.20750, entcon_acc 0.98111, 105.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.94100, tacc(al2) 0.86600, tacc(ob0) 0.98500, tacc(ob1) 0.96200, tacc(ob2) 0.94100, tacc(ob3) 0.96900, tacc(ob4) 0.96400, tacc(ob5) 0.83400, tacc(ob6) 0.97000, tacc(ob7) 0.92800, entcon_acc 0.97484, 10.82 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231003_001641_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 21 \\\n",
    "--val_batch_size 80 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,2e-6,8e-5,8,2e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 2., 1.5, 2.]}\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(169025,21838032).jsonl\" \\\n",
    "--integrated_sentence_facts_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\" \\\n",
    "--triplets_weight 1.0 \\\n",
    "--entcon_weight 1.0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets+entcon)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
