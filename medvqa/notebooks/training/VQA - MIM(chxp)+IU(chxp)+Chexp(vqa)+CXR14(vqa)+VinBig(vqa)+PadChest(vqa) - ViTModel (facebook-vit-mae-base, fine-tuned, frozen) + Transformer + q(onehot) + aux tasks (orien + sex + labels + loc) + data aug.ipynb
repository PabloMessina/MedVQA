{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 pamessina pamessina 1.3G Jan  6 14:27 '/mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt'\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 35\n",
      "   batches_per_epoch: 200\n",
      "   checkpoint_folder: None\n",
      "   iuxray_qa_adapted_reports_filename: qa_adapted_reports__20220904_091601.json\n",
      "   mimiccxr_qa_adapted_reports_filename: qa_adapted_reports__20220904_095810.json\n",
      "   vocab_min_freq: 10\n",
      "   embed_size: 256\n",
      "   question_encoding: one-hot\n",
      "   answer_decoding: transformer\n",
      "   question_hidden_size: 128\n",
      "   answer_hidden_size: 256\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: vitmodel-huggingface\n",
      "   image_local_feat_size: 768\n",
      "   image_encoder_pretrained_weights_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "   freeze_image_encoder: True\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: facebook/vit-mae-base\n",
      "   n_lstm_layers: 1\n",
      "   transf_dec_nhead: 2\n",
      "   transf_dec_dim_forward: 256\n",
      "   transf_dec_num_layers: 2\n",
      "   question_vec_size: 128\n",
      "   dropout_prob: 0\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,4e-4,30,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 150\n",
      "   iters_to_accumulate: 5\n",
      "   num_workers: 4\n",
      "   device: GPU\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   image_size: [224, 224]\n",
      "   mimiccxr_weight: 1.0\n",
      "   chexpert_weight: 0.5\n",
      "   cxr14_weight: 0.4\n",
      "   vinbig_weight: 0.4\n",
      "   iuxray_weight: 0.2\n",
      "   mimiccxr_weight_chexpert_mode: 0.8\n",
      "   iuxray_weight_chexpert_mode: 0.16\n",
      "   padchest_weight: 0.5\n",
      "   mimiccxr_include_chexpert_mode: True\n",
      "   iuxray_include_chexpert_mode: True\n",
      "   use_chexpert_mode_only: False\n",
      "   val_answer_decoding: greedy-search\n",
      "   beam_search_k: None\n",
      "   use_amp: True\n",
      "   medical_tokenization: True\n",
      "   medical_terms_frequency_filename: medical_terms_frequency__20220918_184255.pkl\n",
      "   allowed_questions: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   balanced_dataloading: True\n",
      "   imbalance_reduction_coef: 0.5\n",
      "   iuxray_balanced_metadata_filename: balanced_dataloading_metadata__20220918_210029.pkl\n",
      "   mimiccxr_balanced_metadata_filename: balanced_dataloading_metadata__20220918_210415.pkl\n",
      "   one_question_per_batch: False\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: True\n",
      "   train_iuxray: True\n",
      "   train_chexpert: True\n",
      "   chexpert_mode: vqa\n",
      "   train_cxr14: True\n",
      "   train_vinbig: True\n",
      "   vinbig_training_data: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: True\n",
      "   padchest_training_data_mode: all\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\n",
      "   padchest_val_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\n",
      "   padchest_test_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\n",
      "   binary_loss_name: wbce-c\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: True\n",
      "   classify_chexpert: True\n",
      "   iuxray_chexpert_labels_filename: chexpert_labels_per_report__20220904_113427.pkl\n",
      "   mimiccxr_chexpert_labels_filename: chexpert_labels_per_report__20220904_113605.pkl\n",
      "   classify_questions: True\n",
      "   n_questions: 97\n",
      "   iuxray_question_labels_filename: question_labels_per_report__20220904_105752.pkl\n",
      "   mimiccxr_question_labels_filename: question_labels_per_report__20220904_105753.pkl\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "Debug: balanced_dataloading = True\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mLoading mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.mask_token', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.4.layernorm_after.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,4e-4,30,1e-6\n",
      "1e-06 5 0.0004 30 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=True\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "len(final_transforms) = 16\n",
      "default_prob = 0.3\n",
      "Returning augmented transforms with mode random-color-and-spatial\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=4, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=0, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=3, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=2, one_hot_question_offset=97\n",
      "get_vqa_collate_batch_fn(): dataset_id=6, one_hot_question_offset=111\n",
      "get_vqa_collate_batch_fn(): dataset_id=5, one_hot_question_offset=126\n",
      "get_vqa_collate_batch_fn(): dataset_id=7, one_hot_question_offset=154\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa trainer ...\u001b[0m\n",
      "MIMICCXR_VQA_Trainer: balanced_dataloading = True\n",
      "self.balanced_dataloading = True\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_train_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053,medical_terms_frequency__20220918_184255.pkl).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 150\n",
      "len(self.report_ids) = 2034935, len(set(self.report_ids)) = 223623\n",
      "_generate_datasets_and_dataloaders()\n",
      "self.balanced_dataloading = True\n",
      "_generate_train_dataset__balanced()\n",
      "Balanced train data loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/balanced_train_data(hash=287,3559006002065882738).pkl\n",
      "\tlen(question_datasets) = 92\n",
      "len(self.val_indices) = 16416\n",
      "len(val_indices) = 3258\n",
      "generating training and validation dataloaders ...\n",
      "num_workers = 4\n",
      "Generating perfectly balanced train dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=43016, len(neg_indices)=178811\n",
      "label = 1, onehot=98, len(pos_indices)=43215, len(neg_indices)=178612\n",
      "label = 2, onehot=99, len(pos_indices)=71655, len(neg_indices)=150172\n",
      "label = 3, onehot=100, len(pos_indices)=9668, len(neg_indices)=212159\n",
      "label = 4, onehot=101, len(pos_indices)=73721, len(neg_indices)=148106\n",
      "label = 5, onehot=102, len(pos_indices)=42623, len(neg_indices)=179204\n",
      "label = 6, onehot=103, len(pos_indices)=19790, len(neg_indices)=202037\n",
      "label = 7, onehot=104, len(pos_indices)=37343, len(neg_indices)=184484\n",
      "label = 8, onehot=105, len(pos_indices)=69678, len(neg_indices)=152149\n",
      "label = 9, onehot=106, len(pos_indices)=13071, len(neg_indices)=208756\n",
      "label = 10, onehot=107, len(pos_indices)=68496, len(neg_indices)=153331\n",
      "label = 11, onehot=108, len(pos_indices)=4944, len(neg_indices)=216883\n",
      "label = 12, onehot=109, len(pos_indices)=8414, len(neg_indices)=213413\n",
      "label = 13, onehot=110, len(pos_indices)=87913, len(neg_indices)=133914\n",
      "len(self.train_dataset__chexpert_mode) = 1000000000000000000\n",
      "Generating balanced validation dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 1, onehot=98, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 2, onehot=99, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 3, onehot=100, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 4, onehot=101, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 5, onehot=102, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 6, onehot=103, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 7, onehot=104, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 8, onehot=105, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 9, onehot=106, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 10, onehot=107, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 11, onehot=108, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 12, onehot=109, len(pos_indices)=37, len(neg_indices)=40\n",
      "label = 13, onehot=110, len(pos_indices)=40, len(neg_indices)=40\n",
      "len(self.val_dataset__chexpert_mode) = 1117\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mCreating IU X-Ray vqa trainer ...\u001b[0m\n",
      "self.balanced_dataloading = True\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray/iuxray_preprocessed_train_data__(dataset=qa_adapted_reports__20220904_091601.json;tokenizer=4871,39534,3343246773703667053,medical_terms_frequency__20220918_184255.pkl).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "Assigning all data to training ...\n",
      "All data assigned to train: len(train_indices) = 29046, len(val_indices) = 0\n",
      "batch_size = 150\n",
      "len(self.report_ids) = 29046, len(set(self.report_ids)) = 3806\n",
      "_generate_datasets_and_dataloaders()\n",
      "self.balanced_dataloading = True\n",
      "_generate_train_dataset__balanced()\n",
      "Balanced train data loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray/balanced_train_data(hash=283,2750089748417475840).pkl\n",
      "\tlen(question_datasets) = 42\n",
      "generating training and validation dataloaders ...\n",
      "num_workers = 4\n",
      "Generating perfectly balanced train dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=1414, len(neg_indices)=2392\n",
      "label = 1, onehot=98, len(pos_indices)=387, len(neg_indices)=3419\n",
      "label = 2, onehot=99, len(pos_indices)=675, len(neg_indices)=3131\n",
      "label = 3, onehot=100, len(pos_indices)=232, len(neg_indices)=3574\n",
      "label = 4, onehot=101, len(pos_indices)=708, len(neg_indices)=3098\n",
      "label = 5, onehot=102, len(pos_indices)=153, len(neg_indices)=3653\n",
      "label = 6, onehot=103, len(pos_indices)=43, len(neg_indices)=3763\n",
      "label = 7, onehot=104, len(pos_indices)=138, len(neg_indices)=3668\n",
      "label = 8, onehot=105, len(pos_indices)=368, len(neg_indices)=3438\n",
      "label = 9, onehot=106, len(pos_indices)=100, len(neg_indices)=3706\n",
      "label = 10, onehot=107, len(pos_indices)=295, len(neg_indices)=3511\n",
      "label = 11, onehot=108, len(pos_indices)=71, len(neg_indices)=3735\n",
      "label = 12, onehot=109, len(pos_indices)=119, len(neg_indices)=3687\n",
      "label = 13, onehot=110, len(pos_indices)=220, len(neg_indices)=3586\n",
      "len(self.train_dataset__chexpert_mode) = 1000000000000000000\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mCreating CheXpert vqa trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/workspace/chexpert/CheXpert-v1.0-small/train-val.csv\n",
      "Loading images\n",
      "Loading orientations\n",
      "Loading genders\n",
      "Loading chexpert labels\n",
      "label = 0, onehot=0, len(pos_indices)=22414, len(neg_indices)=201216\n",
      "label = 1, onehot=1, len(pos_indices)=23309, len(neg_indices)=200321\n",
      "label = 2, onehot=2, len(pos_indices)=35151, len(neg_indices)=188479\n",
      "label = 3, onehot=3, len(pos_indices)=10675, len(neg_indices)=212955\n",
      "label = 4, onehot=4, len(pos_indices)=111301, len(neg_indices)=112329\n",
      "label = 5, onehot=5, len(pos_indices)=65274, len(neg_indices)=158356\n",
      "label = 6, onehot=6, len(pos_indices)=42556, len(neg_indices)=181074\n",
      "label = 7, onehot=7, len(pos_indices)=24815, len(neg_indices)=198815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 8, onehot=8, len(pos_indices)=67191, len(neg_indices)=156439\n",
      "label = 9, onehot=9, len(pos_indices)=22601, len(neg_indices)=201029\n",
      "label = 10, onehot=10, len(pos_indices)=97875, len(neg_indices)=125755\n",
      "label = 11, onehot=11, len(pos_indices)=6174, len(neg_indices)=217456\n",
      "label = 12, onehot=12, len(pos_indices)=9680, len(neg_indices)=213950\n",
      "label = 13, onehot=13, len(pos_indices)=117184, len(neg_indices)=106446\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mCreating CXR14 vqa trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/data/chest-x-ray-8/metadata/Data_Entry_2017_v2020.csv\n",
      "Loading images\n",
      "Loading orientations\n",
      "Loading genders\n",
      "Loading CXR14 labels\n",
      "label = 0, onehot=0, len(pos_indices)=60361, len(neg_indices)=51759\n",
      "label = 1, onehot=1, len(pos_indices)=11559, len(neg_indices)=100561\n",
      "label = 2, onehot=2, len(pos_indices)=2776, len(neg_indices)=109344\n",
      "label = 3, onehot=3, len(pos_indices)=4667, len(neg_indices)=107453\n",
      "label = 4, onehot=4, len(pos_indices)=2303, len(neg_indices)=109817\n",
      "label = 5, onehot=5, len(pos_indices)=13317, len(neg_indices)=98803\n",
      "label = 6, onehot=6, len(pos_indices)=2516, len(neg_indices)=109604\n",
      "label = 7, onehot=7, len(pos_indices)=1686, len(neg_indices)=110434\n",
      "label = 8, onehot=8, len(pos_indices)=227, len(neg_indices)=111893\n",
      "label = 9, onehot=9, len(pos_indices)=19894, len(neg_indices)=92226\n",
      "label = 10, onehot=10, len(pos_indices)=5782, len(neg_indices)=106338\n",
      "label = 11, onehot=11, len(pos_indices)=6331, len(neg_indices)=105789\n",
      "label = 12, onehot=12, len(pos_indices)=3385, len(neg_indices)=108735\n",
      "label = 13, onehot=13, len(pos_indices)=1431, len(neg_indices)=110689\n",
      "label = 14, onehot=14, len(pos_indices)=5302, len(neg_indices)=106818\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m15) \u001b[0m\u001b[34mCreating VinBig vqa trainer ...\u001b[0m\n",
      "Loading dataframe from:\n",
      "  /mnt/workspace/vinbig-cxr/dataset-png/annotations/image_labels_train.csv\n",
      "  /mnt/workspace/vinbig-cxr/dataset-png/annotations/image_labels_test.csv\n",
      "Sanity checking train labels ...\n",
      "Done!\n",
      "Generating train dataset and dataloader\n",
      "label = 0, onehot=0, len(pos_indices)=3287, len(neg_indices)=14713\n",
      "label = 1, onehot=1, len(pos_indices)=272, len(neg_indices)=17728\n",
      "label = 2, onehot=2, len(pos_indices)=646, len(neg_indices)=17354\n",
      "label = 3, onehot=3, len(pos_indices)=2609, len(neg_indices)=15391\n",
      "label = 4, onehot=4, len(pos_indices)=29, len(neg_indices)=17971\n",
      "label = 5, onehot=5, len(pos_indices)=449, len(neg_indices)=17551\n",
      "label = 6, onehot=6, len(pos_indices)=13, len(neg_indices)=17987\n",
      "label = 7, onehot=7, len(pos_indices)=81, len(neg_indices)=17919\n",
      "label = 8, onehot=8, len(pos_indices)=139, len(neg_indices)=17861\n",
      "label = 9, onehot=9, len(pos_indices)=607, len(neg_indices)=17393\n",
      "label = 10, onehot=10, len(pos_indices)=671, len(neg_indices)=17329\n",
      "label = 11, onehot=11, len(pos_indices)=1406, len(neg_indices)=16594\n",
      "label = 12, onehot=12, len(pos_indices)=59, len(neg_indices)=17941\n",
      "label = 13, onehot=13, len(pos_indices)=34, len(neg_indices)=17966\n",
      "label = 14, onehot=14, len(pos_indices)=170, len(neg_indices)=17830\n",
      "label = 15, onehot=15, len(pos_indices)=1006, len(neg_indices)=16994\n",
      "label = 16, onehot=16, len(pos_indices)=1143, len(neg_indices)=16857\n",
      "label = 17, onehot=17, len(pos_indices)=2150, len(neg_indices)=15850\n",
      "label = 18, onehot=18, len(pos_indices)=114, len(neg_indices)=17886\n",
      "label = 19, onehot=19, len(pos_indices)=1834, len(neg_indices)=16166\n",
      "label = 20, onehot=20, len(pos_indices)=101, len(neg_indices)=17899\n",
      "label = 21, onehot=21, len(pos_indices)=1228, len(neg_indices)=16772\n",
      "label = 22, onehot=22, len(pos_indices)=37, len(neg_indices)=17963\n",
      "label = 23, onehot=23, len(pos_indices)=371, len(neg_indices)=17629\n",
      "label = 24, onehot=24, len(pos_indices)=1163, len(neg_indices)=16837\n",
      "label = 25, onehot=25, len(pos_indices)=914, len(neg_indices)=17086\n",
      "label = 26, onehot=26, len(pos_indices)=4945, len(neg_indices)=13055\n",
      "label = 27, onehot=27, len(pos_indices)=12652, len(neg_indices)=5348\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m16) \u001b[0m\u001b[34mCreating PadChest vqa trainer ...\u001b[0m\n",
      "../train_vqa.py:1440: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug=debug)\n",
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of rows after filtering: 109821 (dropped rows with duplicate StudyID)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n",
      "Generating answers based on labels...\n",
      "100%|████████████████████████████████| 109821/109821 [00:01<00:00, 61610.35it/s]\n",
      "Done. Example answer: <s> air trapping , callus rib fracture , humeral fracture , kyphosis , vertebral anterior compression , unchanged </s>\n",
      "Generating answers based on localizations...\n",
      "100%|████████████████████████████████| 109821/109821 [00:01<00:00, 73594.19it/s]\n",
      "Done. Example answer: <s> </s>\n",
      "Number of non-empty answers: 63528\n",
      "Generating answers based on labels localizations by sentence...\n",
      "100%|████████████████████████████████| 109821/109821 [00:02<00:00, 42651.18it/s]\n",
      "Done. Example answer: <s> cardiomegaly loc cardiac aortic elongation supra aortic elongation loc supra aortic loc aortic normal </s>\n",
      "Creating dataset and dataloader...\n",
      "Example of positive localization answer: <s> loc bilateral , loc perihilar </s>\n",
      "Example of negative localization answer: <s> </s>\n",
      "193it [00:42,  4.49it/s]\n",
      "Done!\n",
      "len(self.train_dataset) = 1000000000000000000\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m17) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 8\n",
      "len(_val_dataloaders) = 2\n",
      "_train_weights = [1.0, 0.8, 0.2, 0.16, 0.5, 0.4, 0.4, 0.5]\n",
      "merged_dataset_name = mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m18) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m19) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m20) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m21) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m22) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 11.84851, a_loss 8.58174, cD 0.00028, wmdcmp 0.00442, ema 0.00000, oracc 0.37314, orien_loss 1.12916, qlmicf1 0.07478, qlmacf1 0.07390, ql_loss 1.13181, chxlmicf1 0.14168, chxlmacf1 0.14951, chx_loss 1.10079, chxlacc 0.48174, chxlrocaucmic 0.45011, chxlrocaucmac 0.50746, gacc 0.44657, gloss 0.70227, cxr14micf1 0.19334, cxr14macf1 0.14888, cxr14_loss 1.24214, vnbgmicf1 0.13108, vnbgmacf1 0.10288, vnbg_loss 9.77401, b1 0.00050, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01260, padchxlmicf1 0.01030, padchxlzmacf1 0.02918, padchxlzmicf1 0.03976, padchxl_loss 0.95509, padchxlz_loss 1.03052, 195.49 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.00006, wmdcmp 0.00245, ema 0.00000, oracc 0.36287, qlmicf1 0.08233, qlmacf1 0.08086, chxlmicf1 0.16643, chxlmacf1 0.16497, chxlacc 0.48627, chxlrocaucmic 0.46014, chxlrocaucmac 0.50344, 39.44 secs\n",
      "Adjusting learning rate of group 0 to 3.3145e-06.\n",
      "\u001b[1m---- Epoch 2/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 11.55291, a_loss 8.48120, cD 0.00025, wmdcmp 0.00397, ema 0.00005, oracc 0.38660, orien_loss 1.10089, qlmicf1 0.07964, qlmacf1 0.07807, ql_loss 1.13120, chxlmicf1 0.14204, chxlmacf1 0.15023, chx_loss 1.09955, chxlacc 0.48099, chxlrocaucmic 0.45089, chxlrocaucmac 0.50731, gacc 0.44190, gloss 0.69902, cxr14micf1 0.18620, cxr14macf1 0.14575, cxr14_loss 1.24345, vnbgmicf1 0.12601, vnbgmacf1 0.10528, vnbg_loss 9.63589, b1 0.00052, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01209, padchxlmicf1 0.01054, padchxlzmacf1 0.02765, padchxlzmicf1 0.03834, padchxl_loss 0.94558, padchxlz_loss 1.01896, 142.13 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.00008, wmdcmp 0.00222, ema 0.00000, oracc 0.40221, qlmicf1 0.09158, qlmacf1 0.08644, chxlmicf1 0.16536, chxlmacf1 0.16416, chxlacc 0.48465, chxlrocaucmic 0.46182, chxlrocaucmac 0.49854, 37.75 secs\n",
      "Adjusting learning rate of group 0 to 1.0986e-05.\n",
      "\u001b[1m---- Epoch 3/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 11.51079, a_loss 8.12720, cD 0.00011, wmdcmp 0.00124, ema 0.02945, oracc 0.61025, orien_loss 1.01834, qlmicf1 0.10939, qlmacf1 0.09241, ql_loss 1.12575, chxlmicf1 0.14666, chxlmacf1 0.16670, chx_loss 1.10071, chxlacc 0.47905, chxlrocaucmic 0.45760, chxlrocaucmac 0.50897, gacc 0.52058, gloss 0.69152, cxr14micf1 0.18816, cxr14macf1 0.15121, cxr14_loss 1.24087, vnbgmicf1 0.11247, vnbgmacf1 0.10031, vnbg_loss 9.15153, b1 0.00059, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01358, padchxlmicf1 0.01217, padchxlzmacf1 0.02837, padchxlzmicf1 0.03973, padchxl_loss 0.93737, padchxlz_loss 1.01169, 135.83 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.00000, wmdcmp 0.00000, ema 0.03581, oracc 0.62559, qlmicf1 0.14451, qlmacf1 0.10528, chxlmicf1 0.18485, chxlmacf1 0.20963, chxlacc 0.47967, chxlrocaucmic 0.47793, chxlrocaucmac 0.51601, 35.07 secs\n",
      "Adjusting learning rate of group 0 to 3.6411e-05.\n",
      "\u001b[1m---- Epoch 4/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000036) ...\n",
      "loss 10.41273, a_loss 6.96401, cD 0.00006, wmdcmp 0.00001, ema 0.04374, oracc 0.67179, orien_loss 0.84788, qlmicf1 0.18740, qlmacf1 0.11351, ql_loss 1.11738, chxlmicf1 0.24038, chxlmacf1 0.28570, chx_loss 1.09750, chxlacc 0.45520, chxlrocaucmic 0.50094, chxlrocaucmac 0.52045, gacc 0.53878, gloss 0.68851, cxr14micf1 0.15974, cxr14macf1 0.15259, cxr14_loss 1.23754, vnbgmicf1 0.11802, vnbgmacf1 0.13847, vnbg_loss 7.74229, b1 0.00000, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01389, padchxlmicf1 0.01993, padchxlzmacf1 0.02645, padchxlzmicf1 0.03181, padchxl_loss 0.91316, padchxlz_loss 0.99165, 132.33 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.00000, wmdcmp 0.00000, ema 0.03581, oracc 0.62559, qlmicf1 0.23831, qlmacf1 0.12348, chxlmicf1 0.29167, chxlmacf1 0.31209, chxlacc 0.49207, chxlrocaucmic 0.54828, chxlrocaucmac 0.53188, 38.85 secs\n",
      "Adjusting learning rate of group 0 to 1.2068e-04.\n",
      "\u001b[1m---- Epoch 5/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000121) ...\n",
      "loss 8.69666, a_loss 5.34480, cD 0.02325, wmdcmp 0.00404, ema 0.03471, oracc 0.68363, orien_loss 0.65676, qlmicf1 0.24075, qlmacf1 0.12596, ql_loss 1.09012, chxlmicf1 0.22698, chxlmacf1 0.29709, chx_loss 1.09480, chxlacc 0.56474, chxlrocaucmic 0.55045, chxlrocaucmac 0.56333, gacc 0.56638, gloss 0.67884, cxr14micf1 0.11202, cxr14macf1 0.17621, cxr14_loss 1.23692, vnbgmicf1 0.17937, vnbgmacf1 0.18825, vnbg_loss 5.93534, b1 0.00000, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01701, padchxlmicf1 0.04431, padchxlzmacf1 0.02855, padchxlzmicf1 0.04352, padchxl_loss 0.81656, padchxlz_loss 0.91142, 133.92 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.09068, wmdcmp 0.01477, ema 0.00000, oracc 0.70335, qlmicf1 0.23552, qlmacf1 0.13408, chxlmicf1 0.34700, chxlmacf1 0.34611, chxlacc 0.59084, chxlrocaucmic 0.62274, chxlrocaucmac 0.59310, 42.32 secs\n",
      "Adjusting learning rate of group 0 to 4.0000e-04.\n",
      "\u001b[1m---- Epoch 6/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000400) ...\n",
      "loss 8.68176, a_loss 3.71772, cD 0.07234, wmdcmp 0.01661, ema 0.02029, oracc 0.78946, orien_loss 0.51441, qlmicf1 0.24361, qlmacf1 0.14305, ql_loss 1.07150, chxlmicf1 0.32560, chxlmacf1 0.34193, chx_loss 1.08208, chxlacc 0.57399, chxlrocaucmic 0.62226, chxlrocaucmac 0.60621, gacc 0.59662, gloss 0.66904, cxr14micf1 0.15509, cxr14macf1 0.20423, cxr14_loss 1.22423, vnbgmicf1 0.19068, vnbgmacf1 0.20645, vnbg_loss 4.07482, b1 0.01803, b2 0.00812, b3 0.00440, b4 0.00000, padchxlmacf1 0.02047, padchxlmicf1 0.05413, padchxlzmacf1 0.03502, padchxlzmicf1 0.06064, padchxl_loss 0.71090, padchxlz_loss 0.83682, 133.74 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.15129, wmdcmp 0.02610, ema 0.03581, oracc 0.86451, qlmicf1 0.23428, qlmacf1 0.15022, chxlmicf1 0.42239, chxlmacf1 0.36929, chxlacc 0.58176, chxlrocaucmic 0.66635, chxlrocaucmac 0.61858, 44.87 secs\n",
      "Adjusting learning rate of group 0 to 3.2759e-04.\n",
      "\u001b[1m---- Epoch 7/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000328) ...\n",
      "loss 7.04401, a_loss 2.74165, cD 0.11606, wmdcmp 0.02530, ema 0.08859, oracc 0.86037, orien_loss 0.37273, qlmicf1 0.23633, qlmacf1 0.14777, ql_loss 1.06009, chxlmicf1 0.35620, chxlmacf1 0.35384, chx_loss 1.06936, chxlacc 0.57793, chxlrocaucmic 0.64434, chxlrocaucmac 0.62193, gacc 0.65257, gloss 0.64779, cxr14micf1 0.16276, cxr14macf1 0.21149, cxr14_loss 1.20572, vnbgmicf1 0.21965, vnbgmacf1 0.21866, vnbg_loss 3.16088, b1 0.05589, b2 0.03338, b3 0.02060, b4 0.01404, padchxlmacf1 0.02602, padchxlmicf1 0.06623, padchxlzmacf1 0.04001, padchxlzmicf1 0.07218, padchxl_loss 0.68771, padchxlz_loss 0.81534, 134.16 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.15117, wmdcmp 0.02608, ema 0.39928, oracc 0.90151, qlmicf1 0.24816, qlmacf1 0.15413, chxlmicf1 0.40906, chxlmacf1 0.37124, chxlacc 0.58456, chxlrocaucmic 0.66240, chxlrocaucmac 0.61976, 41.59 secs\n",
      "Adjusting learning rate of group 0 to 2.6828e-04.\n",
      "\u001b[1m---- Epoch 8/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000268) ...\n",
      "loss 5.76750, a_loss 2.24605, cD 0.17822, wmdcmp 0.03400, ema 0.29610, oracc 0.88659, orien_loss 0.30125, qlmicf1 0.23880, qlmacf1 0.15055, ql_loss 1.05473, chxlmicf1 0.36384, chxlmacf1 0.35842, chx_loss 1.06356, chxlacc 0.58491, chxlrocaucmic 0.65346, chxlrocaucmac 0.63160, gacc 0.67648, gloss 0.63023, cxr14micf1 0.16305, cxr14macf1 0.21192, cxr14_loss 1.20048, vnbgmicf1 0.23598, vnbgmacf1 0.22791, vnbg_loss 2.63571, b1 0.12139, b2 0.08231, b3 0.05767, b4 0.04073, padchxlmacf1 0.02532, padchxlmicf1 0.07002, padchxlzmacf1 0.04140, padchxlzmicf1 0.07923, padchxl_loss 0.66876, padchxlz_loss 0.79681, 133.06 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.08765, wmdcmp 0.01727, ema 0.54163, oracc 0.91730, qlmicf1 0.24487, qlmacf1 0.15795, chxlmicf1 0.41166, chxlmacf1 0.37678, chxlacc 0.58480, chxlrocaucmic 0.66314, chxlrocaucmac 0.63343, 46.41 secs\n",
      "Adjusting learning rate of group 0 to 2.1971e-04.\n",
      "\u001b[1m---- Epoch 9/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000220) ...\n",
      "loss 6.62931, a_loss 2.01477, cD 0.26355, wmdcmp 0.04471, ema 0.37137, oracc 0.90267, orien_loss 0.26364, qlmicf1 0.23682, qlmacf1 0.15258, ql_loss 1.04606, chxlmicf1 0.36695, chxlmacf1 0.36140, chx_loss 1.06040, chxlacc 0.58751, chxlrocaucmic 0.65879, chxlrocaucmac 0.63857, gacc 0.70367, gloss 0.60907, cxr14micf1 0.16979, cxr14macf1 0.21735, cxr14_loss 1.18935, vnbgmicf1 0.25230, vnbgmacf1 0.23078, vnbg_loss 2.20164, b1 0.22460, b2 0.15113, b3 0.10297, b4 0.07325, padchxlmacf1 0.02720, padchxlmicf1 0.07484, padchxlzmacf1 0.04630, padchxlzmicf1 0.09081, padchxl_loss 0.67132, padchxlz_loss 0.78596, 128.95 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.11269, wmdcmp 0.02701, ema 0.50940, oracc 0.92083, qlmicf1 0.23915, qlmacf1 0.15926, chxlmicf1 0.41260, chxlmacf1 0.37551, chxlacc 0.58841, chxlrocaucmic 0.66815, chxlrocaucmac 0.63563, 45.70 secs\n",
      "Adjusting learning rate of group 0 to 1.7994e-04.\n",
      "\u001b[1m---- Epoch 10/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000180) ...\n",
      "loss 2.74102, a_loss 1.87374, cD 0.35038, wmdcmp 0.05605, ema 0.41161, oracc 0.90947, orien_loss 0.24857, qlmicf1 0.23920, qlmacf1 0.15269, ql_loss 1.04661, chxlmicf1 0.37909, chxlmacf1 0.36570, chx_loss 1.05890, chxlacc 0.59092, chxlrocaucmic 0.66300, chxlrocaucmac 0.64049, gacc 0.73600, gloss 0.58949, cxr14micf1 0.17471, cxr14macf1 0.22015, cxr14_loss 1.18031, vnbgmicf1 0.26880, vnbgmacf1 0.23928, vnbg_loss 1.97471, b1 0.25031, b2 0.16847, b3 0.11781, b4 0.08522, padchxlmacf1 0.02854, padchxlmicf1 0.08133, padchxlzmacf1 0.04653, padchxlzmicf1 0.09717, padchxl_loss 0.65683, padchxlz_loss 0.77912, 128.65 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Validation stage ...\n",
      "cD 0.17626, wmdcmp 0.03209, ema 0.52731, oracc 0.92837, qlmicf1 0.25283, qlmacf1 0.16309, chxlmicf1 0.40924, chxlmacf1 0.37704, chxlacc 0.59660, chxlrocaucmic 0.66588, chxlrocaucmac 0.63743, 49.85 secs\n",
      "Adjusting learning rate of group 0 to 1.4736e-04.\n",
      "\u001b[1m---- Epoch 11/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000147) ...\n",
      "loss 2.57434, a_loss 1.78098, cD 0.40434, wmdcmp 0.06308, ema 0.44062, oracc 0.91943, orien_loss 0.21992, qlmicf1 0.23947, qlmacf1 0.15334, ql_loss 1.04634, chxlmicf1 0.37964, chxlmacf1 0.36675, chx_loss 1.05655, chxlacc 0.59247, chxlrocaucmic 0.66633, chxlrocaucmac 0.64559, gacc 0.74106, gloss 0.57506, cxr14micf1 0.17377, cxr14macf1 0.21957, cxr14_loss 1.18239, vnbgmicf1 0.26283, vnbgmacf1 0.24001, vnbg_loss 1.82465, b1 0.29358, b2 0.19823, b3 0.13637, b4 0.09851, padchxlmacf1 0.02900, padchxlmicf1 0.08352, padchxlzmacf1 0.04598, padchxlzmicf1 0.09122, padchxl_loss 0.64979, padchxlz_loss 0.78345, 128.96 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.22477, wmdcmp 0.03838, ema 0.52551, oracc 0.93756, qlmicf1 0.25245, qlmacf1 0.16341, chxlmicf1 0.42971, chxlmacf1 0.38011, chxlacc 0.59657, chxlrocaucmic 0.67662, chxlrocaucmac 0.63740, 50.94 secs\n",
      "Adjusting learning rate of group 0 to 1.2068e-04.\n",
      "\u001b[1m---- Epoch 12/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000121) ...\n",
      "loss 6.33489, a_loss 1.72033, cD 0.44072, wmdcmp 0.06740, ema 0.44777, oracc 0.92673, orien_loss 0.20548, qlmicf1 0.23941, qlmacf1 0.15478, ql_loss 1.05057, chxlmicf1 0.38410, chxlmacf1 0.36876, chx_loss 1.05371, chxlacc 0.59270, chxlrocaucmic 0.66695, chxlrocaucmac 0.64586, gacc 0.74419, gloss 0.56596, cxr14micf1 0.16907, cxr14macf1 0.21678, cxr14_loss 1.18808, vnbgmicf1 0.27812, vnbgmacf1 0.24758, vnbg_loss 1.71401, b1 0.28186, b2 0.19184, b3 0.13340, b4 0.09714, padchxlmacf1 0.02803, padchxlmicf1 0.08413, padchxlzmacf1 0.04636, padchxlzmicf1 0.09125, padchxl_loss 0.66283, padchxlz_loss 0.80367, 128.28 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.27997, wmdcmp 0.04422, ema 0.55058, oracc 0.93615, qlmicf1 0.24514, qlmacf1 0.16524, chxlmicf1 0.41327, chxlmacf1 0.37945, chxlacc 0.60511, chxlrocaucmic 0.67033, chxlrocaucmac 0.64334, 46.54 secs\n",
      "Adjusting learning rate of group 0 to 9.8835e-05.\n",
      "\u001b[1m---- Epoch 13/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000099) ...\n",
      "loss 1.65707, a_loss 1.68838, cD 0.48187, wmdcmp 0.07335, ema 0.46556, oracc 0.92834, orien_loss 0.19826, qlmicf1 0.24080, qlmacf1 0.15622, ql_loss 1.04286, chxlmicf1 0.38468, chxlmacf1 0.36898, chx_loss 1.05393, chxlacc 0.59511, chxlrocaucmic 0.67041, chxlrocaucmac 0.64879, gacc 0.74995, gloss 0.55357, cxr14micf1 0.17708, cxr14macf1 0.22258, cxr14_loss 1.19057, vnbgmicf1 0.28850, vnbgmacf1 0.25005, vnbg_loss 1.64205, b1 0.28499, b2 0.19212, b3 0.13032, b4 0.09131, padchxlmacf1 0.03011, padchxlmicf1 0.08471, padchxlzmacf1 0.04857, padchxlzmicf1 0.09288, padchxl_loss 0.66600, padchxlz_loss 0.79585, 130.21 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.34821, wmdcmp 0.05527, ema 0.54700, oracc 0.94321, qlmicf1 0.24460, qlmacf1 0.16543, chxlmicf1 0.43810, chxlmacf1 0.38274, chxlacc 0.60527, chxlrocaucmic 0.68763, chxlrocaucmac 0.64440, 42.32 secs\n",
      "Adjusting learning rate of group 0 to 8.0943e-05.\n",
      "\u001b[1m---- Epoch 14/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000081) ...\n",
      "loss 2.41234, a_loss 1.65218, cD 0.51172, wmdcmp 0.07733, ema 0.47386, oracc 0.92709, orien_loss 0.19943, qlmicf1 0.23999, qlmacf1 0.15690, ql_loss 1.03978, chxlmicf1 0.39146, chxlmacf1 0.37259, chx_loss 1.04918, chxlacc 0.59787, chxlrocaucmic 0.67384, chxlrocaucmac 0.65363, gacc 0.75048, gloss 0.54908, cxr14micf1 0.17406, cxr14macf1 0.22195, cxr14_loss 1.18963, vnbgmicf1 0.28203, vnbgmacf1 0.25007, vnbg_loss 1.60845, b1 0.29852, b2 0.20515, b3 0.14229, b4 0.10129, padchxlmacf1 0.02914, padchxlmicf1 0.08040, padchxlzmacf1 0.04630, padchxlzmicf1 0.09377, padchxl_loss 0.65971, padchxlz_loss 0.78067, 131.69 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.38628, wmdcmp 0.06022, ema 0.53894, oracc 0.94274, qlmicf1 0.25215, qlmacf1 0.16750, chxlmicf1 0.42015, chxlmacf1 0.38136, chxlacc 0.60276, chxlrocaucmic 0.67701, chxlrocaucmac 0.64828, 46.42 secs\n",
      "Adjusting learning rate of group 0 to 6.6289e-05.\n",
      "\u001b[1m---- Epoch 15/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000066) ...\n",
      "loss 2.09866, a_loss 1.60884, cD 0.52175, wmdcmp 0.07888, ema 0.46981, oracc 0.93145, orien_loss 0.19162, qlmicf1 0.24080, qlmacf1 0.15717, ql_loss 1.03802, chxlmicf1 0.39056, chxlmacf1 0.37277, chx_loss 1.04994, chxlacc 0.60057, chxlrocaucmic 0.67299, chxlrocaucmac 0.65151, gacc 0.75333, gloss 0.54806, cxr14micf1 0.18360, cxr14macf1 0.22568, cxr14_loss 1.17905, vnbgmicf1 0.29273, vnbgmacf1 0.25207, vnbg_loss 1.56922, b1 0.30436, b2 0.21166, b3 0.14630, b4 0.10463, padchxlmacf1 0.03135, padchxlmicf1 0.08547, padchxlzmacf1 0.04975, padchxlzmicf1 0.09619, padchxl_loss 0.65618, padchxlz_loss 0.80458, 130.95 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.42549, wmdcmp 0.06488, ema 0.54700, oracc 0.94015, qlmicf1 0.24417, qlmacf1 0.16544, chxlmicf1 0.43801, chxlmacf1 0.38658, chxlacc 0.59709, chxlrocaucmic 0.68932, chxlrocaucmac 0.64797, 47.57 secs\n",
      "Adjusting learning rate of group 0 to 5.4288e-05.\n",
      "\u001b[1m---- Epoch 16/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000054) ...\n",
      "loss 1.99602, a_loss 1.60947, cD 0.53957, wmdcmp 0.08102, ema 0.47127, oracc 0.93162, orien_loss 0.18754, qlmicf1 0.24128, qlmacf1 0.15730, ql_loss 1.03992, chxlmicf1 0.38986, chxlmacf1 0.37177, chx_loss 1.04928, chxlacc 0.59861, chxlrocaucmic 0.67395, chxlrocaucmac 0.65407, gacc 0.75571, gloss 0.53998, cxr14micf1 0.17540, cxr14macf1 0.22219, cxr14_loss 1.18382, vnbgmicf1 0.31371, vnbgmacf1 0.26208, vnbg_loss 1.54683, b1 0.32105, b2 0.22345, b3 0.15532, b4 0.11167, padchxlmacf1 0.03280, padchxlmicf1 0.08493, padchxlzmacf1 0.05050, padchxlzmicf1 0.09731, padchxl_loss 0.66622, padchxlz_loss 0.79584, 132.62 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.44481, wmdcmp 0.06767, ema 0.56043, oracc 0.94086, qlmicf1 0.24999, qlmacf1 0.16606, chxlmicf1 0.43589, chxlmacf1 0.38521, chxlacc 0.60235, chxlrocaucmic 0.68662, chxlrocaucmac 0.65133, 41.01 secs\n",
      "Adjusting learning rate of group 0 to 4.4460e-05.\n",
      "\u001b[1m---- Epoch 17/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 6.17695, a_loss 1.60144, cD 0.56426, wmdcmp 0.08371, ema 0.47516, oracc 0.93451, orien_loss 0.18311, qlmicf1 0.24291, qlmacf1 0.15832, ql_loss 1.03596, chxlmicf1 0.38877, chxlmacf1 0.37073, chx_loss 1.04908, chxlacc 0.59580, chxlrocaucmic 0.67374, chxlrocaucmac 0.65238, gacc 0.76457, gloss 0.53382, cxr14micf1 0.18353, cxr14macf1 0.22849, cxr14_loss 1.17720, vnbgmicf1 0.29661, vnbgmacf1 0.25303, vnbg_loss 1.53626, b1 0.31720, b2 0.21503, b3 0.14175, b4 0.09656, padchxlmacf1 0.03101, padchxlmicf1 0.08571, padchxlzmacf1 0.05110, padchxlzmicf1 0.09852, padchxl_loss 0.64748, padchxlz_loss 0.79186, 134.06 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.46998, wmdcmp 0.07093, ema 0.56580, oracc 0.94133, qlmicf1 0.25611, qlmacf1 0.16653, chxlmicf1 0.43108, chxlmacf1 0.38418, chxlacc 0.60593, chxlrocaucmic 0.68531, chxlrocaucmac 0.65179, 36.34 secs\n",
      "Adjusting learning rate of group 0 to 3.6411e-05.\n",
      "\u001b[1m---- Epoch 18/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000036) ...\n",
      "loss 6.09416, a_loss 1.58675, cD 0.55077, wmdcmp 0.08292, ema 0.47861, oracc 0.93128, orien_loss 0.18316, qlmicf1 0.24490, qlmacf1 0.15781, ql_loss 1.04560, chxlmicf1 0.39498, chxlmacf1 0.37450, chx_loss 1.04926, chxlacc 0.60084, chxlrocaucmic 0.67769, chxlrocaucmac 0.65619, gacc 0.76512, gloss 0.53285, cxr14micf1 0.17753, cxr14macf1 0.22430, cxr14_loss 1.18493, vnbgmicf1 0.31220, vnbgmacf1 0.25768, vnbg_loss 1.52671, b1 0.31784, b2 0.22184, b3 0.15393, b4 0.10997, padchxlmacf1 0.03111, padchxlmicf1 0.08845, padchxlzmacf1 0.05122, padchxlzmicf1 0.09873, padchxl_loss 0.65659, padchxlz_loss 0.78198, 138.67 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.51435, wmdcmp 0.07651, ema 0.56401, oracc 0.94369, qlmicf1 0.24980, qlmacf1 0.16672, chxlmicf1 0.43598, chxlmacf1 0.38683, chxlacc 0.60338, chxlrocaucmic 0.68599, chxlrocaucmac 0.65369, 36.28 secs\n",
      "Adjusting learning rate of group 0 to 2.9820e-05.\n",
      "\u001b[1m---- Epoch 19/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 4.39051, a_loss 1.58693, cD 0.56047, wmdcmp 0.08435, ema 0.48417, oracc 0.93240, orien_loss 0.18466, qlmicf1 0.24050, qlmacf1 0.15746, ql_loss 1.03653, chxlmicf1 0.39230, chxlmacf1 0.37243, chx_loss 1.04720, chxlacc 0.60023, chxlrocaucmic 0.67578, chxlrocaucmac 0.65471, gacc 0.76590, gloss 0.52785, cxr14micf1 0.17907, cxr14macf1 0.22383, cxr14_loss 1.18045, vnbgmicf1 0.30602, vnbgmacf1 0.26198, vnbg_loss 1.50041, b1 0.31358, b2 0.21764, b3 0.14729, b4 0.10338, padchxlmacf1 0.03277, padchxlmicf1 0.09064, padchxlzmacf1 0.05065, padchxlzmicf1 0.09543, padchxl_loss 0.65801, padchxlz_loss 0.77535, 136.67 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Validation stage ...\n",
      "cD 0.53705, wmdcmp 0.07975, ema 0.55864, oracc 0.94109, qlmicf1 0.25261, qlmacf1 0.16778, chxlmicf1 0.43787, chxlmacf1 0.38675, chxlacc 0.60709, chxlrocaucmic 0.68839, chxlrocaucmac 0.65191, 39.61 secs\n",
      "Adjusting learning rate of group 0 to 2.4421e-05.\n",
      "\u001b[1m---- Epoch 20/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n",
      "loss 2.39624, a_loss 1.56753, cD 0.59676, wmdcmp 0.08744, ema 0.47856, oracc 0.93437, orien_loss 0.17893, qlmicf1 0.24521, qlmacf1 0.15762, ql_loss 1.03756, chxlmicf1 0.39786, chxlmacf1 0.37558, chx_loss 1.04554, chxlacc 0.60302, chxlrocaucmic 0.68125, chxlrocaucmac 0.65847, gacc 0.77867, gloss 0.51807, cxr14micf1 0.19071, cxr14macf1 0.23228, cxr14_loss 1.17903, vnbgmicf1 0.30966, vnbgmacf1 0.25596, vnbg_loss 1.50777, b1 0.33833, b2 0.23568, b3 0.16072, b4 0.11145, padchxlmacf1 0.03232, padchxlmicf1 0.09459, padchxlzmacf1 0.05141, padchxlzmicf1 0.10348, padchxl_loss 0.63768, padchxlz_loss 0.78665, 135.31 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.56706, wmdcmp 0.08435, ema 0.57296, oracc 0.94604, qlmicf1 0.25340, qlmacf1 0.16912, chxlmicf1 0.43492, chxlmacf1 0.38638, chxlacc 0.60279, chxlrocaucmic 0.68588, chxlrocaucmac 0.65362, 43.60 secs\n",
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "\u001b[1m---- Epoch 21/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 1.51478, a_loss 1.56068, cD 0.60315, wmdcmp 0.08884, ema 0.47679, oracc 0.93408, orien_loss 0.17863, qlmicf1 0.24098, qlmacf1 0.15671, ql_loss 1.03711, chxlmicf1 0.39129, chxlmacf1 0.37256, chx_loss 1.04838, chxlacc 0.59849, chxlrocaucmic 0.67573, chxlrocaucmac 0.65508, gacc 0.77179, gloss 0.52573, cxr14micf1 0.17289, cxr14macf1 0.21839, cxr14_loss 1.18648, vnbgmicf1 0.30770, vnbgmacf1 0.25963, vnbg_loss 1.48312, b1 0.34147, b2 0.24010, b3 0.16660, b4 0.11805, padchxlmacf1 0.03126, padchxlmicf1 0.08706, padchxlzmacf1 0.05008, padchxlzmicf1 0.09509, padchxl_loss 0.64920, padchxlz_loss 0.79433, 136.37 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.57263, wmdcmp 0.08472, ema 0.57296, oracc 0.94439, qlmicf1 0.25471, qlmacf1 0.16867, chxlmicf1 0.43925, chxlmacf1 0.38908, chxlacc 0.60553, chxlrocaucmic 0.68925, chxlrocaucmac 0.65347, 43.43 secs\n",
      "Adjusting learning rate of group 0 to 1.6379e-05.\n",
      "\u001b[1m---- Epoch 22/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 2.31494, a_loss 1.54173, cD 0.58542, wmdcmp 0.08758, ema 0.48324, oracc 0.93459, orien_loss 0.17936, qlmicf1 0.24555, qlmacf1 0.15913, ql_loss 1.03717, chxlmicf1 0.40120, chxlmacf1 0.37680, chx_loss 1.04287, chxlacc 0.60084, chxlrocaucmic 0.68066, chxlrocaucmac 0.65793, gacc 0.76962, gloss 0.51999, cxr14micf1 0.18390, cxr14macf1 0.22617, cxr14_loss 1.17698, vnbgmicf1 0.30891, vnbgmacf1 0.26111, vnbg_loss 1.47348, b1 0.32517, b2 0.22990, b3 0.15985, b4 0.11453, padchxlmacf1 0.03233, padchxlmicf1 0.09287, padchxlzmacf1 0.05172, padchxlzmicf1 0.10159, padchxl_loss 0.64186, padchxlz_loss 0.77623, 133.67 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.57209, wmdcmp 0.08491, ema 0.56222, oracc 0.94251, qlmicf1 0.25352, qlmacf1 0.16738, chxlmicf1 0.43768, chxlmacf1 0.38663, chxlacc 0.60607, chxlrocaucmic 0.68705, chxlrocaucmac 0.65141, 42.37 secs\n",
      "Adjusting learning rate of group 0 to 1.3414e-05.\n",
      "\u001b[1m---- Epoch 23/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 5.97763, a_loss 1.55369, cD 0.59765, wmdcmp 0.08797, ema 0.48408, oracc 0.93597, orien_loss 0.17190, qlmicf1 0.24439, qlmacf1 0.15882, ql_loss 1.03713, chxlmicf1 0.39114, chxlmacf1 0.37281, chx_loss 1.04830, chxlacc 0.60242, chxlrocaucmic 0.67760, chxlrocaucmac 0.65636, gacc 0.76531, gloss 0.52444, cxr14micf1 0.18233, cxr14macf1 0.22638, cxr14_loss 1.17372, vnbgmicf1 0.31527, vnbgmacf1 0.26467, vnbg_loss 1.47591, b1 0.32294, b2 0.22334, b3 0.15055, b4 0.10297, padchxlmacf1 0.03231, padchxlmicf1 0.09286, padchxlzmacf1 0.05193, padchxlzmicf1 0.09748, padchxl_loss 0.63384, padchxlz_loss 0.76850, 132.63 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.59145, wmdcmp 0.08711, ema 0.59266, oracc 0.94369, qlmicf1 0.25583, qlmacf1 0.16897, chxlmicf1 0.43718, chxlmacf1 0.38773, chxlacc 0.60632, chxlrocaucmic 0.68785, chxlrocaucmac 0.65249, 43.86 secs\n",
      "Adjusting learning rate of group 0 to 1.0986e-05.\n",
      "\u001b[1m---- Epoch 24/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 5.95104, a_loss 1.56674, cD 0.60995, wmdcmp 0.08994, ema 0.49068, oracc 0.93633, orien_loss 0.17273, qlmicf1 0.24661, qlmacf1 0.15920, ql_loss 1.04255, chxlmicf1 0.40117, chxlmacf1 0.37761, chx_loss 1.04265, chxlacc 0.60151, chxlrocaucmic 0.68110, chxlrocaucmac 0.66109, gacc 0.76957, gloss 0.52026, cxr14micf1 0.18161, cxr14macf1 0.22644, cxr14_loss 1.17700, vnbgmicf1 0.32060, vnbgmacf1 0.26671, vnbg_loss 1.47489, b1 0.33135, b2 0.23150, b3 0.16062, b4 0.11397, padchxlmacf1 0.03340, padchxlmicf1 0.09699, padchxlzmacf1 0.05341, padchxlzmicf1 0.10731, padchxl_loss 0.65074, padchxlz_loss 0.76910, 129.24 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.59780, wmdcmp 0.08795, ema 0.57565, oracc 0.94604, qlmicf1 0.25317, qlmacf1 0.16833, chxlmicf1 0.44163, chxlmacf1 0.39026, chxlacc 0.60692, chxlrocaucmic 0.69220, chxlrocaucmac 0.65378, 45.05 secs\n",
      "Adjusting learning rate of group 0 to 8.9968e-06.\n",
      "\u001b[1m---- Epoch 25/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 2.26736, a_loss 1.53233, cD 0.59700, wmdcmp 0.08864, ema 0.48371, oracc 0.93737, orien_loss 0.17214, qlmicf1 0.24346, qlmacf1 0.15832, ql_loss 1.03832, chxlmicf1 0.39576, chxlmacf1 0.37391, chx_loss 1.04717, chxlacc 0.60158, chxlrocaucmic 0.68059, chxlrocaucmac 0.65694, gacc 0.77105, gloss 0.51876, cxr14micf1 0.17874, cxr14macf1 0.22598, cxr14_loss 1.17113, vnbgmicf1 0.30610, vnbgmacf1 0.26034, vnbg_loss 1.47910, b1 0.33757, b2 0.23498, b3 0.16200, b4 0.11421, padchxlmacf1 0.03326, padchxlmicf1 0.09547, padchxlzmacf1 0.05015, padchxlzmicf1 0.09909, padchxl_loss 0.64913, padchxlz_loss 0.78917, 129.13 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.60534, wmdcmp 0.08843, ema 0.57923, oracc 0.94840, qlmicf1 0.25213, qlmacf1 0.16787, chxlmicf1 0.43603, chxlmacf1 0.38609, chxlacc 0.60418, chxlrocaucmic 0.68781, chxlrocaucmac 0.65199, 47.95 secs\n",
      "Adjusting learning rate of group 0 to 7.3681e-06.\n",
      "\u001b[1m---- Epoch 26/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.35940, a_loss 1.55135, cD 0.58751, wmdcmp 0.08690, ema 0.48849, oracc 0.93520, orien_loss 0.17393, qlmicf1 0.24307, qlmacf1 0.15724, ql_loss 1.03621, chxlmicf1 0.39838, chxlmacf1 0.37669, chx_loss 1.04467, chxlacc 0.60164, chxlrocaucmic 0.68089, chxlrocaucmac 0.65862, gacc 0.76860, gloss 0.52137, cxr14micf1 0.17596, cxr14macf1 0.22031, cxr14_loss 1.18083, vnbgmicf1 0.32676, vnbgmacf1 0.27085, vnbg_loss 1.47165, b1 0.34961, b2 0.24306, b3 0.16654, b4 0.11677, padchxlmacf1 0.03150, padchxlmicf1 0.08894, padchxlzmacf1 0.04985, padchxlzmicf1 0.09664, padchxl_loss 0.63138, padchxlz_loss 0.77533, 129.21 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.60468, wmdcmp 0.08857, ema 0.56580, oracc 0.94392, qlmicf1 0.25135, qlmacf1 0.16813, chxlmicf1 0.43469, chxlmacf1 0.38566, chxlacc 0.60496, chxlrocaucmic 0.68696, chxlrocaucmac 0.65372, 49.88 secs\n",
      "Adjusting learning rate of group 0 to 6.0342e-06.\n",
      "\u001b[1m---- Epoch 27/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 1.88159, a_loss 1.53741, cD 0.60699, wmdcmp 0.08970, ema 0.48518, oracc 0.93682, orien_loss 0.17410, qlmicf1 0.24398, qlmacf1 0.15921, ql_loss 1.03561, chxlmicf1 0.39894, chxlmacf1 0.37575, chx_loss 1.04387, chxlacc 0.60074, chxlrocaucmic 0.67970, chxlrocaucmac 0.65771, gacc 0.78338, gloss 0.50981, cxr14micf1 0.17963, cxr14macf1 0.22352, cxr14_loss 1.18016, vnbgmicf1 0.32341, vnbgmacf1 0.26680, vnbg_loss 1.47194, b1 0.33666, b2 0.23813, b3 0.16528, b4 0.11724, padchxlmacf1 0.03196, padchxlmicf1 0.08679, padchxlzmacf1 0.05072, padchxlzmicf1 0.09739, padchxl_loss 0.65587, padchxlz_loss 0.78445, 127.16 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.60069, wmdcmp 0.08857, ema 0.59534, oracc 0.94416, qlmicf1 0.25263, qlmacf1 0.16731, chxlmicf1 0.43767, chxlmacf1 0.38853, chxlacc 0.60769, chxlrocaucmic 0.68617, chxlrocaucmac 0.65375, 50.82 secs\n",
      "Adjusting learning rate of group 0 to 4.9418e-06.\n",
      "\u001b[1m---- Epoch 28/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 2.31595, a_loss 1.52522, cD 0.61380, wmdcmp 0.09123, ema 0.48752, oracc 0.93806, orien_loss 0.17091, qlmicf1 0.24232, qlmacf1 0.15783, ql_loss 1.03780, chxlmicf1 0.39516, chxlmacf1 0.37560, chx_loss 1.04574, chxlacc 0.60346, chxlrocaucmic 0.67889, chxlrocaucmac 0.66016, gacc 0.77063, gloss 0.52151, cxr14micf1 0.17821, cxr14macf1 0.22507, cxr14_loss 1.17614, vnbgmicf1 0.31948, vnbgmacf1 0.26510, vnbg_loss 1.46108, b1 0.34666, b2 0.24283, b3 0.16807, b4 0.11845, padchxlmacf1 0.03228, padchxlmicf1 0.09317, padchxlzmacf1 0.05140, padchxlzmicf1 0.10042, padchxl_loss 0.64485, padchxlz_loss 0.78139, 129.96 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.60533, wmdcmp 0.08956, ema 0.57923, oracc 0.94486, qlmicf1 0.25117, qlmacf1 0.16702, chxlmicf1 0.43778, chxlmacf1 0.38621, chxlacc 0.60640, chxlrocaucmic 0.68954, chxlrocaucmac 0.65293, 48.10 secs\n",
      "Adjusting learning rate of group 0 to 4.0471e-06.\n",
      "\u001b[1m---- Epoch 29/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 6.00823, a_loss 1.56474, cD 0.60032, wmdcmp 0.08908, ema 0.48560, oracc 0.93720, orien_loss 0.16919, qlmicf1 0.24361, qlmacf1 0.15890, ql_loss 1.04045, chxlmicf1 0.39621, chxlmacf1 0.37482, chx_loss 1.04628, chxlacc 0.60086, chxlrocaucmic 0.67940, chxlrocaucmac 0.65611, gacc 0.77585, gloss 0.51248, cxr14micf1 0.17492, cxr14macf1 0.22077, cxr14_loss 1.18276, vnbgmicf1 0.33044, vnbgmacf1 0.26996, vnbg_loss 1.45447, b1 0.33955, b2 0.23663, b3 0.16262, b4 0.11522, padchxlmacf1 0.03327, padchxlmicf1 0.09392, padchxlzmacf1 0.05425, padchxlzmicf1 0.10705, padchxl_loss 0.64995, padchxlz_loss 0.78357, 132.58 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.61191, wmdcmp 0.09008, ema 0.57475, oracc 0.94651, qlmicf1 0.25238, qlmacf1 0.16888, chxlmicf1 0.43872, chxlmacf1 0.38821, chxlacc 0.60707, chxlrocaucmic 0.68811, chxlrocaucmac 0.65334, 41.54 secs\n",
      "Adjusting learning rate of group 0 to 3.3145e-06.\n",
      "\u001b[1m---- Epoch 30/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 4.14020, a_loss 1.53340, cD 0.61148, wmdcmp 0.09033, ema 0.48566, oracc 0.93524, orien_loss 0.17609, qlmicf1 0.24209, qlmacf1 0.15778, ql_loss 1.03335, chxlmicf1 0.39759, chxlmacf1 0.37510, chx_loss 1.04504, chxlacc 0.60201, chxlrocaucmic 0.67992, chxlrocaucmac 0.65690, gacc 0.77648, gloss 0.51542, cxr14micf1 0.17965, cxr14macf1 0.22638, cxr14_loss 1.17606, vnbgmicf1 0.30975, vnbgmacf1 0.26278, vnbg_loss 1.47269, b1 0.33567, b2 0.23484, b3 0.16059, b4 0.11261, padchxlmacf1 0.03177, padchxlmicf1 0.09494, padchxlzmacf1 0.04957, padchxlzmicf1 0.10025, padchxl_loss 0.64841, padchxlz_loss 0.77573, 134.08 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.61099, wmdcmp 0.09044, ema 0.58102, oracc 0.94416, qlmicf1 0.25275, qlmacf1 0.16866, chxlmicf1 0.43851, chxlmacf1 0.38751, chxlacc 0.60539, chxlrocaucmic 0.68935, chxlrocaucmac 0.65421, 40.75 secs\n",
      "Adjusting learning rate of group 0 to 2.7144e-06.\n",
      "\u001b[1m---- Epoch 31/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 4.31344, a_loss 1.53180, cD 0.59846, wmdcmp 0.08935, ema 0.48443, oracc 0.93416, orien_loss 0.17258, qlmicf1 0.24062, qlmacf1 0.15787, ql_loss 1.03750, chxlmicf1 0.39362, chxlmacf1 0.37338, chx_loss 1.04650, chxlacc 0.60122, chxlrocaucmic 0.67934, chxlrocaucmac 0.65645, gacc 0.77219, gloss 0.51768, cxr14micf1 0.18361, cxr14macf1 0.22841, cxr14_loss 1.17769, vnbgmicf1 0.30953, vnbgmacf1 0.26095, vnbg_loss 1.47243, b1 0.33018, b2 0.22752, b3 0.15502, b4 0.10858, padchxlmacf1 0.03206, padchxlmicf1 0.08914, padchxlzmacf1 0.04878, padchxlzmicf1 0.09747, padchxl_loss 0.65547, padchxlz_loss 0.77717, 137.51 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.61550, wmdcmp 0.09040, ema 0.55953, oracc 0.94698, qlmicf1 0.25171, qlmacf1 0.16868, chxlmicf1 0.43829, chxlmacf1 0.38859, chxlacc 0.60567, chxlrocaucmic 0.68992, chxlrocaucmac 0.65509, 36.19 secs\n",
      "Adjusting learning rate of group 0 to 2.2230e-06.\n",
      "\u001b[1m---- Epoch 32/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 5.76593, a_loss 1.54275, cD 0.61639, wmdcmp 0.09122, ema 0.49002, oracc 0.94013, orien_loss 0.16769, qlmicf1 0.24259, qlmacf1 0.15828, ql_loss 1.03836, chxlmicf1 0.39879, chxlmacf1 0.37613, chx_loss 1.04529, chxlacc 0.60044, chxlrocaucmic 0.67934, chxlrocaucmac 0.65807, gacc 0.77275, gloss 0.52135, cxr14micf1 0.18708, cxr14macf1 0.22934, cxr14_loss 1.17647, vnbgmicf1 0.31305, vnbgmacf1 0.26463, vnbg_loss 1.47238, b1 0.35366, b2 0.25031, b3 0.17514, b4 0.12308, padchxlmacf1 0.03194, padchxlmicf1 0.08996, padchxlzmacf1 0.05171, padchxlzmicf1 0.10195, padchxl_loss 0.65692, padchxlz_loss 0.77598, 138.14 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.61036, wmdcmp 0.09002, ema 0.57028, oracc 0.94510, qlmicf1 0.25122, qlmacf1 0.16864, chxlmicf1 0.43936, chxlmacf1 0.38940, chxlacc 0.60746, chxlrocaucmic 0.68950, chxlrocaucmac 0.65550, 34.29 secs\n",
      "Adjusting learning rate of group 0 to 1.8206e-06.\n",
      "\u001b[1m---- Epoch 33/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.15400, a_loss 1.54193, cD 0.61295, wmdcmp 0.09077, ema 0.48993, oracc 0.93224, orien_loss 0.17850, qlmicf1 0.24494, qlmacf1 0.15970, ql_loss 1.04319, chxlmicf1 0.39606, chxlmacf1 0.37702, chx_loss 1.04291, chxlacc 0.60267, chxlrocaucmic 0.67773, chxlrocaucmac 0.66029, gacc 0.77705, gloss 0.51707, cxr14micf1 0.17708, cxr14macf1 0.22245, cxr14_loss 1.17683, vnbgmicf1 0.29984, vnbgmacf1 0.25568, vnbg_loss 1.47541, b1 0.34538, b2 0.23870, b3 0.16141, b4 0.11166, padchxlmacf1 0.03307, padchxlmicf1 0.09382, padchxlzmacf1 0.05090, padchxlzmicf1 0.10198, padchxl_loss 0.65018, padchxlz_loss 0.78982, 137.48 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.61505, wmdcmp 0.09110, ema 0.55148, oracc 0.94746, qlmicf1 0.25345, qlmacf1 0.16947, chxlmicf1 0.43670, chxlmacf1 0.38655, chxlacc 0.60640, chxlrocaucmic 0.68799, chxlrocaucmac 0.65443, 39.89 secs\n",
      "Adjusting learning rate of group 0 to 1.4910e-06.\n",
      "\u001b[1m---- Epoch 34/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.37788, a_loss 1.53266, cD 0.61036, wmdcmp 0.08965, ema 0.49127, oracc 0.93624, orien_loss 0.17276, qlmicf1 0.24198, qlmacf1 0.15734, ql_loss 1.03279, chxlmicf1 0.39127, chxlmacf1 0.37361, chx_loss 1.04775, chxlacc 0.60101, chxlrocaucmic 0.67606, chxlrocaucmac 0.65612, gacc 0.77546, gloss 0.51314, cxr14micf1 0.18068, cxr14macf1 0.22500, cxr14_loss 1.18420, vnbgmicf1 0.31299, vnbgmacf1 0.26257, vnbg_loss 1.45485, b1 0.34403, b2 0.24339, b3 0.16914, b4 0.12195, padchxlmacf1 0.03401, padchxlmicf1 0.09642, padchxlzmacf1 0.05488, padchxlzmicf1 0.11035, padchxl_loss 0.64686, padchxlz_loss 0.77354, 135.61 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.61560, wmdcmp 0.09065, ema 0.58013, oracc 0.94557, qlmicf1 0.25429, qlmacf1 0.16956, chxlmicf1 0.43390, chxlmacf1 0.38457, chxlacc 0.60438, chxlrocaucmic 0.68836, chxlrocaucmac 0.65307, 42.95 secs\n",
      "Adjusting learning rate of group 0 to 1.2211e-06.\n",
      "\u001b[1m---- Epoch 35/35\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 6.02869, a_loss 1.54338, cD 0.61307, wmdcmp 0.09052, ema 0.48628, oracc 0.93482, orien_loss 0.17221, qlmicf1 0.24337, qlmacf1 0.15875, ql_loss 1.03807, chxlmicf1 0.39516, chxlmacf1 0.37514, chx_loss 1.04551, chxlacc 0.60194, chxlrocaucmic 0.67863, chxlrocaucmac 0.65724, gacc 0.78190, gloss 0.51064, cxr14micf1 0.17525, cxr14macf1 0.22018, cxr14_loss 1.17925, vnbgmicf1 0.31333, vnbgmacf1 0.25998, vnbg_loss 1.46154, b1 0.34611, b2 0.24315, b3 0.16520, b4 0.11619, padchxlmacf1 0.03313, padchxlmicf1 0.09510, padchxlzmacf1 0.05550, padchxlzmicf1 0.10776, padchxl_loss 0.63778, padchxlz_loss 0.78189, 132.21 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.62117, wmdcmp 0.09076, ema 0.56312, oracc 0.94416, qlmicf1 0.25122, qlmacf1 0.16859, chxlmicf1 0.43734, chxlmacf1 0.38676, chxlacc 0.60586, chxlrocaucmic 0.68947, chxlrocaucmac 0.65443, 45.53 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_vqa.py \\\n",
    "        --epochs 35 \\\n",
    "        --batches-per-epoch 200 \\\n",
    "        --batch-size 150 \\\n",
    "        --num-workers 4 \\\n",
    "        --iters-to-accumulate 5 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,4e-4,30,1e-6\" \\\n",
    "        --use-mimiccxr \\\n",
    "        --use-iuxray \\\n",
    "        --use-chexpert \\\n",
    "        --use-cxr14 \\\n",
    "        --use-vinbig \\\n",
    "        --use-padchest \\\n",
    "        --mimiccxr-include-chexpert-mode \\\n",
    "        --iuxray-include-chexpert-mode \\\n",
    "        --chexpert-mode \"vqa\" \\\n",
    "        --vinbig-training-data 'all' \\\n",
    "        --mimiccxr-weight 1.0 \\\n",
    "        --mimiccxr-weight-chexpert-mode 0.8 \\\n",
    "        --iuxray-weight 0.2 \\\n",
    "        --iuxray-weight-chexpert-mode 0.16 \\\n",
    "        --chexpert-weight 0.5 \\\n",
    "        --cxr14-weight 0.4 \\\n",
    "        --vinbig-weight 0.4 \\\n",
    "        --padchest-weight 0.5 \\\n",
    "        --padchest-train-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-val-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-test-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-training-data-mode \"all\" \\\n",
    "        --mimiccxr-qa-adapted-reports-filename \"qa_adapted_reports__20220904_095810.json\" \\\n",
    "        --iuxray-qa-adapted-reports-filename \"qa_adapted_reports__20220904_091601.json\" \\\n",
    "        --classify-orientation \\\n",
    "        --classify-chexpert \\\n",
    "        --mimiccxr-chexpert-labels-filename \"chexpert_labels_per_report__20220904_113605.pkl\" \\\n",
    "        --iuxray-chexpert-labels-filename \"chexpert_labels_per_report__20220904_113427.pkl\" \\\n",
    "        --classify-questions \\\n",
    "        --n-questions 97 \\\n",
    "        --mimiccxr-question-labels-filename \"question_labels_per_report__20220904_105753.pkl\" \\\n",
    "        --iuxray-question-labels-filename \"question_labels_per_report__20220904_105752.pkl\" \\\n",
    "        --mimiccxr-balanced-metadata-filename \"balanced_dataloading_metadata__20220918_210415.pkl\" \\\n",
    "        --iuxray-balanced-metadata-filename \"balanced_dataloading_metadata__20220918_210029.pkl\" \\\n",
    "        --balanced-dataloading \\\n",
    "        --medical-tokenization \\\n",
    "        --medical-terms-frequency-filename \"medical_terms_frequency__20220918_184255.pkl\" \\\n",
    "        --raw-image-encoding \"vitmodel-huggingface\" \\\n",
    "        --huggingface-model-name \"facebook/vit-mae-base\" \\\n",
    "        --image-encoder-pretrained-weights-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\" \\\n",
    "        --image-size 224 224 \\\n",
    "        --image-local-feat-size 768 \\\n",
    "        --freeze-image-encoder \\\n",
    "        --question-encoding \"one-hot\" \\\n",
    "        --answer-decoding \"transformer\" \\\n",
    "        --binary-loss-name \"wbce-c\" \\\n",
    "        --img-aug-mode \"random-color-and-spatial\" \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 20\n",
      "   batches_per_epoch: 200\n",
      "   checkpoint_folder: models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   iuxray_qa_adapted_reports_filename: None\n",
      "   mimiccxr_qa_adapted_reports_filename: None\n",
      "   vocab_min_freq: 10\n",
      "   embed_size: 256\n",
      "   question_encoding: bilstm\n",
      "   answer_decoding: lstm\n",
      "   question_hidden_size: 128\n",
      "   answer_hidden_size: 256\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: densenet-121\n",
      "   image_local_feat_size: 1024\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: False\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: None\n",
      "   n_lstm_layers: 1\n",
      "   transf_dec_nhead: 2\n",
      "   transf_dec_dim_forward: 256\n",
      "   transf_dec_num_layers: 2\n",
      "   question_vec_size: 128\n",
      "   dropout_prob: 0\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,4e-4,15,3e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 150\n",
      "   iters_to_accumulate: 5\n",
      "   num_workers: 4\n",
      "   device: GPU\n",
      "   img_aug_mode: None\n",
      "   image_size: (256, 256)\n",
      "   mimiccxr_weight: 1\n",
      "   chexpert_weight: 0.3\n",
      "   cxr14_weight: 0.3\n",
      "   vinbig_weight: 0.3\n",
      "   iuxray_weight: 0.05\n",
      "   mimiccxr_weight_chexpert_mode: 0.2\n",
      "   iuxray_weight_chexpert_mode: 0.05\n",
      "   padchest_weight: 0.4\n",
      "   mimiccxr_include_chexpert_mode: False\n",
      "   iuxray_include_chexpert_mode: False\n",
      "   use_chexpert_mode_only: False\n",
      "   val_answer_decoding: greedy-search\n",
      "   beam_search_k: None\n",
      "   use_amp: False\n",
      "   medical_tokenization: False\n",
      "   medical_terms_frequency_filename: None\n",
      "   allowed_questions: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   balanced_dataloading: False\n",
      "   imbalance_reduction_coef: 0.5\n",
      "   iuxray_balanced_metadata_filename: None\n",
      "   mimiccxr_balanced_metadata_filename: None\n",
      "   one_question_per_batch: False\n",
      "   save: True\n",
      "   override_lr: True\n",
      "   train_mimiccxr: False\n",
      "   train_iuxray: False\n",
      "   train_chexpert: False\n",
      "   chexpert_mode: None\n",
      "   train_cxr14: False\n",
      "   train_vinbig: False\n",
      "   vinbig_training_data: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: False\n",
      "   padchest_training_data_mode: train\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: None\n",
      "   padchest_val_study_ids_path: None\n",
      "   padchest_test_study_ids_path: None\n",
      "   binary_loss_name: bce\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: False\n",
      "   classify_chexpert: False\n",
      "   iuxray_chexpert_labels_filename: None\n",
      "   mimiccxr_chexpert_labels_filename: None\n",
      "   classify_questions: False\n",
      "   n_questions: None\n",
      "   iuxray_question_labels_filename: None\n",
      "   mimiccxr_question_labels_filename: None\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Resuming training ------\u001b[0m\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mLoading mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.mask_token', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,4e-4,15,3e-6\n",
      "1e-06 5 0.0004 15 3e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=True\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "len(final_transforms) = 16\n",
      "default_prob = 0.3\n",
      "Returning augmented transforms with mode random-color-and-spatial\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=4, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=0, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=3, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=2, one_hot_question_offset=97\n",
      "get_vqa_collate_batch_fn(): dataset_id=6, one_hot_question_offset=111\n",
      "get_vqa_collate_batch_fn(): dataset_id=5, one_hot_question_offset=126\n",
      "get_vqa_collate_batch_fn(): dataset_id=7, one_hot_question_offset=154\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa trainer ...\u001b[0m\n",
      "MIMICCXR_VQA_Trainer: balanced_dataloading = True\n",
      "self.balanced_dataloading = True\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_train_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053,medical_terms_frequency__20220918_184255.pkl).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 150\n",
      "len(self.report_ids) = 2034935, len(set(self.report_ids)) = 223623\n",
      "_generate_datasets_and_dataloaders()\n",
      "self.balanced_dataloading = True\n",
      "_generate_train_dataset__balanced()\n",
      "Balanced train data loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/balanced_train_data(hash=287,3559006002065882738).pkl\n",
      "\tlen(question_datasets) = 92\n",
      "len(self.val_indices) = 16416\n",
      "len(val_indices) = 3258\n",
      "generating training and validation dataloaders ...\n",
      "num_workers = 4\n",
      "Generating perfectly balanced train dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=43016, len(neg_indices)=178811\n",
      "label = 1, onehot=98, len(pos_indices)=43215, len(neg_indices)=178612\n",
      "label = 2, onehot=99, len(pos_indices)=71655, len(neg_indices)=150172\n",
      "label = 3, onehot=100, len(pos_indices)=9668, len(neg_indices)=212159\n",
      "label = 4, onehot=101, len(pos_indices)=73721, len(neg_indices)=148106\n",
      "label = 5, onehot=102, len(pos_indices)=42623, len(neg_indices)=179204\n",
      "label = 6, onehot=103, len(pos_indices)=19790, len(neg_indices)=202037\n",
      "label = 7, onehot=104, len(pos_indices)=37343, len(neg_indices)=184484\n",
      "label = 8, onehot=105, len(pos_indices)=69678, len(neg_indices)=152149\n",
      "label = 9, onehot=106, len(pos_indices)=13071, len(neg_indices)=208756\n",
      "label = 10, onehot=107, len(pos_indices)=68496, len(neg_indices)=153331\n",
      "label = 11, onehot=108, len(pos_indices)=4944, len(neg_indices)=216883\n",
      "label = 12, onehot=109, len(pos_indices)=8414, len(neg_indices)=213413\n",
      "label = 13, onehot=110, len(pos_indices)=87913, len(neg_indices)=133914\n",
      "len(self.train_dataset__chexpert_mode) = 1000000000000000000\n",
      "Generating balanced validation dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 1, onehot=98, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 2, onehot=99, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 3, onehot=100, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 4, onehot=101, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 5, onehot=102, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 6, onehot=103, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 7, onehot=104, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 8, onehot=105, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 9, onehot=106, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 10, onehot=107, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 11, onehot=108, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 12, onehot=109, len(pos_indices)=37, len(neg_indices)=40\n",
      "label = 13, onehot=110, len(pos_indices)=40, len(neg_indices)=40\n",
      "len(self.val_dataset__chexpert_mode) = 1117\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mCreating IU X-Ray vqa trainer ...\u001b[0m\n",
      "self.balanced_dataloading = True\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray/iuxray_preprocessed_train_data__(dataset=qa_adapted_reports__20220904_091601.json;tokenizer=4871,39534,3343246773703667053,medical_terms_frequency__20220918_184255.pkl).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "Assigning all data to training ...\n",
      "All data assigned to train: len(train_indices) = 29046, len(val_indices) = 0\n",
      "batch_size = 150\n",
      "len(self.report_ids) = 29046, len(set(self.report_ids)) = 3806\n",
      "_generate_datasets_and_dataloaders()\n",
      "self.balanced_dataloading = True\n",
      "_generate_train_dataset__balanced()\n",
      "Balanced train data loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray/balanced_train_data(hash=283,2750089748417475840).pkl\n",
      "\tlen(question_datasets) = 42\n",
      "generating training and validation dataloaders ...\n",
      "num_workers = 4\n",
      "Generating perfectly balanced train dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=1414, len(neg_indices)=2392\n",
      "label = 1, onehot=98, len(pos_indices)=387, len(neg_indices)=3419\n",
      "label = 2, onehot=99, len(pos_indices)=675, len(neg_indices)=3131\n",
      "label = 3, onehot=100, len(pos_indices)=232, len(neg_indices)=3574\n",
      "label = 4, onehot=101, len(pos_indices)=708, len(neg_indices)=3098\n",
      "label = 5, onehot=102, len(pos_indices)=153, len(neg_indices)=3653\n",
      "label = 6, onehot=103, len(pos_indices)=43, len(neg_indices)=3763\n",
      "label = 7, onehot=104, len(pos_indices)=138, len(neg_indices)=3668\n",
      "label = 8, onehot=105, len(pos_indices)=368, len(neg_indices)=3438\n",
      "label = 9, onehot=106, len(pos_indices)=100, len(neg_indices)=3706\n",
      "label = 10, onehot=107, len(pos_indices)=295, len(neg_indices)=3511\n",
      "label = 11, onehot=108, len(pos_indices)=71, len(neg_indices)=3735\n",
      "label = 12, onehot=109, len(pos_indices)=119, len(neg_indices)=3687\n",
      "label = 13, onehot=110, len(pos_indices)=220, len(neg_indices)=3586\n",
      "len(self.train_dataset__chexpert_mode) = 1000000000000000000\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mCreating CheXpert vqa trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/workspace/chexpert/CheXpert-v1.0-small/train-val.csv\n",
      "Loading images\n",
      "Loading orientations\n",
      "Loading genders\n",
      "Loading chexpert labels\n",
      "label = 0, onehot=0, len(pos_indices)=22414, len(neg_indices)=201216\n",
      "label = 1, onehot=1, len(pos_indices)=23309, len(neg_indices)=200321\n",
      "label = 2, onehot=2, len(pos_indices)=35151, len(neg_indices)=188479\n",
      "label = 3, onehot=3, len(pos_indices)=10675, len(neg_indices)=212955\n",
      "label = 4, onehot=4, len(pos_indices)=111301, len(neg_indices)=112329\n",
      "label = 5, onehot=5, len(pos_indices)=65274, len(neg_indices)=158356\n",
      "label = 6, onehot=6, len(pos_indices)=42556, len(neg_indices)=181074\n",
      "label = 7, onehot=7, len(pos_indices)=24815, len(neg_indices)=198815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 8, onehot=8, len(pos_indices)=67191, len(neg_indices)=156439\n",
      "label = 9, onehot=9, len(pos_indices)=22601, len(neg_indices)=201029\n",
      "label = 10, onehot=10, len(pos_indices)=97875, len(neg_indices)=125755\n",
      "label = 11, onehot=11, len(pos_indices)=6174, len(neg_indices)=217456\n",
      "label = 12, onehot=12, len(pos_indices)=9680, len(neg_indices)=213950\n",
      "label = 13, onehot=13, len(pos_indices)=117184, len(neg_indices)=106446\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mCreating CXR14 vqa trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/data/chest-x-ray-8/metadata/Data_Entry_2017_v2020.csv\n",
      "Loading images\n",
      "Loading orientations\n",
      "Loading genders\n",
      "Loading CXR14 labels\n",
      "label = 0, onehot=0, len(pos_indices)=60361, len(neg_indices)=51759\n",
      "label = 1, onehot=1, len(pos_indices)=11559, len(neg_indices)=100561\n",
      "label = 2, onehot=2, len(pos_indices)=2776, len(neg_indices)=109344\n",
      "label = 3, onehot=3, len(pos_indices)=4667, len(neg_indices)=107453\n",
      "label = 4, onehot=4, len(pos_indices)=2303, len(neg_indices)=109817\n",
      "label = 5, onehot=5, len(pos_indices)=13317, len(neg_indices)=98803\n",
      "label = 6, onehot=6, len(pos_indices)=2516, len(neg_indices)=109604\n",
      "label = 7, onehot=7, len(pos_indices)=1686, len(neg_indices)=110434\n",
      "label = 8, onehot=8, len(pos_indices)=227, len(neg_indices)=111893\n",
      "label = 9, onehot=9, len(pos_indices)=19894, len(neg_indices)=92226\n",
      "label = 10, onehot=10, len(pos_indices)=5782, len(neg_indices)=106338\n",
      "label = 11, onehot=11, len(pos_indices)=6331, len(neg_indices)=105789\n",
      "label = 12, onehot=12, len(pos_indices)=3385, len(neg_indices)=108735\n",
      "label = 13, onehot=13, len(pos_indices)=1431, len(neg_indices)=110689\n",
      "label = 14, onehot=14, len(pos_indices)=5302, len(neg_indices)=106818\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m15) \u001b[0m\u001b[34mCreating VinBig vqa trainer ...\u001b[0m\n",
      "Loading dataframe from:\n",
      "  /mnt/workspace/vinbig-cxr/dataset-png/annotations/image_labels_train.csv\n",
      "  /mnt/workspace/vinbig-cxr/dataset-png/annotations/image_labels_test.csv\n",
      "Sanity checking train labels ...\n",
      "Done!\n",
      "Generating train dataset and dataloader\n",
      "label = 0, onehot=0, len(pos_indices)=3287, len(neg_indices)=14713\n",
      "label = 1, onehot=1, len(pos_indices)=272, len(neg_indices)=17728\n",
      "label = 2, onehot=2, len(pos_indices)=646, len(neg_indices)=17354\n",
      "label = 3, onehot=3, len(pos_indices)=2609, len(neg_indices)=15391\n",
      "label = 4, onehot=4, len(pos_indices)=29, len(neg_indices)=17971\n",
      "label = 5, onehot=5, len(pos_indices)=449, len(neg_indices)=17551\n",
      "label = 6, onehot=6, len(pos_indices)=13, len(neg_indices)=17987\n",
      "label = 7, onehot=7, len(pos_indices)=81, len(neg_indices)=17919\n",
      "label = 8, onehot=8, len(pos_indices)=139, len(neg_indices)=17861\n",
      "label = 9, onehot=9, len(pos_indices)=607, len(neg_indices)=17393\n",
      "label = 10, onehot=10, len(pos_indices)=671, len(neg_indices)=17329\n",
      "label = 11, onehot=11, len(pos_indices)=1406, len(neg_indices)=16594\n",
      "label = 12, onehot=12, len(pos_indices)=59, len(neg_indices)=17941\n",
      "label = 13, onehot=13, len(pos_indices)=34, len(neg_indices)=17966\n",
      "label = 14, onehot=14, len(pos_indices)=170, len(neg_indices)=17830\n",
      "label = 15, onehot=15, len(pos_indices)=1006, len(neg_indices)=16994\n",
      "label = 16, onehot=16, len(pos_indices)=1143, len(neg_indices)=16857\n",
      "label = 17, onehot=17, len(pos_indices)=2150, len(neg_indices)=15850\n",
      "label = 18, onehot=18, len(pos_indices)=114, len(neg_indices)=17886\n",
      "label = 19, onehot=19, len(pos_indices)=1834, len(neg_indices)=16166\n",
      "label = 20, onehot=20, len(pos_indices)=101, len(neg_indices)=17899\n",
      "label = 21, onehot=21, len(pos_indices)=1228, len(neg_indices)=16772\n",
      "label = 22, onehot=22, len(pos_indices)=37, len(neg_indices)=17963\n",
      "label = 23, onehot=23, len(pos_indices)=371, len(neg_indices)=17629\n",
      "label = 24, onehot=24, len(pos_indices)=1163, len(neg_indices)=16837\n",
      "label = 25, onehot=25, len(pos_indices)=914, len(neg_indices)=17086\n",
      "label = 26, onehot=26, len(pos_indices)=4945, len(neg_indices)=13055\n",
      "label = 27, onehot=27, len(pos_indices)=12652, len(neg_indices)=5348\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m16) \u001b[0m\u001b[34mCreating PadChest vqa trainer ...\u001b[0m\n",
      "../train_vqa.py:1523: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug = debug)\n",
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of rows after filtering: 109821 (dropped rows with duplicate StudyID)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n",
      "Generating answers based on labels...\n",
      "100%|███████████████████████████████| 109821/109821 [00:00<00:00, 120904.30it/s]\n",
      "Done. Example answer: <s> pectum carinatum </s>\n",
      "Generating answers based on localizations...\n",
      "100%|███████████████████████████████| 109821/109821 [00:00<00:00, 128079.07it/s]\n",
      "Done. Example answer: <s> </s>\n",
      "Number of non-empty answers: 63528\n",
      "Generating answers based on labels localizations by sentence...\n",
      "100%|████████████████████████████████| 109821/109821 [00:01<00:00, 69252.50it/s]\n",
      "Done. Example answer: <s> bronchiectasis loc middle lobe loc bronchi </s>\n",
      "Creating dataset and dataloader...\n",
      "Example of positive localization answer: <s> loc bone , loc right , loc left , loc aortic button , loc tracheal , loc humerus , loc hilar , loc diaphragm </s>\n",
      "Example of negative localization answer: <s> </s>\n",
      "193it [00:27,  7.14it/s]\n",
      "Done!\n",
      "len(self.train_dataset) = 1000000000000000000\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m17) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 8\n",
      "len(_val_dataloaders) = 2\n",
      "_train_weights = [1.0, 0.8, 0.2, 0.16, 0.5, 0.4, 0.4, 0.5]\n",
      "merged_dataset_name = mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m18) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m19) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_27_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4564.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m20) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_27_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4564.pt\n",
      "Loading model and epoch only\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m21) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m22) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 28/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 5.14872, a_loss 1.51831, cD 0.65275, wmdcmp 0.09488, ema 0.48353, oracc 0.93483, orien_loss 0.17361, qlmicf1 0.24370, qlmacf1 0.15793, ql_loss 1.03854, chxlmicf1 0.40114, chxlmacf1 0.37665, chx_loss 1.04472, chxlacc 0.60241, chxlrocaucmic 0.68096, chxlrocaucmac 0.65727, gacc 0.77594, gloss 0.51820, cxr14micf1 0.18133, cxr14macf1 0.22662, cxr14_loss 1.17717, vnbgmicf1 0.30511, vnbgmacf1 0.25823, vnbg_loss 1.46820, b1 0.32375, b2 0.22529, b3 0.15587, b4 0.11035, padchxlmacf1 0.03311, padchxlmicf1 0.09374, padchxlzmacf1 0.05326, padchxlzmicf1 0.10307, padchxl_loss 0.66325, padchxlz_loss 0.80632, 113.85 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.58316, wmdcmp 0.08611, ema 0.57386, oracc 0.94816, qlmicf1 0.24866, qlmacf1 0.16659, chxlmicf1 0.44186, chxlmacf1 0.38955, chxlacc 0.60565, chxlrocaucmic 0.68965, chxlrocaucmac 0.65542, 29.22 secs\n",
      "Adjusting learning rate of group 0 to 3.3145e-06.\n",
      "\u001b[1m---- Epoch 29/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 2.19593, a_loss 1.52204, cD 0.60266, wmdcmp 0.08909, ema 0.48071, oracc 0.93788, orien_loss 0.16864, qlmicf1 0.24337, qlmacf1 0.15865, ql_loss 1.04141, chxlmicf1 0.39382, chxlmacf1 0.37449, chx_loss 1.04409, chxlacc 0.60161, chxlrocaucmic 0.67794, chxlrocaucmac 0.65754, gacc 0.77000, gloss 0.52063, cxr14micf1 0.18387, cxr14macf1 0.22755, cxr14_loss 1.17958, vnbgmicf1 0.31350, vnbgmacf1 0.25895, vnbg_loss 1.47596, b1 0.33623, b2 0.23799, b3 0.16804, b4 0.12115, padchxlmacf1 0.03225, padchxlmicf1 0.09014, padchxlzmacf1 0.05139, padchxlzmicf1 0.09955, padchxl_loss 0.63901, padchxlz_loss 0.76465, 85.57 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.58511, wmdcmp 0.08645, ema 0.57744, oracc 0.94793, qlmicf1 0.25022, qlmacf1 0.16760, chxlmicf1 0.44057, chxlmacf1 0.38822, chxlacc 0.60193, chxlrocaucmic 0.68955, chxlrocaucmac 0.65299, 28.66 secs\n",
      "Adjusting learning rate of group 0 to 1.0986e-05.\n",
      "\u001b[1m---- Epoch 30/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 2.32467, a_loss 1.53710, cD 0.60746, wmdcmp 0.09018, ema 0.49074, oracc 0.93771, orien_loss 0.17028, qlmicf1 0.24314, qlmacf1 0.15790, ql_loss 1.03255, chxlmicf1 0.39483, chxlmacf1 0.37524, chx_loss 1.04589, chxlacc 0.60167, chxlrocaucmic 0.67697, chxlrocaucmac 0.65668, gacc 0.77807, gloss 0.51431, cxr14micf1 0.18891, cxr14macf1 0.23001, cxr14_loss 1.16686, vnbgmicf1 0.30798, vnbgmacf1 0.25884, vnbg_loss 1.45854, b1 0.34272, b2 0.24006, b3 0.16603, b4 0.11704, padchxlmacf1 0.03317, padchxlmicf1 0.09200, padchxlzmacf1 0.05141, padchxlzmicf1 0.09778, padchxl_loss 0.64105, padchxlz_loss 0.78780, 85.96 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.58687, wmdcmp 0.08748, ema 0.57565, oracc 0.94863, qlmicf1 0.24768, qlmacf1 0.16815, chxlmicf1 0.44355, chxlmacf1 0.38945, chxlacc 0.60354, chxlrocaucmic 0.69166, chxlrocaucmac 0.65113, 28.73 secs\n",
      "Adjusting learning rate of group 0 to 3.6411e-05.\n",
      "\u001b[1m---- Epoch 31/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000036) ...\n",
      "loss 1.94464, a_loss 1.53242, cD 0.62018, wmdcmp 0.09154, ema 0.48935, oracc 0.93796, orien_loss 0.17029, qlmicf1 0.24218, qlmacf1 0.15930, ql_loss 1.04289, chxlmicf1 0.39941, chxlmacf1 0.37683, chx_loss 1.04356, chxlacc 0.60093, chxlrocaucmic 0.68022, chxlrocaucmac 0.66037, gacc 0.77765, gloss 0.51258, cxr14micf1 0.18860, cxr14macf1 0.22914, cxr14_loss 1.17731, vnbgmicf1 0.32824, vnbgmacf1 0.27067, vnbg_loss 1.45993, b1 0.32840, b2 0.23078, b3 0.15782, b4 0.10990, padchxlmacf1 0.03187, padchxlmicf1 0.08860, padchxlzmacf1 0.05043, padchxlzmicf1 0.09915, padchxl_loss 0.65353, padchxlz_loss 0.77931, 86.40 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.59198, wmdcmp 0.08942, ema 0.58013, oracc 0.94934, qlmicf1 0.24553, qlmacf1 0.16733, chxlmicf1 0.44346, chxlmacf1 0.39084, chxlacc 0.60549, chxlrocaucmic 0.69365, chxlrocaucmac 0.65558, 28.70 secs\n",
      "Adjusting learning rate of group 0 to 1.2068e-04.\n",
      "\u001b[1m---- Epoch 32/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000121) ...\n",
      "loss 2.17733, a_loss 1.48303, cD 0.64275, wmdcmp 0.09449, ema 0.49333, oracc 0.93731, orien_loss 0.16489, qlmicf1 0.24432, qlmacf1 0.15937, ql_loss 1.03009, chxlmicf1 0.39917, chxlmacf1 0.37737, chx_loss 1.04007, chxlacc 0.60555, chxlrocaucmic 0.68202, chxlrocaucmac 0.66179, gacc 0.76957, gloss 0.50900, cxr14micf1 0.18395, cxr14macf1 0.22753, cxr14_loss 1.18127, vnbgmicf1 0.32408, vnbgmacf1 0.26832, vnbg_loss 1.42873, b1 0.35719, b2 0.25253, b3 0.17448, b4 0.12428, padchxlmacf1 0.03247, padchxlmicf1 0.09103, padchxlzmacf1 0.05348, padchxlzmicf1 0.10036, padchxl_loss 0.64036, padchxlz_loss 0.77609, 86.12 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.66349, wmdcmp 0.09789, ema 0.59714, oracc 0.94675, qlmicf1 0.24183, qlmacf1 0.16562, chxlmicf1 0.44439, chxlmacf1 0.39160, chxlacc 0.60828, chxlrocaucmic 0.69192, chxlrocaucmac 0.65510, 28.86 secs\n",
      "Adjusting learning rate of group 0 to 4.0000e-04.\n",
      "\u001b[1m---- Epoch 33/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000400) ...\n",
      "loss 5.85198, a_loss 1.46649, cD 0.70924, wmdcmp 0.10217, ema 0.49710, oracc 0.93923, orien_loss 0.15333, qlmicf1 0.24390, qlmacf1 0.16033, ql_loss 1.03213, chxlmicf1 0.40041, chxlmacf1 0.37814, chx_loss 1.04120, chxlacc 0.60806, chxlrocaucmic 0.68449, chxlrocaucmac 0.66255, gacc 0.77710, gloss 0.48976, cxr14micf1 0.19009, cxr14macf1 0.23383, cxr14_loss 1.16764, vnbgmicf1 0.33498, vnbgmacf1 0.27489, vnbg_loss 1.38459, b1 0.37399, b2 0.26706, b3 0.18887, b4 0.13823, padchxlmacf1 0.03432, padchxlmicf1 0.08786, padchxlzmacf1 0.05375, padchxlzmicf1 0.09305, padchxl_loss 0.65393, padchxlz_loss 0.76536, 86.79 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.81429, wmdcmp 0.11811, ema 0.57117, oracc 0.95264, qlmicf1 0.25339, qlmacf1 0.17031, chxlmicf1 0.44241, chxlmacf1 0.39139, chxlacc 0.62875, chxlrocaucmic 0.69547, chxlrocaucmac 0.66319, 28.67 secs\n",
      "Adjusting learning rate of group 0 to 2.8867e-04.\n",
      "\u001b[1m---- Epoch 34/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000289) ...\n",
      "loss 4.10114, a_loss 1.38511, cD 0.78074, wmdcmp 0.11204, ema 0.51564, oracc 0.94855, orien_loss 0.13456, qlmicf1 0.24153, qlmacf1 0.16147, ql_loss 1.03085, chxlmicf1 0.40751, chxlmacf1 0.38094, chx_loss 1.03671, chxlacc 0.60995, chxlrocaucmic 0.68940, chxlrocaucmac 0.66752, gacc 0.78276, gloss 0.47109, cxr14micf1 0.19345, cxr14macf1 0.23555, cxr14_loss 1.16739, vnbgmicf1 0.34174, vnbgmacf1 0.28337, vnbg_loss 1.31950, b1 0.39313, b2 0.28590, b3 0.20322, b4 0.14553, padchxlmacf1 0.03758, padchxlmicf1 0.10088, padchxlzmacf1 0.05887, padchxlzmicf1 0.10931, padchxl_loss 0.64222, padchxlz_loss 0.76150, 85.80 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.88224, wmdcmp 0.12635, ema 0.59266, oracc 0.96183, qlmicf1 0.24598, qlmacf1 0.17089, chxlmicf1 0.44300, chxlmacf1 0.39168, chxlacc 0.62451, chxlrocaucmic 0.69618, chxlrocaucmac 0.66359, 28.96 secs\n",
      "Adjusting learning rate of group 0 to 2.0832e-04.\n",
      "\u001b[1m---- Epoch 35/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000208) ...\n",
      "loss 4.07329, a_loss 1.34280, cD 0.83136, wmdcmp 0.11793, ema 0.52138, oracc 0.94616, orien_loss 0.13126, qlmicf1 0.24562, qlmacf1 0.16231, ql_loss 1.02885, chxlmicf1 0.40730, chxlmacf1 0.38300, chx_loss 1.03527, chxlacc 0.61093, chxlrocaucmic 0.68846, chxlrocaucmac 0.66810, gacc 0.79952, gloss 0.45416, cxr14micf1 0.19916, cxr14macf1 0.23928, cxr14_loss 1.16167, vnbgmicf1 0.34470, vnbgmacf1 0.28307, vnbg_loss 1.29365, b1 0.40019, b2 0.29473, b3 0.21416, b4 0.15637, padchxlmacf1 0.03870, padchxlmicf1 0.09896, padchxlzmacf1 0.05683, padchxlzmicf1 0.10442, padchxl_loss 0.64566, padchxlz_loss 0.78102, 87.09 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.91335, wmdcmp 0.13277, ema 0.61594, oracc 0.95594, qlmicf1 0.25733, qlmacf1 0.17449, chxlmicf1 0.44096, chxlmacf1 0.39731, chxlacc 0.60023, chxlrocaucmic 0.69019, chxlrocaucmac 0.66552, 29.99 secs\n",
      "Adjusting learning rate of group 0 to 1.5034e-04.\n",
      "\u001b[1m---- Epoch 36/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000150) ...\n",
      "loss 5.43897, a_loss 1.33526, cD 0.88352, wmdcmp 0.12499, ema 0.51995, oracc 0.95005, orien_loss 0.12192, qlmicf1 0.24915, qlmacf1 0.16405, ql_loss 1.02677, chxlmicf1 0.41260, chxlmacf1 0.38691, chx_loss 1.03018, chxlacc 0.61513, chxlrocaucmic 0.69157, chxlrocaucmac 0.67207, gacc 0.79720, gloss 0.44718, cxr14micf1 0.19936, cxr14macf1 0.23903, cxr14_loss 1.16510, vnbgmicf1 0.34714, vnbgmacf1 0.28577, vnbg_loss 1.29359, b1 0.40343, b2 0.29407, b3 0.20996, b4 0.15307, padchxlmacf1 0.03714, padchxlmicf1 0.09513, padchxlzmacf1 0.05708, padchxlzmicf1 0.09912, padchxl_loss 0.63803, padchxlz_loss 0.78156, 86.31 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.96593, wmdcmp 0.13812, ema 0.58192, oracc 0.95947, qlmicf1 0.24629, qlmacf1 0.17119, chxlmicf1 0.45836, chxlmacf1 0.40277, chxlacc 0.61117, chxlrocaucmic 0.71087, chxlrocaucmac 0.66566, 28.71 secs\n",
      "Adjusting learning rate of group 0 to 1.0849e-04.\n",
      "\u001b[1m---- Epoch 37/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000108) ...\n",
      "loss 1.91973, a_loss 1.30958, cD 0.91354, wmdcmp 0.12754, ema 0.53410, oracc 0.95087, orien_loss 0.12349, qlmicf1 0.24845, qlmacf1 0.16446, ql_loss 1.02347, chxlmicf1 0.42211, chxlmacf1 0.38908, chx_loss 1.02508, chxlacc 0.61924, chxlrocaucmic 0.69808, chxlrocaucmac 0.67623, gacc 0.79876, gloss 0.44869, cxr14micf1 0.19807, cxr14macf1 0.23612, cxr14_loss 1.15980, vnbgmicf1 0.35753, vnbgmacf1 0.28731, vnbg_loss 1.26295, b1 0.39634, b2 0.29292, b3 0.21350, b4 0.15723, padchxlmacf1 0.03734, padchxlmicf1 0.09542, padchxlzmacf1 0.05564, padchxlzmicf1 0.09833, padchxl_loss 0.63797, padchxlz_loss 0.75663, 86.43 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 1.03990, wmdcmp 0.14581, ema 0.60340, oracc 0.96230, qlmicf1 0.24780, qlmacf1 0.17105, chxlmicf1 0.45871, chxlmacf1 0.40314, chxlacc 0.60728, chxlrocaucmic 0.71636, chxlrocaucmac 0.66884, 29.00 secs\n",
      "Adjusting learning rate of group 0 to 7.8297e-05.\n",
      "\u001b[1m---- Epoch 38/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000078) ...\n",
      "loss 2.32546, a_loss 1.31050, cD 0.89960, wmdcmp 0.12647, ema 0.53353, oracc 0.95059, orien_loss 0.11797, qlmicf1 0.24811, qlmacf1 0.16303, ql_loss 1.02648, chxlmicf1 0.41102, chxlmacf1 0.38470, chx_loss 1.02961, chxlacc 0.61345, chxlrocaucmic 0.69375, chxlrocaucmac 0.67324, gacc 0.79556, gloss 0.44284, cxr14micf1 0.19743, cxr14macf1 0.23810, cxr14_loss 1.16139, vnbgmicf1 0.35760, vnbgmacf1 0.28469, vnbg_loss 1.26317, b1 0.40433, b2 0.29840, b3 0.21608, b4 0.15906, padchxlmacf1 0.03724, padchxlmicf1 0.09551, padchxlzmacf1 0.05357, padchxlzmicf1 0.09893, padchxl_loss 0.62995, padchxlz_loss 0.77587, 87.94 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.03275, wmdcmp 0.14510, ema 0.62399, oracc 0.95688, qlmicf1 0.25512, qlmacf1 0.17430, chxlmicf1 0.45722, chxlmacf1 0.40442, chxlacc 0.61275, chxlrocaucmic 0.71222, chxlrocaucmac 0.67248, 28.88 secs\n",
      "Adjusting learning rate of group 0 to 5.6505e-05.\n",
      "\u001b[1m---- Epoch 39/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000057) ...\n",
      "loss 5.35237, a_loss 1.29456, cD 0.91545, wmdcmp 0.12824, ema 0.53933, oracc 0.95308, orien_loss 0.11829, qlmicf1 0.24898, qlmacf1 0.16381, ql_loss 1.02807, chxlmicf1 0.41517, chxlmacf1 0.38584, chx_loss 1.02872, chxlacc 0.61660, chxlrocaucmic 0.69402, chxlrocaucmac 0.67230, gacc 0.80667, gloss 0.43493, cxr14micf1 0.19374, cxr14macf1 0.23477, cxr14_loss 1.16494, vnbgmicf1 0.35184, vnbgmacf1 0.28873, vnbg_loss 1.25998, b1 0.42420, b2 0.31881, b3 0.23633, b4 0.17615, padchxlmacf1 0.03734, padchxlmicf1 0.09920, padchxlzmacf1 0.05772, padchxlzmicf1 0.10097, padchxl_loss 0.62472, padchxlz_loss 0.77892, 86.12 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.07896, wmdcmp 0.15080, ema 0.62399, oracc 0.96230, qlmicf1 0.25865, qlmacf1 0.17480, chxlmicf1 0.45960, chxlmacf1 0.40506, chxlacc 0.61833, chxlrocaucmic 0.71245, chxlrocaucmac 0.67152, 28.82 secs\n",
      "Adjusting learning rate of group 0 to 4.0778e-05.\n",
      "\u001b[1m---- Epoch 40/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000041) ...\n",
      "loss 1.17664, a_loss 1.29077, cD 0.93833, wmdcmp 0.13056, ema 0.53568, oracc 0.95246, orien_loss 0.11721, qlmicf1 0.24954, qlmacf1 0.16464, ql_loss 1.02390, chxlmicf1 0.41477, chxlmacf1 0.38645, chx_loss 1.02653, chxlacc 0.62014, chxlrocaucmic 0.69709, chxlrocaucmac 0.67726, gacc 0.80396, gloss 0.43993, cxr14micf1 0.18891, cxr14macf1 0.23126, cxr14_loss 1.16605, vnbgmicf1 0.37210, vnbgmacf1 0.29348, vnbg_loss 1.23518, b1 0.39905, b2 0.29785, b3 0.21797, b4 0.16141, padchxlmacf1 0.03867, padchxlmicf1 0.10406, padchxlzmacf1 0.05812, padchxlzmicf1 0.10580, padchxl_loss 0.62024, padchxlz_loss 0.76835, 86.60 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.08489, wmdcmp 0.15211, ema 0.62489, oracc 0.96065, qlmicf1 0.25604, qlmacf1 0.17383, chxlmicf1 0.45962, chxlmacf1 0.40478, chxlacc 0.62036, chxlrocaucmic 0.71404, chxlrocaucmac 0.67128, 28.87 secs\n",
      "Adjusting learning rate of group 0 to 2.9428e-05.\n",
      "\u001b[1m---- Epoch 41/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000029) ...\n",
      "loss 2.21002, a_loss 1.28532, cD 0.93142, wmdcmp 0.12946, ema 0.53559, oracc 0.95375, orien_loss 0.11352, qlmicf1 0.24954, qlmacf1 0.16438, ql_loss 1.02197, chxlmicf1 0.42002, chxlmacf1 0.38780, chx_loss 1.02727, chxlacc 0.61924, chxlrocaucmic 0.69887, chxlrocaucmac 0.67482, gacc 0.81010, gloss 0.43802, cxr14micf1 0.20426, cxr14macf1 0.24208, cxr14_loss 1.16182, vnbgmicf1 0.37283, vnbgmacf1 0.29258, vnbg_loss 1.24665, b1 0.42098, b2 0.31855, b3 0.23727, b4 0.17740, padchxlmacf1 0.03953, padchxlmicf1 0.09597, padchxlzmacf1 0.05730, padchxlzmicf1 0.10138, padchxl_loss 0.62068, padchxlz_loss 0.75160, 86.96 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.05942, wmdcmp 0.14871, ema 0.61415, oracc 0.96371, qlmicf1 0.26106, qlmacf1 0.17555, chxlmicf1 0.45304, chxlmacf1 0.40193, chxlacc 0.61528, chxlrocaucmic 0.70727, chxlrocaucmac 0.67180, 28.78 secs\n",
      "Adjusting learning rate of group 0 to 2.1237e-05.\n",
      "\u001b[1m---- Epoch 42/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 1.75692, a_loss 1.26339, cD 0.93763, wmdcmp 0.13141, ema 0.54200, oracc 0.95225, orien_loss 0.11358, qlmicf1 0.24874, qlmacf1 0.16398, ql_loss 1.02214, chxlmicf1 0.41259, chxlmacf1 0.38572, chx_loss 1.02845, chxlacc 0.61492, chxlrocaucmic 0.69356, chxlrocaucmac 0.67303, gacc 0.80396, gloss 0.43627, cxr14micf1 0.19647, cxr14macf1 0.23507, cxr14_loss 1.16308, vnbgmicf1 0.37754, vnbgmacf1 0.29567, vnbg_loss 1.23259, b1 0.41868, b2 0.31611, b3 0.23497, b4 0.17785, padchxlmacf1 0.03862, padchxlmicf1 0.09710, padchxlzmacf1 0.05709, padchxlzmicf1 0.10412, padchxl_loss 0.61975, padchxlz_loss 0.77263, 86.25 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.07294, wmdcmp 0.14989, ema 0.61415, oracc 0.96018, qlmicf1 0.25523, qlmacf1 0.17313, chxlmicf1 0.45361, chxlmacf1 0.40147, chxlacc 0.61136, chxlrocaucmic 0.71007, chxlrocaucmac 0.67063, 28.93 secs\n",
      "Adjusting learning rate of group 0 to 1.5326e-05.\n",
      "\u001b[1m---- Epoch 43/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 1.86717, a_loss 1.27687, cD 0.95088, wmdcmp 0.13188, ema 0.53947, oracc 0.95323, orien_loss 0.11637, qlmicf1 0.25005, qlmacf1 0.16332, ql_loss 1.02538, chxlmicf1 0.41844, chxlmacf1 0.38816, chx_loss 1.02751, chxlacc 0.61769, chxlrocaucmic 0.69683, chxlrocaucmac 0.67710, gacc 0.80600, gloss 0.43400, cxr14micf1 0.20136, cxr14macf1 0.24050, cxr14_loss 1.15577, vnbgmicf1 0.36318, vnbgmacf1 0.28532, vnbg_loss 1.23391, b1 0.42479, b2 0.32067, b3 0.24012, b4 0.17930, padchxlmacf1 0.03835, padchxlmicf1 0.10226, padchxlzmacf1 0.05702, padchxlzmicf1 0.10414, padchxl_loss 0.62434, padchxlz_loss 0.77273, 86.55 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.07440, wmdcmp 0.15031, ema 0.61235, oracc 0.96631, qlmicf1 0.25824, qlmacf1 0.17394, chxlmicf1 0.45825, chxlmacf1 0.40441, chxlacc 0.61794, chxlrocaucmic 0.71288, chxlrocaucmac 0.67354, 28.86 secs\n",
      "Adjusting learning rate of group 0 to 1.1060e-05.\n",
      "\u001b[1m---- Epoch 44/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 5.16601, a_loss 1.28122, cD 0.96311, wmdcmp 0.13358, ema 0.53659, oracc 0.95511, orien_loss 0.11647, qlmicf1 0.24898, qlmacf1 0.16462, ql_loss 1.02121, chxlmicf1 0.41556, chxlmacf1 0.38662, chx_loss 1.02765, chxlacc 0.61788, chxlrocaucmic 0.69644, chxlrocaucmac 0.67589, gacc 0.80876, gloss 0.43263, cxr14micf1 0.20813, cxr14macf1 0.24365, cxr14_loss 1.15218, vnbgmicf1 0.37108, vnbgmacf1 0.29081, vnbg_loss 1.24739, b1 0.42323, b2 0.31517, b3 0.23084, b4 0.16909, padchxlmacf1 0.04048, padchxlmicf1 0.10106, padchxlzmacf1 0.05584, padchxlzmicf1 0.09731, padchxl_loss 0.62365, padchxlz_loss 0.77977, 86.89 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.08083, wmdcmp 0.15143, ema 0.60967, oracc 0.95759, qlmicf1 0.25666, qlmacf1 0.17423, chxlmicf1 0.45845, chxlmacf1 0.40464, chxlacc 0.61879, chxlrocaucmic 0.71009, chxlrocaucmac 0.67052, 28.81 secs\n",
      "Adjusting learning rate of group 0 to 7.9819e-06.\n",
      "\u001b[1m---- Epoch 45/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 5.25910, a_loss 1.28371, cD 0.95588, wmdcmp 0.13355, ema 0.53789, oracc 0.95547, orien_loss 0.11069, qlmicf1 0.24929, qlmacf1 0.16450, ql_loss 1.02911, chxlmicf1 0.41315, chxlmacf1 0.38558, chx_loss 1.02866, chxlacc 0.61619, chxlrocaucmic 0.69463, chxlrocaucmac 0.67316, gacc 0.80763, gloss 0.43254, cxr14micf1 0.19823, cxr14macf1 0.23688, cxr14_loss 1.15876, vnbgmicf1 0.37848, vnbgmacf1 0.29408, vnbg_loss 1.24678, b1 0.41675, b2 0.31134, b3 0.22980, b4 0.17108, padchxlmacf1 0.03775, padchxlmicf1 0.10088, padchxlzmacf1 0.05728, padchxlzmicf1 0.09887, padchxl_loss 0.64024, padchxlz_loss 0.78189, 86.49 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.07378, wmdcmp 0.15037, ema 0.61146, oracc 0.96230, qlmicf1 0.25603, qlmacf1 0.17372, chxlmicf1 0.45768, chxlmacf1 0.40399, chxlacc 0.61460, chxlrocaucmic 0.71169, chxlrocaucmac 0.67038, 29.09 secs\n",
      "Adjusting learning rate of group 0 to 5.7603e-06.\n",
      "\u001b[1m---- Epoch 46/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 3.64517, a_loss 1.29226, cD 0.95592, wmdcmp 0.13297, ema 0.53616, oracc 0.95438, orien_loss 0.11463, qlmicf1 0.24788, qlmacf1 0.16437, ql_loss 1.02645, chxlmicf1 0.41983, chxlmacf1 0.38819, chx_loss 1.02710, chxlacc 0.61707, chxlrocaucmic 0.69747, chxlrocaucmac 0.67539, gacc 0.80933, gloss 0.43000, cxr14micf1 0.19546, cxr14macf1 0.23549, cxr14_loss 1.15454, vnbgmicf1 0.37792, vnbgmacf1 0.29433, vnbg_loss 1.23650, b1 0.42092, b2 0.31592, b3 0.23344, b4 0.17332, padchxlmacf1 0.04186, padchxlmicf1 0.09958, padchxlzmacf1 0.05985, padchxlzmicf1 0.10914, padchxl_loss 0.64451, padchxlz_loss 0.77159, 87.68 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Validation stage ...\n",
      "cD 1.08663, wmdcmp 0.15208, ema 0.61952, oracc 0.96442, qlmicf1 0.25798, qlmacf1 0.17401, chxlmicf1 0.45354, chxlmacf1 0.39987, chxlacc 0.61750, chxlrocaucmic 0.70848, chxlrocaucmac 0.66956, 28.83 secs\n",
      "Adjusting learning rate of group 0 to 4.1570e-06.\n",
      "\u001b[1m---- Epoch 47/47\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 2.23377, a_loss 1.27041, cD 0.95357, wmdcmp 0.13257, ema 0.53894, oracc 0.95240, orien_loss 0.11359, qlmicf1 0.25187, qlmacf1 0.16614, ql_loss 1.01615, chxlmicf1 0.41531, chxlmacf1 0.38741, chx_loss 1.02602, chxlacc 0.61993, chxlrocaucmic 0.69737, chxlrocaucmac 0.67764, gacc 0.81371, gloss 0.43081, cxr14micf1 0.19712, cxr14macf1 0.23741, cxr14_loss 1.16322, vnbgmicf1 0.37299, vnbgmacf1 0.29255, vnbg_loss 1.24093, b1 0.43488, b2 0.32273, b3 0.23454, b4 0.17116, padchxlmacf1 0.03995, padchxlmicf1 0.10535, padchxlzmacf1 0.05961, padchxlzmicf1 0.11003, padchxl_loss 0.61555, padchxlz_loss 0.75567, 86.41 secs\n",
      "(2) Validation stage ...\n",
      "cD 1.10970, wmdcmp 0.15416, ema 0.59534, oracc 0.96254, qlmicf1 0.25854, qlmacf1 0.17438, chxlmicf1 0.45747, chxlmacf1 0.40417, chxlacc 0.61941, chxlrocaucmic 0.70938, chxlrocaucmac 0.67293, 28.89 secs\n",
      "Adjusting learning rate of group 0 to 3.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_vqa.py \\\n",
    "        --checkpoint-folder \"models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --epochs 20 \\\n",
    "        --batches-per-epoch 200 \\\n",
    "        --batch-size 150 \\\n",
    "        --num-workers 4 \\\n",
    "        --iters-to-accumulate 5 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,4e-4,15,3e-6\" \\\n",
    "        --override-lr \\\n",
    "        --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
