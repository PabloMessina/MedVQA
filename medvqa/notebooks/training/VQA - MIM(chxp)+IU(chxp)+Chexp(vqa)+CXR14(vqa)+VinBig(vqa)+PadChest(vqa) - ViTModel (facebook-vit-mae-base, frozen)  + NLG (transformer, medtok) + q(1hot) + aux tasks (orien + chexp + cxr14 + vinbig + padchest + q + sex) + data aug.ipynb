{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 50\n",
      "   batches_per_epoch: 200\n",
      "   checkpoint_folder: None\n",
      "   iuxray_qa_adapted_reports_filename: qa_adapted_reports__20220904_091601.json\n",
      "   mimiccxr_qa_adapted_reports_filename: qa_adapted_reports__20220904_095810.json\n",
      "   vocab_min_freq: 10\n",
      "   embed_size: 256\n",
      "   question_encoding: one-hot\n",
      "   answer_decoding: transformer\n",
      "   question_hidden_size: 128\n",
      "   answer_hidden_size: 256\n",
      "   visual_input_mode: raw-image\n",
      "   raw_image_encoding: vitmodel-huggingface\n",
      "   image_local_feat_size: 768\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   freeze_image_encoder: True\n",
      "   imagenet_pretrained: False\n",
      "   visual_features_mlp_in_dim: None\n",
      "   visual_features_mlp_out_dim: None\n",
      "   visual_features_mlp_hidden_dims: None\n",
      "   iuxray_precomputed_visual_features_path: None\n",
      "   mimiccxr_precomputed_visual_features_path: None\n",
      "   chexpert_precomputed_visual_features_path: None\n",
      "   vinbig_precomputed_visual_features_path: None\n",
      "   clip_version: None\n",
      "   huggingface_model_name: facebook/vit-mae-base\n",
      "   n_lstm_layers: 1\n",
      "   transf_dec_nhead: 2\n",
      "   transf_dec_dim_forward: 256\n",
      "   transf_dec_num_layers: 2\n",
      "   question_vec_size: 128\n",
      "   dropout_prob: 0\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: warmup+decay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: 1e-6,5,4e-4,45,1e-6\n",
      "   warmup_and_cosine_args: None\n",
      "   batch_size: 150\n",
      "   iters_to_accumulate: 5\n",
      "   num_workers: 4\n",
      "   device: GPU\n",
      "   img_aug_mode: random-color-and-spatial\n",
      "   image_size: [224, 224]\n",
      "   mimiccxr_weight: 1.0\n",
      "   chexpert_weight: 0.5\n",
      "   cxr14_weight: 0.4\n",
      "   vinbig_weight: 0.4\n",
      "   iuxray_weight: 0.2\n",
      "   mimiccxr_weight_chexpert_mode: 0.8\n",
      "   iuxray_weight_chexpert_mode: 0.16\n",
      "   padchest_weight: 0.5\n",
      "   mimiccxr_include_chexpert_mode: True\n",
      "   iuxray_include_chexpert_mode: True\n",
      "   use_chexpert_mode_only: False\n",
      "   val_answer_decoding: greedy-search\n",
      "   beam_search_k: None\n",
      "   use_amp: True\n",
      "   medical_tokenization: True\n",
      "   medical_terms_frequency_filename: medical_terms_frequency__20220918_184255.pkl\n",
      "   allowed_questions: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   balanced_dataloading: True\n",
      "   imbalance_reduction_coef: 0.5\n",
      "   iuxray_balanced_metadata_filename: balanced_dataloading_metadata__20220918_210029.pkl\n",
      "   mimiccxr_balanced_metadata_filename: balanced_dataloading_metadata__20220918_210415.pkl\n",
      "   one_question_per_batch: False\n",
      "   save: True\n",
      "   override_lr: False\n",
      "   train_mimiccxr: True\n",
      "   train_iuxray: True\n",
      "   train_chexpert: True\n",
      "   chexpert_mode: vqa\n",
      "   train_cxr14: True\n",
      "   train_vinbig: True\n",
      "   vinbig_training_data: all\n",
      "   vinbig_use_validation: False\n",
      "   train_padchest: True\n",
      "   padchest_training_data_mode: all\n",
      "   padchest_use_validation: False\n",
      "   padchest_train_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\n",
      "   padchest_val_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\n",
      "   padchest_test_study_ids_path: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\n",
      "   binary_loss_name: wbce-c\n",
      "   classify_tags: False\n",
      "   n_medical_tags: None\n",
      "   iuxray_medical_tags_per_report_filename: None\n",
      "   mimiccxr_medical_tags_per_report_filename: None\n",
      "   classify_orientation: True\n",
      "   classify_chexpert: True\n",
      "   iuxray_chexpert_labels_filename: chexpert_labels_per_report__20220904_113427.pkl\n",
      "   mimiccxr_chexpert_labels_filename: chexpert_labels_per_report__20220904_113605.pkl\n",
      "   classify_questions: True\n",
      "   n_questions: 97\n",
      "   iuxray_question_labels_filename: question_labels_per_report__20220904_105752.pkl\n",
      "   mimiccxr_question_labels_filename: question_labels_per_report__20220904_105753.pkl\n",
      "   merge_findings: False\n",
      "\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "Debug: balanced_dataloading = True\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mLoading mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.mask_token', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.4.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using warmup+decay scheduler: 1e-6,5,4e-4,45,1e-6\n",
      "1e-06 5 0.0004 45 1e-06\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "get_engine(): shift_answer=True\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "len(final_transforms) = 16\n",
      "default_prob = 0.3\n",
      "Returning augmented transforms with mode random-color-and-spatial\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=4, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=0, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=3, one_hot_question_offset=0\n",
      "get_vqa_collate_batch_fn(): dataset_id=2, one_hot_question_offset=97\n",
      "get_vqa_collate_batch_fn(): dataset_id=6, one_hot_question_offset=111\n",
      "get_vqa_collate_batch_fn(): dataset_id=5, one_hot_question_offset=126\n",
      "get_vqa_collate_batch_fn(): dataset_id=7, one_hot_question_offset=154\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa trainer ...\u001b[0m\n",
      "MIMICCXR_VQA_Trainer: balanced_dataloading = True\n",
      "self.balanced_dataloading = True\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_train_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053,medical_terms_frequency__20220918_184255.pkl).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 150\n",
      "len(self.report_ids) = 2034935, len(set(self.report_ids)) = 223623\n",
      "_generate_datasets_and_dataloaders()\n",
      "self.balanced_dataloading = True\n",
      "_generate_train_dataset__balanced()\n",
      "Balanced train data loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/balanced_train_data(hash=287,3559006002065882738).pkl\n",
      "\tlen(question_datasets) = 92\n",
      "len(self.val_indices) = 16416\n",
      "len(val_indices) = 3258\n",
      "generating training and validation dataloaders ...\n",
      "num_workers = 4\n",
      "Generating perfectly balanced train dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=43016, len(neg_indices)=178811\n",
      "label = 1, onehot=98, len(pos_indices)=43215, len(neg_indices)=178612\n",
      "label = 2, onehot=99, len(pos_indices)=71655, len(neg_indices)=150172\n",
      "label = 3, onehot=100, len(pos_indices)=9668, len(neg_indices)=212159\n",
      "label = 4, onehot=101, len(pos_indices)=73721, len(neg_indices)=148106\n",
      "label = 5, onehot=102, len(pos_indices)=42623, len(neg_indices)=179204\n",
      "label = 6, onehot=103, len(pos_indices)=19790, len(neg_indices)=202037\n",
      "label = 7, onehot=104, len(pos_indices)=37343, len(neg_indices)=184484\n",
      "label = 8, onehot=105, len(pos_indices)=69678, len(neg_indices)=152149\n",
      "label = 9, onehot=106, len(pos_indices)=13071, len(neg_indices)=208756\n",
      "label = 10, onehot=107, len(pos_indices)=68496, len(neg_indices)=153331\n",
      "label = 11, onehot=108, len(pos_indices)=4944, len(neg_indices)=216883\n",
      "label = 12, onehot=109, len(pos_indices)=8414, len(neg_indices)=213413\n",
      "label = 13, onehot=110, len(pos_indices)=87913, len(neg_indices)=133914\n",
      "len(self.train_dataset__chexpert_mode) = 1000000000000000000\n",
      "Generating balanced validation dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 1, onehot=98, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 2, onehot=99, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 3, onehot=100, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 4, onehot=101, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 5, onehot=102, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 6, onehot=103, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 7, onehot=104, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 8, onehot=105, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 9, onehot=106, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 10, onehot=107, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 11, onehot=108, len(pos_indices)=40, len(neg_indices)=40\n",
      "label = 12, onehot=109, len(pos_indices)=37, len(neg_indices)=40\n",
      "label = 13, onehot=110, len(pos_indices)=40, len(neg_indices)=40\n",
      "len(self.val_dataset__chexpert_mode) = 1117\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mCreating IU X-Ray vqa trainer ...\u001b[0m\n",
      "self.balanced_dataloading = True\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray/iuxray_preprocessed_train_data__(dataset=qa_adapted_reports__20220904_091601.json;tokenizer=4871,39534,3343246773703667053,medical_terms_frequency__20220918_184255.pkl).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "Assigning all data to training ...\n",
      "All data assigned to train: len(train_indices) = 29046, len(val_indices) = 0\n",
      "batch_size = 150\n",
      "len(self.report_ids) = 29046, len(set(self.report_ids)) = 3806\n",
      "_generate_datasets_and_dataloaders()\n",
      "self.balanced_dataloading = True\n",
      "_generate_train_dataset__balanced()\n",
      "Balanced train data loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/iuxray/balanced_train_data(hash=283,2750089748417475840).pkl\n",
      "\tlen(question_datasets) = 42\n",
      "generating training and validation dataloaders ...\n",
      "num_workers = 4\n",
      "Generating perfectly balanced train dataset in chexpert mode ...\n",
      "label = 0, onehot=97, len(pos_indices)=1414, len(neg_indices)=2392\n",
      "label = 1, onehot=98, len(pos_indices)=387, len(neg_indices)=3419\n",
      "label = 2, onehot=99, len(pos_indices)=675, len(neg_indices)=3131\n",
      "label = 3, onehot=100, len(pos_indices)=232, len(neg_indices)=3574\n",
      "label = 4, onehot=101, len(pos_indices)=708, len(neg_indices)=3098\n",
      "label = 5, onehot=102, len(pos_indices)=153, len(neg_indices)=3653\n",
      "label = 6, onehot=103, len(pos_indices)=43, len(neg_indices)=3763\n",
      "label = 7, onehot=104, len(pos_indices)=138, len(neg_indices)=3668\n",
      "label = 8, onehot=105, len(pos_indices)=368, len(neg_indices)=3438\n",
      "label = 9, onehot=106, len(pos_indices)=100, len(neg_indices)=3706\n",
      "label = 10, onehot=107, len(pos_indices)=295, len(neg_indices)=3511\n",
      "label = 11, onehot=108, len(pos_indices)=71, len(neg_indices)=3735\n",
      "label = 12, onehot=109, len(pos_indices)=119, len(neg_indices)=3687\n",
      "label = 13, onehot=110, len(pos_indices)=220, len(neg_indices)=3586\n",
      "len(self.train_dataset__chexpert_mode) = 1000000000000000000\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m13) \u001b[0m\u001b[34mCreating CheXpert vqa trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/workspace/chexpert/CheXpert-v1.0-small/train-val.csv\n",
      "Loading images\n",
      "Loading orientations\n",
      "Loading genders\n",
      "Loading chexpert labels\n",
      "label = 0, onehot=0, len(pos_indices)=22414, len(neg_indices)=201216\n",
      "label = 1, onehot=1, len(pos_indices)=23309, len(neg_indices)=200321\n",
      "label = 2, onehot=2, len(pos_indices)=35151, len(neg_indices)=188479\n",
      "label = 3, onehot=3, len(pos_indices)=10675, len(neg_indices)=212955\n",
      "label = 4, onehot=4, len(pos_indices)=111301, len(neg_indices)=112329\n",
      "label = 5, onehot=5, len(pos_indices)=65274, len(neg_indices)=158356\n",
      "label = 6, onehot=6, len(pos_indices)=42556, len(neg_indices)=181074\n",
      "label = 7, onehot=7, len(pos_indices)=24815, len(neg_indices)=198815\n",
      "label = 8, onehot=8, len(pos_indices)=67191, len(neg_indices)=156439\n",
      "label = 9, onehot=9, len(pos_indices)=22601, len(neg_indices)=201029\n",
      "label = 10, onehot=10, len(pos_indices)=97875, len(neg_indices)=125755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 11, onehot=11, len(pos_indices)=6174, len(neg_indices)=217456\n",
      "label = 12, onehot=12, len(pos_indices)=9680, len(neg_indices)=213950\n",
      "label = 13, onehot=13, len(pos_indices)=117184, len(neg_indices)=106446\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m14) \u001b[0m\u001b[34mCreating CXR14 vqa trainer ...\u001b[0m\n",
      "Loading dataframe from /mnt/data/chest-x-ray-8/metadata/Data_Entry_2017_v2020.csv\n",
      "Loading images\n",
      "Loading orientations\n",
      "Loading genders\n",
      "Loading CXR14 labels\n",
      "label = 0, onehot=0, len(pos_indices)=60361, len(neg_indices)=51759\n",
      "label = 1, onehot=1, len(pos_indices)=11559, len(neg_indices)=100561\n",
      "label = 2, onehot=2, len(pos_indices)=2776, len(neg_indices)=109344\n",
      "label = 3, onehot=3, len(pos_indices)=4667, len(neg_indices)=107453\n",
      "label = 4, onehot=4, len(pos_indices)=2303, len(neg_indices)=109817\n",
      "label = 5, onehot=5, len(pos_indices)=13317, len(neg_indices)=98803\n",
      "label = 6, onehot=6, len(pos_indices)=2516, len(neg_indices)=109604\n",
      "label = 7, onehot=7, len(pos_indices)=1686, len(neg_indices)=110434\n",
      "label = 8, onehot=8, len(pos_indices)=227, len(neg_indices)=111893\n",
      "label = 9, onehot=9, len(pos_indices)=19894, len(neg_indices)=92226\n",
      "label = 10, onehot=10, len(pos_indices)=5782, len(neg_indices)=106338\n",
      "label = 11, onehot=11, len(pos_indices)=6331, len(neg_indices)=105789\n",
      "label = 12, onehot=12, len(pos_indices)=3385, len(neg_indices)=108735\n",
      "label = 13, onehot=13, len(pos_indices)=1431, len(neg_indices)=110689\n",
      "label = 14, onehot=14, len(pos_indices)=5302, len(neg_indices)=106818\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m15) \u001b[0m\u001b[34mCreating VinBig vqa trainer ...\u001b[0m\n",
      "Loading dataframe from:\n",
      "  /mnt/workspace/vinbig-cxr/dataset-png/annotations/image_labels_train.csv\n",
      "  /mnt/workspace/vinbig-cxr/dataset-png/annotations/image_labels_test.csv\n",
      "Sanity checking train labels ...\n",
      "Done!\n",
      "Generating train dataset and dataloader\n",
      "label = 0, onehot=0, len(pos_indices)=3287, len(neg_indices)=14713\n",
      "label = 1, onehot=1, len(pos_indices)=272, len(neg_indices)=17728\n",
      "label = 2, onehot=2, len(pos_indices)=646, len(neg_indices)=17354\n",
      "label = 3, onehot=3, len(pos_indices)=2609, len(neg_indices)=15391\n",
      "label = 4, onehot=4, len(pos_indices)=29, len(neg_indices)=17971\n",
      "label = 5, onehot=5, len(pos_indices)=449, len(neg_indices)=17551\n",
      "label = 6, onehot=6, len(pos_indices)=13, len(neg_indices)=17987\n",
      "label = 7, onehot=7, len(pos_indices)=81, len(neg_indices)=17919\n",
      "label = 8, onehot=8, len(pos_indices)=139, len(neg_indices)=17861\n",
      "label = 9, onehot=9, len(pos_indices)=607, len(neg_indices)=17393\n",
      "label = 10, onehot=10, len(pos_indices)=671, len(neg_indices)=17329\n",
      "label = 11, onehot=11, len(pos_indices)=1406, len(neg_indices)=16594\n",
      "label = 12, onehot=12, len(pos_indices)=59, len(neg_indices)=17941\n",
      "label = 13, onehot=13, len(pos_indices)=34, len(neg_indices)=17966\n",
      "label = 14, onehot=14, len(pos_indices)=170, len(neg_indices)=17830\n",
      "label = 15, onehot=15, len(pos_indices)=1006, len(neg_indices)=16994\n",
      "label = 16, onehot=16, len(pos_indices)=1143, len(neg_indices)=16857\n",
      "label = 17, onehot=17, len(pos_indices)=2150, len(neg_indices)=15850\n",
      "label = 18, onehot=18, len(pos_indices)=114, len(neg_indices)=17886\n",
      "label = 19, onehot=19, len(pos_indices)=1834, len(neg_indices)=16166\n",
      "label = 20, onehot=20, len(pos_indices)=101, len(neg_indices)=17899\n",
      "label = 21, onehot=21, len(pos_indices)=1228, len(neg_indices)=16772\n",
      "label = 22, onehot=22, len(pos_indices)=37, len(neg_indices)=17963\n",
      "label = 23, onehot=23, len(pos_indices)=371, len(neg_indices)=17629\n",
      "label = 24, onehot=24, len(pos_indices)=1163, len(neg_indices)=16837\n",
      "label = 25, onehot=25, len(pos_indices)=914, len(neg_indices)=17086\n",
      "label = 26, onehot=26, len(pos_indices)=4945, len(neg_indices)=13055\n",
      "label = 27, onehot=27, len(pos_indices)=12652, len(neg_indices)=5348\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m16) \u001b[0m\u001b[34mCreating PadChest vqa trainer ...\u001b[0m\n",
      "../train_vqa.py:1440: DtypeWarning: Columns (19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  debug=debug)\n",
      "Number of rows before filtering: 160861\n",
      "Number of rows after filtering: 160754 (dropped nan rows)\n",
      "Number of rows after filtering: 160741 (dropped study ids not in all_study_ids_set)\n",
      "Number of rows after filtering: 160723 (dropped rows with unexpected PatientSex_DICOM)\n",
      "Number of rows after filtering: 160704 (dropped rows with unexpected Projection)\n",
      "Number of rows after filtering: 160692 (dropped rows with broken images)\n",
      "Number of rows after filtering: 109821 (dropped rows with duplicate StudyID)\n",
      "Number of labels: 193\n",
      "Number of localizations: 104\n",
      "Generating answers based on labels...\n",
      "100%|████████████████████████████████| 109821/109821 [00:01<00:00, 78878.51it/s]\n",
      "Done. Example answer: <s> copd signs , nodule , apical pleural thickening </s>\n",
      "Generating answers based on localizations...\n",
      "100%|████████████████████████████████| 109821/109821 [00:01<00:00, 64713.11it/s]\n",
      "Done. Example answer: <s> </s>\n",
      "Number of non-empty answers: 63528\n",
      "Generating answers based on labels localizations by sentence...\n",
      "100%|████████████████████████████████| 109821/109821 [00:02<00:00, 39880.12it/s]\n",
      "Done. Example answer: <s> unchanged </s>\n",
      "Creating dataset and dataloader...\n",
      "Example of positive localization answer: <s> loc right , loc basal , loc diaphragm </s>\n",
      "Example of negative localization answer: <s> </s>\n",
      "193it [00:51,  3.72it/s]\n",
      "Done!\n",
      "len(self.train_dataset) = 1000000000000000000\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m17) \u001b[0m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(_train_dataloaders) = 8\n",
      "len(_val_dataloaders) = 2\n",
      "_train_weights = [1.0, 0.8, 0.2, 0.16, 0.5, 0.4, 0.4, 0.5]\n",
      "merged_dataset_name = mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m18) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m19) \u001b[0m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m20) \u001b[0m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m21) \u001b[0m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metrics_logs.csv\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m22) \u001b[0m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 11.77772, a_loss 8.57556, cD 0.00025, wmdcmp 0.00152, ema 0.00000, oracc 0.33895, orien_loss 1.14384, qlmicf1 0.09084, qlmacf1 0.08501, ql_loss 1.13272, chxlmicf1 0.20945, chxlmacf1 0.19271, chx_loss 1.10351, chxlacc 0.53032, chxlrocaucmic 0.51366, chxlrocaucmac 0.49796, gacc 0.44676, gloss 0.72267, cxr14micf1 0.16093, cxr14macf1 0.12031, cxr14_loss 1.24003, vnbgmicf1 0.25791, vnbgmacf1 0.15741, vnbg_loss 9.73130, b1 0.00008, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01416, padchxlmicf1 0.01397, padchxlzmacf1 0.02531, padchxlzmicf1 0.02403, padchxl_loss 0.96314, padchxlz_loss 1.03012, 163.15 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.00028, wmdcmp 0.00283, ema 0.00000, oracc 0.35248, qlmicf1 0.10105, qlmacf1 0.09183, chxlmicf1 0.23248, chxlmacf1 0.20781, chxlacc 0.52919, chxlrocaucmic 0.51947, chxlrocaucmac 0.49891, 37.32 secs\n",
      "Adjusting learning rate of group 0 to 3.3145e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 2/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 11.59390, a_loss 8.48191, cD 0.00018, wmdcmp 0.00156, ema 0.00000, oracc 0.42746, orien_loss 1.11560, qlmicf1 0.09497, qlmacf1 0.08694, ql_loss 1.13464, chxlmicf1 0.20556, chxlmacf1 0.19982, chx_loss 1.10029, chxlacc 0.51891, chxlrocaucmic 0.50957, chxlrocaucmac 0.50094, gacc 0.44562, gloss 0.71556, cxr14micf1 0.16422, cxr14macf1 0.12536, cxr14_loss 1.23809, vnbgmicf1 0.25125, vnbgmacf1 0.15710, vnbg_loss 9.60647, b1 0.00015, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01481, padchxlmicf1 0.01448, padchxlzmacf1 0.02653, padchxlzmicf1 0.02377, padchxl_loss 0.95893, padchxlz_loss 1.03055, 148.63 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.00047, wmdcmp 0.00238, ema 0.00000, oracc 0.60443, qlmicf1 0.10687, qlmacf1 0.09493, chxlmicf1 0.21622, chxlmacf1 0.21182, chxlacc 0.50658, chxlrocaucmic 0.50940, chxlrocaucmac 0.50298, 47.06 secs\n",
      "Adjusting learning rate of group 0 to 1.0986e-05.\n",
      "\u001b[1m---- Epoch 3/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 11.64689, a_loss 8.14782, cD 0.00047, wmdcmp 0.00078, ema 0.00835, oracc 0.60223, orien_loss 1.02810, qlmicf1 0.11167, qlmacf1 0.09471, ql_loss 1.12890, chxlmicf1 0.19244, chxlmacf1 0.20524, chx_loss 1.10155, chxlacc 0.48715, chxlrocaucmic 0.49261, chxlrocaucmac 0.50491, gacc 0.45749, gloss 0.70066, cxr14micf1 0.17337, cxr14macf1 0.13944, cxr14_loss 1.23904, vnbgmicf1 0.25614, vnbgmacf1 0.16450, vnbg_loss 9.17000, b1 0.00017, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01398, padchxlmicf1 0.01468, padchxlzmacf1 0.02641, padchxlzmicf1 0.02466, padchxl_loss 0.94958, padchxlz_loss 1.01554, 134.54 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.00000, wmdcmp 0.00000, ema 0.03402, oracc 0.63457, qlmicf1 0.13858, qlmacf1 0.10563, chxlmicf1 0.21029, chxlmacf1 0.22085, chxlacc 0.47339, chxlrocaucmic 0.48470, chxlrocaucmac 0.51307, 48.96 secs\n",
      "Adjusting learning rate of group 0 to 3.6411e-05.\n",
      "\u001b[1m---- Epoch 4/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000036) ...\n",
      "loss 10.52477, a_loss 7.00450, cD 0.00000, wmdcmp 0.00000, ema 0.04326, oracc 0.66784, orien_loss 0.85654, qlmicf1 0.20009, qlmacf1 0.11892, ql_loss 1.12244, chxlmicf1 0.17477, chxlmacf1 0.25262, chx_loss 1.09997, chxlacc 0.46689, chxlrocaucmic 0.45939, chxlrocaucmac 0.52313, gacc 0.54958, gloss 0.68626, cxr14micf1 0.15991, cxr14macf1 0.16182, cxr14_loss 1.23865, vnbgmicf1 0.24429, vnbgmacf1 0.17850, vnbg_loss 7.74225, b1 0.00000, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01561, padchxlmicf1 0.01697, padchxlzmacf1 0.02604, padchxlzmicf1 0.03194, padchxl_loss 0.93296, padchxlz_loss 1.00783, 134.15 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.00000, wmdcmp 0.00000, ema 0.03581, oracc 0.63457, qlmicf1 0.26858, qlmacf1 0.12468, chxlmicf1 0.22380, chxlmacf1 0.29944, chxlacc 0.47868, chxlrocaucmic 0.47839, chxlrocaucmac 0.54084, 49.88 secs\n",
      "Adjusting learning rate of group 0 to 1.2068e-04.\n",
      "\u001b[1m---- Epoch 5/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000121) ...\n",
      "loss 8.85030, a_loss 5.38936, cD 0.03571, wmdcmp 0.00633, ema 0.03329, oracc 0.67796, orien_loss 0.66255, qlmicf1 0.23134, qlmacf1 0.12302, ql_loss 1.08660, chxlmicf1 0.24598, chxlmacf1 0.30678, chx_loss 1.09662, chxlacc 0.54435, chxlrocaucmic 0.55691, chxlrocaucmac 0.56419, gacc 0.55691, gloss 0.68325, cxr14micf1 0.12749, cxr14macf1 0.18610, cxr14_loss 1.23409, vnbgmicf1 0.17425, vnbgmacf1 0.18215, vnbg_loss 5.90843, b1 0.00002, b2 0.00000, b3 0.00000, b4 0.00000, padchxlmacf1 0.01521, padchxlmicf1 0.03994, padchxlzmacf1 0.02744, padchxlzmicf1 0.04271, padchxl_loss 0.81282, padchxlz_loss 0.91569, 138.44 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.09144, wmdcmp 0.01451, ema 0.00000, oracc 0.64822, qlmicf1 0.26178, qlmacf1 0.13121, chxlmicf1 0.34943, chxlmacf1 0.34847, chxlacc 0.55409, chxlrocaucmic 0.62266, chxlrocaucmac 0.59232, 48.02 secs\n",
      "Adjusting learning rate of group 0 to 4.0000e-04.\n",
      "\u001b[1m---- Epoch 6/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000400) ...\n",
      "loss 8.82182, a_loss 3.67995, cD 0.07504, wmdcmp 0.01603, ema 0.04116, oracc 0.78278, orien_loss 0.51910, qlmicf1 0.24072, qlmacf1 0.14334, ql_loss 1.06838, chxlmicf1 0.32592, chxlmacf1 0.34101, chx_loss 1.08257, chxlacc 0.57132, chxlrocaucmic 0.62242, chxlrocaucmac 0.60126, gacc 0.59643, gloss 0.66611, cxr14micf1 0.14994, cxr14macf1 0.20132, cxr14_loss 1.22810, vnbgmicf1 0.19684, vnbgmacf1 0.21199, vnbg_loss 3.94353, b1 0.03356, b2 0.01612, b3 0.00740, b4 0.00449, padchxlmacf1 0.02137, padchxlmicf1 0.05349, padchxlzmacf1 0.03619, padchxlzmicf1 0.06251, padchxl_loss 0.70585, padchxlz_loss 0.82503, 141.51 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.14751, wmdcmp 0.02497, ema 0.14324, oracc 0.87662, qlmicf1 0.22283, qlmacf1 0.15151, chxlmicf1 0.39466, chxlmacf1 0.36214, chxlacc 0.57115, chxlrocaucmic 0.65493, chxlrocaucmac 0.60867, 46.77 secs\n",
      "Adjusting learning rate of group 0 to 3.5014e-04.\n",
      "\u001b[1m---- Epoch 7/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000350) ...\n",
      "loss 6.83633, a_loss 2.64791, cD 0.11899, wmdcmp 0.02628, ema 0.17659, oracc 0.85926, orien_loss 0.37272, qlmicf1 0.24216, qlmacf1 0.14835, ql_loss 1.05943, chxlmicf1 0.35632, chxlmacf1 0.35438, chx_loss 1.06926, chxlacc 0.57927, chxlrocaucmic 0.64555, chxlrocaucmac 0.62221, gacc 0.65781, gloss 0.64677, cxr14micf1 0.15941, cxr14macf1 0.21029, cxr14_loss 1.20622, vnbgmicf1 0.22014, vnbgmacf1 0.22172, vnbg_loss 3.00237, b1 0.09438, b2 0.05040, b3 0.03103, b4 0.02095, padchxlmacf1 0.02533, padchxlmicf1 0.06436, padchxlzmacf1 0.04179, padchxlzmicf1 0.07496, padchxl_loss 0.70225, padchxlz_loss 0.81720, 145.69 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.11091, wmdcmp 0.02026, ema 0.43330, oracc 0.90888, qlmicf1 0.24095, qlmacf1 0.15368, chxlmicf1 0.39481, chxlmacf1 0.36809, chxlacc 0.57958, chxlrocaucmic 0.65435, chxlrocaucmac 0.62457, 43.12 secs\n",
      "Adjusting learning rate of group 0 to 3.0649e-04.\n",
      "\u001b[1m---- Epoch 8/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000306) ...\n",
      "loss 5.83233, a_loss 2.19004, cD 0.19821, wmdcmp 0.03671, ema 0.31662, oracc 0.89000, orien_loss 0.29885, qlmicf1 0.23905, qlmacf1 0.14999, ql_loss 1.05294, chxlmicf1 0.36961, chxlmacf1 0.36092, chx_loss 1.06315, chxlacc 0.58467, chxlrocaucmic 0.65460, chxlrocaucmac 0.63171, gacc 0.67933, gloss 0.62412, cxr14micf1 0.16540, cxr14macf1 0.21357, cxr14_loss 1.19700, vnbgmicf1 0.24955, vnbgmacf1 0.22522, vnbg_loss 2.43247, b1 0.18110, b2 0.12192, b3 0.08820, b4 0.06405, padchxlmacf1 0.02551, padchxlmicf1 0.07012, padchxlzmacf1 0.04207, padchxlzmicf1 0.08291, padchxl_loss 0.67535, padchxlz_loss 0.79511, 145.54 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.10453, wmdcmp 0.02187, ema 0.49597, oracc 0.91688, qlmicf1 0.24176, qlmacf1 0.15691, chxlmicf1 0.40749, chxlmacf1 0.37491, chxlacc 0.58593, chxlrocaucmic 0.66129, chxlrocaucmac 0.63184, 43.80 secs\n",
      "Adjusting learning rate of group 0 to 2.6828e-04.\n",
      "\u001b[1m---- Epoch 9/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000268) ...\n",
      "loss 6.49098, a_loss 1.96876, cD 0.29911, wmdcmp 0.05096, ema 0.39122, oracc 0.90440, orien_loss 0.25482, qlmicf1 0.23572, qlmacf1 0.15100, ql_loss 1.05158, chxlmicf1 0.36716, chxlmacf1 0.36118, chx_loss 1.06262, chxlacc 0.58615, chxlrocaucmic 0.65493, chxlrocaucmac 0.63645, gacc 0.71662, gloss 0.60488, cxr14micf1 0.17270, cxr14macf1 0.21900, cxr14_loss 1.19327, vnbgmicf1 0.24328, vnbgmacf1 0.22651, vnbg_loss 2.08791, b1 0.27323, b2 0.18278, b3 0.12746, b4 0.08939, padchxlmacf1 0.02724, padchxlmicf1 0.07315, padchxlzmacf1 0.04420, padchxlzmicf1 0.07788, padchxl_loss 0.68428, padchxlz_loss 0.82202, 141.10 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.12709, wmdcmp 0.03269, ema 0.52731, oracc 0.93007, qlmicf1 0.23061, qlmacf1 0.15791, chxlmicf1 0.42736, chxlmacf1 0.37850, chxlacc 0.58059, chxlrocaucmic 0.67392, chxlrocaucmac 0.63499, 44.29 secs\n",
      "Adjusting learning rate of group 0 to 2.3484e-04.\n",
      "\u001b[1m---- Epoch 10/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000235) ...\n",
      "loss 2.49094, a_loss 1.82606, cD 0.38309, wmdcmp 0.06308, ema 0.42624, oracc 0.91848, orien_loss 0.22338, qlmicf1 0.23887, qlmacf1 0.15385, ql_loss 1.04799, chxlmicf1 0.38212, chxlmacf1 0.36747, chx_loss 1.05678, chxlacc 0.59019, chxlrocaucmic 0.66463, chxlrocaucmac 0.64396, gacc 0.74352, gloss 0.57862, cxr14micf1 0.17043, cxr14macf1 0.21754, cxr14_loss 1.19210, vnbgmicf1 0.28529, vnbgmacf1 0.24858, vnbg_loss 1.93372, b1 0.25976, b2 0.16884, b3 0.11397, b4 0.07874, padchxlmacf1 0.02794, padchxlmicf1 0.07789, padchxlzmacf1 0.04660, padchxlzmicf1 0.08835, padchxl_loss 0.67267, padchxlz_loss 0.80149, 137.44 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.15434, wmdcmp 0.03611, ema 0.51388, oracc 0.93478, qlmicf1 0.24684, qlmacf1 0.16112, chxlmicf1 0.40773, chxlmacf1 0.37866, chxlacc 0.58452, chxlrocaucmic 0.66293, chxlrocaucmac 0.64178, 51.68 secs\n",
      "Adjusting learning rate of group 0 to 2.0556e-04.\n",
      "\u001b[1m---- Epoch 11/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000206) ...\n",
      "loss 2.46707, a_loss 1.72948, cD 0.45016, wmdcmp 0.07116, ema 0.44264, oracc 0.92448, orien_loss 0.20472, qlmicf1 0.24310, qlmacf1 0.15503, ql_loss 1.04130, chxlmicf1 0.38371, chxlmacf1 0.36925, chx_loss 1.05396, chxlacc 0.59191, chxlrocaucmic 0.66826, chxlrocaucmac 0.64645, gacc 0.73768, gloss 0.56326, cxr14micf1 0.17633, cxr14macf1 0.22138, cxr14_loss 1.18580, vnbgmicf1 0.27391, vnbgmacf1 0.23891, vnbg_loss 1.80139, b1 0.28498, b2 0.18998, b3 0.12539, b4 0.08633, padchxlmacf1 0.02979, padchxlmicf1 0.08767, padchxlzmacf1 0.05044, padchxlzmicf1 0.09776, padchxl_loss 0.65368, padchxlz_loss 0.78676, 137.92 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.23446, wmdcmp 0.04547, ema 0.52372, oracc 0.93595, qlmicf1 0.24198, qlmacf1 0.16151, chxlmicf1 0.43117, chxlmacf1 0.38215, chxlacc 0.59265, chxlrocaucmic 0.68171, chxlrocaucmac 0.64386, 51.19 secs\n",
      "Adjusting learning rate of group 0 to 1.7994e-04.\n",
      "\u001b[1m---- Epoch 12/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000180) ...\n",
      "loss 6.29042, a_loss 1.66166, cD 0.50959, wmdcmp 0.07858, ema 0.45890, oracc 0.92451, orien_loss 0.20033, qlmicf1 0.24127, qlmacf1 0.15608, ql_loss 1.04303, chxlmicf1 0.39429, chxlmacf1 0.37408, chx_loss 1.04855, chxlacc 0.59841, chxlrocaucmic 0.67455, chxlrocaucmac 0.65250, gacc 0.75400, gloss 0.54579, cxr14micf1 0.16969, cxr14macf1 0.21743, cxr14_loss 1.18887, vnbgmicf1 0.29246, vnbgmacf1 0.24764, vnbg_loss 1.67928, b1 0.31577, b2 0.21040, b3 0.13613, b4 0.09152, padchxlmacf1 0.02993, padchxlmicf1 0.08951, padchxlzmacf1 0.05021, padchxlzmicf1 0.09436, padchxl_loss 0.65277, padchxlz_loss 0.78128, 139.82 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.29230, wmdcmp 0.04939, ema 0.55953, oracc 0.93525, qlmicf1 0.24145, qlmacf1 0.16274, chxlmicf1 0.43630, chxlmacf1 0.38552, chxlacc 0.60090, chxlrocaucmic 0.68605, chxlrocaucmac 0.65079, 47.67 secs\n",
      "Adjusting learning rate of group 0 to 1.5751e-04.\n",
      "\u001b[1m---- Epoch 13/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000158) ...\n",
      "loss 1.62450, a_loss 1.60815, cD 0.54338, wmdcmp 0.08086, ema 0.47823, oracc 0.92847, orien_loss 0.18836, qlmicf1 0.24333, qlmacf1 0.15753, ql_loss 1.04194, chxlmicf1 0.39071, chxlmacf1 0.37230, chx_loss 1.04982, chxlacc 0.59791, chxlrocaucmic 0.67473, chxlrocaucmac 0.65445, gacc 0.76309, gloss 0.53363, cxr14micf1 0.17631, cxr14macf1 0.22176, cxr14_loss 1.18450, vnbgmicf1 0.28756, vnbgmacf1 0.25588, vnbg_loss 1.57897, b1 0.32349, b2 0.22205, b3 0.14897, b4 0.10324, padchxlmacf1 0.03194, padchxlmicf1 0.08758, padchxlzmacf1 0.05453, padchxlzmicf1 0.09952, padchxl_loss 0.65188, padchxlz_loss 0.77311, 141.21 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.33662, wmdcmp 0.05438, ema 0.55595, oracc 0.94184, qlmicf1 0.24321, qlmacf1 0.16543, chxlmicf1 0.44066, chxlmacf1 0.38812, chxlacc 0.60157, chxlrocaucmic 0.69322, chxlrocaucmac 0.65356, 52.42 secs\n",
      "Adjusting learning rate of group 0 to 1.3787e-04.\n",
      "\u001b[1m---- Epoch 14/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000138) ...\n",
      "loss 2.34272, a_loss 1.56750, cD 0.58453, wmdcmp 0.08490, ema 0.48149, oracc 0.93263, orien_loss 0.18584, qlmicf1 0.24205, qlmacf1 0.15842, ql_loss 1.04069, chxlmicf1 0.39745, chxlmacf1 0.37548, chx_loss 1.04625, chxlacc 0.60153, chxlrocaucmic 0.67956, chxlrocaucmac 0.65773, gacc 0.78095, gloss 0.51858, cxr14micf1 0.18121, cxr14macf1 0.22426, cxr14_loss 1.17967, vnbgmicf1 0.33405, vnbgmacf1 0.26563, vnbg_loss 1.50354, b1 0.32973, b2 0.22678, b3 0.15047, b4 0.10273, padchxlmacf1 0.03219, padchxlmicf1 0.08788, padchxlzmacf1 0.05194, padchxlzmicf1 0.09653, padchxl_loss 0.64974, padchxlz_loss 0.77317, 142.33 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.39984, wmdcmp 0.06133, ema 0.55595, oracc 0.94632, qlmicf1 0.24610, qlmacf1 0.16596, chxlmicf1 0.42997, chxlmacf1 0.38657, chxlacc 0.59469, chxlrocaucmic 0.68159, chxlrocaucmac 0.65518, 52.34 secs\n",
      "Adjusting learning rate of group 0 to 1.2068e-04.\n",
      "\u001b[1m---- Epoch 15/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000121) ...\n",
      "loss 2.06232, a_loss 1.51638, cD 0.62143, wmdcmp 0.08923, ema 0.49052, oracc 0.93460, orien_loss 0.17230, qlmicf1 0.24084, qlmacf1 0.15856, ql_loss 1.03982, chxlmicf1 0.39741, chxlmacf1 0.37650, chx_loss 1.04382, chxlacc 0.60098, chxlrocaucmic 0.67840, chxlrocaucmac 0.65786, gacc 0.77159, gloss 0.51615, cxr14micf1 0.18207, cxr14macf1 0.22542, cxr14_loss 1.17908, vnbgmicf1 0.33050, vnbgmacf1 0.26539, vnbg_loss 1.46807, b1 0.33831, b2 0.23092, b3 0.15240, b4 0.10414, padchxlmacf1 0.03252, padchxlmicf1 0.09328, padchxlzmacf1 0.05389, padchxlzmicf1 0.10143, padchxl_loss 0.63571, padchxlz_loss 0.77574, 139.83 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.51899, wmdcmp 0.07392, ema 0.58997, oracc 0.94749, qlmicf1 0.24969, qlmacf1 0.16604, chxlmicf1 0.44017, chxlmacf1 0.39074, chxlacc 0.60278, chxlrocaucmic 0.69305, chxlrocaucmac 0.65952, 50.17 secs\n",
      "Adjusting learning rate of group 0 to 1.0564e-04.\n",
      "\u001b[1m---- Epoch 16/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000106) ...\n",
      "loss 1.93803, a_loss 1.51236, cD 0.65496, wmdcmp 0.09321, ema 0.49458, oracc 0.93897, orien_loss 0.16715, qlmicf1 0.24634, qlmacf1 0.15980, ql_loss 1.03713, chxlmicf1 0.39948, chxlmacf1 0.37727, chx_loss 1.04251, chxlacc 0.60691, chxlrocaucmic 0.68075, chxlrocaucmac 0.65936, gacc 0.78381, gloss 0.50192, cxr14micf1 0.19047, cxr14macf1 0.23345, cxr14_loss 1.17070, vnbgmicf1 0.32021, vnbgmacf1 0.27194, vnbg_loss 1.44303, b1 0.33399, b2 0.23323, b3 0.15837, b4 0.10985, padchxlmacf1 0.03434, padchxlmicf1 0.09205, padchxlzmacf1 0.05287, padchxlzmicf1 0.09745, padchxl_loss 0.65280, padchxlz_loss 0.78226, 140.44 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.56088, wmdcmp 0.08040, ema 0.59087, oracc 0.94749, qlmicf1 0.25127, qlmacf1 0.16670, chxlmicf1 0.44435, chxlmacf1 0.39293, chxlacc 0.61071, chxlrocaucmic 0.69551, chxlrocaucmac 0.65991, 46.49 secs\n",
      "Adjusting learning rate of group 0 to 9.2470e-05.\n",
      "\u001b[1m---- Epoch 17/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000092) ...\n",
      "loss 5.80660, a_loss 1.49452, cD 0.67450, wmdcmp 0.09668, ema 0.49477, oracc 0.93804, orien_loss 0.16580, qlmicf1 0.24639, qlmacf1 0.15960, ql_loss 1.03367, chxlmicf1 0.40340, chxlmacf1 0.37855, chx_loss 1.04199, chxlacc 0.60497, chxlrocaucmic 0.68490, chxlrocaucmac 0.66198, gacc 0.78162, gloss 0.49696, cxr14micf1 0.19130, cxr14macf1 0.23303, cxr14_loss 1.16997, vnbgmicf1 0.31489, vnbgmacf1 0.26452, vnbg_loss 1.43068, b1 0.35939, b2 0.25301, b3 0.17270, b4 0.12026, padchxlmacf1 0.03520, padchxlmicf1 0.09235, padchxlzmacf1 0.05426, padchxlzmicf1 0.10735, padchxl_loss 0.64755, padchxlz_loss 0.77244, 138.76 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.59011, wmdcmp 0.08530, ema 0.59445, oracc 0.95126, qlmicf1 0.24969, qlmacf1 0.16811, chxlmicf1 0.43945, chxlmacf1 0.39170, chxlacc 0.60723, chxlrocaucmic 0.69191, chxlrocaucmac 0.66151, 49.66 secs\n",
      "Adjusting learning rate of group 0 to 8.0943e-05.\n",
      "\u001b[1m---- Epoch 18/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000081) ...\n",
      "loss 5.88778, a_loss 1.48815, cD 0.68964, wmdcmp 0.09797, ema 0.50144, oracc 0.93812, orien_loss 0.16457, qlmicf1 0.24571, qlmacf1 0.16016, ql_loss 1.03869, chxlmicf1 0.40278, chxlmacf1 0.37895, chx_loss 1.04111, chxlacc 0.60581, chxlrocaucmic 0.68545, chxlrocaucmac 0.66377, gacc 0.78570, gloss 0.49727, cxr14micf1 0.18236, cxr14macf1 0.22637, cxr14_loss 1.17445, vnbgmicf1 0.31970, vnbgmacf1 0.26878, vnbg_loss 1.42278, b1 0.37206, b2 0.26117, b3 0.17702, b4 0.12293, padchxlmacf1 0.03316, padchxlmicf1 0.09010, padchxlzmacf1 0.05272, padchxlzmicf1 0.10190, padchxl_loss 0.64169, padchxlz_loss 0.79479, 140.44 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.60769, wmdcmp 0.08819, ema 0.57833, oracc 0.95597, qlmicf1 0.24711, qlmacf1 0.16743, chxlmicf1 0.44654, chxlmacf1 0.39404, chxlacc 0.61326, chxlrocaucmic 0.69924, chxlrocaucmac 0.65892, 49.49 secs\n",
      "Adjusting learning rate of group 0 to 7.0852e-05.\n",
      "\u001b[1m---- Epoch 19/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000071) ...\n",
      "loss 3.95083, a_loss 1.47972, cD 0.70023, wmdcmp 0.10048, ema 0.50168, oracc 0.94140, orien_loss 0.15758, qlmicf1 0.24429, qlmacf1 0.16028, ql_loss 1.03051, chxlmicf1 0.40202, chxlmacf1 0.37854, chx_loss 1.04035, chxlacc 0.60708, chxlrocaucmic 0.68651, chxlrocaucmac 0.66444, gacc 0.78467, gloss 0.49263, cxr14micf1 0.19951, cxr14macf1 0.23635, cxr14_loss 1.17205, vnbgmicf1 0.32224, vnbgmacf1 0.26968, vnbg_loss 1.39801, b1 0.37044, b2 0.26132, b3 0.17715, b4 0.12313, padchxlmacf1 0.03422, padchxlmicf1 0.09364, padchxlzmacf1 0.05219, padchxlzmicf1 0.09834, padchxl_loss 0.64333, padchxlz_loss 0.77893, 143.39 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.64588, wmdcmp 0.09159, ema 0.61952, oracc 0.95220, qlmicf1 0.25058, qlmacf1 0.16851, chxlmicf1 0.44588, chxlmacf1 0.39554, chxlacc 0.61510, chxlrocaucmic 0.69915, chxlrocaucmac 0.66430, 45.58 secs\n",
      "Adjusting learning rate of group 0 to 6.2020e-05.\n",
      "\u001b[1m---- Epoch 20/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000062) ...\n",
      "loss 2.38160, a_loss 1.44735, cD 0.72351, wmdcmp 0.10223, ema 0.50528, oracc 0.93997, orien_loss 0.15316, qlmicf1 0.24603, qlmacf1 0.15979, ql_loss 1.03283, chxlmicf1 0.40192, chxlmacf1 0.37787, chx_loss 1.03992, chxlacc 0.60859, chxlrocaucmic 0.68687, chxlrocaucmac 0.66515, gacc 0.78476, gloss 0.49367, cxr14micf1 0.18629, cxr14macf1 0.22880, cxr14_loss 1.17024, vnbgmicf1 0.33089, vnbgmacf1 0.27450, vnbg_loss 1.38957, b1 0.37381, b2 0.26633, b3 0.18480, b4 0.12981, padchxlmacf1 0.03472, padchxlmicf1 0.09421, padchxlzmacf1 0.05462, padchxlzmicf1 0.10301, padchxl_loss 0.63618, padchxlz_loss 0.76723, 145.15 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.64948, wmdcmp 0.09385, ema 0.59624, oracc 0.94914, qlmicf1 0.24813, qlmacf1 0.16787, chxlmicf1 0.44617, chxlmacf1 0.39571, chxlacc 0.60526, chxlrocaucmic 0.70153, chxlrocaucmac 0.66303, 43.32 secs\n",
      "Adjusting learning rate of group 0 to 5.4288e-05.\n",
      "\u001b[1m---- Epoch 21/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000054) ...\n",
      "loss 1.42718, a_loss 1.44689, cD 0.74821, wmdcmp 0.10605, ema 0.50897, oracc 0.94196, orien_loss 0.15471, qlmicf1 0.24661, qlmacf1 0.16093, ql_loss 1.03176, chxlmicf1 0.40357, chxlmacf1 0.37930, chx_loss 1.03950, chxlacc 0.60677, chxlrocaucmic 0.68631, chxlrocaucmac 0.66349, gacc 0.78386, gloss 0.48540, cxr14micf1 0.18594, cxr14macf1 0.22900, cxr14_loss 1.17215, vnbgmicf1 0.33265, vnbgmacf1 0.27663, vnbg_loss 1.39166, b1 0.37082, b2 0.26599, b3 0.18653, b4 0.13388, padchxlmacf1 0.03519, padchxlmicf1 0.09278, padchxlzmacf1 0.05623, padchxlzmicf1 0.10508, padchxl_loss 0.64532, padchxlz_loss 0.78291, 146.65 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.66338, wmdcmp 0.09502, ema 0.59534, oracc 0.95126, qlmicf1 0.24859, qlmacf1 0.16766, chxlmicf1 0.44769, chxlmacf1 0.39723, chxlacc 0.61082, chxlrocaucmic 0.70146, chxlrocaucmac 0.66423, 42.55 secs\n",
      "Adjusting learning rate of group 0 to 4.7521e-05.\n",
      "\u001b[1m---- Epoch 22/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000048) ...\n",
      "loss 2.28687, a_loss 1.42889, cD 0.74493, wmdcmp 0.10596, ema 0.50743, oracc 0.94461, orien_loss 0.14543, qlmicf1 0.24307, qlmacf1 0.15975, ql_loss 1.03422, chxlmicf1 0.40644, chxlmacf1 0.38182, chx_loss 1.03676, chxlacc 0.61111, chxlrocaucmic 0.68638, chxlrocaucmac 0.66664, gacc 0.79410, gloss 0.47973, cxr14micf1 0.19038, cxr14macf1 0.23118, cxr14_loss 1.16557, vnbgmicf1 0.34428, vnbgmacf1 0.28825, vnbg_loss 1.36129, b1 0.37542, b2 0.26785, b3 0.18681, b4 0.13327, padchxlmacf1 0.03578, padchxlmicf1 0.09329, padchxlzmacf1 0.05551, padchxlzmicf1 0.10512, padchxl_loss 0.63445, padchxlz_loss 0.75834, 142.93 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.67776, wmdcmp 0.09642, ema 0.61862, oracc 0.95126, qlmicf1 0.25010, qlmacf1 0.16870, chxlmicf1 0.44588, chxlmacf1 0.39469, chxlacc 0.61352, chxlrocaucmic 0.70004, chxlrocaucmac 0.66290, 44.82 secs\n",
      "Adjusting learning rate of group 0 to 4.1597e-05.\n",
      "\u001b[1m---- Epoch 23/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000042) ...\n",
      "loss 5.78585, a_loss 1.43455, cD 0.75229, wmdcmp 0.10619, ema 0.51209, oracc 0.94190, orien_loss 0.15156, qlmicf1 0.24649, qlmacf1 0.16112, ql_loss 1.02646, chxlmicf1 0.40052, chxlmacf1 0.37836, chx_loss 1.04032, chxlacc 0.60708, chxlrocaucmic 0.68662, chxlrocaucmac 0.66426, gacc 0.78483, gloss 0.47826, cxr14micf1 0.19781, cxr14macf1 0.23586, cxr14_loss 1.15874, vnbgmicf1 0.34084, vnbgmacf1 0.28305, vnbg_loss 1.36272, b1 0.37840, b2 0.26948, b3 0.18663, b4 0.13224, padchxlmacf1 0.03442, padchxlmicf1 0.09189, padchxlzmacf1 0.05137, padchxlzmicf1 0.09619, padchxl_loss 0.63152, padchxlz_loss 0.78249, 141.64 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.71485, wmdcmp 0.10145, ema 0.61415, oracc 0.95055, qlmicf1 0.25202, qlmacf1 0.16872, chxlmicf1 0.44400, chxlmacf1 0.39487, chxlacc 0.61347, chxlrocaucmic 0.69859, chxlrocaucmac 0.66314, 48.03 secs\n",
      "Adjusting learning rate of group 0 to 3.6411e-05.\n",
      "\u001b[1m---- Epoch 24/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000036) ...\n",
      "loss 5.65208, a_loss 1.44951, cD 0.76902, wmdcmp 0.10820, ema 0.50686, oracc 0.94358, orien_loss 0.15045, qlmicf1 0.24516, qlmacf1 0.16072, ql_loss 1.03580, chxlmicf1 0.40254, chxlmacf1 0.37916, chx_loss 1.03762, chxlacc 0.60752, chxlrocaucmic 0.68599, chxlrocaucmac 0.66513, gacc 0.78928, gloss 0.47595, cxr14micf1 0.19195, cxr14macf1 0.23323, cxr14_loss 1.16699, vnbgmicf1 0.33735, vnbgmacf1 0.27466, vnbg_loss 1.36320, b1 0.37086, b2 0.26571, b3 0.18623, b4 0.13316, padchxlmacf1 0.03510, padchxlmicf1 0.09146, padchxlzmacf1 0.05418, padchxlzmicf1 0.10158, padchxl_loss 0.63831, padchxlz_loss 0.78117, 138.37 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.70409, wmdcmp 0.10044, ema 0.60519, oracc 0.95079, qlmicf1 0.24843, qlmacf1 0.16804, chxlmicf1 0.44678, chxlmacf1 0.39707, chxlacc 0.61076, chxlrocaucmic 0.70023, chxlrocaucmac 0.66545, 51.08 secs\n",
      "Adjusting learning rate of group 0 to 3.1872e-05.\n",
      "\u001b[1m---- Epoch 25/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 2.01653, a_loss 1.41228, cD 0.76544, wmdcmp 0.10771, ema 0.51352, oracc 0.94221, orien_loss 0.14862, qlmicf1 0.24209, qlmacf1 0.16113, ql_loss 1.03333, chxlmicf1 0.40996, chxlmacf1 0.38230, chx_loss 1.03543, chxlacc 0.60965, chxlrocaucmic 0.68985, chxlrocaucmac 0.66666, gacc 0.78400, gloss 0.47853, cxr14micf1 0.19580, cxr14macf1 0.23605, cxr14_loss 1.16532, vnbgmicf1 0.35205, vnbgmacf1 0.28633, vnbg_loss 1.36078, b1 0.39714, b2 0.28360, b3 0.19738, b4 0.13917, padchxlmacf1 0.03410, padchxlmicf1 0.08723, padchxlzmacf1 0.05430, padchxlzmicf1 0.09891, padchxl_loss 0.63958, padchxlz_loss 0.76229, 136.85 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.71647, wmdcmp 0.10197, ema 0.60967, oracc 0.95385, qlmicf1 0.24679, qlmacf1 0.16847, chxlmicf1 0.44377, chxlmacf1 0.39442, chxlacc 0.61789, chxlrocaucmic 0.69595, chxlrocaucmac 0.66578, 53.68 secs\n",
      "Adjusting learning rate of group 0 to 2.7899e-05.\n",
      "\u001b[1m---- Epoch 26/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 2.27741, a_loss 1.41975, cD 0.77739, wmdcmp 0.10986, ema 0.51271, oracc 0.94338, orien_loss 0.14800, qlmicf1 0.24362, qlmacf1 0.15957, ql_loss 1.03035, chxlmicf1 0.40586, chxlmacf1 0.38232, chx_loss 1.03628, chxlacc 0.61222, chxlrocaucmic 0.68938, chxlrocaucmac 0.66872, gacc 0.78918, gloss 0.47644, cxr14micf1 0.19074, cxr14macf1 0.23392, cxr14_loss 1.16266, vnbgmicf1 0.34499, vnbgmacf1 0.28498, vnbg_loss 1.36677, b1 0.38722, b2 0.27905, b3 0.19427, b4 0.13583, padchxlmacf1 0.03521, padchxlmicf1 0.09433, padchxlzmacf1 0.05436, padchxlzmicf1 0.10052, padchxl_loss 0.63069, padchxlz_loss 0.78122, 140.95 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.73128, wmdcmp 0.10444, ema 0.59893, oracc 0.95173, qlmicf1 0.25188, qlmacf1 0.16888, chxlmicf1 0.44638, chxlmacf1 0.39643, chxlacc 0.61592, chxlrocaucmic 0.70166, chxlrocaucmac 0.66773, 48.04 secs\n",
      "Adjusting learning rate of group 0 to 2.4421e-05.\n",
      "\u001b[1m---- Epoch 27/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000024) ...\n",
      "loss 1.92455, a_loss 1.40987, cD 0.77261, wmdcmp 0.10920, ema 0.51674, oracc 0.94408, orien_loss 0.14627, qlmicf1 0.24235, qlmacf1 0.16012, ql_loss 1.02969, chxlmicf1 0.40396, chxlmacf1 0.37945, chx_loss 1.03768, chxlacc 0.60843, chxlrocaucmic 0.68574, chxlrocaucmac 0.66406, gacc 0.79380, gloss 0.47012, cxr14micf1 0.19860, cxr14macf1 0.23830, cxr14_loss 1.16555, vnbgmicf1 0.33275, vnbgmacf1 0.27324, vnbg_loss 1.35787, b1 0.37968, b2 0.27332, b3 0.19263, b4 0.13775, padchxlmacf1 0.03576, padchxlmicf1 0.10046, padchxlzmacf1 0.05329, padchxlzmicf1 0.09643, padchxl_loss 0.62603, padchxlz_loss 0.75450, 142.26 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.72200, wmdcmp 0.10337, ema 0.62489, oracc 0.95503, qlmicf1 0.25059, qlmacf1 0.16949, chxlmicf1 0.44705, chxlmacf1 0.39821, chxlacc 0.61102, chxlrocaucmic 0.70228, chxlrocaucmac 0.66761, 52.32 secs\n",
      "Adjusting learning rate of group 0 to 2.1377e-05.\n",
      "\u001b[1m---- Epoch 28/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 2.21139, a_loss 1.39920, cD 0.79377, wmdcmp 0.11095, ema 0.51719, oracc 0.94614, orien_loss 0.14229, qlmicf1 0.24435, qlmacf1 0.16091, ql_loss 1.02849, chxlmicf1 0.41168, chxlmacf1 0.38257, chx_loss 1.03586, chxlacc 0.60924, chxlrocaucmic 0.69100, chxlrocaucmac 0.66713, gacc 0.78957, gloss 0.47417, cxr14micf1 0.20192, cxr14macf1 0.23905, cxr14_loss 1.16327, vnbgmicf1 0.33784, vnbgmacf1 0.27617, vnbg_loss 1.36526, b1 0.38334, b2 0.27461, b3 0.18833, b4 0.12797, padchxlmacf1 0.03525, padchxlmicf1 0.09537, padchxlzmacf1 0.05846, padchxlzmicf1 0.10834, padchxl_loss 0.63773, padchxlz_loss 0.77950, 139.45 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.72700, wmdcmp 0.10363, ema 0.62489, oracc 0.95173, qlmicf1 0.25181, qlmacf1 0.16847, chxlmicf1 0.44670, chxlmacf1 0.39806, chxlacc 0.61363, chxlrocaucmic 0.70147, chxlrocaucmac 0.66826, 53.84 secs\n",
      "Adjusting learning rate of group 0 to 1.8712e-05.\n",
      "\u001b[1m---- Epoch 29/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 5.55232, a_loss 1.43582, cD 0.78770, wmdcmp 0.11054, ema 0.51290, oracc 0.94338, orien_loss 0.14505, qlmicf1 0.24757, qlmacf1 0.16170, ql_loss 1.02924, chxlmicf1 0.40777, chxlmacf1 0.38201, chx_loss 1.03459, chxlacc 0.61162, chxlrocaucmic 0.69006, chxlrocaucmac 0.66888, gacc 0.78657, gloss 0.47719, cxr14micf1 0.19548, cxr14macf1 0.23524, cxr14_loss 1.16979, vnbgmicf1 0.35396, vnbgmacf1 0.28432, vnbg_loss 1.35081, b1 0.36385, b2 0.25872, b3 0.17888, b4 0.12420, padchxlmacf1 0.03637, padchxlmicf1 0.09718, padchxlzmacf1 0.05602, padchxlzmicf1 0.10633, padchxl_loss 0.63541, padchxlz_loss 0.77237, 142.01 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.75725, wmdcmp 0.10811, ema 0.61952, oracc 0.95126, qlmicf1 0.25097, qlmacf1 0.16998, chxlmicf1 0.44488, chxlmacf1 0.39578, chxlacc 0.61525, chxlrocaucmic 0.69843, chxlrocaucmac 0.66656, 48.17 secs\n",
      "Adjusting learning rate of group 0 to 1.6379e-05.\n",
      "\u001b[1m---- Epoch 30/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 4.08413, a_loss 1.40517, cD 0.79392, wmdcmp 0.11129, ema 0.51866, oracc 0.94557, orien_loss 0.14410, qlmicf1 0.24560, qlmacf1 0.16101, ql_loss 1.03050, chxlmicf1 0.40624, chxlmacf1 0.38078, chx_loss 1.03544, chxlacc 0.60986, chxlrocaucmic 0.68852, chxlrocaucmac 0.66568, gacc 0.79019, gloss 0.47092, cxr14micf1 0.19824, cxr14macf1 0.23609, cxr14_loss 1.16182, vnbgmicf1 0.36269, vnbgmacf1 0.29004, vnbg_loss 1.33937, b1 0.38846, b2 0.27956, b3 0.19697, b4 0.13976, padchxlmacf1 0.03589, padchxlmicf1 0.09638, padchxlzmacf1 0.05616, padchxlzmicf1 0.10256, padchxl_loss 0.64365, padchxlz_loss 0.78678, 143.37 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.75221, wmdcmp 0.10830, ema 0.61773, oracc 0.95338, qlmicf1 0.25257, qlmacf1 0.16918, chxlmicf1 0.44630, chxlmacf1 0.39678, chxlacc 0.61329, chxlrocaucmic 0.70111, chxlrocaucmac 0.66693, 47.09 secs\n",
      "Adjusting learning rate of group 0 to 1.4337e-05.\n",
      "\u001b[1m---- Epoch 31/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 4.14111, a_loss 1.39589, cD 0.78093, wmdcmp 0.11036, ema 0.51671, oracc 0.94214, orien_loss 0.14786, qlmicf1 0.24595, qlmacf1 0.16124, ql_loss 1.03038, chxlmicf1 0.40715, chxlmacf1 0.38204, chx_loss 1.03582, chxlacc 0.61387, chxlrocaucmic 0.69151, chxlrocaucmac 0.67075, gacc 0.79286, gloss 0.46694, cxr14micf1 0.19864, cxr14macf1 0.23701, cxr14_loss 1.15838, vnbgmicf1 0.34179, vnbgmacf1 0.28054, vnbg_loss 1.34094, b1 0.38809, b2 0.28438, b3 0.20532, b4 0.14918, padchxlmacf1 0.03443, padchxlmicf1 0.09044, padchxlzmacf1 0.05334, padchxlzmicf1 0.09903, padchxl_loss 0.62797, padchxlz_loss 0.77818, 140.14 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76785, wmdcmp 0.10990, ema 0.61773, oracc 0.95079, qlmicf1 0.25379, qlmacf1 0.17025, chxlmicf1 0.44843, chxlmacf1 0.39859, chxlacc 0.61479, chxlrocaucmic 0.70296, chxlrocaucmac 0.66751, 48.26 secs\n",
      "Adjusting learning rate of group 0 to 1.2550e-05.\n",
      "\u001b[1m---- Epoch 32/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 5.44242, a_loss 1.40115, cD 0.79689, wmdcmp 0.11186, ema 0.51856, oracc 0.94521, orien_loss 0.14298, qlmicf1 0.24761, qlmacf1 0.16126, ql_loss 1.03151, chxlmicf1 0.40557, chxlmacf1 0.38106, chx_loss 1.03632, chxlacc 0.60879, chxlrocaucmic 0.68796, chxlrocaucmac 0.66723, gacc 0.79816, gloss 0.46484, cxr14micf1 0.19650, cxr14macf1 0.23436, cxr14_loss 1.17066, vnbgmicf1 0.33610, vnbgmacf1 0.27601, vnbg_loss 1.35135, b1 0.40733, b2 0.29684, b3 0.21166, b4 0.15269, padchxlmacf1 0.03571, padchxlmicf1 0.09488, padchxlzmacf1 0.05790, padchxlzmicf1 0.10782, padchxl_loss 0.63504, padchxlz_loss 0.76774, 143.28 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76096, wmdcmp 0.10929, ema 0.61415, oracc 0.95479, qlmicf1 0.25386, qlmacf1 0.17062, chxlmicf1 0.44363, chxlmacf1 0.39472, chxlacc 0.61239, chxlrocaucmic 0.70037, chxlrocaucmac 0.66721, 46.21 secs\n",
      "Adjusting learning rate of group 0 to 1.0986e-05.\n",
      "\u001b[1m---- Epoch 33/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 2.00094, a_loss 1.40527, cD 0.78617, wmdcmp 0.11075, ema 0.51894, oracc 0.94608, orien_loss 0.14467, qlmicf1 0.24442, qlmacf1 0.16173, ql_loss 1.03371, chxlmicf1 0.41215, chxlmacf1 0.38407, chx_loss 1.03230, chxlacc 0.61079, chxlrocaucmic 0.69055, chxlrocaucmac 0.66779, gacc 0.79543, gloss 0.46933, cxr14micf1 0.18884, cxr14macf1 0.22961, cxr14_loss 1.16623, vnbgmicf1 0.34771, vnbgmacf1 0.28339, vnbg_loss 1.32353, b1 0.38376, b2 0.27757, b3 0.19682, b4 0.14164, padchxlmacf1 0.03743, padchxlmicf1 0.09575, padchxlzmacf1 0.05388, padchxlzmicf1 0.09949, padchxl_loss 0.63193, padchxlz_loss 0.75676, 143.85 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.75043, wmdcmp 0.10782, ema 0.62399, oracc 0.95432, qlmicf1 0.25211, qlmacf1 0.16986, chxlmicf1 0.44989, chxlmacf1 0.40027, chxlacc 0.61745, chxlrocaucmic 0.70284, chxlrocaucmac 0.66811, 43.83 secs\n",
      "Adjusting learning rate of group 0 to 9.6161e-06.\n",
      "\u001b[1m---- Epoch 34/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 2.37472, a_loss 1.38982, cD 0.78347, wmdcmp 0.11062, ema 0.52067, oracc 0.94537, orien_loss 0.14120, qlmicf1 0.24905, qlmacf1 0.16259, ql_loss 1.02741, chxlmicf1 0.40895, chxlmacf1 0.38272, chx_loss 1.03496, chxlacc 0.61257, chxlrocaucmic 0.69127, chxlrocaucmac 0.66941, gacc 0.80010, gloss 0.46441, cxr14micf1 0.18731, cxr14macf1 0.23000, cxr14_loss 1.16772, vnbgmicf1 0.35108, vnbgmacf1 0.28525, vnbg_loss 1.33957, b1 0.39554, b2 0.28257, b3 0.19322, b4 0.13273, padchxlmacf1 0.03606, padchxlmicf1 0.09692, padchxlzmacf1 0.05538, padchxlzmicf1 0.10518, padchxl_loss 0.63589, padchxlz_loss 0.75995, 142.33 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76486, wmdcmp 0.10905, ema 0.62847, oracc 0.95785, qlmicf1 0.25636, qlmacf1 0.17124, chxlmicf1 0.44666, chxlmacf1 0.39743, chxlacc 0.61732, chxlrocaucmic 0.70010, chxlrocaucmac 0.66704, 40.89 secs\n",
      "Adjusting learning rate of group 0 to 8.4174e-06.\n",
      "\u001b[1m---- Epoch 35/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 5.59016, a_loss 1.40411, cD 0.78983, wmdcmp 0.11049, ema 0.51746, oracc 0.94471, orien_loss 0.14317, qlmicf1 0.24871, qlmacf1 0.16243, ql_loss 1.03134, chxlmicf1 0.40711, chxlmacf1 0.38259, chx_loss 1.03376, chxlacc 0.61106, chxlrocaucmic 0.68754, chxlrocaucmac 0.66719, gacc 0.79324, gloss 0.46893, cxr14micf1 0.19659, cxr14macf1 0.23458, cxr14_loss 1.16185, vnbgmicf1 0.34153, vnbgmacf1 0.28112, vnbg_loss 1.33539, b1 0.38802, b2 0.28135, b3 0.20039, b4 0.14469, padchxlmacf1 0.03569, padchxlmicf1 0.09301, padchxlzmacf1 0.05644, padchxlzmicf1 0.10233, padchxl_loss 0.64823, padchxlz_loss 0.78811, 114.99 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76625, wmdcmp 0.11021, ema 0.62220, oracc 0.95456, qlmicf1 0.25192, qlmacf1 0.16942, chxlmicf1 0.44990, chxlmacf1 0.40058, chxlacc 0.61688, chxlrocaucmic 0.70254, chxlrocaucmac 0.66790, 35.86 secs\n",
      "Adjusting learning rate of group 0 to 7.3681e-06.\n",
      "\u001b[1m---- Epoch 36/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.22364, a_loss 1.39565, cD 0.80523, wmdcmp 0.11297, ema 0.52173, oracc 0.94789, orien_loss 0.13445, qlmicf1 0.24647, qlmacf1 0.16224, ql_loss 1.02896, chxlmicf1 0.41200, chxlmacf1 0.38540, chx_loss 1.03351, chxlacc 0.61329, chxlrocaucmic 0.69261, chxlrocaucmac 0.67159, gacc 0.79478, gloss 0.46673, cxr14micf1 0.18530, cxr14macf1 0.23036, cxr14_loss 1.16866, vnbgmicf1 0.35355, vnbgmacf1 0.28788, vnbg_loss 1.33113, b1 0.38168, b2 0.27930, b3 0.19974, b4 0.14526, padchxlmacf1 0.03669, padchxlmicf1 0.09633, padchxlzmacf1 0.05688, padchxlzmicf1 0.10797, padchxl_loss 0.63273, padchxlz_loss 0.76430, 110.10 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76805, wmdcmp 0.10986, ema 0.63026, oracc 0.95220, qlmicf1 0.25167, qlmacf1 0.17021, chxlmicf1 0.44954, chxlmacf1 0.39933, chxlacc 0.61559, chxlrocaucmic 0.70378, chxlrocaucmac 0.66553, 35.42 secs\n",
      "Adjusting learning rate of group 0 to 6.4496e-06.\n",
      "\u001b[1m---- Epoch 37/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 2.32930, a_loss 1.40022, cD 0.80193, wmdcmp 0.11330, ema 0.52528, oracc 0.94645, orien_loss 0.14059, qlmicf1 0.24590, qlmacf1 0.16156, ql_loss 1.02651, chxlmicf1 0.40591, chxlmacf1 0.38202, chx_loss 1.03352, chxlacc 0.61044, chxlrocaucmic 0.68909, chxlrocaucmac 0.66988, gacc 0.79848, gloss 0.46212, cxr14micf1 0.20492, cxr14macf1 0.24184, cxr14_loss 1.16106, vnbgmicf1 0.35261, vnbgmacf1 0.28342, vnbg_loss 1.32071, b1 0.39738, b2 0.28907, b3 0.20652, b4 0.15064, padchxlmacf1 0.03550, padchxlmicf1 0.09014, padchxlzmacf1 0.05356, padchxlzmicf1 0.09672, padchxl_loss 0.63984, padchxlz_loss 0.77419, 107.20 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.77213, wmdcmp 0.11064, ema 0.61235, oracc 0.95479, qlmicf1 0.24983, qlmacf1 0.16933, chxlmicf1 0.44770, chxlmacf1 0.39742, chxlacc 0.61463, chxlrocaucmic 0.70346, chxlrocaucmac 0.66708, 35.12 secs\n",
      "Adjusting learning rate of group 0 to 5.6455e-06.\n",
      "\u001b[1m---- Epoch 38/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 1.82166, a_loss 1.38338, cD 0.81581, wmdcmp 0.11432, ema 0.52371, oracc 0.94440, orien_loss 0.14396, qlmicf1 0.24617, qlmacf1 0.16192, ql_loss 1.02545, chxlmicf1 0.41379, chxlmacf1 0.38415, chx_loss 1.03338, chxlacc 0.61270, chxlrocaucmic 0.69169, chxlrocaucmac 0.66764, gacc 0.80039, gloss 0.45856, cxr14micf1 0.19156, cxr14macf1 0.23312, cxr14_loss 1.16695, vnbgmicf1 0.35902, vnbgmacf1 0.28878, vnbg_loss 1.33816, b1 0.39035, b2 0.28389, b3 0.20048, b4 0.14316, padchxlmacf1 0.03761, padchxlmicf1 0.09681, padchxlzmacf1 0.05915, padchxlzmicf1 0.11101, padchxl_loss 0.64459, padchxlz_loss 0.78921, 107.92 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77069, wmdcmp 0.11025, ema 0.62399, oracc 0.95197, qlmicf1 0.25159, qlmacf1 0.17029, chxlmicf1 0.44117, chxlmacf1 0.39422, chxlacc 0.61293, chxlrocaucmic 0.69554, chxlrocaucmac 0.66441, 34.82 secs\n",
      "Adjusting learning rate of group 0 to 4.9418e-06.\n",
      "\u001b[1m---- Epoch 39/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.81817, a_loss 1.38443, cD 0.79965, wmdcmp 0.11255, ema 0.51871, oracc 0.94670, orien_loss 0.14134, qlmicf1 0.24652, qlmacf1 0.16223, ql_loss 1.02848, chxlmicf1 0.40764, chxlmacf1 0.38283, chx_loss 1.03451, chxlacc 0.61119, chxlrocaucmic 0.69026, chxlrocaucmac 0.66951, gacc 0.79038, gloss 0.46938, cxr14micf1 0.20496, cxr14macf1 0.24201, cxr14_loss 1.16050, vnbgmicf1 0.34178, vnbgmacf1 0.27749, vnbg_loss 1.34661, b1 0.38338, b2 0.27782, b3 0.19746, b4 0.14322, padchxlmacf1 0.03651, padchxlmicf1 0.09490, padchxlzmacf1 0.05680, padchxlzmicf1 0.10710, padchxl_loss 0.61830, padchxlz_loss 0.75005, 106.01 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77211, wmdcmp 0.11055, ema 0.63563, oracc 0.95809, qlmicf1 0.25260, qlmacf1 0.16971, chxlmicf1 0.44960, chxlmacf1 0.39996, chxlacc 0.61602, chxlrocaucmic 0.70345, chxlrocaucmac 0.66807, 34.35 secs\n",
      "Adjusting learning rate of group 0 to 4.3257e-06.\n",
      "\u001b[1m---- Epoch 40/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 5.53675, a_loss 1.39687, cD 0.79590, wmdcmp 0.11214, ema 0.51626, oracc 0.94435, orien_loss 0.14608, qlmicf1 0.24483, qlmacf1 0.16184, ql_loss 1.02643, chxlmicf1 0.40928, chxlmacf1 0.38336, chx_loss 1.03277, chxlacc 0.61043, chxlrocaucmic 0.68960, chxlrocaucmac 0.66816, gacc 0.79886, gloss 0.46093, cxr14micf1 0.18446, cxr14macf1 0.22838, cxr14_loss 1.17898, vnbgmicf1 0.34067, vnbgmacf1 0.27561, vnbg_loss 1.32795, b1 0.39935, b2 0.28834, b3 0.20384, b4 0.14615, padchxlmacf1 0.03745, padchxlmicf1 0.09566, padchxlzmacf1 0.05820, padchxlzmicf1 0.10526, padchxl_loss 0.64124, padchxlz_loss 0.78144, 105.93 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77015, wmdcmp 0.11009, ema 0.62847, oracc 0.95927, qlmicf1 0.25285, qlmacf1 0.17037, chxlmicf1 0.45060, chxlmacf1 0.40061, chxlacc 0.61727, chxlrocaucmic 0.70286, chxlrocaucmac 0.66806, 34.32 secs\n",
      "Adjusting learning rate of group 0 to 3.7865e-06.\n",
      "\u001b[1m---- Epoch 41/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 5.64814, a_loss 1.39294, cD 0.80288, wmdcmp 0.11310, ema 0.51866, oracc 0.94680, orien_loss 0.14103, qlmicf1 0.24611, qlmacf1 0.16202, ql_loss 1.03278, chxlmicf1 0.40913, chxlmacf1 0.38236, chx_loss 1.03392, chxlacc 0.60979, chxlrocaucmic 0.68926, chxlrocaucmac 0.66699, gacc 0.79478, gloss 0.46691, cxr14micf1 0.20304, cxr14macf1 0.23998, cxr14_loss 1.15732, vnbgmicf1 0.34117, vnbgmacf1 0.28249, vnbg_loss 1.34272, b1 0.39358, b2 0.28263, b3 0.19854, b4 0.14350, padchxlmacf1 0.03697, padchxlmicf1 0.09712, padchxlzmacf1 0.05523, padchxlzmicf1 0.10417, padchxl_loss 0.63675, padchxlz_loss 0.75861, 106.23 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.78754, wmdcmp 0.11263, ema 0.62220, oracc 0.95150, qlmicf1 0.25259, qlmacf1 0.17135, chxlmicf1 0.44856, chxlmacf1 0.39908, chxlacc 0.61375, chxlrocaucmic 0.70322, chxlrocaucmac 0.66873, 33.88 secs\n",
      "Adjusting learning rate of group 0 to 3.3145e-06.\n",
      "\u001b[1m---- Epoch 42/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 3.65181, a_loss 1.40140, cD 0.80816, wmdcmp 0.11324, ema 0.52288, oracc 0.94662, orien_loss 0.13726, qlmicf1 0.24899, qlmacf1 0.16240, ql_loss 1.02568, chxlmicf1 0.41039, chxlmacf1 0.38352, chx_loss 1.03525, chxlacc 0.61301, chxlrocaucmic 0.69257, chxlrocaucmac 0.67049, gacc 0.79905, gloss 0.46135, cxr14micf1 0.19896, cxr14macf1 0.23753, cxr14_loss 1.15873, vnbgmicf1 0.34805, vnbgmacf1 0.28325, vnbg_loss 1.33702, b1 0.39338, b2 0.28380, b3 0.20156, b4 0.14590, padchxlmacf1 0.03664, padchxlmicf1 0.09704, padchxlzmacf1 0.05783, padchxlzmicf1 0.10469, padchxl_loss 0.63036, padchxlz_loss 0.76308, 107.71 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77197, wmdcmp 0.11028, ema 0.63384, oracc 0.95409, qlmicf1 0.25355, qlmacf1 0.17086, chxlmicf1 0.44624, chxlmacf1 0.39799, chxlacc 0.61226, chxlrocaucmic 0.70104, chxlrocaucmac 0.66717, 33.44 secs\n",
      "Adjusting learning rate of group 0 to 2.9013e-06.\n",
      "\u001b[1m---- Epoch 43/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 2.23574, a_loss 1.39706, cD 0.81519, wmdcmp 0.11529, ema 0.51727, oracc 0.94775, orien_loss 0.13723, qlmicf1 0.24695, qlmacf1 0.16213, ql_loss 1.03054, chxlmicf1 0.41005, chxlmacf1 0.38290, chx_loss 1.03449, chxlacc 0.60997, chxlrocaucmic 0.68940, chxlrocaucmac 0.66775, gacc 0.78762, gloss 0.46752, cxr14micf1 0.18953, cxr14macf1 0.23123, cxr14_loss 1.17161, vnbgmicf1 0.35345, vnbgmacf1 0.28473, vnbg_loss 1.34008, b1 0.40342, b2 0.29557, b3 0.21204, b4 0.15399, padchxlmacf1 0.03469, padchxlmicf1 0.09180, padchxlzmacf1 0.05542, padchxlzmicf1 0.09774, padchxl_loss 0.63321, padchxlz_loss 0.79401, 95.49 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77307, wmdcmp 0.11061, ema 0.63384, oracc 0.95150, qlmicf1 0.25399, qlmacf1 0.17157, chxlmicf1 0.44913, chxlmacf1 0.39829, chxlacc 0.61744, chxlrocaucmic 0.70239, chxlrocaucmac 0.66727, 31.49 secs\n",
      "Adjusting learning rate of group 0 to 2.5396e-06.\n",
      "\u001b[1m---- Epoch 44/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.40129, a_loss 1.39806, cD 0.80257, wmdcmp 0.11315, ema 0.52158, oracc 0.94828, orien_loss 0.13499, qlmicf1 0.24825, qlmacf1 0.16239, ql_loss 1.02460, chxlmicf1 0.40543, chxlmacf1 0.38127, chx_loss 1.03748, chxlacc 0.61196, chxlrocaucmic 0.68823, chxlrocaucmac 0.66606, gacc 0.79391, gloss 0.46064, cxr14micf1 0.19993, cxr14macf1 0.23750, cxr14_loss 1.15544, vnbgmicf1 0.34076, vnbgmacf1 0.27792, vnbg_loss 1.33540, b1 0.39492, b2 0.28738, b3 0.20473, b4 0.14861, padchxlmacf1 0.03731, padchxlmicf1 0.09685, padchxlzmacf1 0.05677, padchxlzmicf1 0.10826, padchxl_loss 0.63567, padchxlz_loss 0.77601, 89.97 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76515, wmdcmp 0.10986, ema 0.63832, oracc 0.95220, qlmicf1 0.25345, qlmacf1 0.17041, chxlmicf1 0.44771, chxlmacf1 0.39796, chxlacc 0.61517, chxlrocaucmic 0.70165, chxlrocaucmac 0.66753, 30.48 secs\n",
      "Adjusting learning rate of group 0 to 2.2230e-06.\n",
      "\u001b[1m---- Epoch 45/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.20943, a_loss 1.37789, cD 0.80258, wmdcmp 0.11261, ema 0.52610, oracc 0.94597, orien_loss 0.13965, qlmicf1 0.24731, qlmacf1 0.16214, ql_loss 1.02813, chxlmicf1 0.41322, chxlmacf1 0.38488, chx_loss 1.03314, chxlacc 0.61237, chxlrocaucmic 0.69310, chxlrocaucmac 0.67030, gacc 0.78933, gloss 0.46712, cxr14micf1 0.18512, cxr14macf1 0.22897, cxr14_loss 1.17625, vnbgmicf1 0.33860, vnbgmacf1 0.27620, vnbg_loss 1.34068, b1 0.38307, b2 0.27647, b3 0.19434, b4 0.13781, padchxlmacf1 0.03621, padchxlmicf1 0.09365, padchxlzmacf1 0.05439, padchxlzmicf1 0.10040, padchxl_loss 0.64918, padchxlz_loss 0.78446, 88.14 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76343, wmdcmp 0.11010, ema 0.61594, oracc 0.95620, qlmicf1 0.25397, qlmacf1 0.17097, chxlmicf1 0.44894, chxlmacf1 0.39925, chxlacc 0.61582, chxlrocaucmic 0.70127, chxlrocaucmac 0.66677, 30.24 secs\n",
      "Adjusting learning rate of group 0 to 1.9459e-06.\n",
      "\u001b[1m---- Epoch 46/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 5.69912, a_loss 1.39179, cD 0.80223, wmdcmp 0.11300, ema 0.52091, oracc 0.94568, orien_loss 0.14198, qlmicf1 0.24639, qlmacf1 0.16243, ql_loss 1.02823, chxlmicf1 0.40744, chxlmacf1 0.38300, chx_loss 1.03391, chxlacc 0.61266, chxlrocaucmic 0.69003, chxlrocaucmac 0.66979, gacc 0.79865, gloss 0.46354, cxr14micf1 0.19826, cxr14macf1 0.23580, cxr14_loss 1.16454, vnbgmicf1 0.35668, vnbgmacf1 0.28224, vnbg_loss 1.33718, b1 0.40193, b2 0.29242, b3 0.20667, b4 0.14731, padchxlmacf1 0.03749, padchxlmicf1 0.09475, padchxlzmacf1 0.05455, padchxlzmicf1 0.10016, padchxl_loss 0.65470, padchxlz_loss 0.78552, 87.61 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cD 0.76713, wmdcmp 0.10952, ema 0.63205, oracc 0.95338, qlmicf1 0.25177, qlmacf1 0.16921, chxlmicf1 0.44821, chxlmacf1 0.39977, chxlacc 0.61360, chxlrocaucmic 0.70224, chxlrocaucmac 0.66891, 30.09 secs\n",
      "Adjusting learning rate of group 0 to 1.7033e-06.\n",
      "\u001b[1m---- Epoch 47/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 5.72499, a_loss 1.41324, cD 0.81658, wmdcmp 0.11421, ema 0.51865, oracc 0.94659, orien_loss 0.14134, qlmicf1 0.24601, qlmacf1 0.16206, ql_loss 1.02985, chxlmicf1 0.40769, chxlmacf1 0.38198, chx_loss 1.03480, chxlacc 0.61105, chxlrocaucmic 0.69046, chxlrocaucmac 0.66924, gacc 0.79411, gloss 0.46532, cxr14micf1 0.19743, cxr14macf1 0.23552, cxr14_loss 1.16132, vnbgmicf1 0.34349, vnbgmacf1 0.28077, vnbg_loss 1.34169, b1 0.39911, b2 0.28782, b3 0.20300, b4 0.14565, padchxlmacf1 0.03682, padchxlmicf1 0.09537, padchxlzmacf1 0.05877, padchxlzmicf1 0.11006, padchxl_loss 0.63957, padchxlz_loss 0.77833, 87.21 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77161, wmdcmp 0.11065, ema 0.62399, oracc 0.94914, qlmicf1 0.25337, qlmacf1 0.17056, chxlmicf1 0.44843, chxlmacf1 0.39975, chxlacc 0.61515, chxlrocaucmic 0.70323, chxlrocaucmac 0.66889, 30.06 secs\n",
      "Adjusting learning rate of group 0 to 1.4910e-06.\n",
      "\u001b[1m---- Epoch 48/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.09858, a_loss 1.37626, cD 0.81237, wmdcmp 0.11453, ema 0.52143, oracc 0.94486, orien_loss 0.14227, qlmicf1 0.24687, qlmacf1 0.16135, ql_loss 1.03046, chxlmicf1 0.40758, chxlmacf1 0.38243, chx_loss 1.03441, chxlacc 0.61060, chxlrocaucmic 0.69011, chxlrocaucmac 0.66904, gacc 0.79438, gloss 0.46640, cxr14micf1 0.18958, cxr14macf1 0.22781, cxr14_loss 1.17071, vnbgmicf1 0.34073, vnbgmacf1 0.27828, vnbg_loss 1.33773, b1 0.40236, b2 0.29154, b3 0.20806, b4 0.15112, padchxlmacf1 0.03676, padchxlmicf1 0.09308, padchxlzmacf1 0.05418, padchxlzmicf1 0.10049, padchxl_loss 0.62426, padchxlz_loss 0.76776, 85.68 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.76990, wmdcmp 0.10982, ema 0.62399, oracc 0.95809, qlmicf1 0.25340, qlmacf1 0.16984, chxlmicf1 0.44772, chxlmacf1 0.39755, chxlacc 0.61582, chxlrocaucmic 0.70289, chxlrocaucmac 0.66651, 30.19 secs\n",
      "Adjusting learning rate of group 0 to 1.3051e-06.\n",
      "\u001b[1m---- Epoch 49/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.24250, a_loss 1.38693, cD 0.79816, wmdcmp 0.11180, ema 0.52638, oracc 0.94755, orien_loss 0.13737, qlmicf1 0.24661, qlmacf1 0.16211, ql_loss 1.02666, chxlmicf1 0.40945, chxlmacf1 0.38349, chx_loss 1.03250, chxlacc 0.61211, chxlrocaucmic 0.69037, chxlrocaucmac 0.66964, gacc 0.80135, gloss 0.45466, cxr14micf1 0.19235, cxr14macf1 0.23202, cxr14_loss 1.16305, vnbgmicf1 0.34381, vnbgmacf1 0.27889, vnbg_loss 1.31873, b1 0.40300, b2 0.29183, b3 0.20769, b4 0.14759, padchxlmacf1 0.03634, padchxlmicf1 0.09598, padchxlzmacf1 0.05643, padchxlzmicf1 0.10478, padchxl_loss 0.64240, padchxlz_loss 0.78716, 86.72 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77187, wmdcmp 0.11084, ema 0.63563, oracc 0.95456, qlmicf1 0.25238, qlmacf1 0.17001, chxlmicf1 0.44960, chxlmacf1 0.39936, chxlacc 0.61696, chxlrocaucmic 0.70175, chxlrocaucmac 0.66790, 29.92 secs\n",
      "Adjusting learning rate of group 0 to 1.1424e-06.\n",
      "\u001b[1m---- Epoch 50/50\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.82741, a_loss 1.38899, cD 0.79047, wmdcmp 0.11104, ema 0.52446, oracc 0.94653, orien_loss 0.13744, qlmicf1 0.24438, qlmacf1 0.16030, ql_loss 1.02804, chxlmicf1 0.41460, chxlmacf1 0.38512, chx_loss 1.03349, chxlacc 0.61020, chxlrocaucmic 0.69227, chxlrocaucmac 0.66921, gacc 0.79446, gloss 0.46481, cxr14micf1 0.20234, cxr14macf1 0.23955, cxr14_loss 1.15948, vnbgmicf1 0.34706, vnbgmacf1 0.28200, vnbg_loss 1.32520, b1 0.39192, b2 0.28345, b3 0.19993, b4 0.14436, padchxlmacf1 0.03631, padchxlmicf1 0.09188, padchxlzmacf1 0.05605, padchxlzmicf1 0.10466, padchxl_loss 0.64326, padchxlz_loss 0.77209, 86.12 secs\n",
      "(2) Validation stage ...\n",
      "cD 0.77159, wmdcmp 0.11057, ema 0.62310, oracc 0.95715, qlmicf1 0.25491, qlmacf1 0.17016, chxlmicf1 0.44871, chxlmacf1 0.39964, chxlacc 0.61667, chxlrocaucmic 0.70271, chxlrocaucmac 0.66956, 29.81 secs\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "!python ../train_vqa.py \\\n",
    "        --epochs 50 \\\n",
    "        --batches-per-epoch 200 \\\n",
    "        --batch-size 150 \\\n",
    "        --num-workers 4 \\\n",
    "        --iters-to-accumulate 5 \\\n",
    "        --optimizer-name \"adamw\" \\\n",
    "        --scheduler \"warmup+decay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup-and-decay-args \"1e-6,5,4e-4,45,1e-6\" \\\n",
    "        --use-mimiccxr \\\n",
    "        --use-iuxray \\\n",
    "        --use-chexpert \\\n",
    "        --use-cxr14 \\\n",
    "        --use-vinbig \\\n",
    "        --use-padchest \\\n",
    "        --mimiccxr-include-chexpert-mode \\\n",
    "        --iuxray-include-chexpert-mode \\\n",
    "        --chexpert-mode \"vqa\" \\\n",
    "        --vinbig-training-data 'all' \\\n",
    "        --mimiccxr-weight 1.0 \\\n",
    "        --mimiccxr-weight-chexpert-mode 0.8 \\\n",
    "        --iuxray-weight 0.2 \\\n",
    "        --iuxray-weight-chexpert-mode 0.16 \\\n",
    "        --chexpert-weight 0.5 \\\n",
    "        --cxr14-weight 0.4 \\\n",
    "        --vinbig-weight 0.4 \\\n",
    "        --padchest-weight 0.5 \\\n",
    "        --padchest-train-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/train_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-val-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/val_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-test-study-ids-path \"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/padchest/test_study_ids_20221226_161248.txt\" \\\n",
    "        --padchest-training-data-mode \"all\" \\\n",
    "        --mimiccxr-qa-adapted-reports-filename \"qa_adapted_reports__20220904_095810.json\" \\\n",
    "        --iuxray-qa-adapted-reports-filename \"qa_adapted_reports__20220904_091601.json\" \\\n",
    "        --classify-orientation \\\n",
    "        --classify-chexpert \\\n",
    "        --mimiccxr-chexpert-labels-filename \"chexpert_labels_per_report__20220904_113605.pkl\" \\\n",
    "        --iuxray-chexpert-labels-filename \"chexpert_labels_per_report__20220904_113427.pkl\" \\\n",
    "        --classify-questions \\\n",
    "        --n-questions 97 \\\n",
    "        --mimiccxr-question-labels-filename \"question_labels_per_report__20220904_105753.pkl\" \\\n",
    "        --iuxray-question-labels-filename \"question_labels_per_report__20220904_105752.pkl\" \\\n",
    "        --mimiccxr-balanced-metadata-filename \"balanced_dataloading_metadata__20220918_210415.pkl\" \\\n",
    "        --iuxray-balanced-metadata-filename \"balanced_dataloading_metadata__20220918_210029.pkl\" \\\n",
    "        --balanced-dataloading \\\n",
    "        --medical-tokenization \\\n",
    "        --medical-terms-frequency-filename \"medical_terms_frequency__20220918_184255.pkl\" \\\n",
    "        --raw-image-encoding \"vitmodel-huggingface\" \\\n",
    "        --huggingface-model-name \"facebook/vit-mae-base\" \\\n",
    "        --image-size 224 224 \\\n",
    "        --image-local-feat-size 768 \\\n",
    "        --freeze-image-encoder \\\n",
    "        --question-encoding \"one-hot\" \\\n",
    "        --answer-decoding \"transformer\" \\\n",
    "        --binary-loss-name \"wbce-c\" \\\n",
    "        --img-aug-mode \"random-color-and-spatial\" \\\n",
    "        --use-amp \\\n",
    "        --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
