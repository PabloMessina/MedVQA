{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gpt-3.5-turbo-0301_comparisons__part1.jsonl\r\n",
      "'gpt-3.5-turbo-0301_comparisons__part1(obsolete).jsonl'\r\n",
      " gpt-3.5-turbo-0301_comparisons__part2.jsonl\r\n",
      " gpt-3.5-turbo-0301_parsed_sentences.jsonl\r\n",
      " gpt-3.5-turbo-0613_comparisons__part1.jsonl\r\n",
      " gpt-3.5-turbo-0613_comparisons__part2.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\r\n",
      " gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\r\n",
      " gpt-3.5-turbo-0613_parsed_facts__hard.jsonl\r\n",
      " gpt-3.5-turbo-0613_parsed_facts__uniform.jsonl\r\n",
      " gpt-3.5-turbo-0613_parsed_facts__v2.jsonl\r\n",
      "'gpt-3.5-turbo-0613_parsed_facts__v2_offset=20000.jsonl'\r\n",
      "'gpt-3.5-turbo-0613_parsed_facts__v2_offset=40000_uniform.jsonl'\r\n",
      " gpt-3.5-turbo-0613_parsed_sentences__v2.jsonl\r\n",
      "'gpt-3.5-turbo-0613_parsed_sentences__v2(uniform).jsonl'\r\n",
      " gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\r\n",
      " gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\r\n",
      " gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\r\n",
      " gpt-3.5-turbo_parsed_backgrounds.jsonl\r\n",
      " gpt-3.5-turbo_parsed_reports__backup.jsonl\r\n",
      " gpt-3.5-turbo_parsed_reports.jsonl\r\n",
      "'gpt-3.5-turbo_parsed_reports(old).jsonl'\r\n",
      " gpt-3.5-turbo_parsed_sentences.jsonl\r\n",
      " gpt-4-0613_parsed_reports.jsonl\r\n",
      " gpt-4-0613_parsed_sentences.jsonl\r\n",
      " gpt-4-0613_parsed_sentences__v2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 60\n",
      "   batches_per_epoch: 300\n",
      "   batch_size: 400\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230707_215744_fact2metadata(gpt-3.5-..ts__v2;gpt-3.5-..=20000;gpt-3.5-..niform)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 1\n",
      "   override_lr: False\n",
      "   task_name: fact2comparison\n",
      "   val_size: 200\n",
      "   input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 1\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 12607 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Counter:\n",
      "Counter({'no comparison': 50601, 'stable/unchanged': 9334, 'resolved': 5413, 'worsened': 5192, 'improved': 3077, 'progressed': 2658, 'new finding': 1669, 'smaller': 1321, 'increase': 1103, 'decrease': 1066, 'larger': 977, 'unclear comparison': 572, 'position changed': 527, 'reappeared': 368, 'other': 4})\n",
      "\u001b[1mNumber of train examples: 83682\u001b[0m\n",
      "\u001b[1mNumber of val examples: 200\u001b[0m\n",
      "\u001b[1mNumber of total examples: 83882\u001b[0m\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno change in appearance of the right heart border\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "seq2seq_trainer.name =  fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_100601_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_100601_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_51_s2s_loss=0.9648.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230707_215744_fact2metadata(gpt-3.5-..ts__v2;gpt-3.5-..=20000;gpt-3.5-..niform)_Seq2Seq(t5-small)/checkpoint_51_s2s_loss=0.9648.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_100601_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 16.16012, s2s_loss 15.16512, 44.80 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mningd cision\u001b[0m\n",
      "s2s_loss 15.36810, 0.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.0612.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 13.59358, s2s_loss 7.50349, 43.77 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprevious locationd previous multiple\u001b[0m\n",
      "s2s_loss 2.73199, 0.14 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.2529.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 2.82154, s2s_loss 0.71180, 43.46 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.26895, 0.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.7677.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.35581, s2s_loss 0.19411, 43.63 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.14998, 0.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_s2s_loss=0.8664.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 0.18664, s2s_loss 0.14466, 43.57 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.15652, 0.15 secs\n",
      "\u001b[1m---- Epoch 6/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.14436, s2s_loss 0.13505, 43.88 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14898, 0.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.8714.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.13489, s2s_loss 0.13117, 43.51 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14665, 0.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.8733.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.14952, s2s_loss 0.12882, 43.71 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14699, 0.14 secs\n",
      "\u001b[1m---- Epoch 9/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.12608, s2s_loss 0.12776, 43.76 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14680, 0.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.8735.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.11644, s2s_loss 0.12760, 43.93 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14652, 0.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_s2s_loss=0.8737.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.13968, s2s_loss 0.12624, 43.96 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno no no no\u001b[0m\n",
      "s2s_loss 0.14710, 0.16 secs\n",
      "\u001b[1m---- Epoch 12/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.14317, s2s_loss 0.12814, 43.98 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14678, 0.20 secs\n",
      "\u001b[1m---- Epoch 13/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.11772, s2s_loss 0.12030, 43.94 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14926, 0.16 secs\n",
      "\u001b[1m---- Epoch 14/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.11302, s2s_loss 0.10890, 44.28 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14611, 0.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_s2s_loss=0.8754.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.11133, s2s_loss 0.10357, 45.35 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew finding new new\u001b[0m\n",
      "s2s_loss 0.14662, 0.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_s2s_loss=0.8755.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.10518, s2s_loss 0.10135, 47.36 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14550, 0.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_s2s_loss=0.8765.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.11117, s2s_loss 0.10064, 49.97 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14602, 0.20 secs\n",
      "\u001b[1m---- Epoch 18/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.10356, s2s_loss 0.09958, 53.60 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14553, 0.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_s2s_loss=0.8766.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09240, s2s_loss 0.10062, 57.05 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.14638, 0.24 secs\n",
      "\u001b[1m---- Epoch 20/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07786, s2s_loss 0.09827, 61.53 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mimproved improved improved improved\u001b[0m\n",
      "s2s_loss 0.14611, 0.42 secs\n",
      "\u001b[1m---- Epoch 21/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.09152, s2s_loss 0.10031, 65.04 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.14251, 0.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_s2s_loss=0.8786.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.07455, s2s_loss 0.09037, 65.45 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14393, 0.20 secs\n",
      "\u001b[1m---- Epoch 23/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.09346, s2s_loss 0.08601, 65.71 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14335, 0.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_s2s_loss=0.8792.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.08376, s2s_loss 0.08337, 63.39 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14422, 0.43 secs\n",
      "\u001b[1m---- Epoch 25/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.09663, s2s_loss 0.08280, 64.39 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14274, 0.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_s2s_loss=0.8799.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09858, s2s_loss 0.08239, 59.78 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14330, 0.23 secs\n",
      "\u001b[1m---- Epoch 27/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07204, s2s_loss 0.08087, 59.95 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworse improved worse worse\u001b[0m\n",
      "s2s_loss 0.14328, 0.21 secs\n",
      "\u001b[1m---- Epoch 28/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.08799, s2s_loss 0.08156, 61.10 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.14293, 0.23 secs\n",
      "\u001b[1m---- Epoch 29/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.09627, s2s_loss 0.08446, 61.00 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.15691, 0.23 secs\n",
      "\u001b[1m---- Epoch 30/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.09785, s2s_loss 0.07678, 59.99 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.15104, 0.24 secs\n",
      "\u001b[1m---- Epoch 31/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.07414, s2s_loss 0.07248, 62.97 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14875, 0.22 secs\n",
      "\u001b[1m---- Epoch 32/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.07760, s2s_loss 0.07044, 62.22 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mimproved improved improved improved\u001b[0m\n",
      "s2s_loss 0.14930, 0.35 secs\n",
      "\u001b[1m---- Epoch 33/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.06798, s2s_loss 0.06942, 61.08 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno no no no\u001b[0m\n",
      "s2s_loss 0.14936, 0.32 secs\n",
      "\u001b[1m---- Epoch 34/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.07846, s2s_loss 0.06884, 64.96 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.15081, 0.24 secs\n",
      "\u001b[1m---- Epoch 35/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07464, s2s_loss 0.06901, 63.22 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.15094, 0.20 secs\n",
      "\u001b[1m---- Epoch 36/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09035, s2s_loss 0.06805, 66.84 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved comparison resolved resolved\u001b[0m\n",
      "s2s_loss 0.15091, 0.33 secs\n",
      "\u001b[1m---- Epoch 37/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.05212, s2s_loss 0.07223, 63.47 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogressed progress\u001b[0m\n",
      "s2s_loss 0.14520, 0.20 secs\n",
      "\u001b[1m---- Epoch 38/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.08246, s2s_loss 0.06617, 67.71 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew finding new new\u001b[0m\n",
      "s2s_loss 0.15100, 0.23 secs\n",
      "\u001b[1m---- Epoch 39/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.06158, s2s_loss 0.06211, 69.03 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogressed progress\u001b[0m\n",
      "s2s_loss 0.14981, 0.25 secs\n",
      "\u001b[1m---- Epoch 40/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.05606, s2s_loss 0.06008, 64.34 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.14748, 0.22 secs\n",
      "\u001b[1m---- Epoch 41/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.06232, s2s_loss 0.05920, 62.86 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14933, 0.20 secs\n",
      "\u001b[1m---- Epoch 42/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.05949, s2s_loss 0.05899, 61.95 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.14964, 0.23 secs\n",
      "\u001b[1m---- Epoch 43/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07609, s2s_loss 0.05863, 60.79 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved resolved resolved resolved\u001b[0m\n",
      "s2s_loss 0.14939, 0.27 secs\n",
      "\u001b[1m---- Epoch 44/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05034, s2s_loss 0.05816, 61.24 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogressed progress\u001b[0m\n",
      "s2s_loss 0.14940, 0.22 secs\n",
      "\u001b[1m---- Epoch 45/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.05497, s2s_loss 0.06222, 60.78 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16087, 0.28 secs\n",
      "\u001b[1m---- Epoch 46/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.06476, s2s_loss 0.05724, 63.98 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.15716, 0.50 secs\n",
      "\u001b[1m---- Epoch 47/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.07012, s2s_loss 0.05301, 63.89 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable resolved resolved resolved\u001b[0m\n",
      "s2s_loss 0.16187, 0.22 secs\n",
      "\u001b[1m---- Epoch 48/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.05036, s2s_loss 0.05215, 65.12 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16191, 0.28 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 49/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.04579, s2s_loss 0.05103, 65.00 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworse worse worse\u001b[0m\n",
      "s2s_loss 0.16344, 0.24 secs\n",
      "\u001b[1m---- Epoch 50/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.04485, s2s_loss 0.04982, 64.28 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16412, 0.22 secs\n",
      "\u001b[1m---- Epoch 51/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05284, s2s_loss 0.05032, 64.59 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved/unchanged\u001b[0m\n",
      "s2s_loss 0.16401, 0.29 secs\n",
      "\u001b[1m---- Epoch 52/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05284, s2s_loss 0.04981, 64.37 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.16409, 0.35 secs\n",
      "\u001b[1m---- Epoch 53/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06341, s2s_loss 0.05393, 60.77 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16940, 0.22 secs\n",
      "\u001b[1m---- Epoch 54/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.06646, s2s_loss 0.04967, 58.63 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved resolved resolved resolved\u001b[0m\n",
      "s2s_loss 0.15864, 0.23 secs\n",
      "\u001b[1m---- Epoch 55/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.03743, s2s_loss 0.04618, 62.70 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mimproved improved improved improved\u001b[0m\n",
      "s2s_loss 0.16495, 0.23 secs\n",
      "\u001b[1m---- Epoch 56/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.04282, s2s_loss 0.04391, 63.59 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.16720, 0.21 secs\n",
      "\u001b[1m---- Epoch 57/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.03862, s2s_loss 0.04295, 64.63 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16871, 0.49 secs\n",
      "\u001b[1m---- Epoch 58/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.04524, s2s_loss 0.04303, 65.45 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16857, 0.21 secs\n",
      "\u001b[1m---- Epoch 59/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.04780, s2s_loss 0.04259, 62.73 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16837, 0.21 secs\n",
      "\u001b[1m---- Epoch 60/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05976, s2s_loss 0.04224, 62.39 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.16850, 0.20 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "        --pretrained_checkpoint_folder_path \\\n",
    "            \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230707_215744_fact2metadata(gpt-3.5-..ts__v2;gpt-3.5-..=20000;gpt-3.5-..niform)_Seq2Seq(t5-small)\" \\\n",
    "        --epochs 60 \\\n",
    "        --batches_per_epoch 300 \\\n",
    "        --batch_size 400 \\\n",
    "        --num_workers 3 \\\n",
    "        --iters_to_accumulate 1 \\\n",
    "        --optimizer_name \"adamw\" \\\n",
    "        --scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "        --task_name \"fact2comparison\" \\\n",
    "        --integrated_facts_metadata_jsonl_filepath \\\n",
    "            \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "        --input_output_jsonl_filepaths \\\n",
    "            \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "            \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "        --seq2seq_model_name \"t5\" \\\n",
    "        --t5_model_name \"t5-small\" \\\n",
    "        --use_amp \\\n",
    "        --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 60\n",
      "   batches_per_epoch: 400\n",
      "   batch_size: 400\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_100601_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 1\n",
      "   override_lr: False\n",
      "   task_name: fact2comparison\n",
      "   val_size: 200\n",
      "   input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 1\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "Counter:\n",
      "Counter({'no comparison': 52617, 'stable/unchanged': 14599, 'worsened': 8503, 'resolved': 5923, 'improved': 4896, 'progressed': 3366, 'new finding': 2835, 'increase': 1867, 'decrease': 1716, 'larger': 1550, 'smaller': 1436, 'unclear comparison': 985, 'position changed': 946, 'reappeared': 509, 'other': 7})\n",
      "Output: no comparison, train size: 52602, val size: 15, weight: 245.95115785491103\n",
      "Output: worsened, train size: 8488, val size: 15, weight: 170.33405481941352\n",
      "Output: resolved, train size: 5908, val size: 15, weight: 156.96216240553915\n",
      "Output: larger, train size: 1535, val size: 15, weight: 112.0215415998979\n",
      "Output: improved, train size: 4882, val size: 14, weight: 150.14229680995012\n",
      "Output: smaller, train size: 1422, val size: 14, weight: 109.69851212961082\n",
      "Output: progressed, train size: 3352, val size: 14, weight: 137.14298732757413\n",
      "Output: increase, train size: 1853, val size: 14, weight: 117.8450753932758\n",
      "Output: decrease, train size: 1702, val size: 14, weight: 115.19761789554563\n",
      "Output: unclear comparison, train size: 971, val size: 14, weight: 98.47242838287598\n",
      "Output: new finding, train size: 2821, val size: 14, weight: 131.37723655130551\n",
      "Output: stable/unchanged, train size: 14585, val size: 14, weight: 191.32969503180252\n",
      "Output: reappeared, train size: 495, val size: 14, weight: 80.12549804840134\n",
      "Output: other, train size: 7, val size: 0, weight: 7.8812416584010565\n",
      "Output: position changed, train size: 932, val size: 14, weight: 97.30216829638947\n",
      "Number of train examples: 101555\n",
      "Number of val examples: 200\n",
      "Number of total examples: 101755\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mpartial re-expanding pulmonary edema in the left basal region\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "seq2seq_trainer.name =  fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_145334_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_145334_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_25_s2s_loss=0.8799.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_100601_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/checkpoint_25_s2s_loss=0.8799.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_145334_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.29460, s2s_loss 0.25200, 58.02 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworsened\u001b[0m\n",
      "s2s_loss 0.25061, 0.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.7995.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.22280, s2s_loss 0.20946, 56.31 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mimproved improved improved improved\u001b[0m\n",
      "s2s_loss 0.21047, 0.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.8262.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 0.18436, s2s_loss 0.19186, 56.69 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mimprovedd improved improved improved\u001b[0m\n",
      "s2s_loss 0.20727, 0.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.8294.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.19172, s2s_loss 0.16807, 56.77 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparisond no no\u001b[0m\n",
      "s2s_loss 0.20528, 0.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_s2s_loss=0.8323.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 0.18145, s2s_loss 0.14034, 57.29 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.20929, 0.22 secs\n",
      "\u001b[1m---- Epoch 6/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.13121, s2s_loss 0.12764, 56.19 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparisond no no\u001b[0m\n",
      "s2s_loss 0.21102, 0.22 secs\n",
      "\u001b[1m---- Epoch 7/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.12152, s2s_loss 0.12093, 57.25 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mincrease increase increase increase\u001b[0m\n",
      "s2s_loss 0.21493, 0.20 secs\n",
      "\u001b[1m---- Epoch 8/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.12184, s2s_loss 0.11841, 57.14 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mreappeareda\u001b[0m\n",
      "s2s_loss 0.21568, 0.22 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m---- Epoch 9/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.11384, s2s_loss 0.11588, 56.40 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogressed progress\u001b[0m\n",
      "s2s_loss 0.21956, 0.23 secs\n",
      "\u001b[1m---- Epoch 10/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.11967, s2s_loss 0.11477, 59.20 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.21748, 0.23 secs\n",
      "\u001b[1m---- Epoch 11/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09618, s2s_loss 0.11509, 60.86 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mreap comparison unclear unclear\u001b[0m\n",
      "s2s_loss 0.21809, 0.24 secs\n",
      "\u001b[1m---- Epoch 12/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.11799, s2s_loss 0.11505, 63.56 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworse worse worse worse\u001b[0m\n",
      "s2s_loss 0.21827, 0.27 secs\n",
      "\u001b[1m---- Epoch 13/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.09121, s2s_loss 0.11496, 66.49 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworse worse worse\u001b[0m\n",
      "s2s_loss 0.23816, 0.25 secs\n",
      "\u001b[1m---- Epoch 14/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.09170, s2s_loss 0.10022, 69.58 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew finding new new\u001b[0m\n",
      "s2s_loss 0.26116, 0.28 secs\n",
      "\u001b[1m---- Epoch 15/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.10258, s2s_loss 0.09221, 72.21 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.26285, 0.27 secs\n",
      "\u001b[1m---- Epoch 16/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.08176, s2s_loss 0.08807, 72.43 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mposition changed no position\u001b[0m\n",
      "s2s_loss 0.26640, 0.45 secs\n",
      "\u001b[1m---- Epoch 17/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.08171, s2s_loss 0.08769, 73.29 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworsened\u001b[0m\n",
      "s2s_loss 0.26601, 0.26 secs\n",
      "\u001b[1m---- Epoch 18/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.08081, s2s_loss 0.08590, 70.76 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworse worse worse worse\u001b[0m\n",
      "s2s_loss 0.26502, 0.28 secs\n",
      "\u001b[1m---- Epoch 19/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.08940, s2s_loss 0.08486, 71.28 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew finding new new\u001b[0m\n",
      "s2s_loss 0.26471, 0.27 secs\n",
      "\u001b[1m---- Epoch 20/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.08714, s2s_loss 0.08568, 72.58 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35msmaller smaller smaller smaller\u001b[0m\n",
      "s2s_loss 0.26471, 0.27 secs\n",
      "\u001b[1m---- Epoch 21/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.08930, s2s_loss 0.08908, 71.06 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35munclear comparison unclear\u001b[0m\n",
      "s2s_loss 0.28388, 0.26 secs\n",
      "\u001b[1m---- Epoch 22/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.08854, s2s_loss 0.07983, 68.93 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35munclear comparison unclear unclear\u001b[0m\n",
      "s2s_loss 0.30208, 0.42 secs\n",
      "\u001b[1m---- Epoch 23/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.07108, s2s_loss 0.07321, 71.86 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mreappeared\u001b[0m\n",
      "s2s_loss 0.30719, 0.25 secs\n",
      "\u001b[1m---- Epoch 24/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.06590, s2s_loss 0.07009, 86.30 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogress finding progress progress\u001b[0m\n",
      "s2s_loss 0.31735, 0.27 secs\n",
      "\u001b[1m---- Epoch 25/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07191, s2s_loss 0.06920, 77.51 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mincrease increase increase\u001b[0m\n",
      "s2s_loss 0.31556, 0.28 secs\n",
      "\u001b[1m---- Epoch 26/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.06488, s2s_loss 0.06786, 77.31 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparisond no no\u001b[0m\n",
      "s2s_loss 0.31892, 0.36 secs\n",
      "\u001b[1m---- Epoch 27/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05354, s2s_loss 0.06844, 75.91 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworsened\u001b[0m\n",
      "s2s_loss 0.31930, 0.42 secs\n",
      "\u001b[1m---- Epoch 28/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09232, s2s_loss 0.06773, 74.10 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mdecrease decrease decrease decrease\u001b[0m\n",
      "s2s_loss 0.31928, 0.28 secs\n",
      "\u001b[1m---- Epoch 29/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.05538, s2s_loss 0.07300, 74.54 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable changed stable stable\u001b[0m\n",
      "s2s_loss 0.34134, 0.25 secs\n",
      "\u001b[1m---- Epoch 30/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.07189, s2s_loss 0.06503, 78.85 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogressedd improved\u001b[0m\n",
      "s2s_loss 0.35506, 0.55 secs\n",
      "\u001b[1m---- Epoch 31/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.07187, s2s_loss 0.05935, 81.09 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.35105, 0.30 secs\n",
      "\u001b[1m---- Epoch 32/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.06483, s2s_loss 0.05743, 80.73 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mposition changed position position\u001b[0m\n",
      "s2s_loss 0.36301, 0.26 secs\n",
      "\u001b[1m---- Epoch 33/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.05521, s2s_loss 0.05678, 82.51 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew findingd new new\u001b[0m\n",
      "s2s_loss 0.36018, 0.28 secs\n",
      "\u001b[1m---- Epoch 34/60\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "^C iteration 13275\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 484, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 395, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 284, in train_model\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 98, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 63, in step_fn\n",
      "    model_output = model(**model_kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/seq2seq.py\", line 41, in forward\n",
      "    output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1704, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1074, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 693, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 599, in forward\n",
      "    normed_hidden_states = self.layer_norm(hidden_states)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 253, in forward\n",
      "    variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "        --pretrained_checkpoint_folder_path \\\n",
    "            \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_100601_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\" \\\n",
    "        --epochs 60 \\\n",
    "        --batches_per_epoch 400 \\\n",
    "        --batch_size 400 \\\n",
    "        --num_workers 3 \\\n",
    "        --iters_to_accumulate 1 \\\n",
    "        --optimizer_name \"adamw\" \\\n",
    "        --scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "        --lr 1e-6 \\\n",
    "        --warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "        --task_name \"fact2comparison\" \\\n",
    "        --integrated_facts_metadata_jsonl_filepath \\\n",
    "            \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "        --input_output_jsonl_filepaths \\\n",
    "            \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "            \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "        --seq2seq_model_name \"t5\" \\\n",
    "        --t5_model_name \"t5-small\" \\\n",
    "        --use_amp \\\n",
    "        --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 20\n",
      "   batches_per_epoch: 400\n",
      "   batch_size: 400\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: t5\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_145334_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 1\n",
      "   override_lr: False\n",
      "   task_name: fact2comparison\n",
      "   val_size: 200\n",
      "   input_output_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl']\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "   paraphrased_inputs_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl']\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 1\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mwithout evidence to suggest pneumonia\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mabsence of pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mno indication of pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of lung infection\u001b[0m\n",
      "\u001b[1m\u001b[35mno findings suggestive of pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mno radiographic evidence of pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mno infiltrates to suggest pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mno opacities consistent with pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mno consolidation indicative of pneumonia\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of lung inflammation\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mpoor exposure\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35minadequate exposure\u001b[0m\n",
      "\u001b[1m\u001b[35minsufficient exposure\u001b[0m\n",
      "\u001b[1m\u001b[35msuboptimal exposure\u001b[0m\n",
      "\u001b[1m\u001b[35munderexposure\u001b[0m\n",
      "\u001b[1m\u001b[35mimproper exposure\u001b[0m\n",
      "\u001b[1m\u001b[35mlow exposure\u001b[0m\n",
      "\u001b[1m\u001b[35mlack of exposure\u001b[0m\n",
      "\u001b[1m\u001b[35minsubstantial exposure\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited exposure\u001b[0m\n",
      "\u001b[1m\u001b[35mdeficient exposure\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mEKG wires\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35melectrocardiogram wires\u001b[0m\n",
      "\u001b[1m\u001b[35mEKG leads\u001b[0m\n",
      "\u001b[1m\u001b[35mECG cables\u001b[0m\n",
      "\u001b[1m\u001b[35melectrocardiography wires\u001b[0m\n",
      "\u001b[1m\u001b[35mheart monitor wires\u001b[0m\n",
      "\u001b[1m\u001b[35mcardiac monitoring wires\u001b[0m\n",
      "\u001b[1m\u001b[35mheart tracing wires\u001b[0m\n",
      "\u001b[1m\u001b[35melectrocardiograph wires\u001b[0m\n",
      "\u001b[1m\u001b[35mheart rhythm recording wires\u001b[0m\n",
      "\u001b[1m\u001b[35mcardiogram wires\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mautres lesions\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mother abnormalities\u001b[0m\n",
      "\u001b[1m\u001b[35madditional findings\u001b[0m\n",
      "\u001b[1m\u001b[35madditional lesions\u001b[0m\n",
      "\u001b[1m\u001b[35mfurther abnormalities\u001b[0m\n",
      "\u001b[1m\u001b[35madditional pathological changes\u001b[0m\n",
      "\u001b[1m\u001b[35mother anomalies\u001b[0m\n",
      "\u001b[1m\u001b[35madditional irregularities\u001b[0m\n",
      "\u001b[1m\u001b[35madditional abnormalities\u001b[0m\n",
      "\u001b[1m\u001b[35mfurther lesions\u001b[0m\n",
      "\u001b[1m\u001b[35mother pathological findings\u001b[0m\n",
      "--------\n",
      "Loaded 59901 paraphrased inputs in total\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mdegenerative bone disease worsening\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mescalating loss of bone density\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "Added 79040 paraphrased inputs\n",
      "Number of total examples: 180795\n",
      "Counter:\n",
      "Counter({'no comparison': 93296, 'stable/unchanged': 25860, 'worsened': 15270, 'resolved': 10567, 'improved': 8727, 'progressed': 5949, 'new finding': 4856, 'increase': 3417, 'decrease': 3218, 'larger': 2840, 'smaller': 2442, 'unclear comparison': 1844, 'position changed': 1654, 'reappeared': 839, 'other': 16})\n",
      "Output: no comparison, train size: 93281, val size: 15, weight: 272.55684234037693\n",
      "Output: worsened, train size: 15255, val size: 15, weight: 193.1264577858281\n",
      "Output: resolved, train size: 10552, val size: 15, weight: 178.62934219286575\n",
      "Output: larger, train size: 2826, val size: 14, weight: 131.4358092615\n",
      "Output: improved, train size: 8713, val size: 14, weight: 171.32071230674907\n",
      "Output: smaller, train size: 2428, val size: 14, weight: 126.46245566917449\n",
      "Output: progressed, train size: 5935, val size: 14, weight: 157.127035126293\n",
      "Output: increase, train size: 3403, val size: 14, weight: 137.6537018032795\n",
      "Output: decrease, train size: 3204, val size: 14, weight: 135.6213603243321\n",
      "Output: unclear comparison, train size: 1830, val size: 14, weight: 117.45417921785709\n",
      "Output: new finding, train size: 4842, val size: 14, weight: 149.85156459547284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: stable/unchanged, train size: 25846, val size: 14, weight: 214.84680324740802\n",
      "Output: reappeared, train size: 825, val size: 14, weight: 93.86219405241911\n",
      "Output: other, train size: 15, val size: 1, weight: 15.263794126054286\n",
      "Output: position changed, train size: 1640, val size: 14, weight: 114.05129519573283\n",
      "Number of train examples: 180595\n",
      "Number of val examples: 200\n",
      "Number of total examples: 180795\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished azygos vein\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35msmaller\u001b[0m\n",
      "seq2seq_trainer.name =  fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_4_s2s_loss=0.8323.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_145334_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/checkpoint_4_s2s_loss=0.8323.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42589, s2s_loss 0.34880, 58.05 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworse improved worse worse\u001b[0m\n",
      "s2s_loss 0.36012, 0.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.7358.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.29858, s2s_loss 0.31583, 56.98 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mreap unclear resolved reap\u001b[0m\n",
      "s2s_loss 0.30994, 0.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.7631.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 0.27588, s2s_loss 0.28561, 56.94 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mposition changed position position\u001b[0m\n",
      "s2s_loss 0.26635, 0.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.7885.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.26655, s2s_loss 0.24557, 56.55 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogressed progress\u001b[0m\n",
      "s2s_loss 0.21162, 0.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_s2s_loss=0.8231.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 0.19900, s2s_loss 0.21163, 56.70 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mreappeareda\u001b[0m\n",
      "s2s_loss 0.19804, 0.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_s2s_loss=0.8338.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.20510, s2s_loss 0.19700, 57.27 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogressed increase\u001b[0m\n",
      "s2s_loss 0.19102, 0.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.8392.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.20507, s2s_loss 0.18912, 56.13 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable stable stable stable\u001b[0m\n",
      "s2s_loss 0.18183, 0.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.8456.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.19346, s2s_loss 0.18538, 57.70 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35msmaller smaller smaller smaller\u001b[0m\n",
      "s2s_loss 0.18202, 0.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_s2s_loss=0.8458.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.16253, s2s_loss 0.18220, 57.38 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35munclear comparison unclear unclear\u001b[0m\n",
      "s2s_loss 0.18048, 0.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.8470.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.17700, s2s_loss 0.18283, 57.31 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew finding unclear new\u001b[0m\n",
      "s2s_loss 0.18044, 0.21 secs\n",
      "\u001b[1m---- Epoch 11/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.19557, s2s_loss 0.18282, 57.52 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.18047, 0.20 secs\n",
      "\u001b[1m---- Epoch 12/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.18814, s2s_loss 0.18189, 57.27 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mincrease increase increase increase\u001b[0m\n",
      "s2s_loss 0.18018, 0.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_s2s_loss=0.8472.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.18896, s2s_loss 0.18116, 57.64 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mdecrease decrease decrease\u001b[0m\n",
      "s2s_loss 0.16901, 0.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_s2s_loss=0.8545.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.19321, s2s_loss 0.16363, 58.96 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved/unchanged\u001b[0m\n",
      "s2s_loss 0.16471, 0.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_s2s_loss=0.8587.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.15070, s2s_loss 0.15417, 59.99 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35msmallerd smaller smaller smaller\u001b[0m\n",
      "s2s_loss 0.16054, 0.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_s2s_loss=0.8621.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.17191, s2s_loss 0.15017, 61.97 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mnew findingd new new\u001b[0m\n",
      "s2s_loss 0.16015, 0.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_s2s_loss=0.8627.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.15069, s2s_loss 0.14720, 65.95 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35msmaller smaller smaller\u001b[0m\n",
      "s2s_loss 0.15991, 0.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_s2s_loss=0.8631.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.15012, s2s_loss 0.14585, 67.40 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mreappeareda\u001b[0m\n",
      "s2s_loss 0.16016, 0.23 secs\n",
      "\u001b[1m---- Epoch 19/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.16041, s2s_loss 0.14595, 68.81 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mincreased no increase increase\u001b[0m\n",
      "s2s_loss 0.16048, 0.24 secs\n",
      "\u001b[1m---- Epoch 20/20\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.12540, s2s_loss 0.14573, 70.53 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.16050, 0.24 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "    --pretrained_checkpoint_folder_path \\\n",
    "        \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230717_145334_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\" \\\n",
    "    --epochs 20 \\\n",
    "    --batches_per_epoch 400 \\\n",
    "    --batch_size 400 \\\n",
    "    --num_workers 3 \\\n",
    "    --iters_to_accumulate 1 \\\n",
    "    --optimizer_name \"adamw\" \\\n",
    "    --scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "    --lr 1e-6 \\\n",
    "    --warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "    --task_name \"fact2comparison\" \\\n",
    "    --integrated_facts_metadata_jsonl_filepath \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\" \\\n",
    "    --input_output_jsonl_filepaths \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\" \\\n",
    "    --paraphrased_inputs_jsonl_filepaths \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "        \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "    --seq2seq_model_name \"t5\" \\\n",
    "    --t5_model_name \"t5-small\" \\\n",
    "    --use_amp \\\n",
    "    --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 20\n",
      "   batches_per_epoch: 400\n",
      "   batch_size: 400\n",
      "   checkpoint_folder: models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\n",
      "   seq2seq_model_name: None\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   optimizer_name: adamw\n",
      "   lr: 0.001\n",
      "   scheduler: reduce-lr-on-plateau\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: None\n",
      "   iters_to_accumulate: 1\n",
      "   override_lr: False\n",
      "   task_name: None\n",
      "   val_size: 200\n",
      "   input_output_jsonl_filepaths: None\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrased_inputs_jsonl_filepaths: None\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Resuming training ------\u001b[0m\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: t5-small\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 1\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "Loaded 57298 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(578733,58628071).jsonl\n",
      "Loaded 13977 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part1.jsonl\n",
      "Loaded 30480 input/output pairs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_comparisons__part2.jsonl\n",
      "--------\n",
      "\u001b[1mLoaded 14993 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimetres\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in size\u001b[0m\n",
      "\u001b[1m\u001b[35ma length of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35ma measurement of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35mmeasuring 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 centimeters in length\u001b[0m\n",
      "\u001b[1m\u001b[35ma size of 5 cm\u001b[0m\n",
      "\u001b[1m\u001b[35m5 cm in dimension\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14971 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited base\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained base\u001b[0m\n",
      "\u001b[1m\u001b[35mnarrowed base\u001b[0m\n",
      "\u001b[1m\u001b[35mlimited foundation\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted support\u001b[0m\n",
      "\u001b[1m\u001b[35mconstricted base\u001b[0m\n",
      "\u001b[1m\u001b[35mreduced base\u001b[0m\n",
      "\u001b[1m\u001b[35mdiminished base\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted bottom\u001b[0m\n",
      "\u001b[1m\u001b[35mconstrained footing\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14972 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visible\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mnot visualized\u001b[0m\n",
      "\u001b[1m\u001b[35mnot seen\u001b[0m\n",
      "\u001b[1m\u001b[35mnot apparent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot detectable\u001b[0m\n",
      "\u001b[1m\u001b[35mabsent\u001b[0m\n",
      "\u001b[1m\u001b[35mnot present\u001b[0m\n",
      "\u001b[1m\u001b[35munable to visualize\u001b[0m\n",
      "\u001b[1m\u001b[35mnot observable\u001b[0m\n",
      "\u001b[1m\u001b[35mnot evident\u001b[0m\n",
      "\u001b[1m\u001b[35mnot identifiable\u001b[0m\n",
      "--------\n",
      "\u001b[1mLoaded 14965 paraphrased inputs from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel loops\u001b[0m\n",
      "\u001b[1mParaphrases:\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal loops\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal segments\u001b[0m\n",
      "\u001b[1m\u001b[35mbowel segments\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal coils\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal twists\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal convolutions\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal turns\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal curvatures\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal windings\u001b[0m\n",
      "\u001b[1m\u001b[35mintestinal meanders\u001b[0m\n",
      "--------\n",
      "Loaded 59901 paraphrased inputs in total\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mosteoporosis worsening in a debilitated and wheelchair-bound patient\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbacteremia caused by Gram-negative cocci\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mconveyed information to the Emergency Department Quality Assurance nursing group via email at 6 pm\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbarely noticeable 'finger in glove' appearance\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mappearance of Carevalve Aortic valve replacement at the aortic root\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mIdentification of Tetralogy of Fallot in the thorax\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mprogression related to breathing in toxic fumes\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mexcretory nephrograms from previous injection of intravenous contrast for CT examination\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mprobably secondary to left hemidiaphragm phrenic nerve dysfunction\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mperifascial effusion is suspected\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mminor bulging of the left constrophrenic angle\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mfeet showing greater density than surrounding tissues at the right lung base\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlongitudinal opacities in the left upper lobe\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mdramatic rise in radiodensity in the left lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mmessage sent with top priority\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mrestricted assessment of the cardiac and mediastinal contour due to rotation\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mconsolidation in the upper portion of the right lung partially obstructed by the right collarbone\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mconsiderable air trapped beneath the skin in the thoracic/cervical region\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mptx is not visible on this radiograph\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mplaces the Swan- Ganz catheter\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "--------\n",
      "Added 79040 paraphrased inputs\n",
      "Number of total examples: 180795\n",
      "Counter:\n",
      "Counter({'no comparison': 93296, 'stable/unchanged': 25860, 'worsened': 15270, 'resolved': 10567, 'improved': 8727, 'progressed': 5949, 'new finding': 4856, 'increase': 3417, 'decrease': 3218, 'larger': 2840, 'smaller': 2442, 'unclear comparison': 1844, 'position changed': 1654, 'reappeared': 839, 'other': 16})\n",
      "Output: no comparison, train size: 93281, val size: 15, weight: 272.55684234037693\n",
      "Output: worsened, train size: 15255, val size: 15, weight: 193.1264577858281\n",
      "Output: resolved, train size: 10552, val size: 15, weight: 178.62934219286575\n",
      "Output: larger, train size: 2826, val size: 14, weight: 131.4358092615\n",
      "Output: improved, train size: 8713, val size: 14, weight: 171.32071230674907\n",
      "Output: smaller, train size: 2428, val size: 14, weight: 126.46245566917449\n",
      "Output: progressed, train size: 5935, val size: 14, weight: 157.127035126293\n",
      "Output: increase, train size: 3403, val size: 14, weight: 137.6537018032795\n",
      "Output: decrease, train size: 3204, val size: 14, weight: 135.6213603243321\n",
      "Output: unclear comparison, train size: 1830, val size: 14, weight: 117.45417921785709\n",
      "Output: new finding, train size: 4842, val size: 14, weight: 149.85156459547284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: stable/unchanged, train size: 25846, val size: 14, weight: 214.84680324740802\n",
      "Output: reappeared, train size: 825, val size: 14, weight: 93.86219405241911\n",
      "Output: other, train size: 15, val size: 1, weight: 15.263794126054286\n",
      "Output: position changed, train size: 1640, val size: 14, weight: 114.05129519573283\n",
      "Number of train examples: 180595\n",
      "Number of val examples: 200\n",
      "Number of total examples: 180795\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mtrace amount of fluid in the right lung base\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison\u001b[0m\n",
      "seq2seq_trainer.name =  fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_17_s2s_loss=0.8631.pt']\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/checkpoint_17_s2s_loss=0.8631.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 18/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.15777, s2s_loss 0.14610, 58.52 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworsened\u001b[0m\n",
      "s2s_loss 0.13048, 0.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_s2s_loss=0.8834.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.14297, s2s_loss 0.14628, 57.82 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mreappeareda\u001b[0m\n",
      "s2s_loss 0.13009, 0.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_s2s_loss=0.8836.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.12447, s2s_loss 0.14584, 56.11 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.13008, 0.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_20_s2s_loss=0.8837.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 21/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.13835, s2s_loss 0.14897, 57.86 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mimproved improved improved improved\u001b[0m\n",
      "s2s_loss 0.13321, 0.19 secs\n",
      "\u001b[1m---- Epoch 22/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.15471, s2s_loss 0.13653, 58.10 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mposition changed position position\u001b[0m\n",
      "s2s_loss 0.13720, 0.22 secs\n",
      "\u001b[1m---- Epoch 23/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.15197, s2s_loss 0.12854, 58.51 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mlarger larger larger\u001b[0m\n",
      "s2s_loss 0.12853, 0.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_s2s_loss=0.8861.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.14077, s2s_loss 0.12581, 57.94 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mlarger worse worse\u001b[0m\n",
      "s2s_loss 0.13164, 0.21 secs\n",
      "\u001b[1m---- Epoch 25/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.11520, s2s_loss 0.12318, 58.12 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35munclear comparison unclear unclear\u001b[0m\n",
      "s2s_loss 0.13085, 0.20 secs\n",
      "\u001b[1m---- Epoch 26/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.10765, s2s_loss 0.12283, 58.26 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mincreased worse improved\u001b[0m\n",
      "s2s_loss 0.13228, 0.22 secs\n",
      "\u001b[1m---- Epoch 27/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11691, s2s_loss 0.12184, 58.90 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mworsened\u001b[0m\n",
      "s2s_loss 0.13242, 0.23 secs\n",
      "\u001b[1m---- Epoch 28/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09829, s2s_loss 0.12227, 60.03 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mno comparison no no\u001b[0m\n",
      "s2s_loss 0.13261, 0.22 secs\n",
      "\u001b[1m---- Epoch 29/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.13027, s2s_loss 0.12697, 61.22 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved no resolved resolved\u001b[0m\n",
      "s2s_loss 0.13536, 0.23 secs\n",
      "\u001b[1m---- Epoch 30/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.12401, s2s_loss 0.11708, 64.03 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mimproved improved improved\u001b[0m\n",
      "s2s_loss 0.13120, 0.28 secs\n",
      "\u001b[1m---- Epoch 31/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.12196, s2s_loss 0.11031, 65.45 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35msmaller smaller smaller\u001b[0m\n",
      "s2s_loss 0.13370, 0.28 secs\n",
      "\u001b[1m---- Epoch 32/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.10381, s2s_loss 0.10698, 67.60 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable/unchanged\u001b[0m\n",
      "s2s_loss 0.13123, 0.22 secs\n",
      "\u001b[1m---- Epoch 33/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.10887, s2s_loss 0.10630, 67.93 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35msmaller decrease smaller\u001b[0m\n",
      "s2s_loss 0.13306, 0.24 secs\n",
      "\u001b[1m---- Epoch 34/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09862, s2s_loss 0.10389, 68.20 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mresolved resolved resolved resolved\u001b[0m\n",
      "s2s_loss 0.13348, 0.29 secs\n",
      "\u001b[1m---- Epoch 35/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09847, s2s_loss 0.10388, 69.41 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mposition changed position\u001b[0m\n",
      "s2s_loss 0.13378, 0.25 secs\n",
      "\u001b[1m---- Epoch 36/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.11585, s2s_loss 0.10458, 70.04 secs\n",
      "(2) Validation stage ...\n",
      "\u001b[1mRandom output:\u001b[0m\n",
      "\u001b[1m\u001b[35mincrease increase increase\u001b[0m\n",
      "s2s_loss 0.13390, 0.34 secs\n",
      "\u001b[1m---- Epoch 37/37\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "^C iteration 7625\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 484, in <module>\n",
      "    resume_training(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 462, in resume_training\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 286, in train_model\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 98, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 63, in step_fn\n",
      "    model_output = model(**model_kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/seq2seq.py\", line 41, in forward\n",
      "    output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1667, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1074, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 745, in forward\n",
      "    hidden_states = self.layer[-1](hidden_states)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 343, in forward\n",
      "    forwarded_states = self.DenseReluDense(forwarded_states)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 288, in forward\n",
      "    hidden_states = self.wi(hidden_states)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "    --checkpoint_folder \\\n",
    "        \"models/seq2seq/20230718_103406_fact2comparison(gpt-3.5-.._part1;gpt-3.5-.._part2)_Seq2Seq(t5-small)\" \\\n",
    "    --epochs 20 \\\n",
    "    --batches_per_epoch 400 \\\n",
    "    --batch_size 400 \\\n",
    "    --num_workers 3 \\\n",
    "    --save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
