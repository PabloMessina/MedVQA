{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 1000\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: bart\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250112_101312_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 2\n",
      "   override_lr: False\n",
      "   task_name: report2negative_facts\n",
      "   experiment_name: r2nf\n",
      "   multitask_name_list: None\n",
      "   task2weight: {}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrased_inputs_jsonl_filepaths: None\n",
      "   chest_imagenome_phrases2labels_filepath: None\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_v2_input_output_jsonl_filepaths: None\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: None\n",
      "   report_nli_input_output_train_jsonl_filepaths: None\n",
      "   report_nli_input_output_val_jsonl_filepaths: None\n",
      "   report_to_negative_facts_input_output_jsonl_filepaths: ['/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl', '/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl']\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   integrated_report_facts_jsonl_filepath: None\n",
      "   interpret_cxr__label_based_predictions_filepath: None\n",
      "   interpret_cxr_challenge_data_dir: None\n",
      "   mimiccxr_integrated_report_nli_data_filepath: None\n",
      "   report_section_to_generate: None\n",
      "   include_public_test_in_train: False\n",
      "   best_k_classes: None\n",
      "   use_sentence2facts_for_nli: False\n",
      "   use_anli: False\n",
      "   use_multinli: False\n",
      "   use_snli: False\n",
      "   use_report_nli: False\n",
      "   use_fact_based_reports_in_mlm: False\n",
      "   use_report_nli_entailment_dataset: False\n",
      "   use_report_nli_paraphrases_dataset: False\n",
      "   use_numeric_templates: False\n",
      "   only_validate_nli: False\n",
      "   nli1_only_on_train: False\n",
      "   nli1_only_on_val: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: facebook/bart-base\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 2, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "Loaded 10000 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\n",
      "Loaded 79998 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\n",
      "\u001b[1mInput/output example:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mmildly enlarged heart. aorta slightly tortuous. hilar contours within normal limits. mildly engorged pulmonary vasculature. subsegmental atelectasis in both lung bases. no focal consolidation in both lung bases. no pleural effusion. no pneumothorax. chronic deformities of several left sided posterior ribs. cicalage wires in several left sided posterior ribs. mild pulmonary vascular congestion\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"hilar contours within normal limits\": [\"hilar lymphadenopathy\", \"hilar mass\"], \"no focal consolidation in both lung bases\": [\"right lung base consolidation\", \"left lung base consolidation\"], \"no pleural effusion\": [\"pleural effusion\"], \"no pneumothorax\": [\"pneumothorax\"]}\u001b[0m\n",
      "\u001b[93m\u001b[1mShuffling input sentences...\u001b[0m\n",
      "\u001b[93m\u001b[1mShuffling input sentences...\u001b[0m\n",
      "Number of train examples: 89798\n",
      "Number of val examples: 200\n",
      "Number of total examples: 89998\n",
      "seq2seq_trainer.name =  report2negative_facts(r2nf)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250112_125103_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250112_125103_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_67_s2s_loss=0.9123.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250112_101312_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/checkpoint_67_s2s_loss=0.9123.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250112_125103_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43425, s2s_loss 0.23337, 104.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.22934, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.8132.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.19286, s2s_loss 0.13781, 104.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.16379, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.8612.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 0.08571, s2s_loss 0.11179, 103.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.14357, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.8770.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06895, s2s_loss 0.12392, 104.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.15457, 0.54 secs\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 0.11687, s2s_loss 0.10684, 103.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.13610, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_s2s_loss=0.8825.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.09095, s2s_loss 0.09791, 104.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12982, 0.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.8877.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.11998, s2s_loss 0.09215, 104.10 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12928, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.8885.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.07792, s2s_loss 0.08555, 104.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12467, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_s2s_loss=0.8924.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.08620, s2s_loss 0.08056, 101.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12523, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.8924.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.10790, s2s_loss 0.08181, 104.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12260, 0.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_s2s_loss=0.8941.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09552, s2s_loss 0.08260, 104.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12357, 0.53 secs\n",
      "\u001b[1m---- Epoch 12/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06871, s2s_loss 0.08139, 102.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12503, 0.54 secs\n",
      "\u001b[1m---- Epoch 13/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.08141, s2s_loss 0.11236, 104.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.16115, 0.55 secs\n",
      "\u001b[1m---- Epoch 14/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.20274, s2s_loss 0.09994, 104.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.13598, 0.51 secs\n",
      "\u001b[1m---- Epoch 15/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.08596, s2s_loss 0.09318, 104.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12628, 0.53 secs\n",
      "\u001b[1m---- Epoch 16/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.09382, s2s_loss 0.08948, 105.19 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12497, 0.55 secs\n",
      "\u001b[1m---- Epoch 17/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.06283, s2s_loss 0.07978, 104.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12316, 0.52 secs\n",
      "\u001b[1m---- Epoch 18/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.06440, s2s_loss 0.07399, 105.51 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12268, 0.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_s2s_loss=0.8948.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05378, s2s_loss 0.07718, 104.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12233, 0.54 secs\n",
      "\u001b[1m---- Epoch 20/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.09586, s2s_loss 0.08120, 105.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12072, 0.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_20_s2s_loss=0.8955.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 21/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.04200, s2s_loss 0.11138, 106.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.15781, 0.54 secs\n",
      "\u001b[1m---- Epoch 22/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.14851, s2s_loss 0.08978, 106.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.13527, 0.54 secs\n",
      "\u001b[1m---- Epoch 23/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.12379, s2s_loss 0.08511, 105.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12324, 0.54 secs\n",
      "\u001b[1m---- Epoch 24/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.06795, s2s_loss 0.08551, 106.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12042, 0.54 secs\n",
      "\u001b[1m---- Epoch 25/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.08474, s2s_loss 0.07995, 107.33 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12061, 0.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_s2s_loss=0.8957.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.07325, s2s_loss 0.06976, 106.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12059, 0.53 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_s2s_loss=0.8966.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07772, s2s_loss 0.06950, 105.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12108, 0.53 secs\n",
      "\u001b[1m---- Epoch 28/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06773, s2s_loss 0.07579, 106.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12002, 0.54 secs\n",
      "\u001b[1m---- Epoch 29/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.07684, s2s_loss 0.10958, 106.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.15391, 0.56 secs\n",
      "\u001b[1m---- Epoch 30/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.09245, s2s_loss 0.08865, 111.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.13902, 0.67 secs\n",
      "\u001b[1m---- Epoch 31/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.17091, s2s_loss 0.07852, 114.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.13018, 0.55 secs\n",
      "\u001b[1m---- Epoch 32/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.07317, s2s_loss 0.08020, 107.80 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12271, 0.69 secs\n",
      "\u001b[1m---- Epoch 33/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07968, s2s_loss 0.07657, 114.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12096, 0.62 secs\n",
      "\u001b[1m---- Epoch 34/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.09394, s2s_loss 0.06928, 108.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12098, 0.54 secs\n",
      "\u001b[1m---- Epoch 35/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05209, s2s_loss 0.06611, 108.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12211, 0.60 secs\n",
      "\u001b[1m---- Epoch 36/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.04867, s2s_loss 0.06985, 107.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12260, 0.54 secs\n",
      "\u001b[1m---- Epoch 37/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.05013, s2s_loss 0.10629, 106.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.15609, 0.54 secs\n",
      "\u001b[1m---- Epoch 38/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.24491, s2s_loss 0.08845, 107.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.13720, 0.55 secs\n",
      "\u001b[1m---- Epoch 39/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.07359, s2s_loss 0.07807, 107.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12895, 0.54 secs\n",
      "\u001b[1m---- Epoch 40/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.07208, s2s_loss 0.07456, 106.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12442, 0.64 secs\n",
      "\u001b[1m---- Epoch 41/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.08677, s2s_loss 0.07320, 107.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12509, 0.54 secs\n",
      "\u001b[1m---- Epoch 42/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.08570, s2s_loss 0.06680, 110.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12360, 0.56 secs\n",
      "\u001b[1m---- Epoch 43/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.09232, s2s_loss 0.06571, 108.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12487, 0.54 secs\n",
      "\u001b[1m---- Epoch 44/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06196, s2s_loss 0.06674, 109.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.12336, 0.61 secs\n",
      "\u001b[1m---- Epoch 45/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "^C iteration 44275\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 597, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 508, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 329, in train_model\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 97, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 82, in step_fn\n",
      "    gradient_accumulator.step(batch_loss, model)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 39, in step\n",
      "    self.optimizer.zero_grad()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_compile.py\", line 24, in inner\n",
      "    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 451, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 825, in zero_grad\n",
      "    p.grad = None\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250112_101312_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\" \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 1000 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 2 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "--task_name \"report2negative_facts\" \\\n",
    "--report_to_negative_facts_input_output_jsonl_filepaths \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "--seq2seq_model_name \"bart\" \\\n",
    "--bart_model_name \"facebook/bart-base\" \\\n",
    "--experiment_name \"r2nf\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 1000\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: bart\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_044412_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 4\n",
      "   override_lr: False\n",
      "   task_name: report2negative_facts\n",
      "   experiment_name: r2nf\n",
      "   multitask_name_list: None\n",
      "   task2weight: {}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrased_inputs_jsonl_filepaths: None\n",
      "   chest_imagenome_phrases2labels_filepath: None\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_v2_input_output_jsonl_filepaths: None\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: None\n",
      "   report_nli_input_output_train_jsonl_filepaths: None\n",
      "   report_nli_input_output_val_jsonl_filepaths: None\n",
      "   report_to_negative_facts_input_output_jsonl_filepaths: ['/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl', '/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl']\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   integrated_report_facts_jsonl_filepath: None\n",
      "   interpret_cxr__label_based_predictions_filepath: None\n",
      "   interpret_cxr_challenge_data_dir: None\n",
      "   mimiccxr_integrated_report_nli_data_filepath: None\n",
      "   report_section_to_generate: None\n",
      "   include_public_test_in_train: False\n",
      "   best_k_classes: None\n",
      "   use_sentence2facts_for_nli: False\n",
      "   use_anli: False\n",
      "   use_multinli: False\n",
      "   use_snli: False\n",
      "   use_report_nli: False\n",
      "   use_fact_based_reports_in_mlm: False\n",
      "   use_report_nli_entailment_dataset: False\n",
      "   use_report_nli_paraphrases_dataset: False\n",
      "   use_numeric_templates: False\n",
      "   only_validate_nli: False\n",
      "   nli1_only_on_train: False\n",
      "   nli1_only_on_val: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: facebook/bart-base\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "Loaded 10000 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\n",
      "100%|███████████████████████████████████| 10000/10000 [00:05<00:00, 1941.70it/s]\n",
      "Loaded 139995 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\n",
      "100%|█████████████████████████████████| 139995/139995 [01:05<00:00, 2132.38it/s]\n",
      "Number of abnormal sentences: 106146\n",
      "Number of normal sentences: 29786\n",
      "len(train_dataset_1): 1000000000000000000\n",
      "len(train_dataset_2): 1000000000000000000\n",
      "len(val_dataset_1): 400\n",
      "len(val_dataset_2): 400\n",
      "len(merged_train_dataset): 1000000000000000000\n",
      "len(merged_val_dataset): 800\n",
      "\u001b[1m----- Input/output examples from train_dataset_1:\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mbony structures of the thorax within normal limits. no pulmonary metastases. normal cardiomedastinal silhouette. no evidence of newly appeared opacities. pleural effusions are not large. clearing cardiac decompensation. lucent line does not represent a pneumothorax. no peripheral wedge-shaped parenchymal opacities. nasogastric tube extends to the lower body of the stomach. no effusion. no conclusive evidence for acute pulmonary infiltrates. no acute pulmonary process appreciated\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"bony structures of the thorax within normal limits\": [\"bone fracture\", \"bone lesion\"], \"no pulmonary metastases\": [\"pulmonary metastases\"], \"normal cardiomedastinal silhouette\": [\"mediastinal widening\", \"mediastinal lymphadenopathy\"], \"no evidence of newly appeared opacities\": [\"new pulmonary opacity\", \"newly appeared opacities\"], \"pleural effusions are not large\": [\"large pleural effusion\"], \"clearing cardiac decompensation\": [\"cardiac decompensation\"], \"lucent line does not represent a pneumothorax\": [\"pneumothorax\"], \"no peripheral wedge-shaped parenchymal opacities\": [\"pulmonary embolism\"], \"nasogastric tube extends to the lower body of the stomach\": [\"nasogastric tube malposition in the chest\", \"nasogastric tube malposition\"], \"no effusion\": [\"hemothorax\", \"lung effusion\"], \"no conclusive evidence for acute pulmonary infiltrates\": [\"acute pulmonary infiltrates\"], \"no acute pulmonary process appreciated\": [\"acute pneumonia\", \"acute respiratory distress syndrome\"]}\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mdecreased extent of pre-existing opacity in the left lung. vessels not felt to represent a pneumothorax\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"decreased extent of pre-existing opacity in the left lung\": [\"left lung opacity\", \"left lung consolidation\"], \"vessels not felt to represent a pneumothorax\": [\"pneumothorax\"]}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35minterstitial/airspace pulmonary edema with a lower lobe predominance. monitoring devices are essentially unchanged. bilateral air inclusions in the cervical soft tissues. prosthetic mitral valve in unchanged position. no substantial change in the right pneumothorax. confluent opacity at both lung bases. unchanged right basilar opacification.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno focal area of increased radiodensity. no acute bony abnormalities on the PA radiograph. improved ventilation of the left lung parenchyma\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no focal area of increased radiodensity\": [\"pneumonia\"], \"no acute bony abnormalities on the PA radiograph\": [\"acute bony abnormality\"], \"improved ventilation of the left lung parenchyma\": [\"left lung parenchyma collapse\", \"left lung parenchyma atelectasis\"]}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35moverlying soft tissues in the left basilar. abrupt superior and medial turn of left PICC at the level of the right tracheobronchial angle. right mid lung hemorrhage. probable evolving postoperative hematoma. new clavicular lesion. drains in the right upper quadrant. minimal interstitial markings. possible developing focus of infection in the left mid-zone. Port-A-Cath catheter tip at the level of mid SVC. recurrent atelectasis. Left PICC unchanged in position.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "\u001b[1m----- Input/output example from train_dataset_2:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno pleural effusion. clear lungs. stable cardiac contours. no evidence of acute cardiopulmonary disease. no pneumothorax. probable coronary artery bypass graft surgery. status post sternotomy. mild relative elevation of the left hemidiaphragm. low lung volumes. stable mediastinal contours. stable hilar contours.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no pleural effusion\": [\"pleural effusion\"], \"no pneumothorax\": [\"pneumothorax\"], \"clear lungs\": [\"lung consolidation\", \"pulmonary edema\"], \"no evidence of acute cardiopulmonary disease\": [\"acute heart failure\", \"acute pulmonary embolism\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_1:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of an acute cardiopulmonary process. enteric tube ends in the stomach. lungs without signs of aspiration. unchanged appearance of lung bases. no abnormal cardiomediastinal contours. side port in the expected location of the gastroesophageal junction. no remaining pneumothorax. pulmonary vascular pattern without evidence of congestion. unremarkable overall mediastinal contour. absence of vascular enlargement. right IJ line in nominal alignment\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no evidence of an acute cardiopulmonary process\": [\"acute respiratory distress syndrome\", \"acute cardiopulmonary disease\"], \"enteric tube ends in the stomach\": [\"enteric tube malposition\", \"esophageal placement of enteric tube\"], \"lungs without signs of aspiration\": [\"pulmonary aspiration\", \"aspiration\"], \"unchanged appearance of lung bases\": [\"lung base abnormality\"], \"no abnormal cardiomediastinal contours\": [\"cardiomediastinal contour abnormality\"], \"side port in the expected location of the gastroesophageal junction\": [\"malposition of gastroesophageal junction side port\"], \"no remaining pneumothorax\": [\"pneumothorax\"], \"pulmonary vascular pattern without evidence of congestion\": [\"pulmonary vascular congestion\"], \"unremarkable overall mediastinal contour\": [\"mediastinal mass\", \"mediastinal widening\"], \"absence of vascular enlargement\": [\"vascular congestion\"], \"right IJ line in nominal alignment\": [\"right internal jugular line malposition\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_2:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mnormal spine. normal ribcage. normal hilar contours. normal mediastinal contours. no pneumonia. normal size of the cardiac silhouette. no pneumothorax. normal lung volumes. no pleural effusions. no pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"normal lung volumes\": [\"lung volume abnormality\"], \"normal size of the cardiac silhouette\": [\"cardiomegaly\"], \"normal hilar contours\": [\"hilar lymphadenopathy\"], \"normal mediastinal contours\": [\"mediastinal mass\"], \"no pneumonia\": [\"pneumonia\"], \"no pulmonary edema\": [\"pulmonary edema\"], \"no pleural effusions\": [\"pleural effusion\"], \"no pneumothorax\": [\"pneumothorax\"], \"normal spine\": [\"spinal fracture\", \"spinal lesion\"], \"normal ribcage\": [\"rib fracture\", \"rib lesion\"]}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_trainer.name =  report2negative_facts(r2nf)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_055632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_055632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_33_s2s_loss=0.9311.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_044412_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/checkpoint_33_s2s_loss=0.9311.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_055632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.10099, s2s_loss 0.08916, 119.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07948, 1.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.9255.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.06515, s2s_loss 0.08902, 118.75 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07731, 1.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.9272.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 0.06953, s2s_loss 0.08795, 119.14 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07909, 1.74 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.10307, s2s_loss 0.10141, 119.78 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.09256, 1.66 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 0.11443, s2s_loss 0.09408, 118.72 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.08269, 1.70 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.07976, s2s_loss 0.08760, 120.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07483, 1.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.9293.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.09955, s2s_loss 0.08309, 120.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07044, 1.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.9331.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.06339, s2s_loss 0.08145, 120.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07275, 1.71 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.07668, s2s_loss 0.08100, 120.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07002, 1.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.9336.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.08142, s2s_loss 0.08035, 121.31 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07092, 1.68 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06380, s2s_loss 0.07940, 120.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07146, 1.77 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07366, s2s_loss 0.08014, 121.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06986, 1.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_s2s_loss=0.9338.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.09939, s2s_loss 0.09759, 121.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.08751, 1.70 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.12307, s2s_loss 0.08839, 121.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07692, 1.79 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.06119, s2s_loss 0.07944, 120.22 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07006, 1.71 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.07925, s2s_loss 0.07864, 121.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06795, 1.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_s2s_loss=0.9354.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07110, s2s_loss 0.07793, 120.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06591, 1.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_s2s_loss=0.9371.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.08411, s2s_loss 0.07727, 121.17 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06679, 1.73 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.08804, s2s_loss 0.07719, 120.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06830, 1.67 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06995, s2s_loss 0.07718, 120.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06610, 1.71 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.10451, s2s_loss 0.09429, 121.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.08166, 1.71 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.10456, s2s_loss 0.08798, 121.52 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06982, 1.74 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.06350, s2s_loss 0.07899, 121.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06533, 1.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_s2s_loss=0.9375.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.09248, s2s_loss 0.07382, 120.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06317, 1.71 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_s2s_loss=0.9396.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.04645, s2s_loss 0.07122, 120.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06272, 1.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_s2s_loss=0.9402.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.07482, s2s_loss 0.07182, 121.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06555, 1.71 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05681, s2s_loss 0.07244, 120.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06307, 1.68 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05966, s2s_loss 0.07416, 120.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06313, 1.73 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.08645, s2s_loss 0.08991, 121.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.08005, 1.70 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.08448, s2s_loss 0.08356, 121.59 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07032, 1.72 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.09088, s2s_loss 0.07667, 121.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06319, 1.72 secs\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.07409, s2s_loss 0.07119, 120.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06200, 1.71 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_s2s_loss=0.9408.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.04895, s2s_loss 0.07093, 119.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06118, 1.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_s2s_loss=0.9415.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.06796, s2s_loss 0.06949, 120.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06202, 1.65 secs\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.11116, s2s_loss 0.06941, 121.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06275, 1.67 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05577, s2s_loss 0.06826, 120.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06144, 1.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_36_s2s_loss=0.9415.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.08237, s2s_loss 0.08584, 121.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07563, 1.71 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.08727, s2s_loss 0.08038, 120.03 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06893, 1.66 secs\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.08521, s2s_loss 0.07420, 121.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06046, 1.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_39_s2s_loss=0.9418.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.08896, s2s_loss 0.06861, 120.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05942, 1.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_s2s_loss=0.9431.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.09554, s2s_loss 0.06911, 120.62 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06129, 1.71 secs\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.07035, s2s_loss 0.06778, 120.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05756, 1.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_42_s2s_loss=0.9447.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.07603, s2s_loss 0.06749, 120.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05695, 1.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_43_s2s_loss=0.9452.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05964, s2s_loss 0.06860, 119.41 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05709, 1.72 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06855, s2s_loss 0.08309, 119.96 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07475, 1.72 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.07798, s2s_loss 0.07739, 121.26 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06601, 1.69 secs\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.08729, s2s_loss 0.06944, 121.05 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05951, 1.72 secs\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.06380, s2s_loss 0.06602, 119.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05550, 1.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_s2s_loss=0.9465.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.05602, s2s_loss 0.06629, 120.07 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05959, 1.66 secs\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.08341, s2s_loss 0.06546, 121.24 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05891, 1.66 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05878, s2s_loss 0.06560, 120.95 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05793, 1.71 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07377, s2s_loss 0.06647, 120.29 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05683, 1.69 secs\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06981, s2s_loss 0.08101, 118.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07300, 1.73 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.09510, s2s_loss 0.07734, 120.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06063, 1.72 secs\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.08680, s2s_loss 0.06987, 121.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05677, 1.73 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.07670, s2s_loss 0.06295, 120.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05908, 1.67 secs\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.05874, s2s_loss 0.06342, 120.47 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05604, 1.69 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.07731, s2s_loss 0.06218, 119.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05403, 1.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_58_s2s_loss=0.9480.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05894, s2s_loss 0.06259, 119.94 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05675, 1.70 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.03899, s2s_loss 0.06538, 119.89 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05675, 1.72 secs\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06700, s2s_loss 0.07834, 120.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.07056, 1.74 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.07758, s2s_loss 0.07459, 120.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06170, 1.65 secs\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.08764, s2s_loss 0.06742, 120.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05734, 1.69 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.09102, s2s_loss 0.06100, 120.74 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05519, 1.75 secs\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.06656, s2s_loss 0.06255, 120.85 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05405, 1.71 secs\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.04473, s2s_loss 0.06172, 120.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05319, 1.68 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_66_s2s_loss=0.9487.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.04732, s2s_loss 0.06028, 120.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05297, 1.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_67_s2s_loss=0.9490.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06820, s2s_loss 0.06130, 120.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05423, 1.67 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06455, s2s_loss 0.07636, 120.49 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06867, 1.72 secs\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.07212, s2s_loss 0.07153, 120.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05782, 1.72 secs\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.08447, s2s_loss 0.06634, 120.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05464, 1.72 secs\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.06527, s2s_loss 0.06024, 120.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05341, 1.70 secs\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.05861, s2s_loss 0.06148, 119.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05322, 1.72 secs\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.05703, s2s_loss 0.06005, 120.16 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05154, 1.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_74_s2s_loss=0.9502.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05184, s2s_loss 0.05917, 120.56 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05286, 1.69 secs\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05478, s2s_loss 0.06146, 121.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05124, 1.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_76_s2s_loss=0.9503.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.04381, s2s_loss 0.07410, 120.71 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06958, 1.72 secs\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.07374, s2s_loss 0.06962, 122.15 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06061, 1.72 secs\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.05878, s2s_loss 0.06303, 122.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05455, 1.69 secs\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.03830, s2s_loss 0.05846, 121.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05222, 1.67 secs\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.05779, s2s_loss 0.05979, 121.71 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.05085, 1.73 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_81_s2s_loss=0.9508.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.04446, s2s_loss 0.05885, 120.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05222, 1.68 secs\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06736, s2s_loss 0.05834, 121.90 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04970, 1.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_83_s2s_loss=0.9519.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05166, s2s_loss 0.05999, 121.35 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05112, 1.72 secs\n",
      "\u001b[1m---- Epoch 85/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.05980, s2s_loss 0.07271, 120.81 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06263, 1.75 secs\n",
      "\u001b[1m---- Epoch 86/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "   iteration 85675\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_044412_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 1000 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 4 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "--task_name \"report2negative_facts\" \\\n",
    "--report_to_negative_facts_input_output_jsonl_filepaths \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "--seq2seq_model_name \"bart\" \\\n",
    "--bart_model_name \"facebook/bart-base\" \\\n",
    "--experiment_name \"r2nf\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 1000\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: bart\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_055632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 4\n",
      "   override_lr: False\n",
      "   task_name: report2negative_facts\n",
      "   experiment_name: r2nf\n",
      "   multitask_name_list: None\n",
      "   task2weight: {}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrased_inputs_jsonl_filepaths: None\n",
      "   chest_imagenome_phrases2labels_filepath: None\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_v2_input_output_jsonl_filepaths: None\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: None\n",
      "   report_nli_input_output_train_jsonl_filepaths: None\n",
      "   report_nli_input_output_val_jsonl_filepaths: None\n",
      "   report_to_negative_facts_input_output_jsonl_filepaths: ['/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl', '/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl']\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   integrated_report_facts_jsonl_filepath: None\n",
      "   interpret_cxr__label_based_predictions_filepath: None\n",
      "   interpret_cxr_challenge_data_dir: None\n",
      "   mimiccxr_integrated_report_nli_data_filepath: None\n",
      "   report_section_to_generate: None\n",
      "   include_public_test_in_train: False\n",
      "   best_k_classes: None\n",
      "   use_sentence2facts_for_nli: False\n",
      "   use_anli: False\n",
      "   use_multinli: False\n",
      "   use_snli: False\n",
      "   use_report_nli: False\n",
      "   use_fact_based_reports_in_mlm: False\n",
      "   use_report_nli_entailment_dataset: False\n",
      "   use_report_nli_paraphrases_dataset: False\n",
      "   use_numeric_templates: False\n",
      "   only_validate_nli: False\n",
      "   nli1_only_on_train: False\n",
      "   nli1_only_on_val: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: facebook/bart-base\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "Loaded 10000 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\n",
      "100%|███████████████████████████████████| 10000/10000 [00:04<00:00, 2326.13it/s]\n",
      "Loaded 139995 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\n",
      "100%|█████████████████████████████████| 139995/139995 [00:47<00:00, 2951.28it/s]\n",
      "Number of abnormal sentences: 106146\n",
      "Number of normal sentences: 29786\n",
      "len(train_dataset_1): 1000000000000000000\n",
      "len(train_dataset_2): 1000000000000000000\n",
      "len(train_dataset_3): 1000000000000000000\n",
      "len(val_dataset_1): 400\n",
      "len(val_dataset_2): 400\n",
      "len(val_dataset_3): 400\n",
      "len(merged_train_dataset): 1000000000000000000\n",
      "len(merged_val_dataset): 1200\n",
      "\u001b[1m----- Input/output examples from train_dataset_1:\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mopacification in the region of the lingula is not appreciated. soft tissue air collection on the left has completely resolved. no presence of left basal pneumonia. heart probably not substantially dilated. unremarkable upper aerated portion of the right lung\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"opacification in the region of the lingula is not appreciated\": [\"lingula opacification\"], \"soft tissue air collection on the left has completely resolved\": [\"soft tissue air collection on the left\"], \"no presence of left basal pneumonia\": [\"left basal pneumonia\"], \"heart probably not substantially dilated\": [\"significant cardiac dilation\"], \"unremarkable upper aerated portion of the right lung\": [\"right lung mass\", \"right lung consolidation\"]}\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno underlying atelectasis in the left lower lobe. no evidence of abnormalities regarding density of the mediastinal structures. no evidence of a mechanical valve. tracheostomy tube without evidence of pneumomediastinum. correct tube position. no evidence of new aspiration. lungs without definitive evidence of pneumothorax. clear upper two-thirds of the lungs. unremarkable appearance of widely widened thoracic aorta. displacement of the trachea not appreciated. absence of edema\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no underlying atelectasis in the left lower lobe\": [\"left lower lobe atelectasis\"], \"no evidence of abnormalities regarding density of the mediastinal structures\": [\"mediastinal lymphadenopathy\", \"mediastinal density abnormality\"], \"no evidence of a mechanical valve\": [\"mechanical valve presence\"], \"tracheostomy tube without evidence of pneumomediastinum\": [\"pneumomediastinum\"], \"correct tube position\": [\"nasogastric tube malposition\"], \"no evidence of new aspiration\": [\"new aspiration pneumonia\"], \"lungs without definitive evidence of pneumothorax\": [\"pneumothorax\"], \"clear upper two-thirds of the lungs\": [\"opacification in the upper two-thirds of the lungs\", \"pulmonary edema in the upper two-thirds of the lungs\"], \"unremarkable appearance of widely widened thoracic aorta\": [\"thoracic aortic dissection\", \"thoracic aortic aneurysm\"], \"displacement of the trachea not appreciated\": [\"tracheal displacement\"], \"absence of edema\": [\"pulmonary edema\"]}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35msubtle lucency at the right costophrenic angle. collection of gas anteriorly. engorgement of the hila. tortuosity of thoracic aorta. mild interval increase in interstitial pulmonary edema. confirm resolution. severe consolidation in both lower lobes. loop in the wire projecting over the heart. possibility of aspiration in the retrocardiac region. post pyloric feeding tube stable.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "\u001b[1m----- Input/output examples from train_dataset_2:\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mDobbhoff tube in appropriate position within the stomach. multiple old bilateral rib fractures.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 3\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mRight PICC tip at the upper right atrium. no radiographic evidence for latent pulmonary TB\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no radiographic evidence for latent pulmonary TB\": [\"latent pulmonary tuberculosis\"]}\u001b[0m\n",
      "mode: 3\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno pneumothorax following removal of the left pigtail catheter. lucencies at left lung apex.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no pneumothorax following removal of the left pigtail catheter\": [\"post-procedural pneumothorax\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from train_dataset_3:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mconstant appearance of the mediastinum. no signs of significant left atrial enlargement\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"constant appearance of the mediastinum\": [\"mediastinal shift\"], \"no signs of significant left atrial enlargement\": [\"significant left atrial enlargement\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_1:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno new parenchymal pulmonary abnormalities. unchanged pulmonary findings. mild elevation of the left hemidiaphragm unchanged. right basilar atelectasis has essentially cleared. platelike atelectasis at the left lung bases has almost completely resolved. well-defined right costophrenic angle. unremarkable mediastinal clips. unchanged appearance of the chest\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no new parenchymal pulmonary abnormalities\": [\"new parenchymal pulmonary abnormality\"], \"unchanged pulmonary findings\": [\"pulmonary abnormality\"], \"mild elevation of the left hemidiaphragm unchanged\": [\"diaphragmatic paralysis\"], \"right basilar atelectasis has essentially cleared\": [\"right basilar atelectasis\"], \"platelike atelectasis at the left lung bases has almost completely resolved\": [\"left lung base atelectasis\"], \"well-defined right costophrenic angle\": [\"right costophrenic angle blunting\", \"right pleural effusion\"], \"unremarkable mediastinal clips\": [\"mediastinal clip complication\", \"mediastinal clip infection\"], \"unchanged appearance of the chest\": [\"acute chest pathology\", \"new lung abnormality\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_2:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mstable widespread parenchymal opacities. no conventional radiographic evidence of pulmonary metastasis\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"stable widespread parenchymal opacities\": [\"progression of widespread parenchymal opacities\", \"new parenchymal opacities\"], \"no conventional radiographic evidence of pulmonary metastasis\": [\"pulmonary metastasis\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_3:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno relevant change. less dense right paramediastinal structures. less dense right hilar structures. no evidence for acute pneumonia. no pleural effusions. normal size of the cardiac silhouette. unchanged right pectoral Port-A-Cath\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no evidence for acute pneumonia\": [\"acute pneumonia\"], \"no pleural effusions\": [\"pleural effusion\"], \"normal size of the cardiac silhouette\": [\"cardiomegaly\"]}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_trainer.name =  report2negative_facts(r2nf)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_103036_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_103036_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_100_s2s_loss=0.9530.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_055632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/checkpoint_100_s2s_loss=0.9530.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_103036_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.07661, s2s_loss 0.06226, 113.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06832, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_s2s_loss=0.9366.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.04052, s2s_loss 0.06099, 113.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06119, 2.14 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_s2s_loss=0.9424.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 0.05207, s2s_loss 0.06052, 113.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05780, 2.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_s2s_loss=0.9451.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.04409, s2s_loss 0.07486, 112.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06757, 2.07 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 0.07068, s2s_loss 0.06829, 112.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05918, 2.10 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.05974, s2s_loss 0.06221, 112.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05580, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_s2s_loss=0.9466.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.06064, s2s_loss 0.06017, 112.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05257, 2.12 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_s2s_loss=0.9494.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.04625, s2s_loss 0.05856, 113.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05212, 2.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_s2s_loss=0.9499.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.03636, s2s_loss 0.05792, 113.82 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05213, 2.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_s2s_loss=0.9499.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.06020, s2s_loss 0.05704, 113.60 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05095, 2.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_s2s_loss=0.9510.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.06066, s2s_loss 0.05671, 113.08 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05296, 2.12 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.04928, s2s_loss 0.05623, 112.99 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05213, 2.14 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06427, s2s_loss 0.07036, 113.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06138, 2.17 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.07981, s2s_loss 0.06454, 115.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05302, 2.12 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.08216, s2s_loss 0.05729, 114.39 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05372, 2.21 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.05225, s2s_loss 0.05581, 115.18 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05313, 2.23 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.04729, s2s_loss 0.05586, 115.83 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04794, 2.18 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_s2s_loss=0.9535.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.03276, s2s_loss 0.05583, 115.45 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04941, 2.21 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05368, s2s_loss 0.05566, 116.44 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04811, 2.14 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.06894, s2s_loss 0.05587, 115.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04923, 2.19 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.06544, s2s_loss 0.07037, 115.98 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06463, 2.22 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.05896, s2s_loss 0.06537, 117.04 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05398, 2.22 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.03775, s2s_loss 0.05836, 115.84 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04995, 2.17 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.05242, s2s_loss 0.05147, 118.68 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04714, 2.54 secs\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05908, s2s_loss 0.05137, 119.63 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04363, 2.26 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_43_s2s_loss=0.9575.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.04351, s2s_loss 0.05235, 117.13 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04581, 2.50 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.07026, s2s_loss 0.06399, 117.25 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05841, 2.37 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.05322, s2s_loss 0.06055, 118.30 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05071, 2.40 secs\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.05760, s2s_loss 0.05406, 117.23 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04895, 2.51 secs\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.05789, s2s_loss 0.05018, 117.93 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04815, 2.46 secs\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.04004, s2s_loss 0.05209, 120.55 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04304, 2.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_s2s_loss=0.9579.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.05382, s2s_loss 0.05006, 119.86 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04354, 2.69 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.03994, s2s_loss 0.05138, 121.53 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04520, 2.35 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.04904, s2s_loss 0.05314, 120.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04433, 2.70 secs\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.07236, s2s_loss 0.06401, 119.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.06004, 2.39 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.05970, s2s_loss 0.06166, 118.55 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2s_loss 0.04921, 2.59 secs\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.04592, s2s_loss 0.05466, 124.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04606, 2.70 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.04523, s2s_loss 0.04956, 119.87 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04254, 2.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_56_s2s_loss=0.9586.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.03201, s2s_loss 0.04933, 120.77 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04559, 2.41 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.05020, s2s_loss 0.04875, 120.34 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04201, 2.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_58_s2s_loss=0.9591.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.05417, s2s_loss 0.04923, 117.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04428, 2.75 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05528, s2s_loss 0.05167, 119.65 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04386, 2.25 secs\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.07690, s2s_loss 0.06263, 121.66 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05547, 2.22 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.06974, s2s_loss 0.06213, 120.11 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04935, 2.27 secs\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.07412, s2s_loss 0.05491, 120.54 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04650, 2.30 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.04653, s2s_loss 0.04915, 120.02 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04608, 2.45 secs\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.05227, s2s_loss 0.05085, 120.00 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04366, 3.85 secs\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.05302, s2s_loss 0.04888, 122.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04474, 2.51 secs\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.02630, s2s_loss 0.04840, 120.21 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04293, 2.46 secs\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05950, s2s_loss 0.04925, 119.27 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04333, 2.64 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.04658, s2s_loss 0.06177, 120.12 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05899, 2.41 secs\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.06886, s2s_loss 0.05961, 119.32 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04694, 2.50 secs\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.04639, s2s_loss 0.05395, 121.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04588, 2.35 secs\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.04311, s2s_loss 0.04905, 119.67 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04403, 2.48 secs\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.04890, s2s_loss 0.05042, 120.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04105, 2.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_73_s2s_loss=0.9597.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.03292, s2s_loss 0.04870, 120.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04251, 2.37 secs\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.04609, s2s_loss 0.04830, 119.64 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04126, 2.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_75_s2s_loss=0.9597.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.05294, s2s_loss 0.05003, 118.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04061, 2.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_76_s2s_loss=0.9601.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.05117, s2s_loss 0.06007, 119.91 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05734, 2.47 secs\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.06051, s2s_loss 0.05795, 118.70 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04726, 2.26 secs\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.06204, s2s_loss 0.05184, 121.01 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04505, 2.49 secs\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.03566, s2s_loss 0.04739, 118.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04150, 2.66 secs\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.04268, s2s_loss 0.04932, 119.58 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04106, 2.27 secs\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.05804, s2s_loss 0.04811, 118.76 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.03996, 2.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_82_s2s_loss=0.9608.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.04363, s2s_loss 0.04800, 119.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04320, 2.46 secs\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.04389, s2s_loss 0.05074, 116.92 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04488, 2.16 secs\n",
      "\u001b[1m---- Epoch 85/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.05335, s2s_loss 0.05935, 118.40 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.05287, 2.46 secs\n",
      "\u001b[1m---- Epoch 86/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.05022, s2s_loss 0.05855, 117.48 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04662, 2.36 secs\n",
      "\u001b[1m---- Epoch 87/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.10904, s2s_loss 0.05238, 119.36 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04254, 2.24 secs\n",
      "\u001b[1m---- Epoch 88/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.04809, s2s_loss 0.04684, 119.09 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04352, 2.32 secs\n",
      "\u001b[1m---- Epoch 89/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.07325, s2s_loss 0.04766, 118.38 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04302, 2.36 secs\n",
      "\u001b[1m---- Epoch 90/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.04565, s2s_loss 0.04644, 119.61 secs\n",
      "(2) Validation stage ...\n",
      "s2s_loss 0.04370, 2.32 secs\n",
      "\u001b[1m---- Epoch 91/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "^C iteration 90475\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 597, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 508, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_seq2seq.py\", line 329, in train_model\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 97, in step_fn_wrapper\n",
      "    output = step_fn(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/seq2seq.py\", line 63, in step_fn\n",
      "    model_output = model(**model_kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/seq2seq.py\", line 41, in forward\n",
      "    output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1742, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1610, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1163, in forward\n",
      "    embed_pos = self.embed_positions(input)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 129, in forward\n",
      "    return super().forward(positions + self.offset)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 163, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/functional.py\", line 2264, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_055632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 1000 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 4 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "--task_name \"report2negative_facts\" \\\n",
    "--report_to_negative_facts_input_output_jsonl_filepaths \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "--seq2seq_model_name \"bart\" \\\n",
    "--bart_model_name \"facebook/bart-base\" \\\n",
    "--experiment_name \"r2nf\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 1000\n",
      "   batch_size: 20\n",
      "   checkpoint_folder: None\n",
      "   seq2seq_model_name: bart\n",
      "   t5_model_name: t5-small\n",
      "   bart_model_name: facebook/bart-base\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_103036_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 4\n",
      "   override_lr: False\n",
      "   task_name: report2negative_facts\n",
      "   experiment_name: r2nf\n",
      "   multitask_name_list: None\n",
      "   task2weight: {}\n",
      "   val_size: 200\n",
      "   mlm_min_token_count: 20\n",
      "   mlm_masking_fraction: 0.15\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrased_inputs_jsonl_filepaths: None\n",
      "   chest_imagenome_phrases2labels_filepath: None\n",
      "   sentence_to_facts_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_input_output_jsonl_filepaths: None\n",
      "   fact_to_metadata_v2_input_output_jsonl_filepaths: None\n",
      "   fact_to_comparison_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_obs_input_output_jsonl_filepaths: None\n",
      "   chest_imagenome_anatloc_input_output_jsonl_filepaths: None\n",
      "   report_nli_input_output_train_jsonl_filepaths: None\n",
      "   report_nli_input_output_val_jsonl_filepaths: None\n",
      "   report_to_negative_facts_input_output_jsonl_filepaths: ['/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl', '/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl', '/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_sentence_to_negative_facts.jsonl', '/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o-mini_sentence_to_negative_facts.jsonl']\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   integrated_report_facts_jsonl_filepath: None\n",
      "   interpret_cxr__label_based_predictions_filepath: None\n",
      "   interpret_cxr_challenge_data_dir: None\n",
      "   mimiccxr_integrated_report_nli_data_filepath: None\n",
      "   report_section_to_generate: None\n",
      "   include_public_test_in_train: False\n",
      "   best_k_classes: None\n",
      "   use_sentence2facts_for_nli: False\n",
      "   use_anli: False\n",
      "   use_multinli: False\n",
      "   use_snli: False\n",
      "   use_report_nli: False\n",
      "   use_fact_based_reports_in_mlm: False\n",
      "   use_report_nli_entailment_dataset: False\n",
      "   use_report_nli_paraphrases_dataset: False\n",
      "   use_numeric_templates: False\n",
      "   only_validate_nli: False\n",
      "   nli1_only_on_train: False\n",
      "   nli1_only_on_val: False\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of Seq2SeqModel ...\u001b[0m\n",
      "Seq2Seq model:\n",
      "  model_name: facebook/bart-base\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4, max_grad_norm = None\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating Seq2SeqTrainer ...\u001b[0m\n",
      "Loaded 10000 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\n",
      "100%|███████████████████████████████████| 10000/10000 [00:04<00:00, 2362.42it/s]\n",
      "Loaded 139995 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\n",
      "100%|█████████████████████████████████| 139995/139995 [00:47<00:00, 2961.34it/s]\n",
      "Loaded 50436 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_sentence_to_negative_facts.jsonl\n",
      "100%|██████████████████████████████████| 50436/50436 [00:01<00:00, 34714.03it/s]\n",
      "Loaded 348733 input/output pairs from /mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o-mini_sentence_to_negative_facts.jsonl\n",
      "100%|████████████████████████████████| 348733/348733 [00:10<00:00, 33198.61it/s]\n",
      "Number of abnormal sentences: 463962\n",
      "Number of normal sentences: 50588\n",
      "len(train_dataset_1): 1000000000000000000\n",
      "len(train_dataset_2): 1000000000000000000\n",
      "len(train_dataset_3): 1000000000000000000\n",
      "len(val_dataset_1): 500\n",
      "len(val_dataset_2): 500\n",
      "len(val_dataset_3): 500\n",
      "len(merged_train_dataset): 1000000000000000000\n",
      "len(merged_val_dataset): 1500\n",
      "\u001b[1m----- Input/output examples from train_dataset_1:\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThe patient is status post left thoracoplasty.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mUnchanged appearance of previously described NG tube passing through large-sized hiatal hernia before entering abdominal area. difficulty in assessing cardiac size due to pleural effusions. unchanged position of right internal jugular introducer. distended colonic loops in the upper abdomen\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mpost-radiation abnormalities in the left upper lobe area. Stable right and 7th rib deformities. Right trans subclavian right atrial and right ventricular pacer leads are continuous from the right pectoral generator. early infiltrate in the right lower lobe. Consolidation in the right lower lung is improving. appearance representing pulmonary edema. clinical question of evaluating for free intraperitoneal air. unchanged diffuse bilateral pulmonary opacification. Scattered areas of mild linear atelectasis. Lingular pneumonia with evidence of volume loss concerning for an obstructive pneumonia. right infradiaphragmatic air. prominent visibility of the azygos vein in the right tracheobronchial angulation\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 3\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mthoracic aorta without evidence of focal aneurysm. upper lung zones without vascular congestion. endotracheal tube seen in place.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"thoracic aorta without evidence of focal aneurysm\": [\"thoracic aorta aneurysm\"], \"upper lung zones without vascular congestion\": [\"upper lung zone vascular congestion\"]}\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mgrossly unchanged cardio mediastinal contours. edema pattern resolved. no evidence of obvious hematoma. resolution of previously regressing small right-sided apical pneumothorax. interval resolution of the bilateral pleural effusions. significant pleural effusion is unlikely. no acute vascular congestive pattern. orogastric tube position. no new metastatic disease\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"grossly unchanged cardio mediastinal contours\": [\"mediastinal mass\", \"cardiomegaly\"], \"edema pattern resolved\": [\"pulmonary edema\", \"interstitial edema\"], \"no evidence of obvious hematoma\": [\"hematoma\", \"obvious hematoma\"], \"resolution of previously regressing small right-sided apical pneumothorax\": [\"right-sided apical pneumothorax\"], \"interval resolution of the bilateral pleural effusions\": [\"bilateral pleural effusion\"], \"significant pleural effusion is unlikely\": [\"significant pleural effusion\"], \"no acute vascular congestive pattern\": [\"acute vascular congestion\"], \"orogastric tube position\": [\"diaphragmatic hernia\", \"stomach displacement\"], \"no new metastatic disease\": [\"new metastatic disease\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output examples from train_dataset_2:\u001b[0m\n",
      "mode: 3\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mModerate right hydro pneumothorax entirely unchanged since :19 on , including relative volumes of air and fluid in the right pleural space. stable lung parenchyma\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"stable lung parenchyma\": [\"new lung infiltrate\", \"acute lung parenchymal disease\"]}\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno evidence of scarring. stable position of the cervical fixations\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no evidence of scarring\": [\"lung scarring\"], \"stable position of the cervical fixations\": [\"cervical fixation malfunction\", \"cervical fixation displacement\"]}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mproximal weighted portion of the Dobbhoff at the GE junction. ET tube tip now is 4.8 cm above the crania in a standard position.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35munchanged left perihilar mass. Adjacent to the right ninth anterior rib is bony callus from a healing fracture.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mTAVR is in place. persistent collapsed right lower lobe\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 1\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mStatus post right lung biopsy.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "mode: 3\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mINCREASE IN RIGHT PERIHILAR CONSOLIDATION SHOULD BE FOLLOWED WITH FOR POSSIBLE DEVELOPING ATELECTASIS, PNEUMONIA, OR EVEN A PERIHILAR HEMATOMA. left lung is very grossly clear\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"left lung is very grossly clear\": [\"left lung consolidation\", \"left lung opacity\"]}\u001b[0m\n",
      "mode: 3\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno suggestive of a leak. Note is made of erosion of the distal right clavicle, unchanged from .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no suggestive of a leak\": [\"air leak\"]}\u001b[0m\n",
      "mode: 2\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mno signs of acute intrathoracic process\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"no signs of acute intrathoracic process\": [\"acute intrathoracic abnormality\", \"acute intrathoracic hemorrhage\"]}\u001b[0m\n",
      "mode: 3\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mThere is small left apical pneumothorax noted after biopsy, restricted to the lung apex. unremarkable alignment of the sternal wires\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"unremarkable alignment of the sternal wires\": [\"sternal wire displacement\", \"sternal wire misalignment\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from train_dataset_3:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mTop normal heart size with no pulmonary edema.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"normal heart size\": [\"cardiomegaly\"], \"no pulmonary edema\": [\"pulmonary edema\"]}\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mincreased left pleural effusion. interval insertion of a chest tube on the left. obscured cardiomediastinal silhouette. no other complication. right Port-A-Cath in unchanged position. no pneumothorax. clear right lung. unremarkable visualized osseous structures. interval removal of the nasogastric tube. increase in large left pleural effusion\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"clear right lung\": [\"right lung consolidation\", \"right lung mass\"], \"no pneumothorax\": [\"pneumothorax\"], \"no other complication\": [\"surgical complication\"], \"unremarkable visualized osseous structures\": [\"bone fracture\", \"bone lesion\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_1:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mlarge right pleural effusion has increased considerably. intact bilateral acromioclavicular joints. pleural effusions without pneumothorax. Otherwise, the radiograph is unchanged as compared to the previous image, with the exception that the left lung base is minimally better ventilated. This may be slightly more prominent than the study of more than years previously, but appears to be quite dense and sharply round and most likely represents a granulomatous process. little overall change in appearance of the lungs. Left-sided AICD/pacemaker device is again noted with leads in unchanged positions. assessment of position of a Dobbhoff tube. absence of displacement in the right hemithorax. right upper abdomen is otherwise unremarkable. Improved right upper lobe pneumonia, compared to .\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{\"intact bilateral acromioclavicular joints\": [\"acromioclavicular joint arthritis\", \"acromioclavicular joint dislocation\"], \"pleural effusions without pneumothorax\": [\"pneumothorax\"], \"little overall change in appearance of the lungs\": [\"new pleural effusion\", \"new lung consolidation\"], \"absence of displacement in the right hemithorax\": [\"right hemithorax displacement\"], \"right upper abdomen is otherwise unremarkable\": [\"right upper abdominal abnormality\"]}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_2:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mmildly convex contour to the left mid mediastinum. Left IJ central line tip at the level of the cavoatrial junction.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n",
      "\u001b[1m----- Input/output example from val_dataset_3:\u001b[0m\n",
      "\u001b[1mInput:\u001b[0m\n",
      "\u001b[1m\u001b[35mTiny left pneumothorax is clinically insignificant.\u001b[0m\n",
      "\u001b[1mOutput:\u001b[0m\n",
      "\u001b[1m\u001b[35m{}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_trainer.name =  report2negative_facts(r2nf)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250114_104632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250114_104632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_82_s2s_loss=0.9608.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_103036_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/checkpoint_82_s2s_loss=0.9608.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250114_104632_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "   iteration 500\r"
     ]
    }
   ],
   "source": [
    "!python ../train_seq2seq.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/seq2seq/20250113_103036_report2negative_facts(r2nf)_Seq2Seq(facebook-bart-base)\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 1000 \\\n",
    "--batch_size 20 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 4 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\" \\\n",
    "--task_name \"report2negative_facts\" \\\n",
    "--report_to_negative_facts_input_output_jsonl_filepaths \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-1106-preview_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_fact_based_report_to_negative_facts.jsonl\" \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o_sentence_to_negative_facts.jsonl\" \\\n",
    "\"/mnt/data/pamessina_folder_backup_15_10_24/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4o-mini_sentence_to_negative_facts.jsonl\" \\\n",
    "--seq2seq_model_name \"bart\" \\\n",
    "--bart_model_name \"facebook/bart-base\" \\\n",
    "--experiment_name \"r2nf\" \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
