{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af0dd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 800\n",
      "   batch_size: 13\n",
      "   val_batch_size: 100\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: 74\n",
      "   n_chest_imagenome_anatomical_locations: 38\n",
      "   use_aux_task_hidden_layer: True\n",
      "   aux_task_hidden_layer_size: 512\n",
      "   nli_hidden_layer_size: 128\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_folder_paths: ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231013_073812_MIMIC-CXR(triplets+classif+entcont+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)']\n",
      "   freeze_huggingface_model: False\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   fact_decoder_embed_size: 256\n",
      "   fact_decoder_hidden_size: 256\n",
      "   fact_decoder_nhead: 1\n",
      "   fact_decoder_dim_feedforward: 256\n",
      "   fact_decoder_num_layers: 1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 20\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   sentence_autoencoder_loss_weight: 1.5\n",
      "   triplets_filepath: None\n",
      "   triplet_rule_weights: None\n",
      "   integrated_facts_metadata_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(595880,60579117).improved_comparison(6741113).jsonl\n",
      "   paraphrases_jsonl_filepaths: ['/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl', '/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl']\n",
      "   integrated_chest_imagenome_observations_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_observations(9).pkl\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_anatomical_locations(5).pkl\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   integrated_sentence_facts_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\n",
      "   gpt4_radnli_labels_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\n",
      "   sentences_and_cluster_ids_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\n",
      "   dataset_name: MIMIC-CXR(classif+nli+autoencoder)\n",
      "   triplets_weight: 0\n",
      "   metadata_classification_weight: 0\n",
      "   chest_imagenome_observations_classification_weight: 0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0\n",
      "   nli_weight: 6.0\n",
      "   entcon_weight: 0\n",
      "   radgraph_ner_re_weight: 0\n",
      "   sentence_autoencoder_weight: 3.0\n",
      "   use_anli: True\n",
      "   use_multinli: True\n",
      "   use_snli: True\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "Loading integrated sentence facts from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl...\n",
      "Number of integrated sentence facts: 677694\n",
      "Number of entailment samples added: 1323679\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Source: 0 | Label: 0 -> 21667 (2987.98)\n",
      "Source: 1 | Label: 0 -> 3744 (1672.60)\n",
      "Source: 2 | Label: 0 -> 465 (695.76)\n",
      "Source: 3 | Label: 0 -> 473 (701.58)\n",
      "Source: 4 | Label: 0 -> 93 (279.62)\n",
      "Source: 5 | Label: 0 -> 1323679 (8410.16)\n",
      "Source: 0 | Label: 1 -> 34854 (3435.46)\n",
      "Source: 1 | Label: 1 -> 3743 (1672.44)\n",
      "Source: 2 | Label: 1 -> 465 (695.76)\n",
      "Source: 3 | Label: 1 -> 474 (702.30)\n",
      "Source: 4 | Label: 1 -> 281 (538.25)\n",
      "Source: 0 | Label: 2 -> 90988 (4470.43)\n",
      "Source: 1 | Label: 2 -> 3744 (1672.60)\n",
      "Source: 2 | Label: 2 -> 465 (695.76)\n",
      "Source: 3 | Label: 2 -> 474 (702.30)\n",
      "Source: 4 | Label: 2 -> 106 (304.54)\n",
      "----\n",
      "\u001b[1mBuilding train general domain NLI dataset and dataloader...\u001b[0m\n",
      "Loading ANLI...\n",
      "Number of ANLI R1 samples: 18946\n",
      "Number of ANLI R2 samples: 47460\n",
      "Number of ANLI R3 samples: 102859\n",
      "Loading MultiNLI...\n",
      "Number of MultiNLI train samples: 392702\n",
      "Number of MultiNLI dev_matched samples: 10000\n",
      "Number of MultiNLI dev_mismatched samples: 10000\n",
      "Loading SNLI...\n",
      "Number of SNLI train samples: 550152\n",
      "Number of SNLI dev samples: 10000\n",
      "Number of SNLI test samples: 10000\n",
      "Number of general domain samples: 1150647\n",
      "Number of general domain sentences: 1375757\n",
      "Label: 0 -> 382205\n",
      "Label: 1 -> 397295\n",
      "Label: 2 -> 371147\n",
      "NLI dataset examples:\n",
      "\u001b[1mExample 0:\u001b[0m\n",
      "Premise: No detectable pneumothorax.\n",
      "Hypothesis: no detectable pneumothorax\n",
      "Label: 0\n",
      "\u001b[1mExample 1:\u001b[0m\n",
      "Premise: construction worker working.\n",
      "Hypothesis: A worker is working\n",
      "Label: 0\n",
      "\u001b[1mExample 2:\u001b[0m\n",
      "Premise: Left axillary or breast clips are present.\n",
      "Hypothesis: Left breast lumpectomy changes are noted.\n",
      "Label: 1\n",
      "\u001b[1mExample 3:\u001b[0m\n",
      "Premise: Girl in pink tank top grinds on a rail with a skateboard\n",
      "Hypothesis: A girl in orange shorts is grinding on a rail.\n",
      "Label: 1\n",
      "\u001b[1mExample 4:\u001b[0m\n",
      "Premise: Multiple clips are noted within the upper abdomen.\n",
      "Hypothesis: There are no multiple clips within the upper abdomen.\n",
      "Label: 2\n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RadNLI samples: 480\n",
      "Number of GPT-4 RadNLI labels: 960\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 926\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset and dataloader...\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding sentence autoencoder train/val dataset and dataloader...\u001b[0m\n",
      "Number of sentences: 3882542\n",
      "Number of cluster IDs: 3882542\n",
      "Number of clusters: 1000\n",
      "Number of sentences in largest cluster: 27433\n",
      "Number of sentences in smallest cluster: 440\n",
      "Number of sentences after adding general domain sentences: 5258299\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/fact_decoding_vocab(gendoms=1375757)(116, 1862760546599818235).pkl ...\n",
      "Vocabulary size: 33130\n",
      "Number of tokens in vocab: 33130\n",
      "Loading sentence IDs from cache...\n",
      "Number of validation samples: 5000\n",
      "Number of training samples: 5253299\n",
      "Dataset examples:\n",
      "\u001b[1mExample 0:\u001b[0m\n",
      "Sentence: Right lower lobe opacity has improved consistent with decreased pleural effusion and adjacent atelectasis.\n",
      "Sentence ID: [1, 9384, 23051, 22918, 24737, 20593, 21407, 16021, 32890, 16878, 25926, 18153, 12717, 12298, 13223, 68, 2]\n",
      "Sentence ID to string: <START> Right lower lobe opacity has improved consistent with decreased pleural effusion and adjacent atelectasis . <END>\n",
      "\u001b[1mExample 1:\u001b[0m\n",
      "Sentence: A bald lady plays with a fish.\n",
      "Sentence ID: [1, 677, 13497, 22480, 25899, 32890, 12030, 19375, 68, 2]\n",
      "Sentence ID to string: <START> A bald lady plays with a fish . <END>\n",
      "\u001b[1mExample 2:\u001b[0m\n",
      "Sentence: Compared to the prior study there is no significant interval change in central greater than peripheral alveolar infiltrates.\n",
      "Sentence ID: [1, 2786, 31192, 30969, 26464, 30145, 30997, 22106, 24375, 28996, 21938, 15024, 21417, 14952, 20304, 30960, 25585, 12613, 21606, 68, 2]\n",
      "Sentence ID to string: <START> Compared to the prior study there is no significant interval change in central greater than peripheral alveolar infiltrates . <END>\n",
      "\u001b[1mExample 3:\u001b[0m\n",
      "Sentence: A bald man and a girl are outside.\n",
      "Sentence ID: [1, 677, 13497, 23233, 12717, 12030, 20085, 13000, 24965, 68, 2]\n",
      "Sentence ID to string: <START> A bald man and a girl are outside . <END>\n",
      "\u001b[1mExample 4:\u001b[0m\n",
      "Sentence: Bibasilar opacities, likely atelectasis and pleural effusions, are stable.\n",
      "Sentence ID: [1, 1727, 24736, 55, 22802, 13223, 12717, 25926, 18154, 55, 13000, 29751, 68, 2]\n",
      "Sentence ID to string: <START> Bibasilar opacities , likely atelectasis and pleural effusions , are stable . <END>\n",
      "len(_train_dataloaders) = 2\n",
      "len(_train_weights) = 2\n",
      "_train_weights = [6.0, 3.0]\n",
      "len(_val_dataloaders) = 2\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(classif+nli+autoencoder)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: 74\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: 38\n",
      "  use_aux_task_hidden_layer: True\n",
      "  aux_task_hidden_layer_size: 512\n",
      "  do_nli: True\n",
      "  nli_hidden_layer_size: 128\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "  use_fact_decoder: True\n",
      "  fact_decoder_embed_size: 256\n",
      "  fact_decoder_hidden_size: 256\n",
      "  fact_decoder_nhead: 1\n",
      "  fact_decoder_dim_feedforward: 256\n",
      "  fact_decoder_num_layers: 1\n",
      "  fact_decoder_start_idx: 1\n",
      "  fact_decoder_vocab_size: 33130\n",
      "  fact_decoder_dropout_prob: 0\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': None, 'pretrained_checkpoint_folder_paths': ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231013_073812_MIMIC-CXR(triplets+classif+entcont+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)']}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 20, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231225_051135_MIMIC-CXR(classif+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231225_051135_MIMIC-CXR(classif+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_1_cacc+chf1+chf1+cscc+encc+hscc+nlcc+sass+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9160.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231013_073812_MIMIC-CXR(triplets+classif+entcont+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_1_cacc+chf1+chf1+cscc+encc+hscc+nlcc+sass+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9160.pt\n",
      "Skip loading parameter: fact_decoder_embedding_table.weight, required shape: torch.Size([33130, 256]), loaded shape: torch.Size([15773, 256])\n",
      "Skip loading parameter: fact_decoder.embedding_table.weight, required shape: torch.Size([33130, 256]), loaded shape: torch.Size([15773, 256])\n",
      "Skip loading parameter: fact_decoder.W_vocab.weight, required shape: torch.Size([33130, 256]), loaded shape: torch.Size([15773, 256])\n",
      "Skip loading parameter: fact_decoder.W_vocab.bias, required shape: torch.Size([33130]), loaded shape: torch.Size([15773])\n",
      "\u001b[93mWarning: model state dict has 243 keys, loaded state dict has 249 keys, intersection has 239 keys, union has 253 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in model but not in loaded state dict:\u001b[0m\n",
      "\u001b[93m  fact_decoder.embedding_table.weight\u001b[0m\n",
      "\u001b[93m  fact_decoder.W_vocab.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.W_vocab.weight\u001b[0m\n",
      "\u001b[93m  fact_decoder_embedding_table.weight\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  health_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  health_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231225_051135_MIMIC-CXR(classif+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 6.26205, nli_loss 1.13904, nli_acc 0.66119, sae_loss 16.54657, 87.34 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.75810, sae_loss 10.90359, 4.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nli_acc+sae_loss=0.4816.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 5.97704, nli_loss 0.95946, nli_acc 0.64829, sae_loss 15.99343, 83.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74838, sae_loss 10.34067, 4.11 secs\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 5.05342, nli_loss 0.74122, nli_acc 0.64425, sae_loss 13.66165, 85.31 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.74514, sae_loss 7.74997, 4.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nli_acc+sae_loss=0.4849.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 3.24586, nli_loss 0.73109, nli_acc 0.63598, sae_loss 8.29432, 85.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69654, sae_loss 3.89442, 4.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_nli_acc+sae_loss=0.4921.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 2.65692, nli_loss 0.69841, nli_acc 0.64714, sae_loss 6.56660, 86.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69546, sae_loss 3.47489, 3.85 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nli_acc+sae_loss=0.5001.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 2.44265, nli_loss 0.67460, nli_acc 0.66546, sae_loss 5.97212, 86.36 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72138, sae_loss 3.29610, 4.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_nli_acc+sae_loss=0.5190.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 2.32775, nli_loss 0.65828, nli_acc 0.67084, sae_loss 5.67924, 86.03 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72678, sae_loss 3.21274, 4.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_nli_acc+sae_loss=0.5242.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 2.32172, nli_loss 0.64911, nli_acc 0.68249, sae_loss 5.66068, 86.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72462, sae_loss 3.16776, 4.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_nli_acc+sae_loss=0.5246.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 2.25109, nli_loss 0.65670, nli_acc 0.67340, sae_loss 5.43392, 86.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72246, sae_loss 3.14465, 4.17 secs\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 2.27814, nli_loss 0.66115, nli_acc 0.66595, sae_loss 5.52428, 86.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71922, sae_loss 3.13084, 4.13 secs\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.28088, nli_loss 0.65064, nli_acc 0.67687, sae_loss 5.53526, 86.44 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72138, sae_loss 3.12266, 3.98 secs\n",
      "\u001b[1m---- Epoch 12/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.33056, nli_loss 0.64905, nli_acc 0.66922, sae_loss 5.68728, 85.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72138, sae_loss 3.11773, 4.14 secs\n",
      "\u001b[1m---- Epoch 13/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 2.23039, nli_loss 0.63993, nli_acc 0.68467, sae_loss 5.42328, 85.98 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71382, sae_loss 2.83924, 4.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_nli_acc+sae_loss=0.5265.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 2.14938, nli_loss 0.64673, nli_acc 0.67586, sae_loss 5.14907, 86.50 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72138, sae_loss 2.75505, 4.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_nli_acc+sae_loss=0.5325.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 2.05676, nli_loss 0.62160, nli_acc 0.69707, sae_loss 4.92173, 86.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71598, sae_loss 2.72405, 3.88 secs\n",
      "\u001b[1m---- Epoch 16/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 2.05253, nli_loss 0.61926, nli_acc 0.69764, sae_loss 4.92983, 86.45 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72354, sae_loss 2.70892, 4.57 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_nli_acc+sae_loss=0.5364.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 2.05423, nli_loss 0.62205, nli_acc 0.69822, sae_loss 4.91323, 87.00 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72570, sae_loss 2.70161, 4.02 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_nli_acc+sae_loss=0.5378.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.99540, nli_loss 0.61787, nli_acc 0.69303, sae_loss 4.74530, 86.27 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73002, sae_loss 2.69787, 4.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_nli_acc+sae_loss=0.5401.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.02279, nli_loss 0.62178, nli_acc 0.69260, sae_loss 4.83534, 86.56 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73002, sae_loss 2.69583, 4.41 secs\n",
      "\u001b[1m---- Epoch 20/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.00434, nli_loss 0.61172, nli_acc 0.70140, sae_loss 4.78437, 87.68 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73110, sae_loss 2.69474, 4.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_20_nli_acc+sae_loss=0.5412.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 21/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 2.01518, nli_loss 0.61497, nli_acc 0.71121, sae_loss 4.81036, 87.09 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70302, sae_loss 2.62534, 4.33 secs\n",
      "\u001b[1m---- Epoch 22/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.97994, nli_loss 0.61754, nli_acc 0.69994, sae_loss 4.71499, 86.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71274, sae_loss 2.59843, 4.15 secs\n",
      "\u001b[1m---- Epoch 23/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.97952, nli_loss 0.60511, nli_acc 0.71035, sae_loss 4.72319, 86.87 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71922, sae_loss 2.58594, 4.16 secs\n",
      "\u001b[1m---- Epoch 24/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.97476, nli_loss 0.60075, nli_acc 0.71121, sae_loss 4.71765, 86.75 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71922, sae_loss 2.57926, 4.38 secs\n",
      "\u001b[1m---- Epoch 25/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.94768, nli_loss 0.60450, nli_acc 0.70671, sae_loss 4.64414, 87.07 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72354, sae_loss 2.57571, 4.02 secs\n",
      "\u001b[1m---- Epoch 26/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.98087, nli_loss 0.61017, nli_acc 0.70227, sae_loss 4.71714, 85.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72030, sae_loss 2.57379, 4.21 secs\n",
      "\u001b[1m---- Epoch 27/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.98883, nli_loss 0.60352, nli_acc 0.70573, sae_loss 4.75426, 85.79 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72354, sae_loss 2.57281, 4.53 secs\n",
      "\u001b[1m---- Epoch 28/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.96826, nli_loss 0.60441, nli_acc 0.70182, sae_loss 4.70623, 87.25 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72246, sae_loss 2.57219, 4.35 secs\n",
      "\u001b[1m---- Epoch 29/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.97107, nli_loss 0.60289, nli_acc 0.71093, sae_loss 4.70229, 87.08 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71274, sae_loss 2.53271, 4.33 secs\n",
      "\u001b[1m---- Epoch 30/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.97973, nli_loss 0.60015, nli_acc 0.71540, sae_loss 4.73371, 85.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72462, sae_loss 2.51306, 4.05 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_nli_acc+sae_loss=0.5437.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.93246, nli_loss 0.60543, nli_acc 0.70556, sae_loss 4.59649, 87.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72030, sae_loss 2.50198, 4.49 secs\n",
      "\u001b[1m---- Epoch 32/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.90225, nli_loss 0.59330, nli_acc 0.71396, sae_loss 4.51525, 86.97 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72786, sae_loss 2.49641, 4.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_nli_acc+sae_loss=0.5461.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.87896, nli_loss 0.58300, nli_acc 0.71641, sae_loss 4.46603, 86.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72894, sae_loss 2.49302, 4.53 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_nli_acc+sae_loss=0.5470.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.88156, nli_loss 0.58628, nli_acc 0.70844, sae_loss 4.48186, 85.40 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72786, sae_loss 2.49147, 4.31 secs\n",
      "\u001b[1m---- Epoch 35/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.90162, nli_loss 0.59836, nli_acc 0.71093, sae_loss 4.50325, 87.74 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.73002, sae_loss 2.49048, 4.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_35_nli_acc+sae_loss=0.5473.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 36/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.89574, nli_loss 0.58398, nli_acc 0.71338, sae_loss 4.51435, 86.52 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72786, sae_loss 2.49004, 4.34 secs\n",
      "\u001b[1m---- Epoch 37/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.93358, nli_loss 0.59163, nli_acc 0.71147, sae_loss 4.62756, 85.96 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70410, sae_loss 2.45662, 4.14 secs\n",
      "\u001b[1m---- Epoch 38/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.89437, nli_loss 0.60124, nli_acc 0.70530, sae_loss 4.47580, 86.05 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71814, sae_loss 2.43429, 3.87 secs\n",
      "\u001b[1m---- Epoch 39/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.82892, nli_loss 0.58713, nli_acc 0.71997, sae_loss 4.32183, 87.18 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71814, sae_loss 2.41937, 4.34 secs\n",
      "\u001b[1m---- Epoch 41/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.88936, nli_loss 0.58036, nli_acc 0.72175, sae_loss 4.50245, 87.04 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71922, sae_loss 2.41629, 4.16 secs\n",
      "\u001b[1m---- Epoch 42/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.88405, nli_loss 0.58120, nli_acc 0.71958, sae_loss 4.48488, 87.53 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72246, sae_loss 2.41488, 3.77 secs\n",
      "\u001b[1m---- Epoch 43/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.84827, nli_loss 0.58482, nli_acc 0.72097, sae_loss 4.38469, 86.61 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72570, sae_loss 2.41391, 4.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_43_nli_acc+sae_loss=0.5480.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 44/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.85024, nli_loss 0.58698, nli_acc 0.72103, sae_loss 4.37203, 86.71 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72462, sae_loss 2.41347, 3.92 secs\n",
      "\u001b[1m---- Epoch 45/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.89551, nli_loss 0.59632, nli_acc 0.71540, sae_loss 4.48901, 87.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70410, sae_loss 2.37964, 4.01 secs\n",
      "\u001b[1m---- Epoch 46/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.87187, nli_loss 0.59451, nli_acc 0.71723, sae_loss 4.43620, 86.26 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71382, sae_loss 2.36027, 4.56 secs\n",
      "\u001b[1m---- Epoch 47/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.79316, nli_loss 0.57932, nli_acc 0.72305, sae_loss 4.21629, 86.91 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71382, sae_loss 2.35180, 4.57 secs\n",
      "\u001b[1m---- Epoch 48/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.81768, nli_loss 0.57599, nli_acc 0.72637, sae_loss 4.29641, 86.48 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72570, sae_loss 2.34708, 4.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_nli_acc+sae_loss=0.5506.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.82975, nli_loss 0.57205, nli_acc 0.73264, sae_loss 4.35460, 88.28 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.72138, sae_loss 2.34459, 4.13 secs\n",
      "\u001b[1m---- Epoch 50/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.85354, nli_loss 0.56550, nli_acc 0.73286, sae_loss 4.42479, 86.51 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71706, sae_loss 2.34312, 4.42 secs\n",
      "\u001b[1m---- Epoch 51/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.83296, nli_loss 0.57344, nli_acc 0.72420, sae_loss 4.34726, 86.86 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71598, sae_loss 2.34225, 4.19 secs\n",
      "\u001b[1m---- Epoch 52/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.83571, nli_loss 0.56820, nli_acc 0.72918, sae_loss 4.38027, 88.37 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.71490, sae_loss 2.34183, 4.13 secs\n",
      "\u001b[1m---- Epoch 53/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.80335, nli_loss 0.58444, nli_acc 0.72218, sae_loss 4.23661, 86.92 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69762, sae_loss 2.31408, 4.01 secs\n",
      "\u001b[1m---- Epoch 54/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.81352, nli_loss 0.58674, nli_acc 0.72420, sae_loss 4.26249, 87.64 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70194, sae_loss 2.29661, 4.03 secs\n",
      "\u001b[1m---- Epoch 55/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.83584, nli_loss 0.58666, nli_acc 0.71089, sae_loss 4.34358, 86.60 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70842, sae_loss 2.28715, 4.65 secs\n",
      "\u001b[1m---- Epoch 56/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.80519, nli_loss 0.57315, nli_acc 0.73142, sae_loss 4.26465, 87.35 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69762, sae_loss 2.28260, 4.38 secs\n",
      "\u001b[1m---- Epoch 57/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.79994, nli_loss 0.56369, nli_acc 0.73474, sae_loss 4.26782, 86.84 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70302, sae_loss 2.27973, 4.29 secs\n",
      "\u001b[1m---- Epoch 58/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.80476, nli_loss 0.55782, nli_acc 0.73451, sae_loss 4.30803, 87.72 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70950, sae_loss 2.27812, 4.18 secs\n",
      "\u001b[1m---- Epoch 59/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.77090, nli_loss 0.56767, nli_acc 0.72882, sae_loss 4.17285, 87.57 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70734, sae_loss 2.27731, 4.20 secs\n",
      "\u001b[1m---- Epoch 60/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.77246, nli_loss 0.55736, nli_acc 0.73589, sae_loss 4.19811, 86.49 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70842, sae_loss 2.27689, 4.69 secs\n",
      "\u001b[1m---- Epoch 61/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.82126, nli_loss 0.57344, nli_acc 0.73192, sae_loss 4.32628, 87.73 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.67711, sae_loss 2.24826, 4.26 secs\n",
      "\u001b[1m---- Epoch 62/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.78957, nli_loss 0.57859, nli_acc 0.72189, sae_loss 4.20698, 85.67 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69222, sae_loss 2.23179, 4.00 secs\n",
      "\u001b[1m---- Epoch 63/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.75157, nli_loss 0.56470, nli_acc 0.72810, sae_loss 4.12088, 86.99 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69870, sae_loss 2.22294, 4.24 secs\n",
      "\u001b[1m---- Epoch 64/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.72937, nli_loss 0.56067, nli_acc 0.73048, sae_loss 4.07556, 87.59 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.69654, sae_loss 2.21848, 4.10 secs\n",
      "\u001b[1m---- Epoch 65/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.73736, nli_loss 0.56803, nli_acc 0.73055, sae_loss 4.07164, 87.20 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70734, sae_loss 2.21693, 4.28 secs\n",
      "\u001b[1m---- Epoch 66/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.75501, nli_loss 0.56091, nli_acc 0.73474, sae_loss 4.13874, 88.39 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70626, sae_loss 2.21529, 3.80 secs\n",
      "\u001b[1m---- Epoch 67/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.73779, nli_loss 0.56406, nli_acc 0.73696, sae_loss 4.09408, 86.29 secs\n",
      "(2) Validation stage ...\n",
      "nli_acc 0.70626, sae_loss 2.21469, 4.23 secs\n",
      "\u001b[1m---- Epoch 68/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "   iteration 53675\r"
     ]
    }
   ],
   "source": [
    "# --metadata_classification_weight 1.0 \\\n",
    "# --chest_imagenome_observations_classification_weight 1.0 \\\n",
    "# --chest_imagenome_anatomical_locations_classification_weight 1.0 \\\n",
    "\n",
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_paths \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231013_073812_MIMIC-CXR(triplets+classif+entcont+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 800 \\\n",
    "--batch_size 13 \\\n",
    "--val_batch_size 100 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 20 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--integrated_facts_metadata_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(595880,60579117).improved_comparison(6741113).jsonl\" \\\n",
    "--paraphrases_jsonl_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--integrated_chest_imagenome_observations_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_observations(9).pkl\" \\\n",
    "--integrated_chest_imagenome_anatomical_locations_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_anatomical_locations(5).pkl\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--integrated_sentence_facts_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\" \\\n",
    "--gpt4_radnli_labels_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "--sentences_and_cluster_ids_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\" \\\n",
    "--n_chest_imagenome_observations 74 \\\n",
    "--n_chest_imagenome_anatomical_locations 38 \\\n",
    "--nli_weight 6.0 \\\n",
    "--sentence_autoencoder_weight 3.0 \\\n",
    "--sentence_autoencoder_loss_weight 1.5 \\\n",
    "--dataset_name \"MIMIC-CXR(classif+nli+autoencoder)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_aux_task_hidden_layer \\\n",
    "--aux_task_hidden_layer_size 512 \\\n",
    "--nli_hidden_layer_size 128 \\\n",
    "--use_anli \\\n",
    "--use_multinli \\\n",
    "--use_snli \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
