{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cbaad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading integrated sentence facts from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\n",
      "Loading integrated facts metadata from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(595880,60579117).improved_comparison(6741113).jsonl\n",
      "Loding paraphrased anatomical locations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\n",
      "Loding paraphrased anatomical locations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\n",
      "Loding paraphrased anatomical locations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\n",
      "Loding paraphrased observations from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\n",
      "Loding hard triplets from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\n",
      "Loding hard triplets from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\n",
      "Loding hard triplets from /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,3677907674814022869).pkl\n",
      "len(self.cache[\"hashes\"]) = 2592648\n",
      "self.cache[\"embeddings\"].shape = (2592648, 128)\n",
      "100%|███████████████████████████████| 677694/677694 [00:01<00:00, 519641.23it/s]\n",
      "Computing embeddings for 629265 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 232 keys, intersection has 211 keys, union has 232 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "\u001b[93m  spert_rel_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n",
      "\u001b[93m  health_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████| 1574/1574 [07:00<00:00,  3.74it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,3677907674814022869).pkl\n",
      "100%|███████████████████████████████| 629265/629265 [00:00<00:00, 671863.40it/s]\n",
      "Running KMeans clustering with k=400\n",
      "Initialization complete\n",
      "Iteration 0, inertia 160943.03125.\n",
      "Iteration 1, inertia 124543.8515625.\n",
      "Iteration 2, inertia 121443.59375.\n",
      "Iteration 3, inertia 120054.875.\n",
      "Iteration 4, inertia 119228.9140625.\n",
      "Iteration 5, inertia 118680.53125.\n",
      "Iteration 6, inertia 118331.7421875.\n",
      "Iteration 7, inertia 118092.828125.\n",
      "Iteration 8, inertia 117895.7265625.\n",
      "Iteration 9, inertia 117727.21875.\n",
      "Iteration 10, inertia 117587.90625.\n",
      "Iteration 11, inertia 117483.1640625.\n",
      "Iteration 12, inertia 117395.515625.\n",
      "Iteration 13, inertia 117307.125.\n",
      "Iteration 14, inertia 117237.2578125.\n",
      "Iteration 15, inertia 117184.1171875.\n",
      "Iteration 16, inertia 117129.0546875.\n",
      "Iteration 17, inertia 117085.328125.\n",
      "Iteration 18, inertia 117048.6796875.\n",
      "Iteration 19, inertia 117017.9609375.\n",
      "Iteration 20, inertia 116990.234375.\n",
      "Iteration 21, inertia 116966.90625.\n",
      "Iteration 22, inertia 116947.140625.\n",
      "Iteration 23, inertia 116928.5.\n",
      "Iteration 24, inertia 116910.3828125.\n",
      "Iteration 25, inertia 116890.2578125.\n",
      "Iteration 26, inertia 116866.90625.\n",
      "Iteration 27, inertia 116843.859375.\n",
      "Iteration 28, inertia 116820.15625.\n",
      "Iteration 29, inertia 116798.9609375.\n",
      "Iteration 30, inertia 116783.2265625.\n",
      "Iteration 31, inertia 116770.8984375.\n",
      "Iteration 32, inertia 116759.6953125.\n",
      "Iteration 33, inertia 116748.2890625.\n",
      "Iteration 34, inertia 116736.6484375.\n",
      "Iteration 35, inertia 116725.265625.\n",
      "Iteration 36, inertia 116715.3828125.\n",
      "Iteration 37, inertia 116706.4609375.\n",
      "Iteration 38, inertia 116698.015625.\n",
      "Iteration 39, inertia 116689.640625.\n",
      "Iteration 40, inertia 116681.671875.\n",
      "Iteration 41, inertia 116672.6875.\n",
      "Iteration 42, inertia 116662.8046875.\n",
      "Iteration 43, inertia 116649.8671875.\n",
      "Iteration 44, inertia 116638.0390625.\n",
      "Iteration 45, inertia 116632.625.\n",
      "Iteration 46, inertia 116628.7265625.\n",
      "Iteration 47, inertia 116624.5625.\n",
      "Iteration 48, inertia 116620.2578125.\n",
      "Iteration 49, inertia 116617.28125.\n",
      "Iteration 50, inertia 116614.5546875.\n",
      "Iteration 51, inertia 116611.53125.\n",
      "Iteration 52, inertia 116608.515625.\n",
      "Iteration 53, inertia 116605.84375.\n",
      "Iteration 54, inertia 116602.53125.\n",
      "Iteration 55, inertia 116598.9375.\n",
      "Iteration 56, inertia 116594.71875.\n",
      "Iteration 57, inertia 116591.6640625.\n",
      "Iteration 58, inertia 116589.015625.\n",
      "Iteration 59, inertia 116586.75.\n",
      "Iteration 60, inertia 116584.3828125.\n",
      "Iteration 61, inertia 116582.5625.\n",
      "Iteration 62, inertia 116580.9140625.\n",
      "Iteration 63, inertia 116579.15625.\n",
      "Iteration 64, inertia 116577.46875.\n",
      "Iteration 65, inertia 116575.3359375.\n",
      "Iteration 66, inertia 116572.8203125.\n",
      "Iteration 67, inertia 116570.03125.\n",
      "Iteration 68, inertia 116566.9453125.\n",
      "Iteration 69, inertia 116563.53125.\n",
      "Iteration 70, inertia 116560.796875.\n",
      "Iteration 71, inertia 116559.0.\n",
      "Iteration 72, inertia 116557.703125.\n",
      "Iteration 73, inertia 116556.703125.\n",
      "Iteration 74, inertia 116556.3125.\n",
      "Iteration 75, inertia 116555.8828125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, inertia 116555.6484375.\n",
      "Iteration 77, inertia 116555.296875.\n",
      "Iteration 78, inertia 116554.8203125.\n",
      "Iteration 79, inertia 116554.65625.\n",
      "Iteration 80, inertia 116554.40625.\n",
      "Iteration 81, inertia 116554.328125.\n",
      "Iteration 82, inertia 116554.2734375.\n",
      "Iteration 83, inertia 116554.2109375.\n",
      "Iteration 84, inertia 116554.09375.\n",
      "Iteration 85, inertia 116553.9375.\n",
      "Iteration 86, inertia 116553.875.\n",
      "Iteration 87, inertia 116553.828125.\n",
      "Iteration 88, inertia 116553.7265625.\n",
      "Iteration 89, inertia 116553.71875.\n",
      "Iteration 90, inertia 116553.75.\n",
      "Iteration 91, inertia 116553.6875.\n",
      "Iteration 92, inertia 116553.546875.\n",
      "Iteration 93, inertia 116553.5546875.\n",
      "Iteration 94, inertia 116553.359375.\n",
      "Iteration 95, inertia 116553.140625.\n",
      "Iteration 96, inertia 116552.9453125.\n",
      "Iteration 97, inertia 116552.828125.\n",
      "Iteration 98, inertia 116552.609375.\n",
      "Iteration 99, inertia 116552.609375.\n",
      "Iteration 100, inertia 116552.5546875.\n",
      "Iteration 101, inertia 116552.515625.\n",
      "Iteration 102, inertia 116552.4765625.\n",
      "Iteration 103, inertia 116552.453125.\n",
      "Iteration 104, inertia 116552.34375.\n",
      "Iteration 105, inertia 116552.359375.\n",
      "Iteration 106, inertia 116552.328125.\n",
      "Iteration 107, inertia 116552.3125.\n",
      "Iteration 108, inertia 116552.3046875.\n",
      "Iteration 109, inertia 116552.25.\n",
      "Iteration 110, inertia 116552.265625.\n",
      "Iteration 111, inertia 116552.25.\n",
      "Iteration 112, inertia 116552.2109375.\n",
      "Iteration 113, inertia 116552.203125.\n",
      "Iteration 114, inertia 116552.171875.\n",
      "Iteration 115, inertia 116552.1875.\n",
      "Iteration 116, inertia 116552.1640625.\n",
      "Iteration 117, inertia 116552.1796875.\n",
      "Iteration 118, inertia 116552.203125.\n",
      "Iteration 119, inertia 116552.1796875.\n",
      "Iteration 120, inertia 116552.1953125.\n",
      "Iteration 121, inertia 116552.171875.\n",
      "Iteration 122, inertia 116552.171875.\n",
      "Iteration 123, inertia 116552.1640625.\n",
      "Converged at iteration 123: center shift 2.848308326974802e-07 within tolerance 6.551152095198632e-07.\n",
      "Saved kmeans labels to /mnt/workspace/pamessina/medvqa-workspace/cache/kmeans_labels(58655696,1638570761444304226).pkl\n",
      "100%|█████████████████████████████| 2905262/2905262 [00:06<00:00, 453235.76it/s]\n",
      "Computing embeddings for 698303 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 232 keys, intersection has 211 keys, union has 232 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  health_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  nli_classifier.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  spert_size_embeddings.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  nli_classifier.bias\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████| 1746/1746 [04:17<00:00,  6.77it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,3677907674814022869).pkl\n",
      "100%|███████████████████████████████| 698303/698303 [00:01<00:00, 556970.18it/s]\n",
      "Running KMeans clustering with k=400\n",
      "Initialization complete\n",
      "Iteration 0, inertia 1162721.75.\n",
      "Iteration 1, inertia 883819.625.\n",
      "Iteration 2, inertia 859165.25.\n",
      "Iteration 3, inertia 849054.5.\n",
      "Iteration 4, inertia 843562.25.\n",
      "Iteration 5, inertia 840014.5625.\n",
      "Iteration 6, inertia 837486.5.\n",
      "Iteration 7, inertia 835480.8125.\n",
      "Iteration 8, inertia 833798.1875.\n",
      "Iteration 9, inertia 832408.0.\n",
      "Iteration 10, inertia 831365.3125.\n",
      "Iteration 11, inertia 830609.8125.\n",
      "Iteration 12, inertia 830013.625.\n",
      "Iteration 13, inertia 829489.5625.\n",
      "Iteration 14, inertia 829008.5.\n",
      "Iteration 15, inertia 828621.3125.\n",
      "Iteration 16, inertia 828292.5.\n",
      "Iteration 17, inertia 827990.5625.\n",
      "Iteration 18, inertia 827728.875.\n",
      "Iteration 19, inertia 827459.125.\n",
      "Iteration 20, inertia 827166.25.\n",
      "Iteration 21, inertia 826901.3125.\n",
      "Iteration 22, inertia 826682.75.\n",
      "Iteration 23, inertia 826465.1875.\n",
      "Iteration 24, inertia 826241.4375.\n",
      "Iteration 25, inertia 826022.75.\n",
      "Iteration 26, inertia 825826.75.\n",
      "Iteration 27, inertia 825632.375.\n",
      "Iteration 28, inertia 825450.125.\n",
      "Iteration 29, inertia 825285.9375.\n",
      "Iteration 30, inertia 825130.625.\n",
      "Iteration 31, inertia 824967.6875.\n",
      "Iteration 32, inertia 824760.25.\n",
      "Iteration 33, inertia 824522.5.\n",
      "Iteration 34, inertia 824368.0.\n",
      "Iteration 35, inertia 824274.75.\n",
      "Iteration 36, inertia 824208.0.\n",
      "Iteration 37, inertia 824154.9375.\n",
      "Iteration 38, inertia 824113.625.\n",
      "Iteration 39, inertia 824075.625.\n",
      "Iteration 40, inertia 824044.0.\n",
      "Iteration 41, inertia 824014.8125.\n",
      "Iteration 42, inertia 823986.25.\n",
      "Iteration 43, inertia 823961.125.\n",
      "Iteration 44, inertia 823932.5.\n",
      "Iteration 45, inertia 823909.3125.\n",
      "Iteration 46, inertia 823884.6875.\n",
      "Iteration 47, inertia 823858.25.\n",
      "Iteration 48, inertia 823827.6875.\n",
      "Iteration 49, inertia 823794.625.\n",
      "Iteration 50, inertia 823751.0625.\n",
      "Iteration 51, inertia 823702.0.\n",
      "Iteration 52, inertia 823651.4375.\n",
      "Iteration 53, inertia 823594.6875.\n",
      "Iteration 54, inertia 823544.8125.\n",
      "Iteration 55, inertia 823504.1875.\n",
      "Iteration 56, inertia 823463.1875.\n",
      "Iteration 57, inertia 823428.75.\n",
      "Iteration 58, inertia 823393.4375.\n",
      "Iteration 59, inertia 823356.875.\n",
      "Iteration 60, inertia 823319.625.\n",
      "Iteration 61, inertia 823279.375.\n",
      "Iteration 62, inertia 823237.0.\n",
      "Iteration 63, inertia 823204.5.\n",
      "Iteration 64, inertia 823176.0.\n",
      "Iteration 65, inertia 823149.6875.\n",
      "Iteration 66, inertia 823129.8125.\n",
      "Iteration 67, inertia 823111.0.\n",
      "Iteration 68, inertia 823096.125.\n",
      "Iteration 69, inertia 823083.125.\n",
      "Iteration 70, inertia 823072.0.\n",
      "Iteration 71, inertia 823061.1875.\n",
      "Iteration 72, inertia 823049.75.\n",
      "Iteration 73, inertia 823038.5.\n",
      "Iteration 74, inertia 823027.9375.\n",
      "Iteration 75, inertia 823016.4375.\n",
      "Iteration 76, inertia 823003.9375.\n",
      "Iteration 77, inertia 822990.9375.\n",
      "Iteration 78, inertia 822980.4375.\n",
      "Iteration 79, inertia 822973.75.\n",
      "Iteration 80, inertia 822966.0.\n",
      "Iteration 81, inertia 822958.8125.\n",
      "Iteration 82, inertia 822954.3125.\n",
      "Iteration 83, inertia 822947.6875.\n",
      "Iteration 84, inertia 822944.5625.\n",
      "Iteration 85, inertia 822940.4375.\n",
      "Iteration 86, inertia 822937.125.\n",
      "Iteration 87, inertia 822931.375.\n",
      "Iteration 88, inertia 822921.0625.\n",
      "Iteration 89, inertia 822913.5.\n",
      "Iteration 90, inertia 822906.0.\n",
      "Iteration 91, inertia 822899.625.\n",
      "Iteration 92, inertia 822891.875.\n",
      "Iteration 93, inertia 822876.625.\n",
      "Iteration 94, inertia 822851.375.\n",
      "Iteration 95, inertia 822824.5.\n",
      "Iteration 96, inertia 822809.4375.\n",
      "Iteration 97, inertia 822799.375.\n",
      "Iteration 98, inertia 822788.75.\n",
      "Iteration 99, inertia 822777.1875.\n",
      "Iteration 100, inertia 822762.6875.\n",
      "Iteration 101, inertia 822740.5625.\n",
      "Iteration 102, inertia 822716.1875.\n",
      "Iteration 103, inertia 822692.25.\n",
      "Iteration 104, inertia 822671.875.\n",
      "Iteration 105, inertia 822655.25.\n",
      "Iteration 106, inertia 822642.0.\n",
      "Iteration 107, inertia 822631.0625.\n",
      "Iteration 108, inertia 822620.6875.\n",
      "Iteration 109, inertia 822613.6875.\n",
      "Iteration 110, inertia 822605.1875.\n",
      "Iteration 111, inertia 822598.6875.\n",
      "Iteration 112, inertia 822591.9375.\n",
      "Iteration 113, inertia 822589.1875.\n",
      "Iteration 114, inertia 822587.0.\n",
      "Iteration 115, inertia 822584.625.\n",
      "Iteration 116, inertia 822584.25.\n",
      "Iteration 117, inertia 822582.9375.\n",
      "Iteration 118, inertia 822581.9375.\n",
      "Iteration 119, inertia 822582.1875.\n",
      "Iteration 120, inertia 822581.75.\n",
      "Iteration 121, inertia 822580.625.\n",
      "Iteration 122, inertia 822579.25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 123, inertia 822578.75.\n",
      "Iteration 124, inertia 822578.4375.\n",
      "Iteration 125, inertia 822577.75.\n",
      "Iteration 126, inertia 822577.5625.\n",
      "Iteration 127, inertia 822576.625.\n",
      "Iteration 128, inertia 822577.0.\n",
      "Iteration 129, inertia 822577.3125.\n",
      "Iteration 130, inertia 822576.9375.\n",
      "Iteration 131, inertia 822576.3125.\n",
      "Iteration 132, inertia 822576.375.\n",
      "Iteration 133, inertia 822576.375.\n",
      "Iteration 134, inertia 822575.3125.\n",
      "Iteration 135, inertia 822575.0625.\n",
      "Iteration 136, inertia 822574.625.\n",
      "Iteration 137, inertia 822574.75.\n",
      "Iteration 138, inertia 822574.5.\n",
      "Iteration 139, inertia 822574.625.\n",
      "Iteration 140, inertia 822574.3125.\n",
      "Iteration 141, inertia 822574.3125.\n",
      "Iteration 142, inertia 822574.625.\n",
      "Iteration 143, inertia 822574.375.\n",
      "Iteration 144, inertia 822574.75.\n",
      "Iteration 145, inertia 822574.8125.\n",
      "Iteration 146, inertia 822574.6875.\n",
      "Iteration 147, inertia 822574.375.\n",
      "Iteration 148, inertia 822574.3125.\n",
      "Iteration 149, inertia 822574.125.\n",
      "Iteration 150, inertia 822574.6875.\n",
      "Iteration 151, inertia 822574.875.\n",
      "Iteration 152, inertia 822574.8125.\n",
      "Iteration 153, inertia 822574.875.\n",
      "Iteration 154, inertia 822574.625.\n",
      "Iteration 155, inertia 822575.125.\n",
      "Iteration 156, inertia 822575.0.\n",
      "Iteration 157, inertia 822575.125.\n",
      "Iteration 158, inertia 822575.125.\n",
      "Iteration 159, inertia 822574.8125.\n",
      "Iteration 160, inertia 822573.875.\n",
      "Iteration 161, inertia 822574.5.\n",
      "Iteration 162, inertia 822574.125.\n",
      "Iteration 163, inertia 822574.25.\n",
      "Iteration 164, inertia 822574.125.\n",
      "Iteration 165, inertia 822574.25.\n",
      "Iteration 166, inertia 822574.1875.\n",
      "Iteration 167, inertia 822573.875.\n",
      "Iteration 168, inertia 822573.5.\n",
      "Iteration 169, inertia 822573.5.\n",
      "Iteration 170, inertia 822573.3125.\n",
      "Iteration 171, inertia 822573.5.\n",
      "Iteration 172, inertia 822573.625.\n",
      "Iteration 173, inertia 822573.9375.\n",
      "Iteration 174, inertia 822573.4375.\n",
      "Iteration 175, inertia 822573.75.\n",
      "Converged at iteration 175: center shift 6.990890142333228e-07 within tolerance 7.594254799187184e-07.\n",
      "Saved kmeans labels to /mnt/workspace/pamessina/medvqa-workspace/cache/kmeans_labels(150215204,1134394051579003141).pkl\n",
      "100%|███████████████████████████████| 299586/299586 [00:00<00:00, 391663.12it/s]\n",
      "Computing embeddings for 15125 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 232 keys, intersection has 211 keys, union has 232 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  health_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  spert_rel_classifier.bias\u001b[0m\n",
      "\u001b[93m  spert_rel_classifier.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.bias\u001b[0m\n",
      "\u001b[93m  nli_classifier.bias\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  health_status_classifier.bias\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|███████████████████████████████████████████| 38/38 [00:05<00:00,  7.28it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=423,3677907674814022869).pkl\n",
      "100%|█████████████████████████████████| 15125/15125 [00:00<00:00, 325365.80it/s]\n",
      "Running KMeans clustering with k=200\n",
      "Initialization complete\n",
      "Iteration 0, inertia 101417.65625.\n",
      "Iteration 1, inertia 76584.1171875.\n",
      "Iteration 2, inertia 74239.515625.\n",
      "Iteration 3, inertia 73327.3671875.\n",
      "Iteration 4, inertia 72883.0234375.\n",
      "Iteration 5, inertia 72586.1953125.\n",
      "Iteration 6, inertia 72374.84375.\n",
      "Iteration 7, inertia 72240.3828125.\n",
      "Iteration 8, inertia 72146.7890625.\n",
      "Iteration 9, inertia 72067.890625.\n",
      "Iteration 10, inertia 72008.90625.\n",
      "Iteration 11, inertia 71960.75.\n",
      "Iteration 12, inertia 71910.1484375.\n",
      "Iteration 13, inertia 71858.2890625.\n",
      "Iteration 14, inertia 71799.3203125.\n",
      "Iteration 15, inertia 71732.6328125.\n",
      "Iteration 16, inertia 71691.9921875.\n",
      "Iteration 17, inertia 71659.03125.\n",
      "Iteration 18, inertia 71628.2109375.\n",
      "Iteration 19, inertia 71595.5546875.\n",
      "Iteration 20, inertia 71564.6953125.\n",
      "Iteration 21, inertia 71538.9296875.\n",
      "Iteration 22, inertia 71522.0859375.\n",
      "Iteration 23, inertia 71509.0625.\n",
      "Iteration 24, inertia 71495.4921875.\n",
      "Iteration 25, inertia 71478.6328125.\n",
      "Iteration 26, inertia 71462.3125.\n",
      "Iteration 27, inertia 71450.7578125.\n",
      "Iteration 28, inertia 71442.75.\n",
      "Iteration 29, inertia 71437.25.\n",
      "Iteration 30, inertia 71430.859375.\n",
      "Iteration 31, inertia 71418.515625.\n",
      "Iteration 32, inertia 71405.609375.\n",
      "Iteration 33, inertia 71397.515625.\n",
      "Iteration 34, inertia 71388.7421875.\n",
      "Iteration 35, inertia 71381.1796875.\n",
      "Iteration 36, inertia 71377.1015625.\n",
      "Iteration 37, inertia 71374.171875.\n",
      "Iteration 38, inertia 71372.1171875.\n",
      "Iteration 39, inertia 71370.4296875.\n",
      "Iteration 40, inertia 71368.359375.\n",
      "Iteration 41, inertia 71366.28125.\n",
      "Iteration 42, inertia 71364.4296875.\n",
      "Iteration 43, inertia 71362.765625.\n",
      "Iteration 44, inertia 71361.0703125.\n",
      "Iteration 45, inertia 71358.3984375.\n",
      "Iteration 46, inertia 71355.3515625.\n",
      "Iteration 47, inertia 71352.203125.\n",
      "Iteration 48, inertia 71348.8125.\n",
      "Iteration 49, inertia 71345.8046875.\n",
      "Iteration 50, inertia 71343.171875.\n",
      "Iteration 51, inertia 71340.921875.\n",
      "Iteration 52, inertia 71339.4375.\n",
      "Iteration 53, inertia 71337.9296875.\n",
      "Iteration 54, inertia 71336.75.\n",
      "Iteration 55, inertia 71335.5390625.\n",
      "Iteration 56, inertia 71334.1875.\n",
      "Iteration 57, inertia 71332.765625.\n",
      "Iteration 58, inertia 71331.375.\n",
      "Iteration 59, inertia 71330.0859375.\n",
      "Iteration 60, inertia 71328.859375.\n",
      "Iteration 61, inertia 71327.9609375.\n",
      "Iteration 62, inertia 71327.1015625.\n",
      "Iteration 63, inertia 71326.1953125.\n",
      "Iteration 64, inertia 71325.5.\n",
      "Iteration 65, inertia 71325.0078125.\n",
      "Iteration 66, inertia 71324.6484375.\n",
      "Iteration 67, inertia 71324.390625.\n",
      "Iteration 68, inertia 71324.171875.\n",
      "Iteration 69, inertia 71323.984375.\n",
      "Iteration 70, inertia 71323.7734375.\n",
      "Iteration 71, inertia 71323.6640625.\n",
      "Iteration 72, inertia 71323.5546875.\n",
      "Iteration 73, inertia 71323.4453125.\n",
      "Iteration 74, inertia 71323.3046875.\n",
      "Iteration 75, inertia 71323.25.\n",
      "Iteration 76, inertia 71323.1796875.\n",
      "Iteration 77, inertia 71323.140625.\n",
      "Iteration 78, inertia 71323.125.\n",
      "Iteration 79, inertia 71323.078125.\n",
      "Iteration 80, inertia 71323.0390625.\n",
      "Iteration 81, inertia 71323.03125.\n",
      "Iteration 82, inertia 71323.0234375.\n",
      "Iteration 83, inertia 71323.0234375.\n",
      "Iteration 84, inertia 71323.0.\n",
      "Iteration 85, inertia 71322.96875.\n",
      "Iteration 86, inertia 71322.953125.\n",
      "Iteration 87, inertia 71322.9609375.\n",
      "Converged at iteration 87: center shift 7.503813321818598e-07 within tolerance 7.731440477073193e-07.\n",
      "Saved kmeans labels to /mnt/workspace/pamessina/medvqa-workspace/cache/kmeans_labels(10695475,593906493030486071).pkl\n",
      "Saving sentence cluster ids to /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/mimiccxr/export_sentences_and_cluster_ids.py \\\n",
    "--integrated_sentence_facts_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\" \\\n",
    "--integrated_facts_metadata_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(595880,60579117).improved_comparison(6741113).jsonl\" \\\n",
    "--paraphrased_anatomical_locations_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "--paraphrased_observations_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "--hard_triplets_filepaths \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "--batch_size 400 \\\n",
    "--num_workers 3 \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732c9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medvqa.utils.files import load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ff86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = load_pickle(\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1858be74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentences', 'cluster_ids'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbdb44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2idxs = {}\n",
    "for i, c in enumerate(tmp['cluster_ids']):\n",
    "    try:\n",
    "        c2idxs[c].append(i)\n",
    "    except KeyError:\n",
    "        c2idxs[c] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98aa27c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c2idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5db5b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both sides of the lung's lateral base\n",
      "segment of the bilateral basilar area\n",
      "visualization of the lower lung areas\n",
      "bilateral base of the lungs\n",
      "distal bilateral basal lung segments\n",
      "lower lobes of the left lung on both sides\n",
      "lower lobes of the left lungs on both sides\n",
      "lateral aspects of the left lung bases\n",
      "lowermost sections of the lungs on both sides\n",
      "peripheral region of the lung bases\n",
      "bilateral lungs\n",
      "right lung's basal lateral surface\n",
      "lower lung areas on the left side\n",
      "left-sided lower lobes on both sides\n",
      "lateral portions of the lung bases\n",
      "lower lobes bilaterally\n",
      "left lung base lateral aspect\n",
      "lower lung zones in both lungs\n",
      "apical basal in both bases of the pulmonary structures\n",
      "lateral side of the lung bases\n",
      "outer areas of the basilar lung segments\n",
      "lower lungs laterally\n",
      "lower lateral on both sides\n",
      "lower left lung's lateral area\n",
      "lateral lower pulmonary zones\n",
      "dependent portions of both lower lobes\n",
      "bilateral basal predominance manifestation\n",
      "right lung base on the lateral side\n",
      "bilateral sub-segment of the lower segment of the lung\n",
      "bilateral peripheral lung bases\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "c = 804\n",
    "for i in random.sample(c2idxs[c], 30):\n",
    "    print(tmp['sentences'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83bd44d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 200\n",
      "   val_batch_size: None\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   freeze_huggingface_model: True\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   fact_decoder_embed_size: 256\n",
      "   fact_decoder_hidden_size: 256\n",
      "   fact_decoder_nhead: 1\n",
      "   fact_decoder_dim_feedforward: 256\n",
      "   fact_decoder_num_layers: 1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 3\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: None\n",
      "   triplet_rule_weights: None\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   integrated_sentence_facts_jsonl_filepath: None\n",
      "   gpt4_radnli_labels_jsonl_filepath: None\n",
      "   sentences_and_cluster_ids_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\n",
      "   dataset_name: MIMIC-CXR(autoencoder)\n",
      "   triplets_weight: 0\n",
      "   metadata_classification_weight: 0\n",
      "   chest_imagenome_observations_classification_weight: 0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0\n",
      "   nli_weight: 0\n",
      "   entcon_weight: 0\n",
      "   radgraph_ner_re_weight: 0\n",
      "   sentence_autoencoder_weight: 1.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of sentences: 3882542\n",
      "Number of cluster IDs: 3882542\n",
      "Number of clusters: 1000\n",
      "Number of sentences in largest cluster: 27433\n",
      "Number of sentences in smallest cluster: 440\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/fact_decoding_vocab(3882542,219565937).pkl ...\n",
      "Vocabulary size: 15773\n",
      "Number of tokens in vocab: 15773\n",
      "Loading sentence IDs from cache...\n",
      "Number of validation samples: 5000\n",
      "Number of training samples: 3877542\n",
      "len(_train_dataloaders) = 1\n",
      "len(_train_weights) = 1\n",
      "_train_weights = [1.0]\n",
      "len(_val_dataloaders) = 1\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(autoencoder)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "  use_fact_decoder: True\n",
      "  fact_decoder_embed_size: 256\n",
      "  fact_decoder_hidden_size: 256\n",
      "  fact_decoder_nhead: 1\n",
      "  fact_decoder_dim_feedforward: 256\n",
      "  fact_decoder_num_layers: 1\n",
      "  fact_decoder_start_idx: 1\n",
      "  fact_decoder_vocab_size: 15773\n",
      "  fact_decoder_dropout_prob: 0\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Freezing huggingface model\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 3, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_080743_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_080743_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_50_cacc+chf1+chf1+cscc+encc+hscc+nlcc+spss+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9373.pt\n",
      "\u001b[93mWarning: model state dict has 237 keys, loaded state dict has 232 keys, intersection has 211 keys, union has 258 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in model but not in loaded state dict:\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.linear2.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.norm3.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.multihead_attn.out_proj.weight\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.multihead_attn.in_proj_bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.norm1.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.norm1.weight\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.linear1.bias\u001b[0m\n",
      "\u001b[93m  fact_encoder_input_layer.weight\u001b[0m\n",
      "\u001b[93m  fact_decoder.embedding_table.weight\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.norm2.bias\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  aux_task_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  category_classifier.weight\u001b[0m\n",
      "\u001b[93m  health_status_classifier.weight\u001b[0m\n",
      "\u001b[93m  health_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  chest_imagenome_obs_classifier.weight\u001b[0m\n",
      "\u001b[93m  spert_entity_classifier.bias\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  category_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_080743_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 9.79842, sae_loss 9.79842, 69.27 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 9.60107, 3.05 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_sae_loss=0.0942.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 8.69785, sae_loss 8.69785, 67.54 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 7.60571, 3.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_sae_loss=0.1149.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 4.80858, sae_loss 4.80858, 67.41 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 3.44583, 3.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_sae_loss=0.2197.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 2.69437, sae_loss 2.69437, 67.13 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.25935, 3.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_sae_loss=0.3032.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 2.24461, sae_loss 2.24461, 68.39 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.15497, 3.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_sae_loss=0.3161.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 2.17673, sae_loss 2.17673, 68.21 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.10076, 3.18 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_sae_loss=0.3217.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 2.14738, sae_loss 2.14738, 67.91 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.07036, 3.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_sae_loss=0.3249.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 2.10907, sae_loss 2.10907, 68.37 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.05283, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_sae_loss=0.3270.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 2.07937, sae_loss 2.07937, 68.49 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.04267, 3.22 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_sae_loss=0.3283.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 2.08010, sae_loss 2.08010, 68.37 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.03659, 3.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_sae_loss=0.3289.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 2.07297, sae_loss 2.07297, 68.57 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.03295, 3.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_sae_loss=0.3293.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.07600, sae_loss 2.07600, 67.96 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 2.03076, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_sae_loss=0.3295.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.98334, sae_loss 1.98334, 68.88 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.87813, 3.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_sae_loss=0.3462.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.87959, sae_loss 1.87959, 68.60 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.81300, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_sae_loss=0.3547.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.83319, sae_loss 1.83319, 68.50 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.78153, 3.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_sae_loss=0.3589.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.81661, sae_loss 1.81661, 69.04 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.76553, 3.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_sae_loss=0.3609.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.78869, sae_loss 1.78869, 69.23 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.75707, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_sae_loss=0.3623.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.78690, sae_loss 1.78690, 69.52 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.75257, 3.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_sae_loss=0.3628.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.80733, sae_loss 1.80733, 68.03 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.75011, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_sae_loss=0.3629.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.78215, sae_loss 1.78215, 69.20 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.74876, 3.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_20_sae_loss=0.3634.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.74624, sae_loss 1.74624, 69.00 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.65488, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_sae_loss=0.3754.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.67006, sae_loss 1.67006, 69.10 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.61318, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_sae_loss=0.3819.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.63108, sae_loss 1.63108, 69.79 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.59306, 3.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_sae_loss=0.3851.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.62580, sae_loss 1.62580, 69.67 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.58226, 3.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_sae_loss=0.3866.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.61382, sae_loss 1.61382, 70.08 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.57640, 3.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_sae_loss=0.3876.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.60261, sae_loss 1.60261, 69.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.57348, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_sae_loss=0.3881.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.60741, sae_loss 1.60741, 69.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.57186, 3.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_27_sae_loss=0.3883.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.61018, sae_loss 1.61018, 70.17 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.57100, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_28_sae_loss=0.3884.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.57969, sae_loss 1.57969, 69.96 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.50631, 3.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_29_sae_loss=0.3979.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.52145, sae_loss 1.52145, 70.34 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.47515, 3.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_sae_loss=0.4033.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.50339, sae_loss 1.50339, 70.04 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.45924, 3.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_sae_loss=0.4059.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.49175, sae_loss 1.49175, 70.45 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.45137, 3.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_sae_loss=0.4073.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.49637, sae_loss 1.49637, 69.40 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.44711, 3.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_sae_loss=0.4078.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.48465, sae_loss 1.48465, 70.19 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.44481, 3.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_34_sae_loss=0.4084.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.49343, sae_loss 1.49343, 70.51 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.44359, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_35_sae_loss=0.4084.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.47494, sae_loss 1.47494, 70.67 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.44301, 3.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_36_sae_loss=0.4088.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.46396, sae_loss 1.46396, 69.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.39688, 3.51 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_37_sae_loss=0.4161.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.44079, sae_loss 1.44079, 69.65 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.37475, 3.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_38_sae_loss=0.4200.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.41167, sae_loss 1.41167, 70.91 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.36278, 3.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_39_sae_loss=0.4224.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.41344, sae_loss 1.41344, 69.88 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.35702, 3.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_sae_loss=0.4233.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.40640, sae_loss 1.40640, 70.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.35372, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_41_sae_loss=0.4239.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.38445, sae_loss 1.38445, 71.28 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.35206, 3.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_42_sae_loss=0.4246.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.40022, sae_loss 1.40022, 69.85 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.35119, 3.62 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.39975, sae_loss 1.39975, 70.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.35073, 3.38 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.37471, sae_loss 1.37471, 70.87 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.31702, 3.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_45_sae_loss=0.4305.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.35411, sae_loss 1.35411, 70.31 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.29915, 3.57 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_46_sae_loss=0.4339.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.33583, sae_loss 1.33583, 73.59 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.29096, 3.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_sae_loss=0.4357.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.34257, sae_loss 1.34257, 70.05 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.28627, 3.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_sae_loss=0.4363.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.33480, sae_loss 1.33480, 70.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.28379, 3.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_sae_loss=0.4369.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.34304, sae_loss 1.34304, 68.88 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.28250, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_50_sae_loss=0.4370.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.33834, sae_loss 1.33834, 69.05 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.28187, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_51_sae_loss=0.4372.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.32544, sae_loss 1.32544, 70.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.28142, 3.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_52_sae_loss=0.4375.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.31931, sae_loss 1.31931, 70.13 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.25542, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_53_sae_loss=0.4422.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.28908, sae_loss 1.28908, 71.11 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.24241, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_54_sae_loss=0.4450.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.29826, sae_loss 1.29826, 69.00 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.23561, 3.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_55_sae_loss=0.4461.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.28954, sae_loss 1.28954, 70.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.23182, 3.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_56_sae_loss=0.4469.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.27307, sae_loss 1.27307, 71.01 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.22983, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_57_sae_loss=0.4476.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.28580, sae_loss 1.28580, 70.00 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.22883, 3.65 secs\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.29273, sae_loss 1.29273, 70.33 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.22831, 3.41 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.27398, sae_loss 1.27398, 71.02 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.22801, 3.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_60_sae_loss=0.4479.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.27007, sae_loss 1.27007, 70.56 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.20839, 3.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_61_sae_loss=0.4516.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.26092, sae_loss 1.26092, 70.29 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.19727, 3.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_62_sae_loss=0.4538.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.24786, sae_loss 1.24786, 70.67 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.19215, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_63_sae_loss=0.4550.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.24924, sae_loss 1.24924, 70.78 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.18934, 3.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_sae_loss=0.4555.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.24188, sae_loss 1.24188, 70.43 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.18798, 3.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_65_sae_loss=0.4559.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 66/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.22539, sae_loss 1.22539, 71.22 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.18699, 3.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_66_sae_loss=0.4565.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 67/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.24699, sae_loss 1.24699, 68.40 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.18653, 3.50 secs\n",
      "\u001b[1m---- Epoch 68/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.23821, sae_loss 1.23821, 69.31 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.18635, 3.54 secs\n",
      "\u001b[1m---- Epoch 69/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.22383, sae_loss 1.22383, 69.93 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.17081, 3.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_69_sae_loss=0.4596.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 70/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.22917, sae_loss 1.22917, 69.90 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.16318, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_70_sae_loss=0.4609.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 71/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.20762, sae_loss 1.20762, 70.40 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.15813, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_71_sae_loss=0.4623.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 72/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.22255, sae_loss 1.22255, 70.20 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.15547, 3.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_72_sae_loss=0.4625.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 73/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.20434, sae_loss 1.20434, 70.92 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.15438, 3.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_73_sae_loss=0.4631.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 74/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.20950, sae_loss 1.20950, 70.35 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.15372, 3.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_74_sae_loss=0.4631.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 75/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.20908, sae_loss 1.20908, 70.33 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.15335, 3.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_75_sae_loss=0.4632.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 76/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.20999, sae_loss 1.20999, 70.26 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.15322, 3.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_76_sae_loss=0.4632.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 77/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.20510, sae_loss 1.20510, 70.54 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.14181, 3.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_77_sae_loss=0.4656.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 78/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.19009, sae_loss 1.19009, 71.44 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.13381, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_78_sae_loss=0.4674.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 79/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.19942, sae_loss 1.19942, 69.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.12964, 3.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_79_sae_loss=0.4681.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 80/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.17924, sae_loss 1.17924, 71.16 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.12810, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_80_sae_loss=0.4688.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 81/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.18741, sae_loss 1.18741, 69.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.12685, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_81_sae_loss=0.4689.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 82/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.18348, sae_loss 1.18348, 70.52 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.12622, 3.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_82_sae_loss=0.4691.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 83/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.18687, sae_loss 1.18687, 70.12 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.12599, 3.61 secs\n",
      "\u001b[1m---- Epoch 84/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.17653, sae_loss 1.17653, 70.71 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.12587, 3.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_84_sae_loss=0.4693.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 85/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.17196, sae_loss 1.17196, 70.40 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.11605, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_85_sae_loss=0.4714.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 86/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.17527, sae_loss 1.17527, 70.12 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.11020, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_86_sae_loss=0.4725.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 87/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.17086, sae_loss 1.17086, 70.62 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10651, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_87_sae_loss=0.4733.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 88/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.15943, sae_loss 1.15943, 70.90 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10518, 3.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_88_sae_loss=0.4738.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 89/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.15565, sae_loss 1.15565, 71.20 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10419, 3.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_89_sae_loss=0.4741.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 90/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.15733, sae_loss 1.15733, 70.77 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10362, 3.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_90_sae_loss=0.4742.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 91/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.15532, sae_loss 1.15532, 70.63 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10346, 3.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_91_sae_loss=0.4743.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 92/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.16722, sae_loss 1.16722, 69.98 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10328, 3.58 secs\n",
      "\u001b[1m---- Epoch 93/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.16420, sae_loss 1.16420, 70.31 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09453, 3.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_93_sae_loss=0.4759.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 94/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.15142, sae_loss 1.15142, 69.80 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08994, 3.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_94_sae_loss=0.4771.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 95/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.14890, sae_loss 1.14890, 70.17 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08696, 3.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_95_sae_loss=0.4778.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 96/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.15232, sae_loss 1.15232, 70.85 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08530, 3.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_96_sae_loss=0.4781.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 97/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.14094, sae_loss 1.14094, 70.64 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08479, 3.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_97_sae_loss=0.4784.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 98/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.15205, sae_loss 1.15205, 70.86 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08420, 3.67 secs\n",
      "\u001b[1m---- Epoch 99/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.13625, sae_loss 1.13625, 71.25 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08398, 3.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_99_sae_loss=0.4787.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 100/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.14163, sae_loss 1.14163, 70.87 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08389, 3.50 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231004_174626_MIMIC-CXR(triplets+classif+entcont+nli+radgraph)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 200 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 3 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--sentence_autoencoder_weight 1.0 \\\n",
    "--sentences_and_cluster_ids_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\" \\\n",
    "--dataset_name \"MIMIC-CXR(autoencoder)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--freeze_huggingface_model \\\n",
    "--embedding_size 128 \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2aa97cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 120\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 200\n",
      "   val_batch_size: None\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_080743_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   freeze_huggingface_model: True\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   fact_decoder_embed_size: 256\n",
      "   fact_decoder_hidden_size: 256\n",
      "   fact_decoder_nhead: 1\n",
      "   fact_decoder_dim_feedforward: 256\n",
      "   fact_decoder_num_layers: 1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 3\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: None\n",
      "   triplet_rule_weights: None\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   integrated_sentence_facts_jsonl_filepath: None\n",
      "   gpt4_radnli_labels_jsonl_filepath: None\n",
      "   sentences_and_cluster_ids_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\n",
      "   dataset_name: MIMIC-CXR(autoencoder)\n",
      "   triplets_weight: 0\n",
      "   metadata_classification_weight: 0\n",
      "   chest_imagenome_observations_classification_weight: 0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0\n",
      "   nli_weight: 0\n",
      "   entcon_weight: 0\n",
      "   radgraph_ner_re_weight: 0\n",
      "   sentence_autoencoder_weight: 1.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of sentences: 3882542\n",
      "Number of cluster IDs: 3882542\n",
      "Number of clusters: 1000\n",
      "Number of sentences in largest cluster: 27433\n",
      "Number of sentences in smallest cluster: 440\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/fact_decoding_vocab(3882542,219565937).pkl ...\n",
      "Vocabulary size: 15773\n",
      "Number of tokens in vocab: 15773\n",
      "Loading sentence IDs from cache...\n",
      "Number of validation samples: 5000\n",
      "Number of training samples: 3877542\n",
      "len(_train_dataloaders) = 1\n",
      "len(_train_weights) = 1\n",
      "_train_weights = [1.0]\n",
      "len(_val_dataloaders) = 1\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(autoencoder)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "  use_fact_decoder: True\n",
      "  fact_decoder_embed_size: 256\n",
      "  fact_decoder_hidden_size: 256\n",
      "  fact_decoder_nhead: 1\n",
      "  fact_decoder_dim_feedforward: 256\n",
      "  fact_decoder_num_layers: 1\n",
      "  fact_decoder_start_idx: 1\n",
      "  fact_decoder_vocab_size: 15773\n",
      "  fact_decoder_dropout_prob: 0\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_080743_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Freezing huggingface model\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 3, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_110017_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_110017_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_99_sae_loss=0.4787.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_080743_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_99_sae_loss=0.4787.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_110017_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.13395, sae_loss 1.13395, 69.50 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10960, 2.97 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_sae_loss=0.4735.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.15406, sae_loss 1.15406, 67.40 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10927, 3.06 secs\n",
      "\u001b[1m---- Epoch 3/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 1.14505, sae_loss 1.14505, 67.70 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10768, 3.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_sae_loss=0.4736.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.14309, sae_loss 1.14309, 67.36 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.10038, 3.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_sae_loss=0.4752.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 1.13532, sae_loss 1.13532, 67.80 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09607, 3.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_sae_loss=0.4762.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 1.13189, sae_loss 1.13189, 68.29 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09360, 3.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_sae_loss=0.4768.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 1.12506, sae_loss 1.12506, 68.40 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09222, 3.11 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_sae_loss=0.4772.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 1.12340, sae_loss 1.12340, 68.66 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09114, 3.08 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_sae_loss=0.4775.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.12532, sae_loss 1.12532, 67.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09056, 3.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_sae_loss=0.4776.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.11850, sae_loss 1.11850, 68.38 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09029, 3.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_sae_loss=0.4778.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.13355, sae_loss 1.13355, 67.60 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.09011, 3.11 secs\n",
      "\u001b[1m---- Epoch 12/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.11921, sae_loss 1.11921, 68.51 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08993, 3.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_sae_loss=0.4778.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.13041, sae_loss 1.13041, 68.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08467, 3.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_sae_loss=0.4787.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.11703, sae_loss 1.11703, 68.27 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.08030, 3.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_sae_loss=0.4799.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.11749, sae_loss 1.11749, 67.71 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.07863, 3.12 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_sae_loss=0.4802.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.10379, sae_loss 1.10379, 68.88 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.07680, 3.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_sae_loss=0.4809.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.11128, sae_loss 1.11128, 68.93 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.07662, 3.15 secs\n",
      "\u001b[1m---- Epoch 18/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.11161, sae_loss 1.11161, 69.25 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.07619, 3.30 secs\n",
      "\u001b[1m---- Epoch 19/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.10570, sae_loss 1.10570, 70.25 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.07607, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_sae_loss=0.4810.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.11292, sae_loss 1.11292, 69.64 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.07599, 3.21 secs\n",
      "\u001b[1m---- Epoch 21/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.11056, sae_loss 1.11056, 69.85 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.07051, 3.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_sae_loss=0.4821.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.10146, sae_loss 1.10146, 70.05 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.06751, 3.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_sae_loss=0.4829.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.10474, sae_loss 1.10474, 70.22 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.06527, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_sae_loss=0.4833.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.10358, sae_loss 1.10358, 70.32 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.06461, 3.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_sae_loss=0.4835.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.09324, sae_loss 1.09324, 70.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.06392, 3.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_sae_loss=0.4838.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.10475, sae_loss 1.10475, 69.85 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.06367, 3.64 secs\n",
      "\u001b[1m---- Epoch 27/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.10533, sae_loss 1.10533, 70.37 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.06350, 3.64 secs\n",
      "\u001b[1m---- Epoch 28/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.09404, sae_loss 1.09404, 70.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.06342, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_28_sae_loss=0.4839.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 29/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.09895, sae_loss 1.09895, 71.26 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05941, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_29_sae_loss=0.4847.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 30/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.10426, sae_loss 1.10426, 70.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05557, 3.57 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_sae_loss=0.4854.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.09246, sae_loss 1.09246, 71.23 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05390, 3.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_sae_loss=0.4860.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.09576, sae_loss 1.09576, 70.51 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05287, 3.79 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_sae_loss=0.4861.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.09120, sae_loss 1.09120, 71.29 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05237, 3.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_sae_loss=0.4863.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.09813, sae_loss 1.09813, 69.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05233, 3.35 secs\n",
      "\u001b[1m---- Epoch 35/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.08536, sae_loss 1.08536, 70.92 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05215, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_35_sae_loss=0.4865.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 36/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.09124, sae_loss 1.09124, 70.82 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.05208, 3.46 secs\n",
      "\u001b[1m---- Epoch 37/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.08882, sae_loss 1.08882, 71.49 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04882, 3.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_37_sae_loss=0.4872.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 38/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.07916, sae_loss 1.07916, 70.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04549, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_38_sae_loss=0.4881.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 39/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.07891, sae_loss 1.07891, 71.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04393, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_39_sae_loss=0.4884.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 40/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.08370, sae_loss 1.08370, 71.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04309, 3.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_sae_loss=0.4885.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.07012, sae_loss 1.07012, 71.49 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04259, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_41_sae_loss=0.4889.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 42/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.07274, sae_loss 1.07274, 71.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04245, 3.53 secs\n",
      "\u001b[1m---- Epoch 43/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.07640, sae_loss 1.07640, 71.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04222, 3.66 secs\n",
      "\u001b[1m---- Epoch 44/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.08550, sae_loss 1.08550, 70.04 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.04221, 3.62 secs\n",
      "\u001b[1m---- Epoch 45/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.08543, sae_loss 1.08543, 70.21 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03905, 3.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_45_sae_loss=0.4893.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 46/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.07675, sae_loss 1.07675, 71.44 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03644, 3.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_46_sae_loss=0.4901.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 47/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.07061, sae_loss 1.07061, 75.97 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03440, 3.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_sae_loss=0.4907.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.06976, sae_loss 1.06976, 71.61 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03396, 3.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_sae_loss=0.4908.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.06809, sae_loss 1.06809, 71.29 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03358, 3.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_sae_loss=0.4909.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.05687, sae_loss 1.05687, 71.05 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03330, 3.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_50_sae_loss=0.4912.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 51/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.07268, sae_loss 1.07268, 70.95 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03322, 3.66 secs\n",
      "\u001b[1m---- Epoch 52/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.06216, sae_loss 1.06216, 71.74 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03306, 3.58 secs\n",
      "\u001b[1m---- Epoch 53/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.07564, sae_loss 1.07564, 70.83 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03032, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_53_sae_loss=0.4915.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 54/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.06599, sae_loss 1.06599, 71.25 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02745, 3.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_54_sae_loss=0.4923.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 55/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.07245, sae_loss 1.07245, 70.81 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02620, 3.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_55_sae_loss=0.4924.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 56/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.06455, sae_loss 1.06455, 71.43 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02534, 3.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_56_sae_loss=0.4928.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 57/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.07107, sae_loss 1.07107, 70.88 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02495, 3.79 secs\n",
      "\u001b[1m---- Epoch 58/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.06547, sae_loss 1.06547, 70.44 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02486, 3.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_58_sae_loss=0.4929.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 59/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.06487, sae_loss 1.06487, 70.72 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02471, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_59_sae_loss=0.4929.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 60/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.07147, sae_loss 1.07147, 70.86 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02468, 3.59 secs\n",
      "\u001b[1m---- Epoch 61/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.05096, sae_loss 1.05096, 72.55 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.02194, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_61_sae_loss=0.4939.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 62/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.05674, sae_loss 1.05674, 71.09 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01951, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_62_sae_loss=0.4943.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 63/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.05839, sae_loss 1.05839, 71.52 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01838, 3.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_63_sae_loss=0.4945.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 64/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.04770, sae_loss 1.04770, 71.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01801, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_sae_loss=0.4948.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.05343, sae_loss 1.05343, 70.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01750, 3.60 secs\n",
      "\u001b[1m---- Epoch 66/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.05280, sae_loss 1.05280, 71.72 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01732, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_66_sae_loss=0.4949.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 67/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.05232, sae_loss 1.05232, 71.84 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01716, 3.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_67_sae_loss=0.4949.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 68/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.05858, sae_loss 1.05858, 70.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01713, 4.13 secs\n",
      "\u001b[1m---- Epoch 69/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.05067, sae_loss 1.05067, 71.05 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01504, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_69_sae_loss=0.4954.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 70/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.05124, sae_loss 1.05124, 71.17 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01277, 3.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_70_sae_loss=0.4959.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 71/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.05938, sae_loss 1.05938, 70.71 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01129, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_71_sae_loss=0.4960.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 72/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.05464, sae_loss 1.05464, 70.93 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01091, 3.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_72_sae_loss=0.4962.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 73/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.04381, sae_loss 1.04381, 72.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01003, 3.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_73_sae_loss=0.4967.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 74/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.05170, sae_loss 1.05170, 71.63 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00997, 3.60 secs\n",
      "\u001b[1m---- Epoch 75/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.04814, sae_loss 1.04814, 71.00 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.01001, 3.35 secs\n",
      "\u001b[1m---- Epoch 76/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.05267, sae_loss 1.05267, 71.53 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00997, 3.50 secs\n",
      "\u001b[1m---- Epoch 77/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.04760, sae_loss 1.04760, 71.75 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00836, 3.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_77_sae_loss=0.4970.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 78/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.04709, sae_loss 1.04709, 71.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00596, 3.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_78_sae_loss=0.4975.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 79/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.04949, sae_loss 1.04949, 70.77 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_loss 1.00488, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_79_sae_loss=0.4977.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 80/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.04154, sae_loss 1.04154, 71.13 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00455, 3.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_80_sae_loss=0.4980.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 81/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.04263, sae_loss 1.04263, 70.87 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00403, 3.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_81_sae_loss=0.4981.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 82/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.03988, sae_loss 1.03988, 71.65 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00366, 3.82 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_82_sae_loss=0.4982.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 83/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.04634, sae_loss 1.04634, 71.39 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00364, 3.46 secs\n",
      "\u001b[1m---- Epoch 84/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.04564, sae_loss 1.04564, 70.52 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00365, 3.90 secs\n",
      "\u001b[1m---- Epoch 85/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.03686, sae_loss 1.03686, 71.59 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00135, 3.85 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_85_sae_loss=0.4988.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 86/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.03923, sae_loss 1.03923, 71.92 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99939, 3.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_86_sae_loss=0.4992.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 87/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.04143, sae_loss 1.04143, 70.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99824, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_87_sae_loss=0.4994.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 88/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.03579, sae_loss 1.03579, 71.54 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99775, 3.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_88_sae_loss=0.4996.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 89/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.03405, sae_loss 1.03405, 71.42 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99774, 3.89 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_89_sae_loss=0.4997.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 90/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.03619, sae_loss 1.03619, 71.04 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99750, 3.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_90_sae_loss=0.4997.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 91/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.03181, sae_loss 1.03181, 71.58 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99733, 3.98 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_91_sae_loss=0.4998.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 92/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.03545, sae_loss 1.03545, 71.52 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99733, 3.53 secs\n",
      "\u001b[1m---- Epoch 93/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.03067, sae_loss 1.03067, 71.57 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99609, 3.51 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_93_sae_loss=0.5001.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 94/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.03918, sae_loss 1.03918, 71.68 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99377, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_94_sae_loss=0.5004.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 95/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.03963, sae_loss 1.03963, 70.55 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99248, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_95_sae_loss=0.5007.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 96/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.03088, sae_loss 1.03088, 70.93 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99211, 3.60 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_96_sae_loss=0.5010.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 97/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.02829, sae_loss 1.02829, 71.49 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99193, 3.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_97_sae_loss=0.5011.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 98/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.02220, sae_loss 1.02220, 72.54 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99176, 3.75 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_98_sae_loss=0.5013.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 99/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.02525, sae_loss 1.02525, 72.13 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99155, 3.72 secs\n",
      "\u001b[1m---- Epoch 100/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.02733, sae_loss 1.02733, 71.74 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99154, 3.63 secs\n",
      "\u001b[1m---- Epoch 101/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.03342, sae_loss 1.03342, 70.95 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.99111, 3.46 secs\n",
      "\u001b[1m---- Epoch 102/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.03218, sae_loss 1.03218, 71.46 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98884, 3.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_102_sae_loss=0.5017.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 103/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.02553, sae_loss 1.02553, 71.54 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98741, 3.79 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_103_sae_loss=0.5022.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 104/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.02656, sae_loss 1.02656, 71.43 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98694, 3.88 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_104_sae_loss=0.5023.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 105/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.02159, sae_loss 1.02159, 74.51 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98644, 4.00 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_105_sae_loss=0.5025.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 106/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.02752, sae_loss 1.02752, 72.03 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98633, 3.75 secs\n",
      "\u001b[1m---- Epoch 107/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.01878, sae_loss 1.01878, 71.66 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98623, 3.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_107_sae_loss=0.5027.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 108/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.03337, sae_loss 1.03337, 70.92 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98618, 3.73 secs\n",
      "\u001b[1m---- Epoch 109/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.02500, sae_loss 1.02500, 71.57 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98524, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_109_sae_loss=0.5027.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 110/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.02658, sae_loss 1.02658, 71.91 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98328, 3.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_110_sae_loss=0.5031.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 111/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.02583, sae_loss 1.02583, 72.50 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98191, 3.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_111_sae_loss=0.5035.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 112/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.02904, sae_loss 1.02904, 70.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98144, 3.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_112_sae_loss=0.5035.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 113/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.01634, sae_loss 1.01634, 71.11 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98107, 3.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_113_sae_loss=0.5039.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 114/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.01734, sae_loss 1.01734, 71.63 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98098, 3.57 secs\n",
      "\u001b[1m---- Epoch 115/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.02117, sae_loss 1.02117, 71.58 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98088, 3.66 secs\n",
      "\u001b[1m---- Epoch 116/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.01603, sae_loss 1.01603, 71.72 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.98079, 3.78 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_116_sae_loss=0.5040.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 117/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.02731, sae_loss 1.02731, 71.19 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.97966, 3.92 secs\n",
      "\u001b[1m---- Epoch 118/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 1.02955, sae_loss 1.02955, 71.91 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_loss 0.97796, 3.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_118_sae_loss=0.5043.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 119/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 1.01793, sae_loss 1.01793, 72.45 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.97705, 4.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_119_sae_loss=0.5048.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 120/120\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 1.01896, sae_loss 1.01896, 72.50 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.97650, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_120_sae_loss=0.5049.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_080743_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 120 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 200 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 3 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--sentence_autoencoder_weight 1.0 \\\n",
    "--sentences_and_cluster_ids_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\" \\\n",
    "--dataset_name \"MIMIC-CXR(autoencoder)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--freeze_huggingface_model \\\n",
    "--embedding_size 128 \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf6e5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 180\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 100\n",
      "   val_batch_size: None\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_110017_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   freeze_huggingface_model: False\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   fact_decoder_embed_size: 256\n",
      "   fact_decoder_hidden_size: 256\n",
      "   fact_decoder_nhead: 1\n",
      "   fact_decoder_dim_feedforward: 256\n",
      "   fact_decoder_num_layers: 1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 3\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: None\n",
      "   triplet_rule_weights: None\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   integrated_sentence_facts_jsonl_filepath: None\n",
      "   gpt4_radnli_labels_jsonl_filepath: None\n",
      "   sentences_and_cluster_ids_filepath: /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\n",
      "   dataset_name: MIMIC-CXR(autoencoder)\n",
      "   triplets_weight: 0\n",
      "   metadata_classification_weight: 0\n",
      "   chest_imagenome_observations_classification_weight: 0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0\n",
      "   nli_weight: 0\n",
      "   entcon_weight: 0\n",
      "   radgraph_ner_re_weight: 0\n",
      "   sentence_autoencoder_weight: 1.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Number of sentences: 3882542\n",
      "Number of cluster IDs: 3882542\n",
      "Number of clusters: 1000\n",
      "Number of sentences in largest cluster: 27433\n",
      "Number of sentences in smallest cluster: 440\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/fact_decoding_vocab(116, 1862760546599818235).pkl ...\n",
      "Vocabulary size: 15773\n",
      "Number of tokens in vocab: 15773\n",
      "Loading sentence IDs from cache...\n",
      "Number of validation samples: 5000\n",
      "Number of training samples: 3877542\n",
      "len(_train_dataloaders) = 1\n",
      "len(_train_weights) = 1\n",
      "_train_weights = [1.0]\n",
      "len(_val_dataloaders) = 1\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(autoencoder)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "  use_fact_decoder: True\n",
      "  fact_decoder_embed_size: 256\n",
      "  fact_decoder_hidden_size: 256\n",
      "  fact_decoder_nhead: 1\n",
      "  fact_decoder_dim_feedforward: 256\n",
      "  fact_decoder_num_layers: 1\n",
      "  fact_decoder_start_idx: 1\n",
      "  fact_decoder_vocab_size: 15773\n",
      "  fact_decoder_dropout_prob: 0\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_110017_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 3, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_152451_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_152451_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_120_sae_loss=0.5049.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_110017_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_120_sae_loss=0.5049.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_152451_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.10057, sae_loss 1.10057, 79.66 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.03228, 2.83 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_sae_loss=0.4905.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.07744, sae_loss 1.07744, 76.36 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 1.00338, 2.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_sae_loss=0.4974.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 1.05288, sae_loss 1.05288, 78.45 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.95808, 2.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_sae_loss=0.5083.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 1.00353, sae_loss 1.00353, 79.03 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.88215, 2.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_sae_loss=0.5281.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.97000, sae_loss 0.97000, 77.65 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.85077, 2.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_sae_loss=0.5370.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.93839, sae_loss 0.93839, 79.21 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.83449, 2.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_sae_loss=0.5422.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.92606, sae_loss 0.92606, 79.57 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.82509, 2.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_sae_loss=0.5450.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.92769, sae_loss 0.92769, 79.57 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.81985, 2.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_sae_loss=0.5464.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.91758, sae_loss 0.91758, 81.33 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.81716, 2.95 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_sae_loss=0.5474.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.92535, sae_loss 0.92535, 80.27 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.81501, 2.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_sae_loss=0.5478.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.92770, sae_loss 0.92770, 79.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.81426, 2.94 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_sae_loss=0.5479.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.92003, sae_loss 0.92003, 79.88 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.81349, 2.97 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_12_sae_loss=0.5484.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 13/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.91421, sae_loss 0.91421, 79.26 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.78468, 2.99 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_13_sae_loss=0.5565.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 14/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.88502, sae_loss 0.88502, 81.75 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.76958, 3.00 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_14_sae_loss=0.5616.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 15/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.87782, sae_loss 0.87782, 81.80 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.76035, 3.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_sae_loss=0.5645.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.87194, sae_loss 0.87194, 82.43 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.75511, 3.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_sae_loss=0.5662.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.87797, sae_loss 0.87797, 81.86 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.75128, 3.03 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_sae_loss=0.5672.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.87034, sae_loss 0.87034, 82.42 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.74977, 3.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_sae_loss=0.5678.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.86002, sae_loss 0.86002, 83.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.74933, 3.09 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_sae_loss=0.5682.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.86887, sae_loss 0.86887, 83.19 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.74891, 3.40 secs\n",
      "\u001b[1m---- Epoch 21/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.86043, sae_loss 0.86043, 84.35 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.73465, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_21_sae_loss=0.5726.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 22/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.85562, sae_loss 0.85562, 82.71 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.71956, 3.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_sae_loss=0.5773.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.83602, sae_loss 0.83602, 84.49 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.71079, 3.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_sae_loss=0.5805.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.84204, sae_loss 0.84204, 83.22 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.70772, 3.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_sae_loss=0.5813.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.82916, sae_loss 0.82916, 84.63 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.70520, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_25_sae_loss=0.5825.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 26/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.83834, sae_loss 0.83834, 84.16 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.70360, 3.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_sae_loss=0.5827.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.82342, sae_loss 0.82342, 85.67 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.70334, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_27_sae_loss=0.5832.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 28/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.82751, sae_loss 0.82751, 82.97 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.70307, 3.22 secs\n",
      "\u001b[1m---- Epoch 29/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.83090, sae_loss 0.83090, 84.75 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.69254, 3.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_29_sae_loss=0.5864.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 30/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.81440, sae_loss 0.81440, 83.61 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.68042, 3.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_30_sae_loss=0.5907.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 31/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.80749, sae_loss 0.80749, 83.74 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.67385, 3.32 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_31_sae_loss=0.5930.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 32/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.80505, sae_loss 0.80505, 84.59 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.67040, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_32_sae_loss=0.5942.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 33/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.80760, sae_loss 0.80760, 84.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.66940, 3.20 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_33_sae_loss=0.5944.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 34/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.79734, sae_loss 0.79734, 84.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.66797, 3.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_34_sae_loss=0.5952.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 35/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.79667, sae_loss 0.79667, 85.25 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.66757, 3.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_35_sae_loss=0.5954.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 36/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.79751, sae_loss 0.79751, 84.11 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.66763, 3.71 secs\n",
      "\u001b[1m---- Epoch 37/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.80383, sae_loss 0.80383, 83.62 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.66114, 3.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_37_sae_loss=0.5972.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 38/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.79482, sae_loss 0.79482, 86.22 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.65137, 3.34 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_38_sae_loss=0.6007.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 39/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.78633, sae_loss 0.78633, 85.79 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.64629, 3.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_39_sae_loss=0.6027.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 40/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.77034, sae_loss 0.77034, 86.75 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.64279, 3.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_sae_loss=0.6043.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.77538, sae_loss 0.77538, 87.28 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.64137, 3.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_41_sae_loss=0.6046.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 42/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.78413, sae_loss 0.78413, 85.05 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.64130, 3.44 secs\n",
      "\u001b[1m---- Epoch 43/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.77956, sae_loss 0.77956, 86.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.63998, 3.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_43_sae_loss=0.6050.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 44/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.77863, sae_loss 0.77863, 87.10 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.63988, 3.67 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_44_sae_loss=0.6050.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 45/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.77979, sae_loss 0.77979, 85.80 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.63598, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_45_sae_loss=0.6063.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 46/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.76592, sae_loss 0.76592, 86.11 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.62608, 3.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_46_sae_loss=0.6101.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 47/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.75751, sae_loss 0.75751, 85.18 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.62165, 3.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_sae_loss=0.6119.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.75791, sae_loss 0.75791, 85.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.61999, 3.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_sae_loss=0.6124.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.75163, sae_loss 0.75163, 85.09 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.61748, 3.80 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_49_sae_loss=0.6135.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 50/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.74745, sae_loss 0.74745, 84.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.61670, 3.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_50_sae_loss=0.6139.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 51/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.76002, sae_loss 0.76002, 86.21 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.61621, 3.56 secs\n",
      "\u001b[1m---- Epoch 52/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.74946, sae_loss 0.74946, 85.53 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.61664, 3.47 secs\n",
      "\u001b[1m---- Epoch 53/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.76683, sae_loss 0.76683, 83.16 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.61588, 3.17 secs\n",
      "\u001b[1m---- Epoch 54/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.75134, sae_loss 0.75134, 84.35 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.60505, 3.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_54_sae_loss=0.6178.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 55/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.74594, sae_loss 0.74594, 85.39 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.60043, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_55_sae_loss=0.6196.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 56/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.74152, sae_loss 0.74152, 85.34 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.59867, 3.48 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_56_sae_loss=0.6204.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 57/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.75327, sae_loss 0.75327, 83.38 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.59816, 3.11 secs\n",
      "\u001b[1m---- Epoch 58/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.73770, sae_loss 0.73770, 84.51 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.59780, 3.16 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_58_sae_loss=0.6208.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 59/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.74525, sae_loss 0.74525, 82.85 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.59732, 3.33 secs\n",
      "\u001b[1m---- Epoch 60/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.74387, sae_loss 0.74387, 82.44 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.59721, 3.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_60_sae_loss=0.6208.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 61/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.74438, sae_loss 0.74438, 82.73 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.59693, 3.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_61_sae_loss=0.6209.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 62/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.73214, sae_loss 0.73214, 82.59 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.58700, 3.21 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_62_sae_loss=0.6248.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 63/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.73417, sae_loss 0.73417, 84.29 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.58520, 3.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_63_sae_loss=0.6254.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 64/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.73290, sae_loss 0.73290, 82.24 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.58186, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_sae_loss=0.6267.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.72510, sae_loss 0.72510, 83.97 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.58061, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_65_sae_loss=0.6274.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 66/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.72500, sae_loss 0.72500, 84.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.58018, 3.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_66_sae_loss=0.6275.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 67/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.72172, sae_loss 0.72172, 83.77 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.57982, 3.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_67_sae_loss=0.6278.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 68/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.72752, sae_loss 0.72752, 82.93 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.57959, 3.20 secs\n",
      "\u001b[1m---- Epoch 69/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.72527, sae_loss 0.72527, 85.10 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.58212, 3.22 secs\n",
      "\u001b[1m---- Epoch 70/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.72588, sae_loss 0.72588, 83.03 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.57411, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_70_sae_loss=0.6297.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 71/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.71566, sae_loss 0.71566, 84.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.56892, 3.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_71_sae_loss=0.6319.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 72/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.71795, sae_loss 0.71795, 82.85 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.56724, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_72_sae_loss=0.6325.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 73/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.70903, sae_loss 0.70903, 84.23 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.56520, 3.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_73_sae_loss=0.6335.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 74/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.72000, sae_loss 0.72000, 84.61 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.56534, 3.38 secs\n",
      "\u001b[1m---- Epoch 75/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.71652, sae_loss 0.71652, 83.38 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.56519, 3.58 secs\n",
      "\u001b[1m---- Epoch 76/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.71437, sae_loss 0.71437, 82.79 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.56475, 3.19 secs\n",
      "\u001b[1m---- Epoch 77/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.71290, sae_loss 0.71290, 84.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.56663, 3.20 secs\n",
      "\u001b[1m---- Epoch 78/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.70511, sae_loss 0.70511, 84.07 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_loss 0.55919, 3.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_78_sae_loss=0.6359.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 79/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.71136, sae_loss 0.71136, 85.84 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.55601, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_79_sae_loss=0.6368.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 80/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.70007, sae_loss 0.70007, 86.37 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.55401, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_80_sae_loss=0.6380.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 81/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.70250, sae_loss 0.70250, 85.12 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.55195, 3.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_81_sae_loss=0.6387.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 82/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.70944, sae_loss 0.70944, 83.69 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.55146, 3.37 secs\n",
      "\u001b[1m---- Epoch 83/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.70403, sae_loss 0.70403, 83.96 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.55156, 3.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_83_sae_loss=0.6387.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 84/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.69518, sae_loss 0.69518, 84.58 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.55109, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_84_sae_loss=0.6392.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 85/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.71216, sae_loss 0.71216, 82.99 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.55418, 3.08 secs\n",
      "\u001b[1m---- Epoch 86/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.69506, sae_loss 0.69506, 84.38 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.54601, 3.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_86_sae_loss=0.6411.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 87/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.69484, sae_loss 0.69484, 84.59 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.54300, 3.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_87_sae_loss=0.6423.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 88/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.69485, sae_loss 0.69485, 84.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.54125, 3.24 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_88_sae_loss=0.6429.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 89/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.69475, sae_loss 0.69475, 85.09 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.54075, 3.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_89_sae_loss=0.6431.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 90/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.69099, sae_loss 0.69099, 84.27 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.53944, 3.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_90_sae_loss=0.6438.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 91/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.69416, sae_loss 0.69416, 83.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.53980, 3.53 secs\n",
      "\u001b[1m---- Epoch 92/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.69064, sae_loss 0.69064, 84.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.53947, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_92_sae_loss=0.6438.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 93/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.69902, sae_loss 0.69902, 84.90 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.54170, 3.33 secs\n",
      "\u001b[1m---- Epoch 94/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.69331, sae_loss 0.69331, 83.80 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.53696, 3.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_94_sae_loss=0.6446.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 95/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.68538, sae_loss 0.68538, 84.35 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.53292, 3.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_95_sae_loss=0.6464.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 96/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.69069, sae_loss 0.69069, 83.50 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.53015, 3.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_96_sae_loss=0.6473.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 97/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.68229, sae_loss 0.68229, 84.59 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52889, 3.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_97_sae_loss=0.6481.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 98/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.67969, sae_loss 0.67969, 83.27 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52843, 3.30 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_98_sae_loss=0.6484.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 99/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.67908, sae_loss 0.67908, 83.41 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52884, 3.35 secs\n",
      "\u001b[1m---- Epoch 100/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.68176, sae_loss 0.68176, 84.04 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52840, 3.42 secs\n",
      "\u001b[1m---- Epoch 101/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.68593, sae_loss 0.68593, 84.15 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.53262, 3.74 secs\n",
      "\u001b[1m---- Epoch 102/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.68055, sae_loss 0.68055, 84.73 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52645, 3.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_102_sae_loss=0.6491.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 103/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.67857, sae_loss 0.67857, 85.26 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52248, 3.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_103_sae_loss=0.6507.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 104/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.67879, sae_loss 0.67879, 84.73 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52037, 3.84 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_104_sae_loss=0.6515.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 105/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.67486, sae_loss 0.67486, 84.94 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51992, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_105_sae_loss=0.6518.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 106/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.67261, sae_loss 0.67261, 84.91 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51912, 3.38 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_106_sae_loss=0.6522.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 107/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.66921, sae_loss 0.66921, 84.77 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51909, 3.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_107_sae_loss=0.6524.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 108/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.67452, sae_loss 0.67452, 83.98 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51852, 3.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_108_sae_loss=0.6524.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 109/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.67558, sae_loss 0.67558, 84.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.52431, 3.57 secs\n",
      "\u001b[1m---- Epoch 110/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.66899, sae_loss 0.66899, 84.29 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51499, 3.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_110_sae_loss=0.6540.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 111/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.67152, sae_loss 0.67152, 83.84 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51247, 3.88 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_111_sae_loss=0.6549.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 112/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.66700, sae_loss 0.66700, 84.53 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51068, 3.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_112_sae_loss=0.6557.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 113/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.66634, sae_loss 0.66634, 84.20 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50992, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_113_sae_loss=0.6561.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 114/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.66712, sae_loss 0.66712, 84.70 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50935, 3.61 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_114_sae_loss=0.6563.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 115/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.66671, sae_loss 0.66671, 84.06 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50975, 3.33 secs\n",
      "\u001b[1m---- Epoch 116/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.66461, sae_loss 0.66461, 85.87 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50955, 3.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_116_sae_loss=0.6563.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 117/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.67963, sae_loss 0.67963, 83.98 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.51468, 3.58 secs\n",
      "\u001b[1m---- Epoch 118/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.66915, sae_loss 0.66915, 84.81 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50680, 3.63 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_118_sae_loss=0.6572.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 119/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.65891, sae_loss 0.65891, 85.88 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50444, 3.43 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_119_sae_loss=0.6585.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 120/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.65749, sae_loss 0.65749, 84.97 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50267, 3.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_120_sae_loss=0.6593.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 121/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.65575, sae_loss 0.65575, 85.37 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50122, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_121_sae_loss=0.6599.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 122/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.66090, sae_loss 0.66090, 85.23 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50107, 3.55 secs\n",
      "\u001b[1m---- Epoch 123/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.66560, sae_loss 0.66560, 86.48 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50039, 3.53 secs\n",
      "\u001b[1m---- Epoch 124/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.65935, sae_loss 0.65935, 85.29 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50033, 3.27 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_124_sae_loss=0.6601.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 125/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.66201, sae_loss 0.66201, 84.95 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.50761, 3.33 secs\n",
      "\u001b[1m---- Epoch 126/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.65693, sae_loss 0.65693, 85.16 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49850, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_126_sae_loss=0.6610.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 127/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.65391, sae_loss 0.65391, 84.51 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49569, 3.46 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_127_sae_loss=0.6622.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 128/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.65436, sae_loss 0.65436, 85.07 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49500, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_128_sae_loss=0.6625.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 129/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.64737, sae_loss 0.64737, 84.91 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49372, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_129_sae_loss=0.6632.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 130/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.65346, sae_loss 0.65346, 84.78 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49317, 3.68 secs\n",
      "\u001b[1m---- Epoch 131/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.65199, sae_loss 0.65199, 85.96 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49345, 3.20 secs\n",
      "\u001b[1m---- Epoch 132/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.64583, sae_loss 0.64583, 84.61 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49303, 3.37 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_132_sae_loss=0.6636.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 133/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.65932, sae_loss 0.65932, 85.09 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49370, 3.23 secs\n",
      "\u001b[1m---- Epoch 134/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.65392, sae_loss 0.65392, 85.02 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.49051, 3.31 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_134_sae_loss=0.6643.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 135/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.64124, sae_loss 0.64124, 85.78 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48878, 3.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_135_sae_loss=0.6655.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 136/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63723, sae_loss 0.63723, 84.43 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48728, 3.53 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_136_sae_loss=0.6662.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 137/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.64581, sae_loss 0.64581, 84.61 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48631, 3.42 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_137_sae_loss=0.6663.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 138/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.64332, sae_loss 0.64332, 84.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48578, 3.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_138_sae_loss=0.6666.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 139/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.64689, sae_loss 0.64689, 84.53 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48549, 3.55 secs\n",
      "\u001b[1m---- Epoch 140/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63979, sae_loss 0.63979, 85.15 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48532, 3.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_140_sae_loss=0.6669.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 141/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.65396, sae_loss 0.65396, 85.45 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48802, 3.21 secs\n",
      "\u001b[1m---- Epoch 142/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63794, sae_loss 0.63794, 84.92 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48603, 3.30 secs\n",
      "\u001b[1m---- Epoch 143/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.64496, sae_loss 0.64496, 84.47 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48114, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_143_sae_loss=0.6684.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 144/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.64211, sae_loss 0.64211, 84.61 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48011, 3.44 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_144_sae_loss=0.6690.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 145/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63755, sae_loss 0.63755, 85.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47931, 3.49 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_145_sae_loss=0.6695.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 146/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.63679, sae_loss 0.63679, 84.64 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47900, 3.62 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_146_sae_loss=0.6696.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 147/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.63891, sae_loss 0.63891, 85.39 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47881, 3.47 secs\n",
      "\u001b[1m---- Epoch 148/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.63773, sae_loss 0.63773, 84.44 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47855, 3.52 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_148_sae_loss=0.6698.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 149/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64589, sae_loss 0.64589, 84.94 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.48581, 3.54 secs\n",
      "\u001b[1m---- Epoch 150/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.64118, sae_loss 0.64118, 85.33 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47725, 3.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_150_sae_loss=0.6702.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 151/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.63386, sae_loss 0.63386, 84.94 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47440, 4.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_151_sae_loss=0.6716.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 152/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.63979, sae_loss 0.63979, 85.32 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47333, 3.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_152_sae_loss=0.6718.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 153/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.63048, sae_loss 0.63048, 86.03 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47251, 3.84 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_153_sae_loss=0.6725.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 154/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62752, sae_loss 0.62752, 85.27 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47236, 3.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_154_sae_loss=0.6727.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 155/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62798, sae_loss 0.62798, 84.72 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47193, 3.57 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_155_sae_loss=0.6729.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 156/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.63198, sae_loss 0.63198, 83.83 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47186, 3.38 secs\n",
      "\u001b[1m---- Epoch 157/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.64527, sae_loss 0.64527, 83.22 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47559, 3.21 secs\n",
      "\u001b[1m---- Epoch 158/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63338, sae_loss 0.63338, 84.18 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47176, 3.32 secs\n",
      "\u001b[1m---- Epoch 159/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.62848, sae_loss 0.62848, 85.56 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46768, 4.01 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_159_sae_loss=0.6746.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 160/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.62777, sae_loss 0.62777, 86.55 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46754, 3.70 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_160_sae_loss=0.6747.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 161/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.62105, sae_loss 0.62105, 87.63 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46621, 3.66 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_161_sae_loss=0.6755.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 162/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62652, sae_loss 0.62652, 86.56 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46592, 3.90 secs\n",
      "\u001b[1m---- Epoch 163/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.62408, sae_loss 0.62408, 88.17 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46558, 3.73 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_163_sae_loss=0.6757.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 164/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.62784, sae_loss 0.62784, 85.57 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46552, 3.24 secs\n",
      "\u001b[1m---- Epoch 165/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.63198, sae_loss 0.63198, 84.50 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.47094, 3.36 secs\n",
      "\u001b[1m---- Epoch 166/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.63753, sae_loss 0.63753, 83.89 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46520, 3.53 secs\n",
      "\u001b[1m---- Epoch 167/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.62877, sae_loss 0.62877, 84.42 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46305, 3.77 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_167_sae_loss=0.6765.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 168/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.61749, sae_loss 0.61749, 86.06 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46185, 3.59 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_168_sae_loss=0.6775.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 169/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.62510, sae_loss 0.62510, 85.52 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46008, 3.64 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_169_sae_loss=0.6779.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 170/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.62681, sae_loss 0.62681, 85.14 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45997, 3.49 secs\n",
      "\u001b[1m---- Epoch 171/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.61848, sae_loss 0.61848, 85.66 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45992, 3.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_171_sae_loss=0.6783.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 172/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.61952, sae_loss 0.61952, 85.65 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45996, 3.64 secs\n",
      "\u001b[1m---- Epoch 173/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.62307, sae_loss 0.62307, 84.98 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.46892, 3.71 secs\n",
      "\u001b[1m---- Epoch 174/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.62471, sae_loss 0.62471, 85.49 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45973, 3.37 secs\n",
      "\u001b[1m---- Epoch 175/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.61344, sae_loss 0.61344, 86.12 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45719, 3.39 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_175_sae_loss=0.6796.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 176/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.61619, sae_loss 0.61619, 85.00 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45591, 3.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_176_sae_loss=0.6800.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 177/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.60757, sae_loss 0.60757, 86.37 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45551, 3.41 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_177_sae_loss=0.6805.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 178/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.60656, sae_loss 0.60656, 86.30 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45499, 3.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_178_sae_loss=0.6808.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 179/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.61894, sae_loss 0.61894, 84.76 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45483, 3.19 secs\n",
      "\u001b[1m---- Epoch 180/180\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.61503, sae_loss 0.61503, 86.97 secs\n",
      "(2) Validation stage ...\n",
      "sae_loss 0.45432, 3.72 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231010_110017_MIMIC-CXR(autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 180 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 100 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 3 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--sentence_autoencoder_weight 1.0 \\\n",
    "--sentences_and_cluster_ids_filepath \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/sentence_cluster_ids((2589, 1439222964121874843)).pkl\" \\\n",
    "--dataset_name \"MIMIC-CXR(autoencoder)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
