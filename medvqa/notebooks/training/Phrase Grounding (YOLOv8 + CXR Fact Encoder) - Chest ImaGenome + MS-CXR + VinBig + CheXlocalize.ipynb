{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5b5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mObtaining embeddings for each sentence...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_91_cacc+chf1+chf1+cscc+encc+hscc+nlcc+sass+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9248.pt']\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]Loading cached text embeddings from /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=426,702090393457262989).pkl\n",
      "len(self.cache[\"hashes\"]) = 29579\n",
      "self.cache[\"embeddings\"].shape = (29579, 128)\n",
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 421.74it/s]\n",
      "Computing embeddings for 5 new texts\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "checkpoint_names = ['checkpoint_91_cacc+chf1+chf1+cscc+encc+hscc+nlcc+sass+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9248.pt']\n",
      "Loading model weights from /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231018_230637_MIMIC-CXR(triplets+classif+entcont+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_91_cacc+chf1+chf1+cscc+encc+hscc+nlcc+sass+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9248.pt\n",
      "\u001b[93mWarning: model state dict has 211 keys, loaded state dict has 253 keys, intersection has 211 keys, union has 253 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in loaded state dict but not in model:\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.norm1.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.multihead_attn.out_proj.weight\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.self_attn.in_proj_bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.decoder.layers.0.self_attn.in_proj_weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.bias\u001b[0m\n",
      "\u001b[93m  chest_imagenome_anatloc_classifier.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder.W_vocab.bias\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  fact_decoder_input_layer.weight\u001b[0m\n",
      "\u001b[93m  comparison_status_classifier.weight\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "Saving updated cache to /mnt/workspace/pamessina/medvqa-workspace/cache/text_embeddings_cache(hash=426,702090393457262989).pkl\n",
      "100%|██████████████████████████████████████████| 5/5 [00:00<00:00, 80971.12it/s]\n",
      "\u001b[1m\u001b[34mSaving output to:\u001b[0m \u001b[1m\u001b[34m/mnt/workspace/pamessina/medvqa-workspace/cache/chexlocalize/class_phrase_embeddings(hash=322,2913146136894882617).pkl\u001b[0m\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/chexlocalize/precompute_phrase_embeddings.py \\\n",
    "--model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--model_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231018_230637_MIMIC-CXR(triplets+classif+entcont+nli+autoencoder)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8833170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 125\n",
      "   batches_per_epoch: 600\n",
      "   max_images_per_batch: 35\n",
      "   max_phrases_per_batch: 600\n",
      "   max_phrases_per_image: 30\n",
      "   val_batch_size_factor: 1.5\n",
      "   checkpoint_folder: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_folder_paths: ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231219_195906_mscxr+chst-img-anat+vinbig_PhraseGrounder(yolov8l,128,256)']\n",
      "   freeze_image_encoder: False\n",
      "   raw_image_encoding: yolov8\n",
      "   num_regions: 169\n",
      "   image_local_feat_size: 512\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   yolov8_model_name_or_path: yolov8l.pt\n",
      "   yolov8_model_alias: yolov8l\n",
      "   phrase_embedding_size: 128\n",
      "   regions_width: 13\n",
      "   regions_height: 13\n",
      "   qkv_size: 256\n",
      "   phrase_classifier_hidden_size: 256\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,3e-5,8,1e-6,3e-5,8,1e-6\n",
      "   iters_to_accumulate: 12\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   attention_supervision_loss_weight: 3.0\n",
      "   phrase_classifier_loss_weight: 2.0\n",
      "   foreground_loss_weight: 1.5\n",
      "   background_loss_weight: 1.0\n",
      "   num_train_workers: 2\n",
      "   num_val_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   image_size: [416, 416]\n",
      "   dicom_id_to_pos_neg_facts_filepath: None\n",
      "   mscxr_phrase2embedding_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/mscxr_phrase2embedding.pkl\n",
      "   chest_imagenome_bbox_phrase_embeddings_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/chest_imagenome/bbox_phrase_embeddings.pkl\n",
      "   vinbig_bbox_phrase_embeddings_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/vinbig/bbox_phrase_embeddings.pkl\n",
      "   chexlocalize_class_phrase_embeddings_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/chexlocalize/class_phrase_embeddings(hash=322,2913146136894882617).pkl\n",
      "   exclude_noisy_images: False\n",
      "   mimiccxr_facts_weight: 1.0\n",
      "   chest_imagenome_anatlocs_weight: 2.5\n",
      "   mscxr_weight: 1.0\n",
      "   vinbig_weight: 6.0\n",
      "   chexlocalize_weight: 2.5\n",
      "   img_aug_mode: None\n",
      "   pos_area_prior: 0.4\n",
      "   neg_area_prior: 0.0\n",
      "   use_mimiccxr_facts_for_train: False\n",
      "   use_mscxr_for_train: True\n",
      "   use_mscxr_for_test: True\n",
      "   use_chest_imagenome_for_train: True\n",
      "   use_chest_imagenome_gold_for_test: True\n",
      "   use_vinbig_for_train: True\n",
      "   use_vinbig_for_test: True\n",
      "   use_chexlocalize_for_train: True\n",
      "   use_chexlocalize_for_test: True\n",
      "   vinbig_training_data_mode: train\n",
      "   mask_exponent: 0.4\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "source_image_size_mode: medium_512\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of PhraseGrounder ...\u001b[0m\n",
      "\u001b[93mWARNING: Unused kwargs: {'pretrained_checkpoint_folder_path': None, 'pretrained_checkpoint_folder_paths': ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231219_195906_mscxr+chst-img-anat+vinbig_PhraseGrounder(yolov8l,128,256)']}\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: yolov8\n",
      "  nc_list: [36, 22]\n",
      "\u001b[1mCreating YOLOv8 model for multiple datasets\u001b[0m\n",
      "\u001b[1m   1. Creating DetectionModel for 36 classes\u001b[0m\n",
      "Overriding model.yaml nc=80 with nc=36\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.Conv                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.C2f                   [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.C2f                   [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.C2f                   [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.C2f                   [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.C2f                   [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.C2f                   [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5610556  ultralytics.nn.modules.Detect                [36, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43657596 parameters, 43657580 gradients, 165.6 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[1m   2. Creating DetectionModel for 22 classes\u001b[0m\n",
      "Overriding model.yaml nc=80 with nc=22\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.Conv                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.C2f                   [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.C2f                   [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.C2f                   [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.C2f                   [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.C2f                   [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18                  -1  3   4592640  ultralytics.nn.modules.C2f                   [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5599762  ultralytics.nn.modules.Detect                [22, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43646802 parameters, 43646786 gradients, 165.5 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[1m   Creating final YOLOv8 model with 2 detection layers\u001b[0m\n",
      "Overriding model.yaml nc=80 with nc=36\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.Conv                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.C2f                   [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.C2f                   [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.C2f                   [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.C2f                   [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.C2f                   [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.C2f                   [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5610556  ultralytics.nn.modules.Detect                [36, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43657596 parameters, 43657580 gradients, 165.6 GFLOPs\n",
      "\n",
      "Transferred 510/510 items from pretrained weights\n",
      "  self.global_feat_size = 1024\n",
      "  self.local_feat_size = 512\n",
      "  Initializing auxiliary tasks\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,3e-5,8,1e-6,3e-5,8,1e-6\n",
      "1e-06 3 3e-05 8 1e-06 3e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 3e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "\u001b[1m\u001b[35mUsing YOLOv8MultiDetectionLayersLoss\u001b[0m\n",
      "foreground_loss_weight: 1.5\n",
      "background_loss_weight: 1.0\n",
      "Using multiple detection layers in yolov8\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 12, max_grad_norm = None\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "Using multiple detection layers in yolov8\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating CheXLocalize Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "Loding class_phrase_embeddings_filepath and bbox_phrases from /mnt/workspace/pamessina/medvqa-workspace/cache/chexlocalize/class_phrase_embeddings(hash=322,2913146136894882617).pkl...\n",
      "class_phrase_embeddings.shape = (10, 128)\n",
      "len(class_phrases) = 10\n",
      "\t Enlarged Cardiomediastinum\n",
      "\t Cardiomegaly\n",
      "\t Lung Lesion\n",
      "\t Airspace Opacity\n",
      "\t Edema\n",
      "\t Consolidation\n",
      "\t Atelectasis\n",
      "\t Pneumothorax\n",
      "\t Pleural Effusion\n",
      "\t Support Devices\n",
      "Compute phrase grounding masks and labels\n",
      "Without masks: 216/902\n",
      "Generating train dataset and dataloader\n",
      "batch_size = 35\n",
      "Generating val dataset and dataloader\n",
      "batch_size = 52\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating VinBig Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "Loading bounding boxes\n",
      "Anomalous bboxes found: 29 of 37367\n",
      "class_id_offset: 0\n",
      "Anomalous bboxes found: 0 of 2697\n",
      "class_id_offset: 0\n",
      "  Loaded 18000 bounding boxes\n",
      "Loding bbox_phrase_embeddings and bbox_phrases from /mnt/workspace/pamessina/medvqa-workspace/cache/vinbig/bbox_phrase_embeddings.pkl...\n",
      "bbox_phrase_embeddings.shape = (22, 128)\n",
      "len(bbox_phrases) = 22\n",
      "\t Aortic enlargement\n",
      "\t Atelectasis\n",
      "\t Calcification\n",
      "\t Cardiomegaly\n",
      "\t Clavicle fracture\n",
      "\t Consolidation\n",
      "\t Edema\n",
      "\t Emphysema\n",
      "\t Enlarged PA\n",
      "\t ILD\n",
      "\t Infiltration\n",
      "\t Lung Opacity\n",
      "\t Lung cavity\n",
      "\t Lung cyst\n",
      "\t Mediastinal shift\n",
      "\t Nodule/Mass\n",
      "\t Other lesion\n",
      "\t Pleural effusion\n",
      "\t Pleural thickening\n",
      "\t Pneumothorax\n",
      "\t Pulmonary fibrosis\n",
      "\t Rib fracture\n",
      "Compute phrase grounding masks and labels\n",
      "Generating train dataset and dataloader\n",
      "len(train_indices) = 15000\n",
      "Edema: 13\n",
      "Clavicle fracture: 27\n",
      "Lung cyst: 33\n",
      "Lung cavity: 51\n",
      "Emphysema: 81\n",
      "Rib fracture: 90\n",
      "Pneumothorax: 96\n",
      "Enlarged PA: 131\n",
      "Mediastinal shift: 150\n",
      "Atelectasis: 187\n",
      "Consolidation: 353\n",
      "ILD: 397\n",
      "Calcification: 458\n",
      "Infiltration: 613\n",
      "Nodule/Mass: 841\n",
      "Pleural effusion: 1038\n",
      "Other lesion: 1154\n",
      "Lung Opacity: 1331\n",
      "Pulmonary fibrosis: 1621\n",
      "Pleural thickening: 2010\n",
      "Cardiomegaly: 2316\n",
      "Aortic enlargement: 3098\n",
      "No labels: 10478\n",
      "Group sizes: [10478, 771, 383, 380, 331, 323, 299, 295, 261, 260, 233, 156, 137, 122, 120, 102, 83, 79, 70, 45, 72]\n",
      "  len(indices) = 10478, weight = 2381.9852437292775\n",
      "  len(indices) = 771, weight = 882.1360585110112\n",
      "  len(indices) = 383, weight = 631.8938966609495\n",
      "  len(indices) = 380, weight = 629.3909790409404\n",
      "  len(indices) = 331, weight = 586.5207376353635\n",
      "  len(indices) = 323, weight = 579.1323556126193\n",
      "  len(indices) = 299, weight = 556.223803123097\n",
      "  len(indices) = 295, weight = 552.290605339315\n",
      "  len(indices) = 261, weight = 517.3766629445327\n",
      "  len(indices) = 260, weight = 516.3066389499386\n",
      "  len(indices) = 233, weight = 486.36392431157844\n",
      "  len(indices) = 156, weight = 386.6879164039802\n",
      "  len(indices) = 137, weight = 357.61347438695543\n",
      "  len(indices) = 122, weight = 332.9187999914028\n",
      "  len(indices) = 120, weight = 329.49416694030464\n",
      "  len(indices) = 102, weight = 297.0647831080699\n",
      "  len(indices) = 83, weight = 259.088791948487\n",
      "  len(indices) = 79, weight = 250.497443897163\n",
      "  len(indices) = 70, weight = 230.2655804515992\n",
      "  len(indices) = 45, weight = 165.6367630898417\n",
      "  len(indices) = 72, weight = 234.87654776633605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating val dataset and dataloader\n",
      "len(self.test_indices) = 3000\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mCreating MIMIC-CXR Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "mask_exponent = 0.4\n",
      "len(forbidden_train_dicom_ids) = 1000\n",
      "\u001b[1mPreparing MS-CXR dataset and dataloader for training/testing...\u001b[0m\n",
      "len(dicom_id_2_phrases_and_masks) = 1047\n",
      "len(phrase2embedding) = 715\n",
      "Using image size mode: medium_512\n",
      "227835it [00:00, 427489.15it/s]\n",
      "Total number of images: 1047\n",
      "Number of phrases: 1, # images: 939\n",
      "Number of phrases: 2, # images: 103\n",
      "Number of phrases: 3, # images: 5\n",
      "\u001b[1mPreparing Chest Imagenome dataset and dataloader for training...\u001b[0m\n",
      "Loding bbox_phrase_embeddings and bbox_phrases from /mnt/workspace/pamessina/medvqa-workspace/cache/chest_imagenome/bbox_phrase_embeddings.pkl...\n",
      "bbox_phrase_embeddings.shape = (36, 128)\n",
      "len(bbox_phrases) = 36\n",
      "\t right lung\n",
      "\t right upper lung zone\n",
      "\t right mid lung zone\n",
      "\t right lower lung zone\n",
      "\t right hilar structures\n",
      "\t right apical zone\n",
      "\t right costophrenic angle\n",
      "\t right cardiophrenic angle\n",
      "\t right hemidiaphragm\n",
      "\t left lung\n",
      "\t left upper lung zone\n",
      "\t left mid lung zone\n",
      "\t left lower lung zone\n",
      "\t left hilar structures\n",
      "\t left apical zone\n",
      "\t left costophrenic angle\n",
      "\t left hemidiaphragm\n",
      "\t trachea\n",
      "\t spine\n",
      "\t right clavicle\n",
      "\t left clavicle\n",
      "\t aortic arch\n",
      "\t mediastinum\n",
      "\t upper mediastinum\n",
      "\t svc\n",
      "\t cardiac silhouette\n",
      "\t left cardiac silhouette\n",
      "\t right cardiac silhouette\n",
      "\t cavoatrial junction\n",
      "\t right atrium\n",
      "\t descending aorta\n",
      "\t carina\n",
      "\t left upper abdomen\n",
      "\t right upper abdomen\n",
      "\t abdomen\n",
      "\t left cardiophrenic angle\n",
      "Using image size mode: medium_512\n",
      "\u001b[1mLoading precomputed bbox_coords_and_presence_and_mask from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/bbox_coords_and_presence_and_mask(13,13,243310).pkl...\u001b[0m\n",
      "227835it [00:29, 7831.31it/s]\n",
      "Total number of images: 242310\n",
      "\u001b[1mPreparing Chest Imagenome dataset and dataloader for testing...\u001b[0m\n",
      "Loding bbox_phrase_embeddings and bbox_phrases from /mnt/workspace/pamessina/medvqa-workspace/cache/chest_imagenome/bbox_phrase_embeddings.pkl...\n",
      "bbox_phrase_embeddings.shape = (26, 128)\n",
      "len(bbox_phrases) = 26\n",
      "\t right lung\n",
      "\t right upper lung zone\n",
      "\t right mid lung zone\n",
      "\t right lower lung zone\n",
      "\t right hilar structures\n",
      "\t right apical zone\n",
      "\t right costophrenic angle\n",
      "\t right hemidiaphragm\n",
      "\t left lung\n",
      "\t left upper lung zone\n",
      "\t left mid lung zone\n",
      "\t left lower lung zone\n",
      "\t left hilar structures\n",
      "\t left apical zone\n",
      "\t left costophrenic angle\n",
      "\t left hemidiaphragm\n",
      "\t trachea\n",
      "\t right clavicle\n",
      "\t left clavicle\n",
      "\t aortic arch\n",
      "\t upper mediastinum\n",
      "\t svc\n",
      "\t cardiac silhouette\n",
      "\t cavoatrial junction\n",
      "\t right atrium\n",
      "\t carina\n",
      "Using image size mode: medium_512\n",
      "227835it [00:02, 93980.22it/s]\n",
      "Total number of images: 1000\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(mimiccxr_trainer.train_mscxr_dataloader) = 31\n",
      "len(mimiccxr_trainer.test_mscxr_dataloader) = 22\n",
      "len(mimiccxr_trainer.train_chest_imagenome_dataloader) = 15145\n",
      "len(mimiccxr_trainer.test_chest_imagenome_gold_dataloader) = 30\n",
      "len(vinbig_trainer.train_dataloader) = 37037037037037038\n",
      "len(vinbig_trainer.val_dataloader) = 75\n",
      "len(chexlocalize_trainer.train_dataloader) = 26\n",
      "len(chexlocalize_trainer.val_dataloader) = 18\n",
      "len(_train_dataloaders) = 4\n",
      "len(_val_dataloaders) = 4\n",
      "_train_weights = [1.0, 2.5, 6.0, 2.5]\n",
      "merged_dataset_name = mscxr+chst-img-anat+vinbig+chexloc\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231220_111103_mscxr+chst-img-anat+vinbig+chexloc_PhraseGrounder(yolov8l,128,256)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231220_111103_mscxr+chst-img-anat+vinbig+chexloc_PhraseGrounder(yolov8l,128,256)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m14) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_107_ciss+ciou+ciou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7490.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231219_195906_mscxr+chst-img-anat+vinbig_PhraseGrounder(yolov8l,128,256)/checkpoint_107_ciss+ciou+ciou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7490.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m15) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231220_111103_mscxr+chst-img-anat+vinbig+chexloc_PhraseGrounder(yolov8l,128,256)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m16) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 2.13639, cibg_y8_loss 2.01940, cibg_y8_box_loss 0.66601, cibg_y8_cls_loss 0.32536, cibg_y8_dfl_loss 1.02803, cibg_att_sup_loss 0.31182, cibg_segmask_iou 0.73834, pg_att_sup_loss 0.04247, pg_segmask_iou 0.97000, vbg_y8_loss 1.02339, vbg_y8_box_loss 0.12269, vbg_y8_cls_loss 0.13711, vbg_y8_dfl_loss 0.76358, vbg_phrcls_loss 0.00226, vbg_phrase_acc 0.99986, vbg_att_sup_loss 0.09996, vbg_segmask_iou 0.70681, cl_att_sup_loss 3.38473, cl_segmask_iou 0.20905, cl_phrcls_loss 1.82025, cl_phrase_acc 0.76986, 189.84 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.97061, cibg_att_sup_loss 0.14176, cibg_segmask_iou 0.63550, pg_att_sup_loss 0.28044, pg_segmask_iou 0.54416, vbg_y8_bbox_iou 0.35158, vbg_phrase_acc 0.96950, vbg_att_sup_loss 0.09732, vbg_segmask_iou 0.27731, cl_att_sup_loss 0.71590, cl_segmask_iou 0.24625, cl_phrase_acc 0.77916, 54.04 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.6669.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.85706, cibg_y8_loss 2.03456, cibg_y8_box_loss 0.67211, cibg_y8_cls_loss 0.33026, cibg_y8_dfl_loss 1.03219, cibg_att_sup_loss 0.37157, cibg_segmask_iou 0.70051, pg_att_sup_loss 0.06245, pg_segmask_iou 0.94976, vbg_y8_loss 1.02683, vbg_y8_box_loss 0.12435, vbg_y8_cls_loss 0.13817, vbg_y8_dfl_loss 0.76431, vbg_phrcls_loss 0.00236, vbg_phrase_acc 0.99984, vbg_att_sup_loss 0.10588, vbg_segmask_iou 0.70192, cl_att_sup_loss 2.87067, cl_segmask_iou 0.27482, cl_phrcls_loss 0.88798, cl_phrase_acc 0.82491, 191.93 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96777, cibg_att_sup_loss 0.19894, cibg_segmask_iou 0.54915, pg_att_sup_loss 0.20213, pg_segmask_iou 0.61727, vbg_y8_bbox_iou 0.20562, vbg_phrase_acc 0.96562, vbg_att_sup_loss 0.10814, vbg_segmask_iou 0.25240, cl_att_sup_loss 0.60046, cl_segmask_iou 0.30606, cl_phrase_acc 0.80033, 54.55 secs\n",
      "\u001b[1m---- Epoch 3/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 1.64731, cibg_y8_loss 2.05358, cibg_y8_box_loss 0.68231, cibg_y8_cls_loss 0.33545, cibg_y8_dfl_loss 1.03582, cibg_att_sup_loss 0.42653, cibg_segmask_iou 0.66489, pg_att_sup_loss 0.09758, pg_segmask_iou 0.93362, vbg_y8_loss 1.04931, vbg_y8_box_loss 0.13726, vbg_y8_cls_loss 0.14707, vbg_y8_dfl_loss 0.76499, vbg_phrcls_loss 0.00283, vbg_phrase_acc 0.99981, vbg_att_sup_loss 0.11794, vbg_segmask_iou 0.69847, cl_att_sup_loss 2.00071, cl_segmask_iou 0.35804, cl_phrcls_loss 0.57905, cl_phrase_acc 0.87308, 189.90 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96725, cibg_att_sup_loss 0.26127, cibg_segmask_iou 0.45601, pg_att_sup_loss 0.20109, pg_segmask_iou 0.59825, vbg_y8_bbox_iou 0.47689, vbg_phrase_acc 0.97005, vbg_att_sup_loss 0.09563, vbg_segmask_iou 0.26607, cl_att_sup_loss 0.43653, cl_segmask_iou 0.37638, cl_phrase_acc 0.87428, 54.12 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.6901.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 1.53551, cibg_y8_loss 2.08219, cibg_y8_box_loss 0.69440, cibg_y8_cls_loss 0.34524, cibg_y8_dfl_loss 1.04255, cibg_att_sup_loss 0.36766, cibg_segmask_iou 0.70376, pg_att_sup_loss 0.13530, pg_segmask_iou 0.90863, vbg_y8_loss 1.17636, vbg_y8_box_loss 0.20854, vbg_y8_cls_loss 0.19443, vbg_y8_dfl_loss 0.77339, vbg_phrcls_loss 0.00647, vbg_phrase_acc 0.99921, vbg_att_sup_loss 0.15912, vbg_segmask_iou 0.67866, cl_att_sup_loss 1.24113, cl_segmask_iou 0.40611, cl_phrcls_loss 0.40466, cl_phrase_acc 0.91211, 192.10 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96213, cibg_att_sup_loss 0.38788, cibg_segmask_iou 0.29980, pg_att_sup_loss 0.33634, pg_segmask_iou 0.43452, vbg_y8_bbox_iou 0.31915, vbg_phrase_acc 0.96556, vbg_att_sup_loss 0.11689, vbg_segmask_iou 0.18903, cl_att_sup_loss 0.35343, cl_segmask_iou 0.39665, cl_phrase_acc 0.85831, 53.90 secs\n",
      "\u001b[1m---- Epoch 5/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 1.40443, cibg_y8_loss 2.06761, cibg_y8_box_loss 0.68741, cibg_y8_cls_loss 0.34072, cibg_y8_dfl_loss 1.03948, cibg_att_sup_loss 0.33330, cibg_segmask_iou 0.72678, pg_att_sup_loss 0.11988, pg_segmask_iou 0.92275, vbg_y8_loss 1.20067, vbg_y8_box_loss 0.22431, vbg_y8_cls_loss 0.20173, vbg_y8_dfl_loss 0.77463, vbg_phrcls_loss 0.00956, vbg_phrase_acc 0.99870, vbg_att_sup_loss 0.16626, vbg_segmask_iou 0.67642, cl_att_sup_loss 0.82035, cl_segmask_iou 0.43832, cl_phrcls_loss 0.16848, cl_phrase_acc 0.96999, 193.45 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96740, cibg_att_sup_loss 0.37112, cibg_segmask_iou 0.32602, pg_att_sup_loss 0.20790, pg_segmask_iou 0.58488, vbg_y8_bbox_iou 0.30763, vbg_phrase_acc 0.96758, vbg_att_sup_loss 0.10532, vbg_segmask_iou 0.22618, cl_att_sup_loss 0.27239, cl_segmask_iou 0.43237, cl_phrase_acc 0.93348, 54.79 secs\n",
      "\u001b[1m---- Epoch 6/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 1.27780, cibg_y8_loss 2.03933, cibg_y8_box_loss 0.67656, cibg_y8_cls_loss 0.32958, cibg_y8_dfl_loss 1.03319, cibg_att_sup_loss 0.31540, cibg_segmask_iou 0.73478, pg_att_sup_loss 0.09292, pg_segmask_iou 0.94163, vbg_y8_loss 1.11046, vbg_y8_box_loss 0.17326, vbg_y8_cls_loss 0.16872, vbg_y8_dfl_loss 0.76847, vbg_phrcls_loss 0.00462, vbg_phrase_acc 0.99947, vbg_att_sup_loss 0.14129, vbg_segmask_iou 0.68721, cl_att_sup_loss 0.64578, cl_segmask_iou 0.46203, cl_phrcls_loss 0.08046, cl_phrase_acc 0.98966, 189.60 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96427, cibg_att_sup_loss 0.32196, cibg_segmask_iou 0.39443, pg_att_sup_loss 0.23653, pg_segmask_iou 0.57719, vbg_y8_bbox_iou 0.10607, vbg_phrase_acc 0.95392, vbg_att_sup_loss 0.12619, vbg_segmask_iou 0.19194, cl_att_sup_loss 0.35130, cl_segmask_iou 0.40888, cl_phrase_acc 0.89102, 54.72 secs\n",
      "\u001b[1m---- Epoch 7/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 1.22336, cibg_y8_loss 2.04309, cibg_y8_box_loss 0.67778, cibg_y8_cls_loss 0.33057, cibg_y8_dfl_loss 1.03473, cibg_att_sup_loss 0.31822, cibg_segmask_iou 0.73634, pg_att_sup_loss 0.08793, pg_segmask_iou 0.94019, vbg_y8_loss 1.06472, vbg_y8_box_loss 0.14749, vbg_y8_cls_loss 0.15167, vbg_y8_dfl_loss 0.76556, vbg_phrcls_loss 0.00229, vbg_phrase_acc 0.99983, vbg_att_sup_loss 0.12602, vbg_segmask_iou 0.69491, cl_att_sup_loss 0.56421, cl_segmask_iou 0.47928, cl_phrcls_loss 0.04816, cl_phrase_acc 0.99449, 191.64 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96976, cibg_att_sup_loss 0.36376, cibg_segmask_iou 0.34334, pg_att_sup_loss 0.16884, pg_segmask_iou 0.64744, vbg_y8_bbox_iou 0.51528, vbg_phrase_acc 0.96783, vbg_att_sup_loss 0.09094, vbg_segmask_iou 0.27916, cl_att_sup_loss 0.23845, cl_segmask_iou 0.47150, cl_phrase_acc 0.96253, 54.35 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7157.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 1.18417, cibg_y8_loss 2.02085, cibg_y8_box_loss 0.66752, cibg_y8_cls_loss 0.32659, cibg_y8_dfl_loss 1.02674, cibg_att_sup_loss 0.31216, cibg_segmask_iou 0.73838, pg_att_sup_loss 0.07875, pg_segmask_iou 0.94889, vbg_y8_loss 1.04152, vbg_y8_box_loss 0.13413, vbg_y8_cls_loss 0.14271, vbg_y8_dfl_loss 0.76468, vbg_phrcls_loss 0.00197, vbg_phrase_acc 0.99989, vbg_att_sup_loss 0.11515, vbg_segmask_iou 0.69954, cl_att_sup_loss 0.51001, cl_segmask_iou 0.49175, cl_phrcls_loss 0.02871, cl_phrase_acc 0.99779, 190.29 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96827, cibg_att_sup_loss 0.36608, cibg_segmask_iou 0.34585, pg_att_sup_loss 0.18724, pg_segmask_iou 0.63180, vbg_y8_bbox_iou 0.30208, vbg_phrase_acc 0.96138, vbg_att_sup_loss 0.10431, vbg_segmask_iou 0.24115, cl_att_sup_loss 0.25017, cl_segmask_iou 0.46673, cl_phrase_acc 0.95310, 54.66 secs\n",
      "\u001b[1m---- Epoch 9/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.17511, cibg_y8_loss 2.05003, cibg_y8_box_loss 0.68110, cibg_y8_cls_loss 0.33309, cibg_y8_dfl_loss 1.03584, cibg_att_sup_loss 0.32045, cibg_segmask_iou 0.73517, pg_att_sup_loss 0.07356, pg_segmask_iou 0.94976, vbg_y8_loss 1.02839, vbg_y8_box_loss 0.12674, vbg_y8_cls_loss 0.13779, vbg_y8_dfl_loss 0.76386, vbg_phrcls_loss 0.00174, vbg_phrase_acc 0.99990, vbg_att_sup_loss 0.10925, vbg_segmask_iou 0.70153, cl_att_sup_loss 0.48153, cl_segmask_iou 0.50030, cl_phrcls_loss 0.02459, cl_phrase_acc 0.99783, 193.70 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96954, cibg_att_sup_loss 0.36266, cibg_segmask_iou 0.34881, pg_att_sup_loss 0.16808, pg_segmask_iou 0.65289, vbg_y8_bbox_iou 0.49146, vbg_phrase_acc 0.96579, vbg_att_sup_loss 0.09134, vbg_segmask_iou 0.28224, cl_att_sup_loss 0.22286, cl_segmask_iou 0.48351, cl_phrase_acc 0.96120, 54.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7175.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.15922, cibg_y8_loss 2.03217, cibg_y8_box_loss 0.67232, cibg_y8_cls_loss 0.32829, cibg_y8_dfl_loss 1.03155, cibg_att_sup_loss 0.31390, cibg_segmask_iou 0.73885, pg_att_sup_loss 0.07397, pg_segmask_iou 0.94818, vbg_y8_loss 1.02185, vbg_y8_box_loss 0.12282, vbg_y8_cls_loss 0.13525, vbg_y8_dfl_loss 0.76378, vbg_phrcls_loss 0.00146, vbg_phrase_acc 0.99994, vbg_att_sup_loss 0.10710, vbg_segmask_iou 0.70351, cl_att_sup_loss 0.45687, cl_segmask_iou 0.50791, cl_phrcls_loss 0.01878, cl_phrase_acc 0.99910, 195.00 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96959, cibg_att_sup_loss 0.36555, cibg_segmask_iou 0.34687, pg_att_sup_loss 0.16855, pg_segmask_iou 0.65165, vbg_y8_bbox_iou 0.58396, vbg_phrase_acc 0.96718, vbg_att_sup_loss 0.08671, vbg_segmask_iou 0.29706, cl_att_sup_loss 0.22646, cl_segmask_iou 0.48410, cl_phrase_acc 0.95865, 54.69 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7255.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.15244, cibg_y8_loss 2.03293, cibg_y8_box_loss 0.67358, cibg_y8_cls_loss 0.32796, cibg_y8_dfl_loss 1.03139, cibg_att_sup_loss 0.31268, cibg_segmask_iou 0.73792, pg_att_sup_loss 0.06783, pg_segmask_iou 0.95740, vbg_y8_loss 1.01699, vbg_y8_box_loss 0.12063, vbg_y8_cls_loss 0.13322, vbg_y8_dfl_loss 0.76315, vbg_phrcls_loss 0.00150, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.10402, vbg_segmask_iou 0.70588, cl_att_sup_loss 0.44738, cl_segmask_iou 0.51178, cl_phrcls_loss 0.01757, cl_phrase_acc 0.99922, 192.91 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96971, cibg_att_sup_loss 0.37544, cibg_segmask_iou 0.33535, pg_att_sup_loss 0.17013, pg_segmask_iou 0.64898, vbg_y8_bbox_iou 0.66876, vbg_phrase_acc 0.96929, vbg_att_sup_loss 0.08369, vbg_segmask_iou 0.30595, cl_att_sup_loss 0.22902, cl_segmask_iou 0.48441, cl_phrase_acc 0.95288, 54.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7311.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.14904, cibg_y8_loss 2.03656, cibg_y8_box_loss 0.67658, cibg_y8_cls_loss 0.32794, cibg_y8_dfl_loss 1.03204, cibg_att_sup_loss 0.31294, cibg_segmask_iou 0.73685, pg_att_sup_loss 0.06753, pg_segmask_iou 0.95128, vbg_y8_loss 1.01511, vbg_y8_box_loss 0.11936, vbg_y8_cls_loss 0.13241, vbg_y8_dfl_loss 0.76334, vbg_phrcls_loss 0.00143, vbg_phrase_acc 0.99994, vbg_att_sup_loss 0.10216, vbg_segmask_iou 0.70514, cl_att_sup_loss 0.43798, cl_segmask_iou 0.51241, cl_phrcls_loss 0.01603, cl_phrase_acc 0.99945, 195.39 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cibg_y8_bbox_iou 0.96975, cibg_att_sup_loss 0.36888, cibg_segmask_iou 0.34389, pg_att_sup_loss 0.16032, pg_segmask_iou 0.66447, vbg_y8_bbox_iou 0.57032, vbg_phrase_acc 0.96695, vbg_att_sup_loss 0.08738, vbg_segmask_iou 0.29430, cl_att_sup_loss 0.22372, cl_segmask_iou 0.48924, cl_phrase_acc 0.95643, 54.70 secs\n",
      "\u001b[1m---- Epoch 13/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 1.31992, cibg_y8_loss 2.07598, cibg_y8_box_loss 0.69340, cibg_y8_cls_loss 0.34207, cibg_y8_dfl_loss 1.04051, cibg_att_sup_loss 0.32326, cibg_segmask_iou 0.73306, pg_att_sup_loss 0.11913, pg_segmask_iou 0.92365, vbg_y8_loss 1.15737, vbg_y8_box_loss 0.20045, vbg_y8_cls_loss 0.18473, vbg_y8_dfl_loss 0.77219, vbg_phrcls_loss 0.01334, vbg_phrase_acc 0.99804, vbg_att_sup_loss 0.16552, vbg_segmask_iou 0.67981, cl_att_sup_loss 0.51808, cl_segmask_iou 0.49146, cl_phrcls_loss 0.16370, cl_phrase_acc 0.97213, 194.95 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96077, cibg_att_sup_loss 0.43456, cibg_segmask_iou 0.25371, pg_att_sup_loss 0.34187, pg_segmask_iou 0.44215, vbg_y8_bbox_iou 0.32143, vbg_phrase_acc 0.96005, vbg_att_sup_loss 0.11503, vbg_segmask_iou 0.17481, cl_att_sup_loss 0.32062, cl_segmask_iou 0.42218, cl_phrase_acc 0.88925, 54.33 secs\n",
      "\u001b[1m---- Epoch 14/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 1.31484, cibg_y8_loss 2.07361, cibg_y8_box_loss 0.69173, cibg_y8_cls_loss 0.34015, cibg_y8_dfl_loss 1.04173, cibg_att_sup_loss 0.32373, cibg_segmask_iou 0.73370, pg_att_sup_loss 0.10073, pg_segmask_iou 0.93816, vbg_y8_loss 1.22213, vbg_y8_box_loss 0.23828, vbg_y8_cls_loss 0.20668, vbg_y8_dfl_loss 0.77717, vbg_phrcls_loss 0.01312, vbg_phrase_acc 0.99819, vbg_att_sup_loss 0.17909, vbg_segmask_iou 0.67371, cl_att_sup_loss 0.42533, cl_segmask_iou 0.50681, cl_phrcls_loss 0.05387, cl_phrase_acc 0.99474, 195.54 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96640, cibg_att_sup_loss 0.36972, cibg_segmask_iou 0.35083, pg_att_sup_loss 0.25021, pg_segmask_iou 0.56892, vbg_y8_bbox_iou 0.17999, vbg_phrase_acc 0.95342, vbg_att_sup_loss 0.11471, vbg_segmask_iou 0.20067, cl_att_sup_loss 0.29606, cl_segmask_iou 0.45444, cl_phrase_acc 0.93160, 55.08 secs\n",
      "\u001b[1m---- Epoch 15/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 1.21160, cibg_y8_loss 2.06692, cibg_y8_box_loss 0.68907, cibg_y8_cls_loss 0.33775, cibg_y8_dfl_loss 1.04010, cibg_att_sup_loss 0.32567, cibg_segmask_iou 0.73413, pg_att_sup_loss 0.08380, pg_segmask_iou 0.94419, vbg_y8_loss 1.10974, vbg_y8_box_loss 0.17452, vbg_y8_cls_loss 0.16684, vbg_y8_dfl_loss 0.76838, vbg_phrcls_loss 0.00389, vbg_phrase_acc 0.99967, vbg_att_sup_loss 0.14011, vbg_segmask_iou 0.68965, cl_att_sup_loss 0.35928, cl_segmask_iou 0.53372, cl_phrcls_loss 0.02128, cl_phrase_acc 0.99899, 197.35 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96713, cibg_att_sup_loss 0.35434, cibg_segmask_iou 0.36052, pg_att_sup_loss 0.25499, pg_segmask_iou 0.55439, vbg_y8_bbox_iou 0.59681, vbg_phrase_acc 0.96286, vbg_att_sup_loss 0.09906, vbg_segmask_iou 0.24052, cl_att_sup_loss 0.28267, cl_segmask_iou 0.47617, cl_phrase_acc 0.92749, 54.48 secs\n",
      "\u001b[1m---- Epoch 16/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.15349, cibg_y8_loss 2.02806, cibg_y8_box_loss 0.67358, cibg_y8_cls_loss 0.32514, cibg_y8_dfl_loss 1.02934, cibg_att_sup_loss 0.30388, cibg_segmask_iou 0.74188, pg_att_sup_loss 0.07096, pg_segmask_iou 0.95605, vbg_y8_loss 1.05543, vbg_y8_box_loss 0.14367, vbg_y8_cls_loss 0.14689, vbg_y8_dfl_loss 0.76487, vbg_phrcls_loss 0.00186, vbg_phrase_acc 0.99992, vbg_att_sup_loss 0.12122, vbg_segmask_iou 0.69818, cl_att_sup_loss 0.33250, cl_segmask_iou 0.54713, cl_phrcls_loss 0.01552, cl_phrase_acc 0.99924, 196.19 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96891, cibg_att_sup_loss 0.35842, cibg_segmask_iou 0.36158, pg_att_sup_loss 0.20084, pg_segmask_iou 0.62517, vbg_y8_bbox_iou 0.30334, vbg_phrase_acc 0.95580, vbg_att_sup_loss 0.10482, vbg_segmask_iou 0.23607, cl_att_sup_loss 0.23210, cl_segmask_iou 0.50849, cl_phrase_acc 0.95510, 55.55 secs\n",
      "\u001b[1m---- Epoch 17/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.13342, cibg_y8_loss 2.03375, cibg_y8_box_loss 0.67444, cibg_y8_cls_loss 0.32666, cibg_y8_dfl_loss 1.03264, cibg_att_sup_loss 0.30685, cibg_segmask_iou 0.74075, pg_att_sup_loss 0.06484, pg_segmask_iou 0.95313, vbg_y8_loss 1.03284, vbg_y8_box_loss 0.13068, vbg_y8_cls_loss 0.13832, vbg_y8_dfl_loss 0.76384, vbg_phrcls_loss 0.00161, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.11070, vbg_segmask_iou 0.70428, cl_att_sup_loss 0.31409, cl_segmask_iou 0.55529, cl_phrcls_loss 0.01143, cl_phrase_acc 0.99961, 197.97 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96920, cibg_att_sup_loss 0.39004, cibg_segmask_iou 0.32841, pg_att_sup_loss 0.18699, pg_segmask_iou 0.63752, vbg_y8_bbox_iou 0.64468, vbg_phrase_acc 0.96802, vbg_att_sup_loss 0.08764, vbg_segmask_iou 0.28885, cl_att_sup_loss 0.21950, cl_segmask_iou 0.51891, cl_phrase_acc 0.96042, 55.02 secs\n",
      "\u001b[1m---- Epoch 18/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.12827, cibg_y8_loss 2.04981, cibg_y8_box_loss 0.68178, cibg_y8_cls_loss 0.33030, cibg_y8_dfl_loss 1.03773, cibg_att_sup_loss 0.31140, cibg_segmask_iou 0.73820, pg_att_sup_loss 0.06328, pg_segmask_iou 0.95466, vbg_y8_loss 1.02191, vbg_y8_box_loss 0.12409, vbg_y8_cls_loss 0.13406, vbg_y8_dfl_loss 0.76375, vbg_phrcls_loss 0.00136, vbg_phrase_acc 0.99996, vbg_att_sup_loss 0.10546, vbg_segmask_iou 0.70569, cl_att_sup_loss 0.30924, cl_segmask_iou 0.56117, cl_phrcls_loss 0.01100, cl_phrase_acc 0.99956, 199.54 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96992, cibg_att_sup_loss 0.37033, cibg_segmask_iou 0.34979, pg_att_sup_loss 0.18002, pg_segmask_iou 0.65352, vbg_y8_bbox_iou 0.47106, vbg_phrase_acc 0.96286, vbg_att_sup_loss 0.09350, vbg_segmask_iou 0.27296, cl_att_sup_loss 0.21555, cl_segmask_iou 0.52663, cl_phrase_acc 0.96408, 54.95 secs\n",
      "\u001b[1m---- Epoch 19/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.11677, cibg_y8_loss 2.02993, cibg_y8_box_loss 0.67243, cibg_y8_cls_loss 0.32703, cibg_y8_dfl_loss 1.03047, cibg_att_sup_loss 0.30596, cibg_segmask_iou 0.73910, pg_att_sup_loss 0.06108, pg_segmask_iou 0.96064, vbg_y8_loss 1.01486, vbg_y8_box_loss 0.12053, vbg_y8_cls_loss 0.13140, vbg_y8_dfl_loss 0.76292, vbg_phrcls_loss 0.00157, vbg_phrase_acc 0.99995, vbg_att_sup_loss 0.10351, vbg_segmask_iou 0.70721, cl_att_sup_loss 0.30164, cl_segmask_iou 0.56308, cl_phrcls_loss 0.01068, cl_phrase_acc 0.99965, 197.19 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96969, cibg_att_sup_loss 0.36884, cibg_segmask_iou 0.35196, pg_att_sup_loss 0.18304, pg_segmask_iou 0.64789, vbg_y8_bbox_iou 0.58483, vbg_phrase_acc 0.96588, vbg_att_sup_loss 0.08820, vbg_segmask_iou 0.29082, cl_att_sup_loss 0.21049, cl_segmask_iou 0.52846, cl_phrase_acc 0.96585, 55.09 secs\n",
      "\u001b[1m---- Epoch 20/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.11700, cibg_y8_loss 2.04492, cibg_y8_box_loss 0.67976, cibg_y8_cls_loss 0.32968, cibg_y8_dfl_loss 1.03549, cibg_att_sup_loss 0.31293, cibg_segmask_iou 0.73948, pg_att_sup_loss 0.05862, pg_segmask_iou 0.95574, vbg_y8_loss 1.01149, vbg_y8_box_loss 0.11838, vbg_y8_cls_loss 0.13041, vbg_y8_dfl_loss 0.76271, vbg_phrcls_loss 0.00124, vbg_phrase_acc 0.99998, vbg_att_sup_loss 0.09998, vbg_segmask_iou 0.70883, cl_att_sup_loss 0.30026, cl_segmask_iou 0.56637, cl_phrcls_loss 0.00954, cl_phrase_acc 0.99977, 199.62 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96987, cibg_att_sup_loss 0.37984, cibg_segmask_iou 0.33995, pg_att_sup_loss 0.17407, pg_segmask_iou 0.66175, vbg_y8_bbox_iou 0.49876, vbg_phrase_acc 0.96308, vbg_att_sup_loss 0.09282, vbg_segmask_iou 0.27446, cl_att_sup_loss 0.20631, cl_segmask_iou 0.53358, cl_phrase_acc 0.96674, 55.03 secs\n",
      "\u001b[1m---- Epoch 21/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 1.22005, cibg_y8_loss 2.05110, cibg_y8_box_loss 0.68396, cibg_y8_cls_loss 0.33408, cibg_y8_dfl_loss 1.03307, cibg_att_sup_loss 0.30779, cibg_segmask_iou 0.73971, pg_att_sup_loss 0.08278, pg_segmask_iou 0.95176, vbg_y8_loss 1.13002, vbg_y8_box_loss 0.18826, vbg_y8_cls_loss 0.17169, vbg_y8_dfl_loss 0.77007, vbg_phrcls_loss 0.00641, vbg_phrase_acc 0.99920, vbg_att_sup_loss 0.15685, vbg_segmask_iou 0.68668, cl_att_sup_loss 0.31842, cl_segmask_iou 0.55958, cl_phrcls_loss 0.04197, cl_phrase_acc 0.99388, 198.06 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cibg_y8_bbox_iou 0.95118, cibg_att_sup_loss 0.36907, cibg_segmask_iou 0.35073, pg_att_sup_loss 0.40307, pg_segmask_iou 0.41075, vbg_y8_bbox_iou 0.05908, vbg_phrase_acc 0.94448, vbg_att_sup_loss 0.14549, vbg_segmask_iou 0.14948, cl_att_sup_loss 0.50857, cl_segmask_iou 0.33678, cl_phrase_acc 0.83326, 55.83 secs\n",
      "\u001b[1m---- Epoch 22/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 1.27860, cibg_y8_loss 2.05676, cibg_y8_box_loss 0.68464, cibg_y8_cls_loss 0.33532, cibg_y8_dfl_loss 1.03680, cibg_att_sup_loss 0.31188, cibg_segmask_iou 0.73945, pg_att_sup_loss 0.09597, pg_segmask_iou 0.93726, vbg_y8_loss 1.21021, vbg_y8_box_loss 0.23416, vbg_y8_cls_loss 0.19971, vbg_y8_dfl_loss 0.77633, vbg_phrcls_loss 0.01450, vbg_phrase_acc 0.99814, vbg_att_sup_loss 0.17754, vbg_segmask_iou 0.67500, cl_att_sup_loss 0.33088, cl_segmask_iou 0.55256, cl_phrcls_loss 0.03400, cl_phrase_acc 0.99548, 199.89 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.95464, cibg_att_sup_loss 0.41651, cibg_segmask_iou 0.30004, pg_att_sup_loss 0.37812, pg_segmask_iou 0.42877, vbg_y8_bbox_iou 0.30711, vbg_phrase_acc 0.96139, vbg_att_sup_loss 0.12233, vbg_segmask_iou 0.15730, cl_att_sup_loss 0.37292, cl_segmask_iou 0.41591, cl_phrase_acc 0.90133, 54.71 secs\n",
      "\u001b[1m---- Epoch 23/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 1.17703, cibg_y8_loss 2.04596, cibg_y8_box_loss 0.68126, cibg_y8_cls_loss 0.33122, cibg_y8_dfl_loss 1.03349, cibg_att_sup_loss 0.30661, cibg_segmask_iou 0.74102, pg_att_sup_loss 0.07827, pg_segmask_iou 0.94866, vbg_y8_loss 1.10024, vbg_y8_box_loss 0.17038, vbg_y8_cls_loss 0.16235, vbg_y8_dfl_loss 0.76751, vbg_phrcls_loss 0.00473, vbg_phrase_acc 0.99956, vbg_att_sup_loss 0.13595, vbg_segmask_iou 0.69266, cl_att_sup_loss 0.27946, cl_segmask_iou 0.57656, cl_phrcls_loss 0.00820, cl_phrase_acc 0.99965, 200.35 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96940, cibg_att_sup_loss 0.34383, cibg_segmask_iou 0.38194, pg_att_sup_loss 0.20407, pg_segmask_iou 0.62347, vbg_y8_bbox_iou 0.38200, vbg_phrase_acc 0.96159, vbg_att_sup_loss 0.09906, vbg_segmask_iou 0.24849, cl_att_sup_loss 0.23978, cl_segmask_iou 0.52915, cl_phrase_acc 0.95388, 55.27 secs\n",
      "\u001b[1m---- Epoch 24/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.12685, cibg_y8_loss 2.01908, cibg_y8_box_loss 0.66939, cibg_y8_cls_loss 0.32336, cibg_y8_dfl_loss 1.02633, cibg_att_sup_loss 0.30349, cibg_segmask_iou 0.74344, pg_att_sup_loss 0.05883, pg_segmask_iou 0.96170, vbg_y8_loss 1.04673, vbg_y8_box_loss 0.13980, vbg_y8_cls_loss 0.14243, vbg_y8_dfl_loss 0.76451, vbg_phrcls_loss 0.00258, vbg_phrase_acc 0.99983, vbg_att_sup_loss 0.11629, vbg_segmask_iou 0.70254, cl_att_sup_loss 0.25800, cl_segmask_iou 0.59190, cl_phrcls_loss 0.00737, cl_phrase_acc 0.99972, 198.74 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96850, cibg_att_sup_loss 0.36214, cibg_segmask_iou 0.36122, pg_att_sup_loss 0.23394, pg_segmask_iou 0.58696, vbg_y8_bbox_iou 0.31185, vbg_phrase_acc 0.95576, vbg_att_sup_loss 0.10646, vbg_segmask_iou 0.22477, cl_att_sup_loss 0.29745, cl_segmask_iou 0.49556, cl_phrase_acc 0.92661, 55.28 secs\n",
      "\u001b[1m---- Epoch 25/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.11626, cibg_y8_loss 2.04751, cibg_y8_box_loss 0.68351, cibg_y8_cls_loss 0.32788, cibg_y8_dfl_loss 1.03611, cibg_att_sup_loss 0.30961, cibg_segmask_iou 0.74152, pg_att_sup_loss 0.05867, pg_segmask_iou 0.95736, vbg_y8_loss 1.02405, vbg_y8_box_loss 0.12711, vbg_y8_cls_loss 0.13346, vbg_y8_dfl_loss 0.76348, vbg_phrcls_loss 0.00204, vbg_phrase_acc 0.99991, vbg_att_sup_loss 0.10711, vbg_segmask_iou 0.70594, cl_att_sup_loss 0.25224, cl_segmask_iou 0.60020, cl_phrcls_loss 0.00556, cl_phrase_acc 0.99970, 201.10 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96975, cibg_att_sup_loss 0.36507, cibg_segmask_iou 0.35932, pg_att_sup_loss 0.19167, pg_segmask_iou 0.64054, vbg_y8_bbox_iou 0.44964, vbg_phrase_acc 0.96282, vbg_att_sup_loss 0.09437, vbg_segmask_iou 0.26722, cl_att_sup_loss 0.22910, cl_segmask_iou 0.54149, cl_phrase_acc 0.95920, 55.36 secs\n",
      "\u001b[1m---- Epoch 26/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.10633, cibg_y8_loss 2.04370, cibg_y8_box_loss 0.67946, cibg_y8_cls_loss 0.33181, cibg_y8_dfl_loss 1.03243, cibg_att_sup_loss 0.31288, cibg_segmask_iou 0.73898, pg_att_sup_loss 0.05565, pg_segmask_iou 0.96115, vbg_y8_loss 1.01339, vbg_y8_box_loss 0.12109, vbg_y8_cls_loss 0.12987, vbg_y8_dfl_loss 0.76243, vbg_phrcls_loss 0.00154, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.10136, vbg_segmask_iou 0.70832, cl_att_sup_loss 0.24753, cl_segmask_iou 0.60422, cl_phrcls_loss 0.00489, cl_phrase_acc 0.99986, 199.11 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96909, cibg_att_sup_loss 0.39405, cibg_segmask_iou 0.32886, pg_att_sup_loss 0.20182, pg_segmask_iou 0.62551, vbg_y8_bbox_iou 0.64404, vbg_phrase_acc 0.96867, vbg_att_sup_loss 0.08941, vbg_segmask_iou 0.28092, cl_att_sup_loss 0.24168, cl_segmask_iou 0.53368, cl_phrase_acc 0.95410, 55.12 secs\n",
      "\u001b[1m---- Epoch 27/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.09676, cibg_y8_loss 2.02819, cibg_y8_box_loss 0.67416, cibg_y8_cls_loss 0.32613, cibg_y8_dfl_loss 1.02790, cibg_att_sup_loss 0.30592, cibg_segmask_iou 0.74228, pg_att_sup_loss 0.04944, pg_segmask_iou 0.96448, vbg_y8_loss 1.00835, vbg_y8_box_loss 0.11787, vbg_y8_cls_loss 0.12778, vbg_y8_dfl_loss 0.76269, vbg_phrcls_loss 0.00159, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.09999, vbg_segmask_iou 0.71036, cl_att_sup_loss 0.24260, cl_segmask_iou 0.60611, cl_phrcls_loss 0.00413, cl_phrase_acc 0.99986, 199.56 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96996, cibg_att_sup_loss 0.38020, cibg_segmask_iou 0.34563, pg_att_sup_loss 0.18620, pg_segmask_iou 0.64714, vbg_y8_bbox_iou 0.54471, vbg_phrase_acc 0.96605, vbg_att_sup_loss 0.08926, vbg_segmask_iou 0.28621, cl_att_sup_loss 0.21775, cl_segmask_iou 0.55261, cl_phrase_acc 0.96696, 55.35 secs\n",
      "\u001b[1m---- Epoch 28/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.09578, cibg_y8_loss 2.03886, cibg_y8_box_loss 0.67731, cibg_y8_cls_loss 0.32752, cibg_y8_dfl_loss 1.03403, cibg_att_sup_loss 0.30501, cibg_segmask_iou 0.74171, pg_att_sup_loss 0.05221, pg_segmask_iou 0.96012, vbg_y8_loss 1.00483, vbg_y8_box_loss 0.11595, vbg_y8_cls_loss 0.12640, vbg_y8_dfl_loss 0.76247, vbg_phrcls_loss 0.00140, vbg_phrase_acc 0.99996, vbg_att_sup_loss 0.09702, vbg_segmask_iou 0.71175, cl_att_sup_loss 0.24320, cl_segmask_iou 0.60938, cl_phrcls_loss 0.00397, cl_phrase_acc 0.99998, 199.70 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.97004, cibg_att_sup_loss 0.38141, cibg_segmask_iou 0.34378, pg_att_sup_loss 0.17931, pg_segmask_iou 0.65554, vbg_y8_bbox_iou 0.48798, vbg_phrase_acc 0.96309, vbg_att_sup_loss 0.09133, vbg_segmask_iou 0.27975, cl_att_sup_loss 0.21840, cl_segmask_iou 0.55360, cl_phrase_acc 0.96486, 55.12 secs\n",
      "\u001b[1m---- Epoch 29/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 1.21028, cibg_y8_loss 2.06983, cibg_y8_box_loss 0.69445, cibg_y8_cls_loss 0.33678, cibg_y8_dfl_loss 1.03860, cibg_att_sup_loss 0.30770, cibg_segmask_iou 0.73976, pg_att_sup_loss 0.08174, pg_segmask_iou 0.95125, vbg_y8_loss 1.12794, vbg_y8_box_loss 0.18788, vbg_y8_cls_loss 0.16999, vbg_y8_dfl_loss 0.77007, vbg_phrcls_loss 0.00435, vbg_phrase_acc 0.99947, vbg_att_sup_loss 0.15053, vbg_segmask_iou 0.68875, cl_att_sup_loss 0.28457, cl_segmask_iou 0.59196, cl_phrcls_loss 0.03576, cl_phrase_acc 0.99453, 197.54 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96555, cibg_att_sup_loss 0.36353, cibg_segmask_iou 0.36164, pg_att_sup_loss 0.32337, pg_segmask_iou 0.49699, vbg_y8_bbox_iou 0.09469, vbg_phrase_acc 0.94405, vbg_att_sup_loss 0.13117, vbg_segmask_iou 0.15902, cl_att_sup_loss 0.40844, cl_segmask_iou 0.42452, cl_phrase_acc 0.85743, 55.48 secs\n",
      "\u001b[1m---- Epoch 30/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 1.26485, cibg_y8_loss 2.07672, cibg_y8_box_loss 0.69446, cibg_y8_cls_loss 0.34056, cibg_y8_dfl_loss 1.04170, cibg_att_sup_loss 0.31796, cibg_segmask_iou 0.73738, pg_att_sup_loss 0.09415, pg_segmask_iou 0.93851, vbg_y8_loss 1.20729, vbg_y8_box_loss 0.23339, vbg_y8_cls_loss 0.19804, vbg_y8_dfl_loss 0.77586, vbg_phrcls_loss 0.01058, vbg_phrase_acc 0.99866, vbg_att_sup_loss 0.17116, vbg_segmask_iou 0.67725, cl_att_sup_loss 0.28285, cl_segmask_iou 0.57702, cl_phrcls_loss 0.02245, cl_phrase_acc 0.99716, 198.99 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cibg_y8_bbox_iou 0.96107, cibg_att_sup_loss 0.44044, cibg_segmask_iou 0.28145, pg_att_sup_loss 0.31429, pg_segmask_iou 0.48397, vbg_y8_bbox_iou 0.58197, vbg_phrase_acc 0.96677, vbg_att_sup_loss 0.10716, vbg_segmask_iou 0.20745, cl_att_sup_loss 0.31790, cl_segmask_iou 0.47755, cl_phrase_acc 0.91729, 54.34 secs\n",
      "\u001b[1m---- Epoch 31/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 1.16516, cibg_y8_loss 2.04803, cibg_y8_box_loss 0.68170, cibg_y8_cls_loss 0.33050, cibg_y8_dfl_loss 1.03583, cibg_att_sup_loss 0.30564, cibg_segmask_iou 0.74137, pg_att_sup_loss 0.07148, pg_segmask_iou 0.95185, vbg_y8_loss 1.09471, vbg_y8_box_loss 0.16880, vbg_y8_cls_loss 0.15874, vbg_y8_dfl_loss 0.76717, vbg_phrcls_loss 0.00520, vbg_phrase_acc 0.99952, vbg_att_sup_loss 0.13568, vbg_segmask_iou 0.69390, cl_att_sup_loss 0.23920, cl_segmask_iou 0.60842, cl_phrcls_loss 0.00589, cl_phrase_acc 0.99979, 198.54 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96385, cibg_att_sup_loss 0.44046, cibg_segmask_iou 0.28562, pg_att_sup_loss 0.29384, pg_segmask_iou 0.51632, vbg_y8_bbox_iou 0.67385, vbg_phrase_acc 0.96917, vbg_att_sup_loss 0.10355, vbg_segmask_iou 0.22721, cl_att_sup_loss 0.32641, cl_segmask_iou 0.48597, cl_phrase_acc 0.91840, 54.60 secs\n",
      "\u001b[1m---- Epoch 32/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.12227, cibg_y8_loss 2.04751, cibg_y8_box_loss 0.68335, cibg_y8_cls_loss 0.32853, cibg_y8_dfl_loss 1.03563, cibg_att_sup_loss 0.30484, cibg_segmask_iou 0.74231, pg_att_sup_loss 0.06056, pg_segmask_iou 0.96194, vbg_y8_loss 1.04193, vbg_y8_box_loss 0.13857, vbg_y8_cls_loss 0.13952, vbg_y8_dfl_loss 0.76384, vbg_phrcls_loss 0.00266, vbg_phrase_acc 0.99981, vbg_att_sup_loss 0.11472, vbg_segmask_iou 0.70284, cl_att_sup_loss 0.22405, cl_segmask_iou 0.62139, cl_phrcls_loss 0.00389, cl_phrase_acc 0.99991, 198.27 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96817, cibg_att_sup_loss 0.39088, cibg_segmask_iou 0.33906, pg_att_sup_loss 0.20661, pg_segmask_iou 0.62408, vbg_y8_bbox_iou 0.52332, vbg_phrase_acc 0.96824, vbg_att_sup_loss 0.09515, vbg_segmask_iou 0.26173, cl_att_sup_loss 0.23992, cl_segmask_iou 0.55076, cl_phrase_acc 0.95565, 54.86 secs\n",
      "\u001b[1m---- Epoch 33/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.09947, cibg_y8_loss 2.02888, cibg_y8_box_loss 0.67439, cibg_y8_cls_loss 0.32591, cibg_y8_dfl_loss 1.02858, cibg_att_sup_loss 0.30243, cibg_segmask_iou 0.74343, pg_att_sup_loss 0.05635, pg_segmask_iou 0.95882, vbg_y8_loss 1.01954, vbg_y8_box_loss 0.12536, vbg_y8_cls_loss 0.13135, vbg_y8_dfl_loss 0.76283, vbg_phrcls_loss 0.00211, vbg_phrase_acc 0.99989, vbg_att_sup_loss 0.10522, vbg_segmask_iou 0.70725, cl_att_sup_loss 0.21579, cl_segmask_iou 0.62925, cl_phrcls_loss 0.00335, cl_phrase_acc 0.99991, 199.78 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96821, cibg_att_sup_loss 0.38705, cibg_segmask_iou 0.34282, pg_att_sup_loss 0.22101, pg_segmask_iou 0.60873, vbg_y8_bbox_iou 0.65779, vbg_phrase_acc 0.96977, vbg_att_sup_loss 0.09222, vbg_segmask_iou 0.27177, cl_att_sup_loss 0.26710, cl_segmask_iou 0.53248, cl_phrase_acc 0.94401, 54.91 secs\n",
      "\u001b[1m---- Epoch 34/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.09173, cibg_y8_loss 2.03751, cibg_y8_box_loss 0.67889, cibg_y8_cls_loss 0.32709, cibg_y8_dfl_loss 1.03153, cibg_att_sup_loss 0.30355, cibg_segmask_iou 0.74277, pg_att_sup_loss 0.04956, pg_segmask_iou 0.96753, vbg_y8_loss 1.00931, vbg_y8_box_loss 0.11899, vbg_y8_cls_loss 0.12768, vbg_y8_dfl_loss 0.76264, vbg_phrcls_loss 0.00158, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.09941, vbg_segmask_iou 0.71081, cl_att_sup_loss 0.21144, cl_segmask_iou 0.63355, cl_phrcls_loss 0.00329, cl_phrase_acc 0.99993, 197.80 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96998, cibg_att_sup_loss 0.38139, cibg_segmask_iou 0.34989, pg_att_sup_loss 0.18609, pg_segmask_iou 0.65365, vbg_y8_bbox_iou 0.48137, vbg_phrase_acc 0.96589, vbg_att_sup_loss 0.09258, vbg_segmask_iou 0.27856, cl_att_sup_loss 0.22268, cl_segmask_iou 0.56864, cl_phrase_acc 0.95976, 55.09 secs\n",
      "\u001b[1m---- Epoch 35/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.08691, cibg_y8_loss 2.03228, cibg_y8_box_loss 0.67456, cibg_y8_cls_loss 0.32513, cibg_y8_dfl_loss 1.03259, cibg_att_sup_loss 0.30736, cibg_segmask_iou 0.74337, pg_att_sup_loss 0.04907, pg_segmask_iou 0.96183, vbg_y8_loss 1.00322, vbg_y8_box_loss 0.11564, vbg_y8_cls_loss 0.12492, vbg_y8_dfl_loss 0.76266, vbg_phrcls_loss 0.00181, vbg_phrase_acc 0.99992, vbg_att_sup_loss 0.09615, vbg_segmask_iou 0.71180, cl_att_sup_loss 0.21187, cl_segmask_iou 0.63497, cl_phrcls_loss 0.00322, cl_phrase_acc 0.99998, 198.83 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96997, cibg_att_sup_loss 0.38735, cibg_segmask_iou 0.34368, pg_att_sup_loss 0.19227, pg_segmask_iou 0.64551, vbg_y8_bbox_iou 0.56385, vbg_phrase_acc 0.96845, vbg_att_sup_loss 0.08816, vbg_segmask_iou 0.29351, cl_att_sup_loss 0.23977, cl_segmask_iou 0.55742, cl_phrase_acc 0.95554, 54.94 secs\n",
      "\u001b[1m---- Epoch 36/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.08563, cibg_y8_loss 2.04313, cibg_y8_box_loss 0.67896, cibg_y8_cls_loss 0.32804, cibg_y8_dfl_loss 1.03613, cibg_att_sup_loss 0.31054, cibg_segmask_iou 0.74243, pg_att_sup_loss 0.05156, pg_segmask_iou 0.96107, vbg_y8_loss 0.99768, vbg_y8_box_loss 0.11323, vbg_y8_cls_loss 0.12307, vbg_y8_dfl_loss 0.76138, vbg_phrcls_loss 0.00157, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.09417, vbg_segmask_iou 0.71312, cl_att_sup_loss 0.20933, cl_segmask_iou 0.63866, cl_phrcls_loss 0.00321, cl_phrase_acc 0.99993, 199.51 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96941, cibg_att_sup_loss 0.40654, cibg_segmask_iou 0.32305, pg_att_sup_loss 0.19677, pg_segmask_iou 0.63843, vbg_y8_bbox_iou 0.59393, vbg_phrase_acc 0.96908, vbg_att_sup_loss 0.08948, vbg_segmask_iou 0.28608, cl_att_sup_loss 0.24456, cl_segmask_iou 0.55406, cl_phrase_acc 0.95466, 54.79 secs\n",
      "\u001b[1m---- Epoch 37/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 1.18875, cibg_y8_loss 2.05477, cibg_y8_box_loss 0.68628, cibg_y8_cls_loss 0.33263, cibg_y8_dfl_loss 1.03586, cibg_att_sup_loss 0.30617, cibg_segmask_iou 0.74232, pg_att_sup_loss 0.07699, pg_segmask_iou 0.95563, vbg_y8_loss 1.11795, vbg_y8_box_loss 0.18324, vbg_y8_cls_loss 0.16551, vbg_y8_dfl_loss 0.76920, vbg_phrcls_loss 0.00609, vbg_phrase_acc 0.99921, vbg_att_sup_loss 0.14899, vbg_segmask_iou 0.69098, cl_att_sup_loss 0.23729, cl_segmask_iou 0.62121, cl_phrcls_loss 0.02169, cl_phrase_acc 0.99710, 195.90 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.95819, cibg_att_sup_loss 0.43933, cibg_segmask_iou 0.29112, pg_att_sup_loss 0.36012, pg_segmask_iou 0.43200, vbg_y8_bbox_iou 0.33789, vbg_phrase_acc 0.96109, vbg_att_sup_loss 0.12631, vbg_segmask_iou 0.15300, cl_att_sup_loss 0.40656, cl_segmask_iou 0.43823, cl_phrase_acc 0.86685, 54.15 secs\n",
      "\u001b[1m---- Epoch 38/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 1.25599, cibg_y8_loss 2.07549, cibg_y8_box_loss 0.69607, cibg_y8_cls_loss 0.33744, cibg_y8_dfl_loss 1.04198, cibg_att_sup_loss 0.31427, cibg_segmask_iou 0.73952, pg_att_sup_loss 0.08939, pg_segmask_iou 0.94339, vbg_y8_loss 1.20253, vbg_y8_box_loss 0.23130, vbg_y8_cls_loss 0.19601, vbg_y8_dfl_loss 0.77522, vbg_phrcls_loss 0.01681, vbg_phrase_acc 0.99786, vbg_att_sup_loss 0.17188, vbg_segmask_iou 0.67941, cl_att_sup_loss 0.24498, cl_segmask_iou 0.60954, cl_phrcls_loss 0.01935, cl_phrase_acc 0.99735, 198.94 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96580, cibg_att_sup_loss 0.41303, cibg_segmask_iou 0.31132, pg_att_sup_loss 0.27628, pg_segmask_iou 0.53760, vbg_y8_bbox_iou 0.58092, vbg_phrase_acc 0.96464, vbg_att_sup_loss 0.10540, vbg_segmask_iou 0.22301, cl_att_sup_loss 0.32068, cl_segmask_iou 0.49146, cl_phrase_acc 0.92927, 55.17 secs\n",
      "\u001b[1m---- Epoch 39/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 1.16484, cibg_y8_loss 2.05244, cibg_y8_box_loss 0.68614, cibg_y8_cls_loss 0.33093, cibg_y8_dfl_loss 1.03537, cibg_att_sup_loss 0.30772, cibg_segmask_iou 0.74336, pg_att_sup_loss 0.06733, pg_segmask_iou 0.95771, vbg_y8_loss 1.09852, vbg_y8_box_loss 0.17130, vbg_y8_cls_loss 0.15984, vbg_y8_dfl_loss 0.76738, vbg_phrcls_loss 0.00601, vbg_phrase_acc 0.99941, vbg_att_sup_loss 0.13802, vbg_segmask_iou 0.69317, cl_att_sup_loss 0.21422, cl_segmask_iou 0.63166, cl_phrcls_loss 0.00780, cl_phrase_acc 0.99949, 198.42 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cibg_y8_bbox_iou 0.96005, cibg_att_sup_loss 0.45825, cibg_segmask_iou 0.26157, pg_att_sup_loss 0.38798, pg_segmask_iou 0.42012, vbg_y8_bbox_iou 0.27752, vbg_phrase_acc 0.96448, vbg_att_sup_loss 0.12785, vbg_segmask_iou 0.15497, cl_att_sup_loss 0.43673, cl_segmask_iou 0.41914, cl_phrase_acc 0.87561, 54.40 secs\n",
      "\u001b[1m---- Epoch 40/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.12487, cibg_y8_loss 2.07178, cibg_y8_box_loss 0.69460, cibg_y8_cls_loss 0.33553, cibg_y8_dfl_loss 1.04165, cibg_att_sup_loss 0.31047, cibg_segmask_iou 0.73960, pg_att_sup_loss 0.05680, pg_segmask_iou 0.95977, vbg_y8_loss 1.04376, vbg_y8_box_loss 0.14020, vbg_y8_cls_loss 0.13996, vbg_y8_dfl_loss 0.76360, vbg_phrcls_loss 0.00251, vbg_phrase_acc 0.99985, vbg_att_sup_loss 0.11522, vbg_segmask_iou 0.70207, cl_att_sup_loss 0.20154, cl_segmask_iou 0.64391, cl_phrcls_loss 0.00526, cl_phrase_acc 0.99970, 196.95 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96992, cibg_att_sup_loss 0.36616, cibg_segmask_iou 0.36386, pg_att_sup_loss 0.20317, pg_segmask_iou 0.63811, vbg_y8_bbox_iou 0.25136, vbg_phrase_acc 0.95880, vbg_att_sup_loss 0.10420, vbg_segmask_iou 0.24360, cl_att_sup_loss 0.24608, cl_segmask_iou 0.56071, cl_phrase_acc 0.96120, 55.35 secs\n",
      "\u001b[1m---- Epoch 41/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.09649, cibg_y8_loss 2.04560, cibg_y8_box_loss 0.68276, cibg_y8_cls_loss 0.32813, cibg_y8_dfl_loss 1.03472, cibg_att_sup_loss 0.30341, cibg_segmask_iou 0.74282, pg_att_sup_loss 0.05147, pg_segmask_iou 0.96201, vbg_y8_loss 1.01726, vbg_y8_box_loss 0.12445, vbg_y8_cls_loss 0.12976, vbg_y8_dfl_loss 0.76305, vbg_phrcls_loss 0.00218, vbg_phrase_acc 0.99990, vbg_att_sup_loss 0.10390, vbg_segmask_iou 0.70983, cl_att_sup_loss 0.19358, cl_segmask_iou 0.65455, cl_phrcls_loss 0.00395, cl_phrase_acc 0.99988, 198.88 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96921, cibg_att_sup_loss 0.39231, cibg_segmask_iou 0.33729, pg_att_sup_loss 0.21075, pg_segmask_iou 0.62311, vbg_y8_bbox_iou 0.47831, vbg_phrase_acc 0.96742, vbg_att_sup_loss 0.09394, vbg_segmask_iou 0.26818, cl_att_sup_loss 0.23640, cl_segmask_iou 0.56532, cl_phrase_acc 0.96696, 54.88 secs\n",
      "\u001b[1m---- Epoch 42/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.08457, cibg_y8_loss 2.04097, cibg_y8_box_loss 0.67983, cibg_y8_cls_loss 0.32684, cibg_y8_dfl_loss 1.03430, cibg_att_sup_loss 0.30222, cibg_segmask_iou 0.74318, pg_att_sup_loss 0.04740, pg_segmask_iou 0.96826, vbg_y8_loss 1.00525, vbg_y8_box_loss 0.11784, vbg_y8_cls_loss 0.12561, vbg_y8_dfl_loss 0.76181, vbg_phrcls_loss 0.00206, vbg_phrase_acc 0.99990, vbg_att_sup_loss 0.09836, vbg_segmask_iou 0.71240, cl_att_sup_loss 0.18773, cl_segmask_iou 0.65878, cl_phrcls_loss 0.00243, cl_phrase_acc 1.00000, 196.95 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96938, cibg_att_sup_loss 0.40898, cibg_segmask_iou 0.31791, pg_att_sup_loss 0.20470, pg_segmask_iou 0.63082, vbg_y8_bbox_iou 0.53778, vbg_phrase_acc 0.96861, vbg_att_sup_loss 0.09292, vbg_segmask_iou 0.27215, cl_att_sup_loss 0.25516, cl_segmask_iou 0.55693, cl_phrase_acc 0.96319, 54.79 secs\n",
      "\u001b[1m---- Epoch 43/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.07719, cibg_y8_loss 2.02611, cibg_y8_box_loss 0.67211, cibg_y8_cls_loss 0.32500, cibg_y8_dfl_loss 1.02900, cibg_att_sup_loss 0.30387, cibg_segmask_iou 0.74199, pg_att_sup_loss 0.04580, pg_segmask_iou 0.96418, vbg_y8_loss 0.99959, vbg_y8_box_loss 0.11457, vbg_y8_cls_loss 0.12316, vbg_y8_dfl_loss 0.76186, vbg_phrcls_loss 0.00182, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.09588, vbg_segmask_iou 0.71215, cl_att_sup_loss 0.18643, cl_segmask_iou 0.66001, cl_phrcls_loss 0.00231, cl_phrase_acc 0.99998, 199.03 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96950, cibg_att_sup_loss 0.40285, cibg_segmask_iou 0.32505, pg_att_sup_loss 0.20028, pg_segmask_iou 0.63634, vbg_y8_bbox_iou 0.53446, vbg_phrase_acc 0.96853, vbg_att_sup_loss 0.09181, vbg_segmask_iou 0.27716, cl_att_sup_loss 0.23933, cl_segmask_iou 0.56744, cl_phrase_acc 0.96608, 55.11 secs\n",
      "\u001b[1m---- Epoch 44/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 1.07421, cibg_y8_loss 2.03274, cibg_y8_box_loss 0.67766, cibg_y8_cls_loss 0.32487, cibg_y8_dfl_loss 1.03021, cibg_att_sup_loss 0.29908, cibg_segmask_iou 0.74401, pg_att_sup_loss 0.04665, pg_segmask_iou 0.96447, vbg_y8_loss 0.99535, vbg_y8_box_loss 0.11217, vbg_y8_cls_loss 0.12169, vbg_y8_dfl_loss 0.76148, vbg_phrcls_loss 0.00165, vbg_phrase_acc 0.99994, vbg_att_sup_loss 0.09313, vbg_segmask_iou 0.71498, cl_att_sup_loss 0.18714, cl_segmask_iou 0.66366, cl_phrcls_loss 0.00227, cl_phrase_acc 1.00000, 197.94 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.97010, cibg_att_sup_loss 0.39961, cibg_segmask_iou 0.32877, pg_att_sup_loss 0.19174, pg_segmask_iou 0.64968, vbg_y8_bbox_iou 0.44309, vbg_phrase_acc 0.96644, vbg_att_sup_loss 0.09372, vbg_segmask_iou 0.27210, cl_att_sup_loss 0.22956, cl_segmask_iou 0.57757, cl_phrase_acc 0.96907, 55.02 secs\n",
      "\u001b[1m---- Epoch 45/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000030) ...\n",
      "loss 1.17392, cibg_y8_loss 2.07231, cibg_y8_box_loss 0.69400, cibg_y8_cls_loss 0.33723, cibg_y8_dfl_loss 1.04108, cibg_att_sup_loss 0.31118, cibg_segmask_iou 0.74140, pg_att_sup_loss 0.07850, pg_segmask_iou 0.95464, vbg_y8_loss 1.10692, vbg_y8_box_loss 0.17842, vbg_y8_cls_loss 0.16006, vbg_y8_dfl_loss 0.76844, vbg_phrcls_loss 0.00341, vbg_phrase_acc 0.99966, vbg_att_sup_loss 0.14109, vbg_segmask_iou 0.69610, cl_att_sup_loss 0.20323, cl_segmask_iou 0.65217, cl_phrcls_loss 0.01333, cl_phrase_acc 0.99834, 198.07 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.91033, cibg_att_sup_loss 0.49817, cibg_segmask_iou 0.19748, pg_att_sup_loss 0.63459, pg_segmask_iou 0.16721, vbg_y8_bbox_iou 0.08659, vbg_phrase_acc 0.96139, vbg_att_sup_loss 0.16496, vbg_segmask_iou 0.07232, cl_att_sup_loss 0.64587, cl_segmask_iou 0.25394, cl_phrase_acc 0.79867, 51.48 secs\n",
      "\u001b[1m---- Epoch 46/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 1.23304, cibg_y8_loss 2.06062, cibg_y8_box_loss 0.69123, cibg_y8_cls_loss 0.33414, cibg_y8_dfl_loss 1.03525, cibg_att_sup_loss 0.30982, cibg_segmask_iou 0.74242, pg_att_sup_loss 0.09510, pg_segmask_iou 0.93867, vbg_y8_loss 1.18162, vbg_y8_box_loss 0.22196, vbg_y8_cls_loss 0.18678, vbg_y8_dfl_loss 0.77288, vbg_phrcls_loss 0.00827, vbg_phrase_acc 0.99901, vbg_att_sup_loss 0.16562, vbg_segmask_iou 0.68301, cl_att_sup_loss 0.23077, cl_segmask_iou 0.62651, cl_phrcls_loss 0.02613, cl_phrase_acc 0.99679, 200.08 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96496, cibg_att_sup_loss 0.35936, cibg_segmask_iou 0.36748, pg_att_sup_loss 0.31605, pg_segmask_iou 0.51133, vbg_y8_bbox_iou 0.25977, vbg_phrase_acc 0.96609, vbg_att_sup_loss 0.11924, vbg_segmask_iou 0.17334, cl_att_sup_loss 0.40287, cl_segmask_iou 0.43437, cl_phrase_acc 0.89956, 54.92 secs\n",
      "\u001b[1m---- Epoch 47/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000011) ...\n",
      "loss 1.15173, cibg_y8_loss 2.05162, cibg_y8_box_loss 0.68536, cibg_y8_cls_loss 0.33039, cibg_y8_dfl_loss 1.03587, cibg_att_sup_loss 0.30766, cibg_segmask_iou 0.74301, pg_att_sup_loss 0.06717, pg_segmask_iou 0.95896, vbg_y8_loss 1.08595, vbg_y8_box_loss 0.16488, vbg_y8_cls_loss 0.15443, vbg_y8_dfl_loss 0.76664, vbg_phrcls_loss 0.00436, vbg_phrase_acc 0.99956, vbg_att_sup_loss 0.13259, vbg_segmask_iou 0.69623, cl_att_sup_loss 0.20027, cl_segmask_iou 0.64668, cl_phrcls_loss 0.00690, cl_phrase_acc 0.99963, 197.74 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96928, cibg_att_sup_loss 0.38884, cibg_segmask_iou 0.34114, pg_att_sup_loss 0.19651, pg_segmask_iou 0.64205, vbg_y8_bbox_iou 0.25086, vbg_phrase_acc 0.96026, vbg_att_sup_loss 0.10605, vbg_segmask_iou 0.23524, cl_att_sup_loss 0.22943, cl_segmask_iou 0.57353, cl_phrase_acc 0.97217, 55.54 secs\n",
      "\u001b[1m---- Epoch 48/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 1.11400, cibg_y8_loss 2.07648, cibg_y8_box_loss 0.69746, cibg_y8_cls_loss 0.33309, cibg_y8_dfl_loss 1.04593, cibg_att_sup_loss 0.31168, cibg_segmask_iou 0.74249, pg_att_sup_loss 0.05786, pg_segmask_iou 0.95863, vbg_y8_loss 1.03249, vbg_y8_box_loss 0.13452, vbg_y8_cls_loss 0.13529, vbg_y8_dfl_loss 0.76267, vbg_phrcls_loss 0.00195, vbg_phrase_acc 0.99989, vbg_att_sup_loss 0.11086, vbg_segmask_iou 0.70845, cl_att_sup_loss 0.18332, cl_segmask_iou 0.66412, cl_phrcls_loss 0.00384, cl_phrase_acc 0.99982, 199.56 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cibg_y8_bbox_iou 0.96418, cibg_att_sup_loss 0.45706, cibg_segmask_iou 0.26878, pg_att_sup_loss 0.29931, pg_segmask_iou 0.50970, vbg_y8_bbox_iou 0.55039, vbg_phrase_acc 0.96777, vbg_att_sup_loss 0.10688, vbg_segmask_iou 0.21457, cl_att_sup_loss 0.34763, cl_segmask_iou 0.48977, cl_phrase_acc 0.91851, 54.53 secs\n",
      "\u001b[1m---- Epoch 49/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 1.08754, cibg_y8_loss 2.04079, cibg_y8_box_loss 0.68011, cibg_y8_cls_loss 0.32718, cibg_y8_dfl_loss 1.03350, cibg_att_sup_loss 0.30603, cibg_segmask_iou 0.74295, pg_att_sup_loss 0.05268, pg_segmask_iou 0.96115, vbg_y8_loss 1.01101, vbg_y8_box_loss 0.12147, vbg_y8_cls_loss 0.12669, vbg_y8_dfl_loss 0.76286, vbg_phrcls_loss 0.00154, vbg_phrase_acc 0.99993, vbg_att_sup_loss 0.10140, vbg_segmask_iou 0.71221, cl_att_sup_loss 0.17632, cl_segmask_iou 0.67209, cl_phrcls_loss 0.00249, cl_phrase_acc 0.99993, 200.97 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96907, cibg_att_sup_loss 0.39296, cibg_segmask_iou 0.33658, pg_att_sup_loss 0.20463, pg_segmask_iou 0.63464, vbg_y8_bbox_iou 0.36882, vbg_phrase_acc 0.96564, vbg_att_sup_loss 0.10017, vbg_segmask_iou 0.24828, cl_att_sup_loss 0.25013, cl_segmask_iou 0.56851, cl_phrase_acc 0.96386, 55.09 secs\n",
      "\u001b[1m---- Epoch 50/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 1.07602, cibg_y8_loss 2.03952, cibg_y8_box_loss 0.67967, cibg_y8_cls_loss 0.32695, cibg_y8_dfl_loss 1.03290, cibg_att_sup_loss 0.30566, cibg_segmask_iou 0.74393, pg_att_sup_loss 0.04811, pg_segmask_iou 0.96835, vbg_y8_loss 0.99866, vbg_y8_box_loss 0.11504, vbg_y8_cls_loss 0.12245, vbg_y8_dfl_loss 0.76117, vbg_phrcls_loss 0.00108, vbg_phrase_acc 0.99996, vbg_att_sup_loss 0.09455, vbg_segmask_iou 0.71631, cl_att_sup_loss 0.17177, cl_segmask_iou 0.67753, cl_phrcls_loss 0.00240, cl_phrase_acc 0.99995, 199.12 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96989, cibg_att_sup_loss 0.36181, cibg_segmask_iou 0.37092, pg_att_sup_loss 0.20780, pg_segmask_iou 0.63230, vbg_y8_bbox_iou 0.35467, vbg_phrase_acc 0.96297, vbg_att_sup_loss 0.10125, vbg_segmask_iou 0.25203, cl_att_sup_loss 0.25659, cl_segmask_iou 0.56338, cl_phrase_acc 0.96009, 55.53 secs\n",
      "\u001b[1m---- Epoch 51/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 1.06276, cibg_y8_loss 2.00508, cibg_y8_box_loss 0.66427, cibg_y8_cls_loss 0.31619, cibg_y8_dfl_loss 1.02463, cibg_att_sup_loss 0.29333, cibg_segmask_iou 0.74889, pg_att_sup_loss 0.04762, pg_segmask_iou 0.96315, vbg_y8_loss 0.99274, vbg_y8_box_loss 0.11137, vbg_y8_cls_loss 0.11994, vbg_y8_dfl_loss 0.76142, vbg_phrcls_loss 0.00120, vbg_phrase_acc 0.99996, vbg_att_sup_loss 0.09240, vbg_segmask_iou 0.71652, cl_att_sup_loss 0.17455, cl_segmask_iou 0.67813, cl_phrcls_loss 0.00203, cl_phrase_acc 1.00000, 200.50 secs\n",
      "(2) Validation stage ...\n",
      "cibg_y8_bbox_iou 0.96951, cibg_att_sup_loss 0.38202, cibg_segmask_iou 0.34872, pg_att_sup_loss 0.20895, pg_segmask_iou 0.62892, vbg_y8_bbox_iou 0.51135, vbg_phrase_acc 0.96821, vbg_att_sup_loss 0.09542, vbg_segmask_iou 0.26389, cl_att_sup_loss 0.25445, cl_segmask_iou 0.56475, cl_phrase_acc 0.96175, 55.30 secs\n",
      "\u001b[1m---- Epoch 52/125\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "^C\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 935, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 809, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 504, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 134, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1069, in _run_once_on_dataset_as_gen\n",
      "    self._fire_event(Events.ITERATION_COMPLETED)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 425, in _fire_event\n",
      "    func(*first, *(event_args + others), **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/condition_aware_loss.py\", line 36, in iteration_completed_handler\n",
      "    self.update(self.output_transform(output))\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/condition_aware_loss.py\", line 19, in update\n",
      "    loss = loss.item()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_phrase_grounding.py \\\n",
    "--pretrained_checkpoint_folder_paths \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231219_195906_mscxr+chst-img-anat+vinbig_PhraseGrounder(yolov8l,128,256)\" \\\n",
    "--epochs 125 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--max_images_per_batch 35 \\\n",
    "--max_phrases_per_batch 600 \\\n",
    "--max_phrases_per_image 30 \\\n",
    "--val_batch_size_factor 1.5 \\\n",
    "--image_size 416 416 \\\n",
    "--image_local_feat_size 512 \\\n",
    "--num_regions 169 \\\n",
    "--regions_width 13 \\\n",
    "--regions_height 13 \\\n",
    "--yolov8_model_name_or_path \"yolov8l.pt\" \\\n",
    "--yolov8_model_alias \"yolov8l\" \\\n",
    "--phrase_embedding_size 128 \\\n",
    "--qkv_size 256 \\\n",
    "--phrase_classifier_hidden_size 256 \\\n",
    "--num_train_workers 2 \\\n",
    "--num_val_workers 2 \\\n",
    "--iters_to_accumulate 12 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,3e-5,8,1e-6,3e-5,8,1e-6\" \\\n",
    "--use_chest_imagenome_for_train \\\n",
    "--use_chest_imagenome_gold_for_test \\\n",
    "--use_mscxr_for_train \\\n",
    "--use_mscxr_for_test \\\n",
    "--use_vinbig_for_train \\\n",
    "--use_vinbig_for_test \\\n",
    "--use_chexlocalize_for_train \\\n",
    "--use_chexlocalize_for_test \\\n",
    "--chest_imagenome_bbox_phrase_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/chest_imagenome/bbox_phrase_embeddings.pkl\" \\\n",
    "--vinbig_bbox_phrase_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/vinbig/bbox_phrase_embeddings.pkl\" \\\n",
    "--mscxr_phrase2embedding_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/mscxr_phrase2embedding.pkl\" \\\n",
    "--chexlocalize_class_phrase_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/chexlocalize/class_phrase_embeddings(hash=322,2913146136894882617).pkl\" \\\n",
    "--attention_supervision_loss_weight 3.0 \\\n",
    "--phrase_classifier_loss_weight 2.0 \\\n",
    "--foreground_loss_weight 1.5 \\\n",
    "--mask_exponent 0.4 \\\n",
    "--chest_imagenome_anatlocs_weight 2.5 \\\n",
    "--mscxr_weight 1.0 \\\n",
    "--vinbig_weight 6.0 \\\n",
    "--chexlocalize_weight 2.5 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8288f907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 10\n",
      "   batches_per_epoch: 150\n",
      "   max_images_per_batch: 35\n",
      "   max_phrases_per_batch: 600\n",
      "   max_phrases_per_image: 30\n",
      "   val_batch_size_factor: 1.5\n",
      "   checkpoint_folder: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   pretrained_checkpoint_folder_paths: ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231220_111103_mscxr+chst-img-anat+vinbig+chexloc_PhraseGrounder(yolov8l,128,256)']\n",
      "   freeze_image_encoder: True\n",
      "   raw_image_encoding: yolov8\n",
      "   num_regions: 169\n",
      "   image_local_feat_size: 512\n",
      "   image_encoder_pretrained_weights_path: None\n",
      "   yolov8_model_name_or_path: yolov8l.pt\n",
      "   yolov8_model_alias: yolov8l\n",
      "   phrase_embedding_size: 128\n",
      "   regions_width: 13\n",
      "   regions_height: 13\n",
      "   qkv_size: 256\n",
      "   phrase_classifier_hidden_size: 256\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-07\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-7,3,3e-6,8,1e-7,3e-6,8,1e-7\n",
      "   iters_to_accumulate: 10000\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   attention_supervision_loss_weight: 3.0\n",
      "   phrase_classifier_loss_weight: 2.0\n",
      "   foreground_loss_weight: 1.5\n",
      "   background_loss_weight: 1.0\n",
      "   num_train_workers: 2\n",
      "   num_val_workers: 2\n",
      "   device: GPU\n",
      "   use_amp: False\n",
      "   image_size: [416, 416]\n",
      "   dicom_id_to_pos_neg_facts_filepath: None\n",
      "   mscxr_phrase2embedding_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/mscxr_phrase2embedding.pkl\n",
      "   chest_imagenome_bbox_phrase_embeddings_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/chest_imagenome/bbox_phrase_embeddings.pkl\n",
      "   vinbig_bbox_phrase_embeddings_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/vinbig/bbox_phrase_embeddings.pkl\n",
      "   chexlocalize_class_phrase_embeddings_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/chexlocalize/class_phrase_embeddings(hash=322,2913146136894882617).pkl\n",
      "   exclude_noisy_images: False\n",
      "   mimiccxr_facts_weight: 1.0\n",
      "   chest_imagenome_anatlocs_weight: 0.0\n",
      "   mscxr_weight: 1.0\n",
      "   vinbig_weight: 0.0\n",
      "   chexlocalize_weight: 1.0\n",
      "   img_aug_mode: None\n",
      "   pos_area_prior: 0.4\n",
      "   neg_area_prior: 0.0\n",
      "   use_mimiccxr_facts_for_train: False\n",
      "   use_mscxr_for_train: True\n",
      "   use_mscxr_for_test: True\n",
      "   use_chest_imagenome_for_train: True\n",
      "   use_chest_imagenome_gold_for_test: False\n",
      "   use_vinbig_for_train: True\n",
      "   use_vinbig_for_test: False\n",
      "   use_chexlocalize_for_train: True\n",
      "   use_chexlocalize_for_test: True\n",
      "   vinbig_training_data_mode: train\n",
      "   mask_exponent: 0.4\n",
      "   save: False\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "source_image_size_mode: medium_512\n",
      "\u001b[93m\u001b[1mWARNING: use_chest_imagenome_for_train is True but chest_imagenome_anatlocs_weight is 0\u001b[0m\n",
      "\u001b[93m\u001b[1mWARNING: use_vinbig_for_train is True but vinbig_weight is 0\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of PhraseGrounder ...\u001b[0m\n",
      "\u001b[93mWARNING: Unused kwargs: {'pretrained_checkpoint_folder_path': None, 'pretrained_checkpoint_folder_paths': ['/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231220_111103_mscxr+chst-img-anat+vinbig+chexloc_PhraseGrounder(yolov8l,128,256)']}\u001b[0m\n",
      "MultiPurposeVisualModule()\n",
      "  Initializing raw_image_encoder: yolov8\n",
      "  nc_list: [36, 22]\n",
      "\u001b[1mCreating YOLOv8 model for multiple datasets\u001b[0m\n",
      "\u001b[1m   1. Creating DetectionModel for 36 classes\u001b[0m\n",
      "Overriding model.yaml nc=80 with nc=36\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.Conv                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.C2f                   [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.C2f                   [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.C2f                   [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.C2f                   [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.C2f                   [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.C2f                   [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5610556  ultralytics.nn.modules.Detect                [36, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43657596 parameters, 43657580 gradients, 165.6 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[1m   2. Creating DetectionModel for 22 classes\u001b[0m\n",
      "Overriding model.yaml nc=80 with nc=22\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.Conv                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.C2f                   [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.C2f                   [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.C2f                   [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.C2f                   [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15                  -1  3   1247744  ultralytics.nn.modules.C2f                   [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.C2f                   [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5599762  ultralytics.nn.modules.Detect                [22, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43646802 parameters, 43646786 gradients, 165.5 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[1m   Creating final YOLOv8 model with 2 detection layers\u001b[0m\n",
      "Overriding model.yaml nc=80 with nc=36\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.Conv                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.C2f                   [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.C2f                   [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.C2f                   [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.C2f                   [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.C2f                   [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.C2f                   [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.Conv                  [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.C2f                   [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5610556  ultralytics.nn.modules.Detect                [36, [256, 512, 512]]         \n",
      "Model summary: 365 layers, 43657596 parameters, 43657580 gradients, 165.6 GFLOPs\n",
      "\n",
      "Transferred 510/510 items from pretrained weights\n",
      "  self.global_feat_size = 1024\n",
      "  self.local_feat_size = 512\n",
      "  Initializing auxiliary tasks\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-7,3,3e-6,8,1e-7,3e-6,8,1e-7\n",
      "1e-07 3 3e-06 8 1e-07 3e-06 8 1e-07\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 3e-06\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "\u001b[1m\u001b[35mUsing YOLOv8MultiDetectionLayersLoss\u001b[0m\n",
      "foreground_loss_weight: 1.5\n",
      "background_loss_weight: 1.0\n",
      "Using multiple detection layers in yolov8\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10000, max_grad_norm = None\n",
      "foreground_loss_weight: 1.0\n",
      "background_loss_weight: 1.0\n",
      "Using multiple detection layers in yolov8\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mCreating CheXLocalize Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "Loding class_phrase_embeddings_filepath and bbox_phrases from /mnt/workspace/pamessina/medvqa-workspace/cache/chexlocalize/class_phrase_embeddings(hash=322,2913146136894882617).pkl...\n",
      "class_phrase_embeddings.shape = (10, 128)\n",
      "len(class_phrases) = 10\n",
      "\t Enlarged Cardiomediastinum\n",
      "\t Cardiomegaly\n",
      "\t Lung Lesion\n",
      "\t Airspace Opacity\n",
      "\t Edema\n",
      "\t Consolidation\n",
      "\t Atelectasis\n",
      "\t Pneumothorax\n",
      "\t Pleural Effusion\n",
      "\t Support Devices\n",
      "Compute phrase grounding masks and labels\n",
      "Without masks: 216/902\n",
      "Generating train dataset and dataloader\n",
      "batch_size = 35\n",
      "Generating val dataset and dataloader\n",
      "batch_size = 52\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mCreating MIMIC-CXR Phrase Grounding Trainer ...\u001b[0m\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "get_image_transform()\n",
      "  Using bounding box aware transforms\n",
      "    Returning default transform (no augmentation)\n",
      "mask_exponent = 0.4\n",
      "len(forbidden_train_dicom_ids) = 0\n",
      "\u001b[1mPreparing MS-CXR dataset and dataloader for training/testing...\u001b[0m\n",
      "len(dicom_id_2_phrases_and_masks) = 1047\n",
      "len(phrase2embedding) = 715\n",
      "Using image size mode: medium_512\n",
      "227835it [00:00, 429508.54it/s]\n",
      "Total number of images: 1047\n",
      "Number of phrases: 1, # images: 939\n",
      "Number of phrases: 2, # images: 103\n",
      "Number of phrases: 3, # images: 5\n",
      "\u001b[1mPreparing Chest Imagenome dataset and dataloader for training...\u001b[0m\n",
      "Loding bbox_phrase_embeddings and bbox_phrases from /mnt/workspace/pamessina/medvqa-workspace/cache/chest_imagenome/bbox_phrase_embeddings.pkl...\n",
      "bbox_phrase_embeddings.shape = (36, 128)\n",
      "len(bbox_phrases) = 36\n",
      "\t right lung\n",
      "\t right upper lung zone\n",
      "\t right mid lung zone\n",
      "\t right lower lung zone\n",
      "\t right hilar structures\n",
      "\t right apical zone\n",
      "\t right costophrenic angle\n",
      "\t right cardiophrenic angle\n",
      "\t right hemidiaphragm\n",
      "\t left lung\n",
      "\t left upper lung zone\n",
      "\t left mid lung zone\n",
      "\t left lower lung zone\n",
      "\t left hilar structures\n",
      "\t left apical zone\n",
      "\t left costophrenic angle\n",
      "\t left hemidiaphragm\n",
      "\t trachea\n",
      "\t spine\n",
      "\t right clavicle\n",
      "\t left clavicle\n",
      "\t aortic arch\n",
      "\t mediastinum\n",
      "\t upper mediastinum\n",
      "\t svc\n",
      "\t cardiac silhouette\n",
      "\t left cardiac silhouette\n",
      "\t right cardiac silhouette\n",
      "\t cavoatrial junction\n",
      "\t right atrium\n",
      "\t descending aorta\n",
      "\t carina\n",
      "\t left upper abdomen\n",
      "\t right upper abdomen\n",
      "\t abdomen\n",
      "\t left cardiophrenic angle\n",
      "Using image size mode: medium_512\n",
      "\u001b[1mLoading precomputed bbox_coords_and_presence_and_mask from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/bbox_coords_and_presence_and_mask(13,13,243310).pkl...\u001b[0m\n",
      "227835it [00:29, 7774.86it/s]\n",
      "Total number of images: 243310\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mCreating dataloaders ...\u001b[0m\n",
      "len(mimiccxr_trainer.train_mscxr_dataloader) = 31\n",
      "len(mimiccxr_trainer.test_mscxr_dataloader) = 22\n",
      "len(chexlocalize_trainer.train_dataloader) = 26\n",
      "len(chexlocalize_trainer.val_dataloader) = 18\n",
      "len(_train_dataloaders) = 2\n",
      "len(_val_dataloaders) = 2\n",
      "_train_weights = [1.0, 1.0]\n",
      "merged_dataset_name = mscxr+chexloc\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_11_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7311.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231220_111103_mscxr+chst-img-anat+vinbig+chexloc_PhraseGrounder(yolov8l,128,256)/checkpoint_11_ciss+ciou+ciou+clss+clcc+clou+pgss+pgou+vbss+vbcc+vbou+vbou=0.7311.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m14) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/10\u001b[0m\n",
      "(1) Training stage (lr = 0.000000) ...\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "ConditionAwareSegmaskIOU:compute(): self._acc_score: 3711.476843672339, self._count: 3948\n",
      "ConditionAwareSegmaskIOU:compute(): self._acc_score: 4689.864430239427, self._count: 9101\n",
      "loss 0.29821, pg_att_sup_loss 0.09441, pg_segmask_iou 0.94009, cl_att_sup_loss 0.47607, cl_segmask_iou 0.51531, cl_phrcls_loss 0.02594, cl_phrase_acc 0.99793, 62.89 secs\n",
      "(2) Validation stage ...\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=cl\n",
      "ConditionAwareSegmaskIOU:compute(): self._acc_score: 1091.5079976874404, self._count: 1160\n",
      "ConditionAwareSegmaskIOU:compute(): self._acc_score: 1124.0661733275438, self._count: 2177\n",
      "pg_att_sup_loss 0.02115, pg_segmask_iou 0.94096, cl_att_sup_loss 0.13093, cl_segmask_iou 0.51634, cl_phrase_acc 0.99789, 16.05 secs\n",
      "\u001b[1m---- Epoch 2/10\u001b[0m\n",
      "(1) Training stage (lr = 0.000000) ...\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "step_fn(): flag=cl\n",
      "step_fn(): flag=pg\n",
      "^C\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 950, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 824, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_phrase_grounding.py\", line 519, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 134, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/phrase_grounding.py\", line 482, in step_fn\n",
      "    output = step_fn__phrase_grounding(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/phrase_grounding.py\", line 173, in step_fn__phrase_grounding\n",
      "    attention_supervision_loss = compute_balanced_segmentation_loss(sigmoid_attention, phrase_grounding_masks,\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/segmentation_loss.py\", line 5, in compute_balanced_segmentation_loss\n",
      "    nonzero_count = torch.sum(gt_nonzero).item()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_phrase_grounding.py \\\n",
    "--pretrained_checkpoint_folder_paths \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/phrase_grounding/20231220_111103_mscxr+chst-img-anat+vinbig+chexloc_PhraseGrounder(yolov8l,128,256)\" \\\n",
    "--epochs 10 \\\n",
    "--batches_per_epoch 150 \\\n",
    "--max_images_per_batch 35 \\\n",
    "--max_phrases_per_batch 600 \\\n",
    "--max_phrases_per_image 30 \\\n",
    "--val_batch_size_factor 1.5 \\\n",
    "--image_size 416 416 \\\n",
    "--image_local_feat_size 512 \\\n",
    "--num_regions 169 \\\n",
    "--regions_width 13 \\\n",
    "--regions_height 13 \\\n",
    "--yolov8_model_name_or_path \"yolov8l.pt\" \\\n",
    "--yolov8_model_alias \"yolov8l\" \\\n",
    "--phrase_embedding_size 128 \\\n",
    "--qkv_size 256 \\\n",
    "--phrase_classifier_hidden_size 256 \\\n",
    "--num_train_workers 2 \\\n",
    "--num_val_workers 2 \\\n",
    "--iters_to_accumulate 10000 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-7 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-7,3,3e-6,8,1e-7,3e-6,8,1e-7\" \\\n",
    "--use_chest_imagenome_for_train \\\n",
    "--use_mscxr_for_train \\\n",
    "--use_mscxr_for_test \\\n",
    "--use_vinbig_for_train \\\n",
    "--use_chexlocalize_for_train \\\n",
    "--use_chexlocalize_for_test \\\n",
    "--chest_imagenome_bbox_phrase_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/chest_imagenome/bbox_phrase_embeddings.pkl\" \\\n",
    "--vinbig_bbox_phrase_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/vinbig/bbox_phrase_embeddings.pkl\" \\\n",
    "--mscxr_phrase2embedding_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/mscxr_phrase2embedding.pkl\" \\\n",
    "--chexlocalize_class_phrase_embeddings_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/chexlocalize/class_phrase_embeddings(hash=322,2913146136894882617).pkl\" \\\n",
    "--attention_supervision_loss_weight 3.0 \\\n",
    "--phrase_classifier_loss_weight 2.0 \\\n",
    "--foreground_loss_weight 1.5 \\\n",
    "--mask_exponent 0.4 \\\n",
    "--chest_imagenome_anatlocs_weight 0 \\\n",
    "--mscxr_weight 1.0 \\\n",
    "--vinbig_weight 0 \\\n",
    "--chexlocalize_weight 1.0 \\\n",
    "--freeze_image_encoder \\\n",
    "--no_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7494550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n ../datasets/dataloading_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d1db698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3,\n",
       "       3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3,\n",
       "       2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_balancedly_distributed_class_indices([0, 0, 1, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
