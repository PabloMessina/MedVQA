{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb248563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ../train_fact_embedding.py \\\n",
    "# --pretrained_checkpoint_folder_path \\\n",
    "# \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230924_175553_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "# --epochs 300 \\\n",
    "# --batches_per_epoch 500 \\\n",
    "# --batch_size 45 \\\n",
    "# --num_workers 3 \\\n",
    "# --iters_to_accumulate 8 \\\n",
    "# --optimizer_name \"adamw\" \\\n",
    "# --scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "# --lr 1e-6 \\\n",
    "# --warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "# --triplets_filepath \\\n",
    "# \"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000).pkl\" \\\n",
    "# --triplet_rule_weights \\\n",
    "# \"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 1., 1., 2.]}\" \\\n",
    "# --integrated_facts_metadata_jsonl_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(595880,60579117).improved_comparison(6741113).jsonl\" \\\n",
    "# --paraphrases_jsonl_filepaths \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "# --integrated_chest_imagenome_observations_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_observations(9).pkl\" \\\n",
    "# --integrated_chest_imagenome_anatomical_locations_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_anatomical_locations(5).pkl\" \\\n",
    "# --integrated_nli_jsonl_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "# --gpt4_radnli_labels_jsonl_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "# --n_chest_imagenome_observations 74 \\\n",
    "# --n_chest_imagenome_anatomical_locations 38 \\\n",
    "# --triplets_weight 1.0 \\\n",
    "# --metadata_classification_weight 0 \\\n",
    "# --chest_imagenome_observations_classification_weight 0 \\\n",
    "# --chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "# --nli_weight 0 \\\n",
    "# --entcon_weight 0 \\\n",
    "# --dataset_name \"MIMIC-CXR(triplets-only)\" \\\n",
    "# --huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "# --embedding_size 128 \\\n",
    "# --use_amp \\\n",
    "# --no_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2446644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 25\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   triplet_loss_weight: 2.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.5, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   gpt4_radnli_labels_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\n",
      "   dataset_name: MIMIC-CXR(triplets+entcont)\n",
      "   triplets_weight: 2.0\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 1.0\n",
      "   entcon_weight: 0.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: True\n",
      "  nli_hidden_layer_size: 128\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 2.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.5\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Number of entailment samples: 26442\n",
      "Number of neutral samples: 39817\n",
      "Number of contradiction samples: 95777\n",
      "----\n",
      "Number of RadNLI samples: 480\n",
      "Number of GPT-4 RadNLI labels: 960\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 926\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset and dataloader...\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n",
      "len(_train_dataloaders) = 2\n",
      "len(_train_weights) = 2\n",
      "_train_weights = [2.0, 1.0]\n",
      "len(_val_dataloaders) = 2\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets+entcont)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_015217_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_015217_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_20_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9437.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_20_encc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9437.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mWarning: model state dict has 215 keys, loaded state dict has 211 keys, intersection has 211 keys, union has 215 keys.\u001b[0m\n",
      "\u001b[93mExamples of keys in model but not in loaded state dict:\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.weight\u001b[0m\n",
      "\u001b[93m  nli_hidden_layer.bias\u001b[0m\n",
      "\u001b[93m  nli_classifier.weight\u001b[0m\n",
      "\u001b[93m  nli_classifier.bias\u001b[0m\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_015217_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.92653, triplet_loss 0.84018, nli_loss 1.09924, nli_acc 0.33460, 84.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92700, tacc(al2) 0.86300, tacc(ob0) 0.99400, tacc(ob1) 0.96700, tacc(ob2) 0.94300, tacc(ob3) 0.96900, tacc(ob4) 0.95700, tacc(ob5) 0.78900, tacc(ob6) 0.97200, tacc(ob7) 0.93100, nli_acc 0.31210, 20.77 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8055.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.92447, triplet_loss 0.83901, nli_loss 1.09540, nli_acc 0.38020, 85.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.93000, tacc(al2) 0.87300, tacc(ob0) 0.99500, tacc(ob1) 0.96300, tacc(ob2) 0.94100, tacc(ob3) 0.96400, tacc(ob4) 0.95500, tacc(ob5) 0.83200, tacc(ob6) 0.97100, tacc(ob7) 0.92700, nli_acc 0.35421, 20.88 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8186.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.93059, triplet_loss 0.85660, nli_loss 1.07857, nli_acc 0.45940, 85.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.91400, tacc(al2) 0.88100, tacc(ob0) 0.99100, tacc(ob1) 0.95800, tacc(ob2) 0.94100, tacc(ob3) 0.95800, tacc(ob4) 0.95300, tacc(ob5) 0.84200, tacc(ob6) 0.96300, tacc(ob7) 0.91300, nli_acc 0.42333, 20.47 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_4_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8298.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.91661, triplet_loss 0.85905, nli_loss 1.03172, nli_acc 0.55820, 85.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.91800, tacc(al2) 0.87100, tacc(ob0) 0.98300, tacc(ob1) 0.96600, tacc(ob2) 0.93500, tacc(ob3) 0.96600, tacc(ob4) 0.95400, tacc(ob5) 0.81500, tacc(ob6) 0.96200, tacc(ob7) 0.92100, nli_acc 0.54212, 20.90 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8501.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.89462, triplet_loss 0.84742, nli_loss 0.98901, nli_acc 0.56580, 85.78 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92900, tacc(al2) 0.86600, tacc(ob0) 0.99000, tacc(ob1) 0.96300, tacc(ob2) 0.94200, tacc(ob3) 0.96900, tacc(ob4) 0.95700, tacc(ob5) 0.83700, tacc(ob6) 0.97000, tacc(ob7) 0.92000, nli_acc 0.57775, 20.78 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_6_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8593.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.88354, triplet_loss 0.84549, nli_loss 0.95963, nli_acc 0.59960, 84.73 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92500, tacc(al2) 0.87200, tacc(ob0) 0.99200, tacc(ob1) 0.96300, tacc(ob2) 0.94500, tacc(ob3) 0.96900, tacc(ob4) 0.95200, tacc(ob5) 0.84400, tacc(ob6) 0.97200, tacc(ob7) 0.92400, nli_acc 0.55400, 20.80 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8614.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.87501, triplet_loss 0.84443, nli_loss 0.93617, nli_acc 0.61660, 86.60 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.85600, tacc(ob0) 0.99100, tacc(ob1) 0.96400, tacc(ob2) 0.94400, tacc(ob3) 0.96700, tacc(ob4) 0.94900, tacc(ob5) 0.83500, tacc(ob6) 0.96800, tacc(ob7) 0.92100, nli_acc 0.53564, 20.88 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.87221, triplet_loss 0.84251, nli_loss 0.93161, nli_acc 0.60880, 84.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92500, tacc(al2) 0.85700, tacc(ob0) 0.99200, tacc(ob1) 0.96200, tacc(ob2) 0.94200, tacc(ob3) 0.96600, tacc(ob4) 0.95100, tacc(ob5) 0.83800, tacc(ob6) 0.96800, tacc(ob7) 0.92500, nli_acc 0.54644, 20.69 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.87123, triplet_loss 0.84110, nli_loss 0.93149, nli_acc 0.60460, 84.20 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92800, tacc(al2) 0.86300, tacc(ob0) 0.99100, tacc(ob1) 0.96200, tacc(ob2) 0.94100, tacc(ob3) 0.96900, tacc(ob4) 0.94900, tacc(ob5) 0.83500, tacc(ob6) 0.96700, tacc(ob7) 0.92500, nli_acc 0.53888, 20.90 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.86613, triplet_loss 0.84013, nli_loss 0.91814, nli_acc 0.61820, 85.23 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92900, tacc(al2) 0.86200, tacc(ob0) 0.99100, tacc(ob1) 0.96300, tacc(ob2) 0.94000, tacc(ob3) 0.96700, tacc(ob4) 0.95100, tacc(ob5) 0.83800, tacc(ob6) 0.96800, tacc(ob7) 0.92600, nli_acc 0.54968, 20.91 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8614.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.86584, triplet_loss 0.83596, nli_loss 0.92559, nli_acc 0.60920, 85.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92800, tacc(al2) 0.86200, tacc(ob0) 0.99100, tacc(ob1) 0.96300, tacc(ob2) 0.94100, tacc(ob3) 0.96700, tacc(ob4) 0.95100, tacc(ob5) 0.83700, tacc(ob6) 0.96800, tacc(ob7) 0.92500, nli_acc 0.54536, 20.69 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.87563, triplet_loss 0.85666, nli_loss 0.91356, nli_acc 0.60600, 85.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.91600, tacc(al2) 0.86500, tacc(ob0) 0.98800, tacc(ob1) 0.95000, tacc(ob2) 0.93800, tacc(ob3) 0.96500, tacc(ob4) 0.95500, tacc(ob5) 0.80100, tacc(ob6) 0.94700, tacc(ob7) 0.91700, nli_acc 0.57019, 20.74 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.85671, triplet_loss 0.85417, nli_loss 0.86178, nli_acc 0.63480, 85.59 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.91200, tacc(al2) 0.87500, tacc(ob0) 0.99200, tacc(ob1) 0.96100, tacc(ob2) 0.94500, tacc(ob3) 0.96700, tacc(ob4) 0.95700, tacc(ob5) 0.82300, tacc(ob6) 0.95900, tacc(ob7) 0.92600, nli_acc 0.54752, 20.81 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.83843, triplet_loss 0.85032, nli_loss 0.81466, nli_acc 0.65840, 85.20 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92100, tacc(al2) 0.87400, tacc(ob0) 0.99200, tacc(ob1) 0.96100, tacc(ob2) 0.94800, tacc(ob3) 0.96600, tacc(ob4) 0.95600, tacc(ob5) 0.83000, tacc(ob6) 0.96600, tacc(ob7) 0.92000, nli_acc 0.54104, 20.96 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8634.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.82715, triplet_loss 0.84697, nli_loss 0.78752, nli_acc 0.67080, 84.16 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92200, tacc(al2) 0.85800, tacc(ob0) 0.99300, tacc(ob1) 0.96300, tacc(ob2) 0.94200, tacc(ob3) 0.96700, tacc(ob4) 0.96200, tacc(ob5) 0.83300, tacc(ob6) 0.96700, tacc(ob7) 0.92300, nli_acc 0.57343, 20.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_16_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8680.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92400, tacc(al2) 0.85800, tacc(ob0) 0.99200, tacc(ob1) 0.96100, tacc(ob2) 0.94000, tacc(ob3) 0.96900, tacc(ob4) 0.96000, tacc(ob5) 0.83100, tacc(ob6) 0.96000, tacc(ob7) 0.92100, nli_acc 0.56263, 20.74 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.82416, triplet_loss 0.84271, nli_loss 0.78706, nli_acc 0.67300, 85.27 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92500, tacc(al2) 0.85800, tacc(ob0) 0.99300, tacc(ob1) 0.96200, tacc(ob2) 0.94500, tacc(ob3) 0.97100, tacc(ob4) 0.95800, tacc(ob5) 0.83900, tacc(ob6) 0.96400, tacc(ob7) 0.92100, nli_acc 0.57559, 20.76 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8692.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.81740, triplet_loss 0.84127, nli_loss 0.76966, nli_acc 0.68360, 85.49 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92300, tacc(al2) 0.85600, tacc(ob0) 0.99100, tacc(ob1) 0.96200, tacc(ob2) 0.94200, tacc(ob3) 0.97100, tacc(ob4) 0.95900, tacc(ob5) 0.83400, tacc(ob6) 0.96200, tacc(ob7) 0.92000, nli_acc 0.58315, 20.75 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_19_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8697.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.81946, triplet_loss 0.83798, nli_loss 0.78244, nli_acc 0.67280, 84.53 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92300, tacc(al2) 0.85800, tacc(ob0) 0.99100, tacc(ob1) 0.96100, tacc(ob2) 0.94400, tacc(ob3) 0.97100, tacc(ob4) 0.95900, tacc(ob5) 0.83600, tacc(ob6) 0.96300, tacc(ob7) 0.92300, nli_acc 0.57991, 20.99 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.83563, triplet_loss 0.85747, nli_loss 0.79196, nli_acc 0.65740, 84.58 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92200, tacc(al2) 0.87500, tacc(ob0) 0.98600, tacc(ob1) 0.96500, tacc(ob2) 0.94500, tacc(ob3) 0.95900, tacc(ob4) 0.94500, tacc(ob5) 0.84300, tacc(ob6) 0.96100, tacc(ob7) 0.91600, nli_acc 0.54860, 20.80 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.82498, triplet_loss 0.86042, nli_loss 0.75409, nli_acc 0.67280, 86.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92200, tacc(al2) 0.88800, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.94300, tacc(ob3) 0.97200, tacc(ob4) 0.95700, tacc(ob5) 0.84500, tacc(ob6) 0.96000, tacc(ob7) 0.92700, nli_acc 0.57235, 20.54 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8708.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.80787, triplet_loss 0.85734, nli_loss 0.70893, nli_acc 0.70180, 85.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92600, tacc(al2) 0.88100, tacc(ob0) 0.99000, tacc(ob1) 0.96900, tacc(ob2) 0.94600, tacc(ob3) 0.97100, tacc(ob4) 0.96100, tacc(ob5) 0.84100, tacc(ob6) 0.96100, tacc(ob7) 0.92800, nli_acc 0.56587, 19.74 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8741.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.79473, triplet_loss 0.84835, nli_loss 0.68748, nli_acc 0.70720, 84.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92600, tacc(al2) 0.88100, tacc(ob0) 0.98900, tacc(ob1) 0.97100, tacc(ob2) 0.94300, tacc(ob3) 0.96900, tacc(ob4) 0.95300, tacc(ob5) 0.84100, tacc(ob6) 0.96200, tacc(ob7) 0.93200, nli_acc 0.57775, 20.97 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8759.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.79527, triplet_loss 0.84933, nli_loss 0.68714, nli_acc 0.70600, 83.05 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92600, tacc(al2) 0.87500, tacc(ob0) 0.99000, tacc(ob1) 0.97000, tacc(ob2) 0.94400, tacc(ob3) 0.96700, tacc(ob4) 0.95600, tacc(ob5) 0.83800, tacc(ob6) 0.96300, tacc(ob7) 0.93500, nli_acc 0.54968, 20.79 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.79194, triplet_loss 0.84146, nli_loss 0.69290, nli_acc 0.70300, 85.22 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92500, tacc(al2) 0.87200, tacc(ob0) 0.99000, tacc(ob1) 0.97000, tacc(ob2) 0.93900, tacc(ob3) 0.96900, tacc(ob4) 0.95900, tacc(ob5) 0.83300, tacc(ob6) 0.96000, tacc(ob7) 0.93400, nli_acc 0.55832, 20.87 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.78904, triplet_loss 0.84748, nli_loss 0.67214, nli_acc 0.72320, 85.51 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92600, tacc(al2) 0.87300, tacc(ob0) 0.98900, tacc(ob1) 0.97100, tacc(ob2) 0.93900, tacc(ob3) 0.96900, tacc(ob4) 0.95700, tacc(ob5) 0.83300, tacc(ob6) 0.96100, tacc(ob7) 0.93400, nli_acc 0.56048, 20.72 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.78754, triplet_loss 0.83889, nli_loss 0.68484, nli_acc 0.71260, 85.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92700, tacc(al2) 0.87300, tacc(ob0) 0.99000, tacc(ob1) 0.97100, tacc(ob2) 0.93800, tacc(ob3) 0.96900, tacc(ob4) 0.95700, tacc(ob5) 0.83200, tacc(ob6) 0.96200, tacc(ob7) 0.93400, nli_acc 0.55940, 20.83 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.81044, triplet_loss 0.85822, nli_loss 0.71486, nli_acc 0.69380, 85.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.91800, tacc(al2) 0.86500, tacc(ob0) 0.98700, tacc(ob1) 0.95600, tacc(ob2) 0.92300, tacc(ob3) 0.96400, tacc(ob4) 0.94100, tacc(ob5) 0.82600, tacc(ob6) 0.95400, tacc(ob7) 0.91800, nli_acc 0.52592, 20.85 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.80802, triplet_loss 0.86082, nli_loss 0.70243, nli_acc 0.68860, 83.58 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.91700, tacc(al2) 0.85700, tacc(ob0) 0.98900, tacc(ob1) 0.96300, tacc(ob2) 0.94500, tacc(ob3) 0.96100, tacc(ob4) 0.94700, tacc(ob5) 0.84600, tacc(ob6) 0.96400, tacc(ob7) 0.92300, nli_acc 0.55616, 20.96 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.78679, triplet_loss 0.85103, nli_loss 0.65832, nli_acc 0.71600, 84.69 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92700, tacc(al2) 0.86900, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.94300, tacc(ob3) 0.96500, tacc(ob4) 0.94900, tacc(ob5) 0.82800, tacc(ob6) 0.96000, tacc(ob7) 0.92400, nli_acc 0.55292, 20.70 secs\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.77839, triplet_loss 0.85103, nli_loss 0.63312, nli_acc 0.73760, 86.53 secs\n",
      "(2) Validation stage ...\n",
      "^C iteration 125\n",
      "Engine run is terminating due to exception: \n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 676, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 587, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 364, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 965, in _internal_run_as_gen\n",
      "    self._fire_event(Events.EPOCH_COMPLETED)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 425, in _fire_event\n",
      "    func(*first, *(event_args + others), **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 108, in <lambda>\n",
      "    trainer_engine.add_event_handler(Events.EPOCH_COMPLETED, lambda : validator_engine.run(val_dataloader,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 302, in step_fn_wrapper\n",
      "    output = step_fn__triplet_ranking(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 58, in step_fn__triplet_ranking\n",
      "    n_embeddings = model(input_ids=n_input_ids, attention_mask=n_attention_mask)['text_embeddings']\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/fact_encoder.py\", line 124, in forward\n",
      "    text_embeddings = self.model.get_projected_text_embeddings(input_ids=input_ids, attention_mask=attention_mask)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1358, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1020, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 610, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 537, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/pytorch_utils.py\", line 248, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 550, in feed_forward_chunk\n",
      "    layer_output = self.output(intermediate_output, attention_output)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 464, in forward\n",
      "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/normalization.py\", line 190, in forward\n",
      "    return F.layer_norm(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/functional.py\", line 2515, in layer_norm\n",
      "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_225344_MIMIC-CXR(triplets+classif)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 25 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 2., 1.5, 2.]}\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--gpt4_radnli_labels_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "--triplets_weight 2.0 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 1.0 \\\n",
    "--entcon_weight 0 \\\n",
    "--triplet_loss_weight 2.0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets+nli)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--nli_hidden_layer_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4967b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 100\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 25\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_015217_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 5\n",
      "   override_lr: False\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.5\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.5, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   gpt4_radnli_labels_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\n",
      "   dataset_name: MIMIC-CXR(triplets+nli)\n",
      "   triplets_weight: 1.0\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 1.0\n",
      "   entcon_weight: 0.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: True\n",
      "  nli_hidden_layer_size: 128\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_015217_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 5\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 2.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.5\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Number of entailment samples: 26442\n",
      "Number of neutral samples: 39817\n",
      "Number of contradiction samples: 95777\n",
      "----\n",
      "Number of RadNLI samples: 480\n",
      "Number of GPT-4 RadNLI labels: 960\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 926\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset and dataloader...\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n",
      "len(_train_dataloaders) = 2\n",
      "len(_train_weights) = 2\n",
      "_train_weights = [1.0, 1.0]\n",
      "len(_val_dataloaders) = 2\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets+nli)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_025244_MIMIC-CXR(triplets+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_025244_MIMIC-CXR(triplets+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_24_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8759.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_015217_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_24_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8759.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_025244_MIMIC-CXR(triplets+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.72379, triplet_loss 0.42291, nli_loss 1.02467, nli_acc 0.71427, 78.50 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.92700, tacc(al2) 0.88000, tacc(ob0) 0.98900, tacc(ob1) 0.97100, tacc(ob2) 0.94100, tacc(ob3) 0.97000, tacc(ob4) 0.95800, tacc(ob5) 0.83600, tacc(ob6) 0.96200, tacc(ob7) 0.93000, nli_acc 0.58963, 20.55 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8773.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.72624, triplet_loss 0.42323, nli_loss 1.02926, nli_acc 0.71160, 66.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92700, tacc(al2) 0.87400, tacc(ob0) 0.98900, tacc(ob1) 0.97100, tacc(ob2) 0.93800, tacc(ob3) 0.96600, tacc(ob4) 0.95700, tacc(ob5) 0.82900, tacc(ob6) 0.95900, tacc(ob7) 0.92800, nli_acc 0.56479, 20.53 secs\n",
      "\u001b[1m---- Epoch 3/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.71725, triplet_loss 0.42327, nli_loss 1.01123, nli_acc 0.72040, 80.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92900, tacc(al2) 0.87000, tacc(ob0) 0.98700, tacc(ob1) 0.96600, tacc(ob2) 0.94000, tacc(ob3) 0.96800, tacc(ob4) 0.95700, tacc(ob5) 0.82700, tacc(ob6) 0.95600, tacc(ob7) 0.92100, nli_acc 0.55832, 20.74 secs\n",
      "\u001b[1m---- Epoch 4/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.74040, triplet_loss 0.43616, nli_loss 1.04465, nli_acc 0.70040, 81.31 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92700, tacc(al2) 0.87000, tacc(ob0) 0.98500, tacc(ob1) 0.95400, tacc(ob2) 0.93500, tacc(ob3) 0.96000, tacc(ob4) 0.95400, tacc(ob5) 0.78600, tacc(ob6) 0.93600, tacc(ob7) 0.91400, nli_acc 0.55400, 20.86 secs\n",
      "\u001b[1m---- Epoch 5/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.70369, triplet_loss 0.43210, nli_loss 0.97528, nli_acc 0.72213, 81.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92400, tacc(al2) 0.88200, tacc(ob0) 0.98600, tacc(ob1) 0.96100, tacc(ob2) 0.93900, tacc(ob3) 0.96600, tacc(ob4) 0.94200, tacc(ob5) 0.79400, tacc(ob6) 0.95000, tacc(ob7) 0.91800, nli_acc 0.57019, 20.81 secs\n",
      "\u001b[1m---- Epoch 6/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.68768, triplet_loss 0.43275, nli_loss 0.94262, nli_acc 0.72813, 82.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.91900, tacc(al2) 0.87300, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.93500, tacc(ob3) 0.96100, tacc(ob4) 0.94900, tacc(ob5) 0.78300, tacc(ob6) 0.94800, tacc(ob7) 0.90700, nli_acc 0.53996, 20.45 secs\n",
      "\u001b[1m---- Epoch 7/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n",
      "loss 0.66635, triplet_loss 0.42982, nli_loss 0.90288, nli_acc 0.74440, 81.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92400, tacc(al2) 0.86700, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.93900, tacc(ob3) 0.96900, tacc(ob4) 0.95400, tacc(ob5) 0.80500, tacc(ob6) 0.95300, tacc(ob7) 0.91500, nli_acc 0.55940, 20.92 secs\n",
      "\u001b[1m---- Epoch 8/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.66006, triplet_loss 0.42875, nli_loss 0.89138, nli_acc 0.73920, 81.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92300, tacc(al2) 0.86900, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.93800, tacc(ob3) 0.96700, tacc(ob4) 0.95200, tacc(ob5) 0.79400, tacc(ob6) 0.94700, tacc(ob7) 0.91800, nli_acc 0.54968, 20.92 secs\n",
      "\u001b[1m---- Epoch 9/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.65425, triplet_loss 0.42882, nli_loss 0.87968, nli_acc 0.75387, 80.91 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92400, tacc(al2) 0.86300, tacc(ob0) 0.99000, tacc(ob1) 0.96400, tacc(ob2) 0.93900, tacc(ob3) 0.96800, tacc(ob4) 0.95400, tacc(ob5) 0.80100, tacc(ob6) 0.95200, tacc(ob7) 0.91300, nli_acc 0.55940, 20.89 secs\n",
      "\u001b[1m---- Epoch 10/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.65843, triplet_loss 0.42788, nli_loss 0.88897, nli_acc 0.74453, 75.09 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92300, tacc(al2) 0.86300, tacc(ob0) 0.99000, tacc(ob1) 0.96400, tacc(ob2) 0.94000, tacc(ob3) 0.96900, tacc(ob4) 0.95500, tacc(ob5) 0.80200, tacc(ob6) 0.95200, tacc(ob7) 0.91500, nli_acc 0.56156, 20.74 secs\n",
      "\u001b[1m---- Epoch 11/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.65512, triplet_loss 0.43140, nli_loss 0.87885, nli_acc 0.74693, 81.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92400, tacc(al2) 0.86100, tacc(ob0) 0.98900, tacc(ob1) 0.96400, tacc(ob2) 0.94100, tacc(ob3) 0.97000, tacc(ob4) 0.95400, tacc(ob5) 0.80000, tacc(ob6) 0.95200, tacc(ob7) 0.91500, nli_acc 0.55832, 21.02 secs\n",
      "\u001b[1m---- Epoch 12/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.65415, triplet_loss 0.42866, nli_loss 0.87964, nli_acc 0.75453, 80.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92400, tacc(al2) 0.86200, tacc(ob0) 0.98900, tacc(ob1) 0.96400, tacc(ob2) 0.93900, tacc(ob3) 0.97000, tacc(ob4) 0.95300, tacc(ob5) 0.79900, tacc(ob6) 0.95200, tacc(ob7) 0.91500, nli_acc 0.55940, 20.93 secs\n",
      "\u001b[1m---- Epoch 13/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.69400, triplet_loss 0.43794, nli_loss 0.95007, nli_acc 0.72560, 81.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93000, tacc(al2) 0.85600, tacc(ob0) 0.97900, tacc(ob1) 0.95500, tacc(ob2) 0.93200, tacc(ob3) 0.96500, tacc(ob4) 0.94900, tacc(ob5) 0.78700, tacc(ob6) 0.93600, tacc(ob7) 0.90400, nli_acc 0.54320, 20.96 secs\n",
      "\u001b[1m---- Epoch 14/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.66184, triplet_loss 0.43340, nli_loss 0.89029, nli_acc 0.74253, 81.37 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.93100, tacc(al2) 0.86900, tacc(ob0) 0.98400, tacc(ob1) 0.95600, tacc(ob2) 0.93600, tacc(ob3) 0.96600, tacc(ob4) 0.93100, tacc(ob5) 0.79100, tacc(ob6) 0.95300, tacc(ob7) 0.92400, nli_acc 0.52700, 20.85 secs\n",
      "\u001b[1m---- Epoch 15/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.64273, triplet_loss 0.43234, nli_loss 0.85312, nli_acc 0.76160, 81.24 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98300, tacc(al1) 0.93100, tacc(al2) 0.87600, tacc(ob0) 0.98500, tacc(ob1) 0.96600, tacc(ob2) 0.93700, tacc(ob3) 0.97100, tacc(ob4) 0.94400, tacc(ob5) 0.80100, tacc(ob6) 0.95200, tacc(ob7) 0.91700, nli_acc 0.58099, 20.91 secs\n",
      "\u001b[1m---- Epoch 16/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.62684, triplet_loss 0.43341, nli_loss 0.82027, nli_acc 0.77000, 80.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92900, tacc(al2) 0.87900, tacc(ob0) 0.98600, tacc(ob1) 0.96700, tacc(ob2) 0.93900, tacc(ob3) 0.97000, tacc(ob4) 0.94000, tacc(ob5) 0.80000, tacc(ob6) 0.95200, tacc(ob7) 0.91500, nli_acc 0.58099, 20.71 secs\n",
      "\u001b[1m---- Epoch 17/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.62187, triplet_loss 0.43043, nli_loss 0.81330, nli_acc 0.77120, 81.02 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93200, tacc(al2) 0.87900, tacc(ob0) 0.98600, tacc(ob1) 0.96700, tacc(ob2) 0.94000, tacc(ob3) 0.96800, tacc(ob4) 0.94300, tacc(ob5) 0.80300, tacc(ob6) 0.95600, tacc(ob7) 0.91600, nli_acc 0.59287, 21.01 secs\n",
      "\u001b[1m---- Epoch 18/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.61851, triplet_loss 0.42624, nli_loss 0.81078, nli_acc 0.77027, 81.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93200, tacc(al2) 0.87900, tacc(ob0) 0.98600, tacc(ob1) 0.96600, tacc(ob2) 0.94100, tacc(ob3) 0.96700, tacc(ob4) 0.94400, tacc(ob5) 0.80100, tacc(ob6) 0.95200, tacc(ob7) 0.91300, nli_acc 0.58423, 21.02 secs\n",
      "\u001b[1m---- Epoch 19/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.61302, triplet_loss 0.43084, nli_loss 0.79521, nli_acc 0.77187, 81.58 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93200, tacc(al2) 0.88100, tacc(ob0) 0.98700, tacc(ob1) 0.96600, tacc(ob2) 0.94000, tacc(ob3) 0.96800, tacc(ob4) 0.94500, tacc(ob5) 0.80400, tacc(ob6) 0.95300, tacc(ob7) 0.91600, nli_acc 0.57667, 20.93 secs\n",
      "\u001b[1m---- Epoch 20/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.61403, triplet_loss 0.43243, nli_loss 0.79563, nli_acc 0.77853, 81.60 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98200, tacc(al1) 0.93300, tacc(al2) 0.88000, tacc(ob0) 0.98600, tacc(ob1) 0.96700, tacc(ob2) 0.93900, tacc(ob3) 0.96900, tacc(ob4) 0.94400, tacc(ob5) 0.80300, tacc(ob6) 0.95200, tacc(ob7) 0.91600, nli_acc 0.58639, 20.86 secs\n",
      "\u001b[1m---- Epoch 21/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.65522, triplet_loss 0.43418, nli_loss 0.87625, nli_acc 0.75093, 80.35 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98000, tacc(al1) 0.92200, tacc(al2) 0.87900, tacc(ob0) 0.98200, tacc(ob1) 0.95600, tacc(ob2) 0.93600, tacc(ob3) 0.96400, tacc(ob4) 0.94000, tacc(ob5) 0.78500, tacc(ob6) 0.95300, tacc(ob7) 0.90900, nli_acc 0.58099, 20.94 secs\n",
      "\u001b[1m---- Epoch 22/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.63204, triplet_loss 0.43704, nli_loss 0.82704, nli_acc 0.76360, 82.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92100, tacc(al2) 0.86600, tacc(ob0) 0.98800, tacc(ob1) 0.95900, tacc(ob2) 0.94800, tacc(ob3) 0.96200, tacc(ob4) 0.94600, tacc(ob5) 0.79500, tacc(ob6) 0.95300, tacc(ob7) 0.91200, nli_acc 0.56695, 20.99 secs\n",
      "\u001b[1m---- Epoch 23/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.61772, triplet_loss 0.43386, nli_loss 0.80157, nli_acc 0.77067, 65.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92100, tacc(al2) 0.86300, tacc(ob0) 0.98600, tacc(ob1) 0.96100, tacc(ob2) 0.94100, tacc(ob3) 0.96700, tacc(ob4) 0.94000, tacc(ob5) 0.78700, tacc(ob6) 0.94400, tacc(ob7) 0.91100, nli_acc 0.58963, 20.87 secs\n",
      "\u001b[1m---- Epoch 24/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.59157, triplet_loss 0.42974, nli_loss 0.75340, nli_acc 0.79227, 82.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.91900, tacc(al2) 0.86800, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.95000, tacc(ob3) 0.96700, tacc(ob4) 0.94600, tacc(ob5) 0.79600, tacc(ob6) 0.94900, tacc(ob7) 0.91100, nli_acc 0.59827, 20.83 secs\n",
      "\u001b[1m---- Epoch 25/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.60122, triplet_loss 0.42921, nli_loss 0.77324, nli_acc 0.78027, 81.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92200, tacc(al2) 0.86400, tacc(ob0) 0.98900, tacc(ob1) 0.96000, tacc(ob2) 0.94300, tacc(ob3) 0.96600, tacc(ob4) 0.94700, tacc(ob5) 0.79300, tacc(ob6) 0.94300, tacc(ob7) 0.91400, nli_acc 0.59071, 19.85 secs\n",
      "\u001b[1m---- Epoch 26/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.60362, triplet_loss 0.43641, nli_loss 0.77084, nli_acc 0.78067, 80.66 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92200, tacc(al2) 0.87000, tacc(ob0) 0.98900, tacc(ob1) 0.96200, tacc(ob2) 0.94300, tacc(ob3) 0.96600, tacc(ob4) 0.94700, tacc(ob5) 0.79400, tacc(ob6) 0.94400, tacc(ob7) 0.91600, nli_acc 0.58747, 20.90 secs\n",
      "\u001b[1m---- Epoch 27/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.60201, triplet_loss 0.43214, nli_loss 0.77188, nli_acc 0.78480, 82.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92300, tacc(al2) 0.87000, tacc(ob0) 0.98900, tacc(ob1) 0.96300, tacc(ob2) 0.94200, tacc(ob3) 0.96700, tacc(ob4) 0.94800, tacc(ob5) 0.79700, tacc(ob6) 0.94500, tacc(ob7) 0.91600, nli_acc 0.59179, 20.91 secs\n",
      "\u001b[1m---- Epoch 28/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.59397, triplet_loss 0.42959, nli_loss 0.75836, nli_acc 0.79307, 81.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92300, tacc(al2) 0.86900, tacc(ob0) 0.98900, tacc(ob1) 0.96400, tacc(ob2) 0.94300, tacc(ob3) 0.96600, tacc(ob4) 0.94600, tacc(ob5) 0.79400, tacc(ob6) 0.94600, tacc(ob7) 0.91600, nli_acc 0.58531, 20.82 secs\n",
      "\u001b[1m---- Epoch 29/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.63109, triplet_loss 0.43512, nli_loss 0.82706, nli_acc 0.76613, 80.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.91100, tacc(al2) 0.87400, tacc(ob0) 0.98000, tacc(ob1) 0.94000, tacc(ob2) 0.94200, tacc(ob3) 0.95800, tacc(ob4) 0.94400, tacc(ob5) 0.78500, tacc(ob6) 0.94400, tacc(ob7) 0.89900, nli_acc 0.53132, 20.74 secs\n",
      "\u001b[1m---- Epoch 30/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.61472, triplet_loss 0.43756, nli_loss 0.79187, nli_acc 0.77253, 79.69 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92200, tacc(al2) 0.87000, tacc(ob0) 0.98600, tacc(ob1) 0.95600, tacc(ob2) 0.93600, tacc(ob3) 0.96500, tacc(ob4) 0.93600, tacc(ob5) 0.79000, tacc(ob6) 0.95500, tacc(ob7) 0.91400, nli_acc 0.57343, 21.02 secs\n",
      "\u001b[1m---- Epoch 31/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.60186, triplet_loss 0.43405, nli_loss 0.76968, nli_acc 0.77973, 82.01 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.91900, tacc(al2) 0.87200, tacc(ob0) 0.98100, tacc(ob1) 0.96300, tacc(ob2) 0.94800, tacc(ob3) 0.96100, tacc(ob4) 0.94200, tacc(ob5) 0.79400, tacc(ob6) 0.95200, tacc(ob7) 0.91000, nli_acc 0.59611, 20.98 secs\n",
      "\u001b[1m---- Epoch 32/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.58592, triplet_loss 0.43339, nli_loss 0.73845, nli_acc 0.78933, 65.31 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.91700, tacc(al2) 0.86600, tacc(ob0) 0.98400, tacc(ob1) 0.96200, tacc(ob2) 0.94700, tacc(ob3) 0.96100, tacc(ob4) 0.94100, tacc(ob5) 0.80300, tacc(ob6) 0.95400, tacc(ob7) 0.91700, nli_acc 0.60043, 20.37 secs\n",
      "\u001b[1m---- Epoch 33/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.58544, triplet_loss 0.43263, nli_loss 0.73825, nli_acc 0.79613, 80.09 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.91700, tacc(al2) 0.86100, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.94400, tacc(ob3) 0.96300, tacc(ob4) 0.94500, tacc(ob5) 0.79500, tacc(ob6) 0.95000, tacc(ob7) 0.91400, nli_acc 0.60259, 20.99 secs\n",
      "\u001b[1m---- Epoch 34/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.57670, triplet_loss 0.43269, nli_loss 0.72071, nli_acc 0.80227, 81.04 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.91800, tacc(al2) 0.86200, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.94800, tacc(ob3) 0.96100, tacc(ob4) 0.94500, tacc(ob5) 0.79400, tacc(ob6) 0.94800, tacc(ob7) 0.91000, nli_acc 0.60367, 21.03 secs\n",
      "\u001b[1m---- Epoch 35/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.58401, triplet_loss 0.43284, nli_loss 0.73518, nli_acc 0.79627, 81.53 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92000, tacc(al2) 0.86200, tacc(ob0) 0.98500, tacc(ob1) 0.96100, tacc(ob2) 0.94800, tacc(ob3) 0.96100, tacc(ob4) 0.94400, tacc(ob5) 0.79500, tacc(ob6) 0.94700, tacc(ob7) 0.91100, nli_acc 0.60151, 20.72 secs\n",
      "\u001b[1m---- Epoch 36/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.56841, triplet_loss 0.43103, nli_loss 0.70580, nli_acc 0.80347, 78.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92000, tacc(al2) 0.86400, tacc(ob0) 0.98300, tacc(ob1) 0.96000, tacc(ob2) 0.94800, tacc(ob3) 0.96000, tacc(ob4) 0.94400, tacc(ob5) 0.79400, tacc(ob6) 0.94700, tacc(ob7) 0.91000, nli_acc 0.60475, 20.79 secs\n",
      "\u001b[1m---- Epoch 37/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.60148, triplet_loss 0.43878, nli_loss 0.76418, nli_acc 0.78227, 81.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92300, tacc(al2) 0.86400, tacc(ob0) 0.98200, tacc(ob1) 0.95200, tacc(ob2) 0.93600, tacc(ob3) 0.95900, tacc(ob4) 0.94700, tacc(ob5) 0.79700, tacc(ob6) 0.95200, tacc(ob7) 0.89000, nli_acc 0.59827, 20.54 secs\n",
      "\u001b[1m---- Epoch 38/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.59905, triplet_loss 0.43690, nli_loss 0.76120, nli_acc 0.78893, 78.49 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.93300, tacc(al2) 0.85400, tacc(ob0) 0.98600, tacc(ob1) 0.96400, tacc(ob2) 0.93000, tacc(ob3) 0.95700, tacc(ob4) 0.94300, tacc(ob5) 0.79800, tacc(ob6) 0.95000, tacc(ob7) 0.90100, nli_acc 0.59611, 20.82 secs\n",
      "\u001b[1m---- Epoch 39/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.58483, triplet_loss 0.43547, nli_loss 0.73419, nli_acc 0.78947, 79.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92500, tacc(al2) 0.85700, tacc(ob0) 0.98300, tacc(ob1) 0.96300, tacc(ob2) 0.93900, tacc(ob3) 0.96300, tacc(ob4) 0.94300, tacc(ob5) 0.79400, tacc(ob6) 0.96500, tacc(ob7) 0.91300, nli_acc 0.60043, 20.49 secs\n",
      "\u001b[1m---- Epoch 40/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.56772, triplet_loss 0.43113, nli_loss 0.70432, nli_acc 0.80680, 78.30 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92400, tacc(al2) 0.86200, tacc(ob0) 0.98700, tacc(ob1) 0.95700, tacc(ob2) 0.93500, tacc(ob3) 0.96300, tacc(ob4) 0.93900, tacc(ob5) 0.78900, tacc(ob6) 0.95800, tacc(ob7) 0.91100, nli_acc 0.61231, 20.55 secs\n",
      "\u001b[1m---- Epoch 41/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.57345, triplet_loss 0.43049, nli_loss 0.71641, nli_acc 0.79760, 78.49 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92700, tacc(al2) 0.86700, tacc(ob0) 0.98800, tacc(ob1) 0.96300, tacc(ob2) 0.93600, tacc(ob3) 0.96400, tacc(ob4) 0.94400, tacc(ob5) 0.79600, tacc(ob6) 0.96400, tacc(ob7) 0.91600, nli_acc 0.60799, 20.72 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_41_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8788.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 42/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.56751, triplet_loss 0.43019, nli_loss 0.70484, nli_acc 0.80053, 77.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92300, tacc(al2) 0.86000, tacc(ob0) 0.98700, tacc(ob1) 0.96100, tacc(ob2) 0.93500, tacc(ob3) 0.96600, tacc(ob4) 0.94600, tacc(ob5) 0.79700, tacc(ob6) 0.96100, tacc(ob7) 0.92000, nli_acc 0.61771, 20.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_42_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8796.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 43/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.56359, triplet_loss 0.42902, nli_loss 0.69816, nli_acc 0.80640, 79.06 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92400, tacc(al2) 0.86500, tacc(ob0) 0.98700, tacc(ob1) 0.96200, tacc(ob2) 0.93700, tacc(ob3) 0.96600, tacc(ob4) 0.94600, tacc(ob5) 0.79400, tacc(ob6) 0.95800, tacc(ob7) 0.91500, nli_acc 0.61123, 20.47 secs\n",
      "\u001b[1m---- Epoch 44/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.57183, triplet_loss 0.43497, nli_loss 0.70868, nli_acc 0.80213, 78.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92400, tacc(al2) 0.86400, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.93700, tacc(ob3) 0.96600, tacc(ob4) 0.94700, tacc(ob5) 0.79500, tacc(ob6) 0.95900, tacc(ob7) 0.91500, nli_acc 0.60799, 20.52 secs\n",
      "\u001b[1m---- Epoch 45/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.58648, triplet_loss 0.43889, nli_loss 0.73407, nli_acc 0.79573, 62.99 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92000, tacc(al2) 0.86600, tacc(ob0) 0.98200, tacc(ob1) 0.95800, tacc(ob2) 0.93900, tacc(ob3) 0.95100, tacc(ob4) 0.92500, tacc(ob5) 0.79500, tacc(ob6) 0.95100, tacc(ob7) 0.90100, nli_acc 0.56911, 20.56 secs\n",
      "\u001b[1m---- Epoch 46/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.59665, triplet_loss 0.43866, nli_loss 0.75463, nli_acc 0.78960, 78.05 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92700, tacc(al2) 0.87000, tacc(ob0) 0.98900, tacc(ob1) 0.96300, tacc(ob2) 0.92800, tacc(ob3) 0.96200, tacc(ob4) 0.94200, tacc(ob5) 0.80400, tacc(ob6) 0.95500, tacc(ob7) 0.91000, nli_acc 0.62743, 20.57 secs\n",
      "\u001b[1m---- Epoch 47/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.57184, triplet_loss 0.43196, nli_loss 0.71173, nli_acc 0.79640, 77.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92100, tacc(al2) 0.88000, tacc(ob0) 0.98900, tacc(ob1) 0.96200, tacc(ob2) 0.94600, tacc(ob3) 0.96600, tacc(ob4) 0.94200, tacc(ob5) 0.80800, tacc(ob6) 0.96000, tacc(ob7) 0.91200, nli_acc 0.62419, 20.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_47_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8817.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 48/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.55409, triplet_loss 0.43787, nli_loss 0.67030, nli_acc 0.81960, 78.83 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92400, tacc(al2) 0.88100, tacc(ob0) 0.99100, tacc(ob1) 0.96600, tacc(ob2) 0.94500, tacc(ob3) 0.96400, tacc(ob4) 0.94400, tacc(ob5) 0.80700, tacc(ob6) 0.95800, tacc(ob7) 0.91400, nli_acc 0.61879, 20.19 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_48_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8844.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 49/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.56196, triplet_loss 0.43230, nli_loss 0.69163, nli_acc 0.80933, 79.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98000, tacc(al1) 0.92700, tacc(al2) 0.87900, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.94200, tacc(ob3) 0.96500, tacc(ob4) 0.94600, tacc(ob5) 0.80900, tacc(ob6) 0.95500, tacc(ob7) 0.91300, nli_acc 0.62635, 20.71 secs\n",
      "\u001b[1m---- Epoch 50/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.56008, triplet_loss 0.43333, nli_loss 0.68682, nli_acc 0.81133, 79.72 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92800, tacc(al2) 0.88100, tacc(ob0) 0.99000, tacc(ob1) 0.96500, tacc(ob2) 0.94000, tacc(ob3) 0.96500, tacc(ob4) 0.94600, tacc(ob5) 0.80000, tacc(ob6) 0.95500, tacc(ob7) 0.91200, nli_acc 0.63283, 20.61 secs\n",
      "\u001b[1m---- Epoch 51/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.55533, triplet_loss 0.43174, nli_loss 0.67891, nli_acc 0.80800, 79.27 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92900, tacc(al2) 0.87900, tacc(ob0) 0.98900, tacc(ob1) 0.96500, tacc(ob2) 0.94100, tacc(ob3) 0.96600, tacc(ob4) 0.94600, tacc(ob5) 0.80300, tacc(ob6) 0.95500, tacc(ob7) 0.91200, nli_acc 0.63067, 20.65 secs\n",
      "\u001b[1m---- Epoch 52/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.54221, triplet_loss 0.43197, nli_loss 0.65244, nli_acc 0.82533, 72.40 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92800, tacc(al2) 0.87900, tacc(ob0) 0.98900, tacc(ob1) 0.96400, tacc(ob2) 0.94200, tacc(ob3) 0.96600, tacc(ob4) 0.94600, tacc(ob5) 0.80300, tacc(ob6) 0.95500, tacc(ob7) 0.91400, nli_acc 0.62959, 16.92 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_52_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8854.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 53/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.57172, triplet_loss 0.43625, nli_loss 0.70719, nli_acc 0.80440, 77.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92100, tacc(al2) 0.87000, tacc(ob0) 0.98100, tacc(ob1) 0.95700, tacc(ob2) 0.93000, tacc(ob3) 0.96200, tacc(ob4) 0.94500, tacc(ob5) 0.77800, tacc(ob6) 0.93600, tacc(ob7) 0.90700, nli_acc 0.59395, 20.59 secs\n",
      "\u001b[1m---- Epoch 54/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.57540, triplet_loss 0.43687, nli_loss 0.71393, nli_acc 0.80053, 79.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92800, tacc(al2) 0.87700, tacc(ob0) 0.98600, tacc(ob1) 0.96300, tacc(ob2) 0.94300, tacc(ob3) 0.95900, tacc(ob4) 0.95100, tacc(ob5) 0.80800, tacc(ob6) 0.95700, tacc(ob7) 0.90100, nli_acc 0.59395, 20.60 secs\n",
      "\u001b[1m---- Epoch 55/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.56282, triplet_loss 0.43122, nli_loss 0.69443, nli_acc 0.81000, 80.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.91900, tacc(al2) 0.87700, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.94800, tacc(ob3) 0.96200, tacc(ob4) 0.94100, tacc(ob5) 0.80700, tacc(ob6) 0.96000, tacc(ob7) 0.90100, nli_acc 0.64363, 20.89 secs\n",
      "\u001b[1m---- Epoch 56/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.54629, triplet_loss 0.43434, nli_loss 0.65824, nli_acc 0.81787, 79.40 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92300, tacc(al2) 0.87300, tacc(ob0) 0.98500, tacc(ob1) 0.96500, tacc(ob2) 0.94700, tacc(ob3) 0.96200, tacc(ob4) 0.94600, tacc(ob5) 0.81000, tacc(ob6) 0.95000, tacc(ob7) 0.90500, nli_acc 0.64363, 20.84 secs\n",
      "\u001b[1m---- Epoch 57/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54289, triplet_loss 0.43324, nli_loss 0.65255, nli_acc 0.81640, 79.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92400, tacc(al2) 0.87700, tacc(ob0) 0.98900, tacc(ob1) 0.96600, tacc(ob2) 0.95300, tacc(ob3) 0.96500, tacc(ob4) 0.94500, tacc(ob5) 0.81500, tacc(ob6) 0.95700, tacc(ob7) 0.90300, nli_acc 0.63283, 20.56 secs\n",
      "\u001b[1m---- Epoch 58/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.54152, triplet_loss 0.42729, nli_loss 0.65575, nli_acc 0.81840, 78.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92200, tacc(al2) 0.87700, tacc(ob0) 0.98900, tacc(ob1) 0.96800, tacc(ob2) 0.95100, tacc(ob3) 0.96500, tacc(ob4) 0.94600, tacc(ob5) 0.81200, tacc(ob6) 0.95600, tacc(ob7) 0.90400, nli_acc 0.63607, 20.71 secs\n",
      "\u001b[1m---- Epoch 59/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.54908, triplet_loss 0.43463, nli_loss 0.66354, nli_acc 0.81440, 78.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92200, tacc(al2) 0.87400, tacc(ob0) 0.98800, tacc(ob1) 0.96700, tacc(ob2) 0.95000, tacc(ob3) 0.96700, tacc(ob4) 0.94600, tacc(ob5) 0.80900, tacc(ob6) 0.95500, tacc(ob7) 0.90400, nli_acc 0.62743, 20.58 secs\n",
      "\u001b[1m---- Epoch 60/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.54635, triplet_loss 0.43170, nli_loss 0.66100, nli_acc 0.81613, 79.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92200, tacc(al2) 0.87600, tacc(ob0) 0.98800, tacc(ob1) 0.96700, tacc(ob2) 0.95000, tacc(ob3) 0.96700, tacc(ob4) 0.94700, tacc(ob5) 0.80800, tacc(ob6) 0.95500, tacc(ob7) 0.90400, nli_acc 0.63283, 20.64 secs\n",
      "\u001b[1m---- Epoch 61/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.55932, triplet_loss 0.44299, nli_loss 0.67566, nli_acc 0.81213, 76.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92400, tacc(al2) 0.87800, tacc(ob0) 0.98000, tacc(ob1) 0.95100, tacc(ob2) 0.93600, tacc(ob3) 0.95800, tacc(ob4) 0.94100, tacc(ob5) 0.79800, tacc(ob6) 0.93300, tacc(ob7) 0.90900, nli_acc 0.63067, 20.63 secs\n",
      "\u001b[1m---- Epoch 62/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.56535, triplet_loss 0.43841, nli_loss 0.69230, nli_acc 0.80187, 78.80 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.91400, tacc(al2) 0.88200, tacc(ob0) 0.98800, tacc(ob1) 0.96000, tacc(ob2) 0.93900, tacc(ob3) 0.96300, tacc(ob4) 0.94300, tacc(ob5) 0.80600, tacc(ob6) 0.94800, tacc(ob7) 0.91300, nli_acc 0.65659, 20.71 secs\n",
      "\u001b[1m---- Epoch 63/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.55973, triplet_loss 0.43769, nli_loss 0.68177, nli_acc 0.80840, 78.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92400, tacc(al2) 0.86700, tacc(ob0) 0.98500, tacc(ob1) 0.96900, tacc(ob2) 0.93700, tacc(ob3) 0.96700, tacc(ob4) 0.94100, tacc(ob5) 0.80200, tacc(ob6) 0.94900, tacc(ob7) 0.91400, nli_acc 0.61987, 20.54 secs\n",
      "\u001b[1m---- Epoch 64/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.53968, triplet_loss 0.43473, nli_loss 0.64463, nli_acc 0.82440, 79.13 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92900, tacc(al2) 0.86800, tacc(ob0) 0.98800, tacc(ob1) 0.96300, tacc(ob2) 0.94600, tacc(ob3) 0.96700, tacc(ob4) 0.94400, tacc(ob5) 0.81100, tacc(ob6) 0.94900, tacc(ob7) 0.91900, nli_acc 0.65011, 20.58 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_64_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8877.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 65/100\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "^C iteration 38475\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 676, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 587, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 364, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 302, in step_fn_wrapper\n",
      "    output = step_fn__triplet_ranking(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 72, in step_fn__triplet_ranking\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_015217_MIMIC-CXR(triplets+entcont)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 100 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 25 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 5 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 2., 1.5, 2.]}\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--gpt4_radnli_labels_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "--triplets_weight 1.0 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 1.0 \\\n",
    "--entcon_weight 0 \\\n",
    "--triplet_loss_weight 1.0 \\\n",
    "--nli_loss_weight 1.5 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets+nli)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--nli_hidden_layer_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c8947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 80\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 25\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_025244_MIMIC-CXR(triplets+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "   iters_to_accumulate: 8\n",
      "   override_lr: False\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 2.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.5, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   gpt4_radnli_labels_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\n",
      "   dataset_name: MIMIC-CXR(triplets+entcon+nli)\n",
      "   triplets_weight: 1.5\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 1.5\n",
      "   entcon_weight: 1.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: True\n",
      "  nli_hidden_layer_size: 128\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_025244_MIMIC-CXR(triplets+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\n",
      "1e-06 3 0.0001 8 1e-06 0.0001 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0001\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 8\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 2.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.5\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n",
      "Number of entailment samples: 26442\n",
      "Number of neutral samples: 39817\n",
      "Number of contradiction samples: 95777\n",
      "----\n",
      "\u001b[1mBuilding train entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 26442\n",
      "Number of contradiction samples: 95777\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'A repeat study would be recommended.', 'ent_h': 'The patient needs to undergo another chest X-ray.', 'con_p': 'Left PICC tip is in the cavoatrial junction or upper right atrium.', 'con_h': 'New right central line at the cavoatrial junction or upper right atrium.'}\n",
      "{'ent_p': 'There are findings consistent with COPD including increased AP diameter of the chest and flattening of the diaphragms.', 'ent_h': 'Lungs are clear but hyperinflated with increased AP diameter of the chest and flattened hemidiaphragms.', 'con_p': 'Right lower lobe mass appears grossly unchanged compared to the most recent PET-CT from .', 'con_h': \"The mass in the right lower lobe of the patient's lung has significantly increased in size since the last PET-CT scan.\"}\n",
      "{'ent_p': 'There is no evidence of lung abscess or pneumothorax.', 'ent_h': 'No pneumothorax is present.', 'con_p': 'Right upper lobe pleural surface thickening is new since .', 'con_h': 'Right lower lobe peribronchial wall thickening is noted.'}\n",
      "{'ent_p': 'There is no consolidation, edema, pleural effusion or pneumothorax.', 'ent_h': 'There is no evidence of pulmonary edema, pleural effusion, or pneumothorax.', 'con_p': 'Otherwise, the hilar and cardiomediastinal contours are normal.', 'con_h': 'Right apical pneumothorax and pneumomediastinum appear stable.'}\n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RadNLI samples: 480\n",
      "Number of GPT-4 RadNLI labels: 960\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 926\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset and dataloader...\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 290\n",
      "Number of contradiction samples: 335\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'There is no pulmonary vascular congestion.', 'ent_h': 'Heart size is upper limit of normal with no signs of pleural effusion or pulmonary congestion.', 'con_p': 'Heart size cannot be assessed.', 'con_h': 'Heart size is normal.'}\n",
      "{'ent_p': 'There is a tiny right pleural effusion.', 'ent_h': 'Trace pleural effusion at the right base.', 'con_p': 'Heart size is normal.', 'con_h': 'Heart size cannot be assessed.'}\n",
      "{'ent_p': 'No pleural effusions.', 'ent_h': 'No discrete consolidation, pleural effusion, pneumothorax, or pulmonary edema is identified.', 'con_p': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.', 'con_h': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.'}\n",
      "{'ent_p': 'Stable extent of the pre-existing left pleural effusion.', 'ent_h': 'Stable extent of a previously existing left pleural effusion.', 'con_p': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.', 'con_h': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.'}\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n",
      "len(_train_dataloaders) = 3\n",
      "len(_train_weights) = 3\n",
      "_train_weights = [1.5, 1.5, 1.0]\n",
      "len(_val_dataloaders) = 3\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets+entcon+nli)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_045139_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_045139_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_64_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8877.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_025244_MIMIC-CXR(triplets+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_64_nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8877.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_045139_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.58006, triplet_loss 0.43462, nli_loss 0.83787, nli_acc 0.83324, entcon_loss 0.41151, entcon_acc 0.89253, 85.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.86800, tacc(ob0) 0.98800, tacc(ob1) 0.96400, tacc(ob2) 0.94600, tacc(ob3) 0.96700, tacc(ob4) 0.94300, tacc(ob5) 0.80900, tacc(ob6) 0.94700, tacc(ob7) 0.91900, nli_acc 0.64039, entcon_acc 0.92836, 21.97 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8937.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.57138, triplet_loss 0.43541, nli_loss 0.83439, nli_acc 0.82898, entcon_loss 0.38084, entcon_acc 0.91813, 84.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92600, tacc(al2) 0.86300, tacc(ob0) 0.98500, tacc(ob1) 0.96600, tacc(ob2) 0.94400, tacc(ob3) 0.96400, tacc(ob4) 0.94200, tacc(ob5) 0.80500, tacc(ob6) 0.94600, tacc(ob7) 0.92100, nli_acc 0.63931, entcon_acc 0.95224, 22.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8955.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000022) ...\n",
      "loss 0.56436, triplet_loss 0.43174, nli_loss 0.83871, nli_acc 0.83342, entcon_loss 0.35177, entcon_acc 0.93280, 70.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92600, tacc(al2) 0.86900, tacc(ob0) 0.98500, tacc(ob1) 0.96600, tacc(ob2) 0.94000, tacc(ob3) 0.96200, tacc(ob4) 0.94000, tacc(ob5) 0.79900, tacc(ob6) 0.94900, tacc(ob7) 0.91800, nli_acc 0.65011, entcon_acc 0.95522, 21.93 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8970.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.57868, triplet_loss 0.43888, nli_loss 0.88362, nli_acc 0.82151, entcon_loss 0.33097, entcon_acc 0.94293, 85.75 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96700, tacc(al1) 0.92100, tacc(al2) 0.86500, tacc(ob0) 0.98600, tacc(ob1) 0.96500, tacc(ob2) 0.94200, tacc(ob3) 0.96500, tacc(ob4) 0.94600, tacc(ob5) 0.80700, tacc(ob6) 0.95900, tacc(ob7) 0.90700, nli_acc 0.59287, entcon_acc 0.95224, 22.14 secs\n",
      "\u001b[1m---- Epoch 5/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000056) ...\n",
      "loss 0.59120, triplet_loss 0.44014, nli_loss 0.91576, nli_acc 0.81262, entcon_loss 0.33098, entcon_acc 0.93440, 85.48 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96600, tacc(al1) 0.91900, tacc(al2) 0.86900, tacc(ob0) 0.98300, tacc(ob1) 0.95300, tacc(ob2) 0.93600, tacc(ob3) 0.95300, tacc(ob4) 0.93600, tacc(ob5) 0.77900, tacc(ob6) 0.94400, tacc(ob7) 0.90900, nli_acc 0.62203, entcon_acc 0.94328, 21.97 secs\n",
      "\u001b[1m---- Epoch 6/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.57723, triplet_loss 0.43647, nli_loss 0.88800, nli_acc 0.81884, entcon_loss 0.32223, entcon_acc 0.93387, 85.86 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92900, tacc(al2) 0.86500, tacc(ob0) 0.98200, tacc(ob1) 0.96600, tacc(ob2) 0.93700, tacc(ob3) 0.96300, tacc(ob4) 0.94200, tacc(ob5) 0.79700, tacc(ob6) 0.95900, tacc(ob7) 0.90700, nli_acc 0.63283, entcon_acc 0.94925, 22.00 secs\n",
      "\u001b[1m---- Epoch 7/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000018) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.56322, triplet_loss 0.43634, nli_loss 0.85620, nli_acc 0.82916, entcon_loss 0.31409, entcon_acc 0.94160, 84.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92900, tacc(al2) 0.86700, tacc(ob0) 0.98400, tacc(ob1) 0.96200, tacc(ob2) 0.93500, tacc(ob3) 0.96500, tacc(ob4) 0.93500, tacc(ob5) 0.79500, tacc(ob6) 0.95900, tacc(ob7) 0.90100, nli_acc 0.63823, entcon_acc 0.96119, 22.34 secs\n",
      "\u001b[1m---- Epoch 8/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.55958, triplet_loss 0.43949, nli_loss 0.83958, nli_acc 0.82791, entcon_loss 0.31969, entcon_acc 0.94267, 87.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92800, tacc(al2) 0.86600, tacc(ob0) 0.98600, tacc(ob1) 0.96200, tacc(ob2) 0.93900, tacc(ob3) 0.96700, tacc(ob4) 0.93900, tacc(ob5) 0.80600, tacc(ob6) 0.95700, tacc(ob7) 0.90400, nli_acc 0.63499, entcon_acc 0.93433, 21.99 secs\n",
      "\u001b[1m---- Epoch 9/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.55294, triplet_loss 0.43308, nli_loss 0.83612, nli_acc 0.82987, entcon_loss 0.30795, entcon_acc 0.95147, 86.91 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93000, tacc(al2) 0.86200, tacc(ob0) 0.98600, tacc(ob1) 0.96400, tacc(ob2) 0.93400, tacc(ob3) 0.96600, tacc(ob4) 0.93800, tacc(ob5) 0.79500, tacc(ob6) 0.95700, tacc(ob7) 0.90800, nli_acc 0.63715, entcon_acc 0.96119, 22.26 secs\n",
      "\u001b[1m---- Epoch 10/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.55480, triplet_loss 0.43511, nli_loss 0.83811, nli_acc 0.83129, entcon_loss 0.30935, entcon_acc 0.94853, 86.67 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.86400, tacc(ob0) 0.98500, tacc(ob1) 0.96300, tacc(ob2) 0.93600, tacc(ob3) 0.96700, tacc(ob4) 0.93900, tacc(ob5) 0.79500, tacc(ob6) 0.95700, tacc(ob7) 0.90700, nli_acc 0.64579, entcon_acc 0.95821, 22.12 secs\n",
      "\u001b[1m---- Epoch 11/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.54772, triplet_loss 0.43281, nli_loss 0.81997, nli_acc 0.83467, entcon_loss 0.31171, entcon_acc 0.94373, 85.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.86400, tacc(ob0) 0.98500, tacc(ob1) 0.96500, tacc(ob2) 0.93500, tacc(ob3) 0.96600, tacc(ob4) 0.94000, tacc(ob5) 0.79700, tacc(ob6) 0.95700, tacc(ob7) 0.90800, nli_acc 0.64687, entcon_acc 0.94627, 22.39 secs\n",
      "\u001b[1m---- Epoch 12/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.55078, triplet_loss 0.43063, nli_loss 0.83049, nli_acc 0.83040, entcon_loss 0.31143, entcon_acc 0.94693, 86.45 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.86400, tacc(ob0) 0.98600, tacc(ob1) 0.96500, tacc(ob2) 0.93500, tacc(ob3) 0.96600, tacc(ob4) 0.93800, tacc(ob5) 0.79600, tacc(ob6) 0.95700, tacc(ob7) 0.90800, nli_acc 0.64579, entcon_acc 0.94627, 22.38 secs\n",
      "\u001b[1m---- Epoch 13/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.57251, triplet_loss 0.44012, nli_loss 0.87864, nli_acc 0.81796, entcon_loss 0.31190, entcon_acc 0.95147, 87.19 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.92300, tacc(al2) 0.86400, tacc(ob0) 0.98100, tacc(ob1) 0.96200, tacc(ob2) 0.93800, tacc(ob3) 0.96400, tacc(ob4) 0.94700, tacc(ob5) 0.79800, tacc(ob6) 0.95700, tacc(ob7) 0.91500, nli_acc 0.59287, entcon_acc 0.95821, 21.73 secs\n",
      "\u001b[1m---- Epoch 14/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.57577, triplet_loss 0.44124, nli_loss 0.88528, nli_acc 0.82258, entcon_loss 0.31330, entcon_acc 0.93627, 85.88 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92000, tacc(al2) 0.85900, tacc(ob0) 0.98100, tacc(ob1) 0.96000, tacc(ob2) 0.93900, tacc(ob3) 0.96800, tacc(ob4) 0.94300, tacc(ob5) 0.79300, tacc(ob6) 0.95300, tacc(ob7) 0.90800, nli_acc 0.66199, entcon_acc 0.94925, 22.45 secs\n",
      "\u001b[1m---- Epoch 15/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.56839, triplet_loss 0.43644, nli_loss 0.87581, nli_acc 0.82080, entcon_loss 0.30517, entcon_acc 0.94800, 86.37 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.92500, tacc(al2) 0.86000, tacc(ob0) 0.98200, tacc(ob1) 0.96200, tacc(ob2) 0.94300, tacc(ob3) 0.96700, tacc(ob4) 0.94200, tacc(ob5) 0.79600, tacc(ob6) 0.94900, tacc(ob7) 0.90100, nli_acc 0.64687, entcon_acc 0.92537, 22.26 secs\n",
      "\u001b[1m---- Epoch 16/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.55385, triplet_loss 0.43736, nli_loss 0.83829, nli_acc 0.82720, entcon_loss 0.30191, entcon_acc 0.94667, 86.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92600, tacc(al2) 0.86000, tacc(ob0) 0.98400, tacc(ob1) 0.96000, tacc(ob2) 0.94100, tacc(ob3) 0.96700, tacc(ob4) 0.93800, tacc(ob5) 0.79000, tacc(ob6) 0.95400, tacc(ob7) 0.90100, nli_acc 0.65443, entcon_acc 0.94627, 22.17 secs\n",
      "\u001b[1m---- Epoch 17/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54069, triplet_loss 0.43765, nli_loss 0.79983, nli_acc 0.83627, entcon_loss 0.30652, entcon_acc 0.95173, 70.65 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92700, tacc(al2) 0.86000, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.93700, tacc(ob3) 0.96800, tacc(ob4) 0.93900, tacc(ob5) 0.79300, tacc(ob6) 0.95300, tacc(ob7) 0.90300, nli_acc 0.65767, entcon_acc 0.96716, 22.29 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_17_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8973.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 18/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.53297, triplet_loss 0.43241, nli_loss 0.78513, nli_acc 0.84498, entcon_loss 0.30556, entcon_acc 0.95227, 86.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92500, tacc(al2) 0.86300, tacc(ob0) 0.98500, tacc(ob1) 0.95900, tacc(ob2) 0.93900, tacc(ob3) 0.96800, tacc(ob4) 0.93900, tacc(ob5) 0.79000, tacc(ob6) 0.95300, tacc(ob7) 0.90300, nli_acc 0.65659, entcon_acc 0.96119, 21.32 secs\n",
      "\u001b[1m---- Epoch 19/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.53828, triplet_loss 0.43611, nli_loss 0.79900, nli_acc 0.84053, entcon_loss 0.30044, entcon_acc 0.95333, 86.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92400, tacc(al2) 0.86500, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.93900, tacc(ob3) 0.96800, tacc(ob4) 0.94200, tacc(ob5) 0.78900, tacc(ob6) 0.95300, tacc(ob7) 0.90400, nli_acc 0.65767, entcon_acc 0.92239, 21.99 secs\n",
      "\u001b[1m---- Epoch 20/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.53970, triplet_loss 0.43296, nli_loss 0.80423, nli_acc 0.83644, entcon_loss 0.30301, entcon_acc 0.94533, 88.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92300, tacc(al2) 0.86600, tacc(ob0) 0.98500, tacc(ob1) 0.96000, tacc(ob2) 0.93900, tacc(ob3) 0.96800, tacc(ob4) 0.94200, tacc(ob5) 0.79100, tacc(ob6) 0.95400, tacc(ob7) 0.90400, nli_acc 0.65767, entcon_acc 0.95224, 22.16 secs\n",
      "\u001b[1m---- Epoch 21/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.56918, triplet_loss 0.43930, nli_loss 0.87495, nli_acc 0.81476, entcon_loss 0.30535, entcon_acc 0.95040, 84.91 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92500, tacc(al2) 0.86000, tacc(ob0) 0.98900, tacc(ob1) 0.95300, tacc(ob2) 0.93500, tacc(ob3) 0.95700, tacc(ob4) 0.94300, tacc(ob5) 0.77200, tacc(ob6) 0.95200, tacc(ob7) 0.92000, nli_acc 0.64255, entcon_acc 0.94925, 22.48 secs\n",
      "\u001b[1m---- Epoch 22/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.56907, triplet_loss 0.43774, nli_loss 0.87727, nli_acc 0.81671, entcon_loss 0.30377, entcon_acc 0.95253, 88.30 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96500, tacc(al1) 0.92900, tacc(al2) 0.85700, tacc(ob0) 0.98700, tacc(ob1) 0.95300, tacc(ob2) 0.93700, tacc(ob3) 0.96100, tacc(ob4) 0.93800, tacc(ob5) 0.78200, tacc(ob6) 0.95200, tacc(ob7) 0.90300, nli_acc 0.65659, entcon_acc 0.94627, 22.27 secs\n",
      "\u001b[1m---- Epoch 23/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.54945, triplet_loss 0.43581, nli_loss 0.82678, nli_acc 0.83342, entcon_loss 0.30392, entcon_acc 0.95067, 85.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.93300, tacc(al2) 0.85700, tacc(ob0) 0.98700, tacc(ob1) 0.95400, tacc(ob2) 0.94100, tacc(ob3) 0.96400, tacc(ob4) 0.94700, tacc(ob5) 0.79900, tacc(ob6) 0.95800, tacc(ob7) 0.90300, nli_acc 0.63931, entcon_acc 0.94925, 22.05 secs\n",
      "\u001b[1m---- Epoch 24/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.54276, triplet_loss 0.43246, nli_loss 0.81404, nli_acc 0.83431, entcon_loss 0.30127, entcon_acc 0.95467, 86.09 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tacc(al0) 0.97000, tacc(al1) 0.92900, tacc(al2) 0.86500, tacc(ob0) 0.98900, tacc(ob1) 0.96200, tacc(ob2) 0.94300, tacc(ob3) 0.96300, tacc(ob4) 0.94500, tacc(ob5) 0.79700, tacc(ob6) 0.95700, tacc(ob7) 0.90900, nli_acc 0.64039, entcon_acc 0.95224, 22.05 secs\n",
      "\u001b[1m---- Epoch 25/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.53549, triplet_loss 0.43845, nli_loss 0.79321, nli_acc 0.83964, entcon_loss 0.29444, entcon_acc 0.95760, 85.25 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93000, tacc(al2) 0.86300, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.94000, tacc(ob3) 0.96300, tacc(ob4) 0.94600, tacc(ob5) 0.79200, tacc(ob6) 0.95500, tacc(ob7) 0.91100, nli_acc 0.64147, entcon_acc 0.93433, 22.19 secs\n",
      "\u001b[1m---- Epoch 26/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.52487, triplet_loss 0.43221, nli_loss 0.76843, nli_acc 0.84693, entcon_loss 0.29851, entcon_acc 0.95200, 85.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92900, tacc(al2) 0.86300, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.94300, tacc(ob3) 0.96200, tacc(ob4) 0.94700, tacc(ob5) 0.78900, tacc(ob6) 0.95600, tacc(ob7) 0.90800, nli_acc 0.64579, entcon_acc 0.94030, 21.96 secs\n",
      "\u001b[1m---- Epoch 27/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.51818, triplet_loss 0.43398, nli_loss 0.74764, nli_acc 0.85404, entcon_loss 0.30029, entcon_acc 0.95227, 85.06 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.92900, tacc(al2) 0.86300, tacc(ob0) 0.98800, tacc(ob1) 0.96200, tacc(ob2) 0.94300, tacc(ob3) 0.96400, tacc(ob4) 0.94800, tacc(ob5) 0.78800, tacc(ob6) 0.95600, tacc(ob7) 0.90800, nli_acc 0.64579, entcon_acc 0.94925, 22.18 secs\n",
      "\u001b[1m---- Epoch 28/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.53422, triplet_loss 0.43140, nli_loss 0.79441, nli_acc 0.83644, entcon_loss 0.29816, entcon_acc 0.95653, 87.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.92800, tacc(al2) 0.86500, tacc(ob0) 0.98800, tacc(ob1) 0.96000, tacc(ob2) 0.94300, tacc(ob3) 0.96400, tacc(ob4) 0.94700, tacc(ob5) 0.79000, tacc(ob6) 0.95700, tacc(ob7) 0.90700, nli_acc 0.64147, entcon_acc 0.93731, 22.32 secs\n",
      "\u001b[1m---- Epoch 29/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000100) ...\n",
      "loss 0.56448, triplet_loss 0.43694, nli_loss 0.86464, nli_acc 0.81956, entcon_loss 0.30555, entcon_acc 0.94933, 86.16 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.92700, tacc(al2) 0.85200, tacc(ob0) 0.97600, tacc(ob1) 0.93200, tacc(ob2) 0.92600, tacc(ob3) 0.96200, tacc(ob4) 0.92700, tacc(ob5) 0.77300, tacc(ob6) 0.92700, tacc(ob7) 0.88800, nli_acc 0.61015, entcon_acc 0.95522, 21.94 secs\n",
      "\u001b[1m---- Epoch 30/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000052) ...\n",
      "loss 0.57092, triplet_loss 0.43601, nli_loss 0.88623, nli_acc 0.81369, entcon_loss 0.30032, entcon_acc 0.94853, 85.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93000, tacc(al2) 0.86600, tacc(ob0) 0.98300, tacc(ob1) 0.95500, tacc(ob2) 0.93600, tacc(ob3) 0.96300, tacc(ob4) 0.94800, tacc(ob5) 0.77100, tacc(ob6) 0.94100, tacc(ob7) 0.90500, nli_acc 0.64579, entcon_acc 0.95821, 22.19 secs\n",
      "\u001b[1m---- Epoch 31/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.55449, triplet_loss 0.43792, nli_loss 0.83861, nli_acc 0.82809, entcon_loss 0.30318, entcon_acc 0.94133, 85.48 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92400, tacc(al2) 0.86100, tacc(ob0) 0.98500, tacc(ob1) 0.95600, tacc(ob2) 0.94100, tacc(ob3) 0.96800, tacc(ob4) 0.94500, tacc(ob5) 0.79000, tacc(ob6) 0.94800, tacc(ob7) 0.90200, nli_acc 0.63391, entcon_acc 0.95821, 22.18 secs\n",
      "\u001b[1m---- Epoch 32/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.54011, triplet_loss 0.43490, nli_loss 0.80663, nli_acc 0.83609, entcon_loss 0.29813, entcon_acc 0.95067, 86.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92100, tacc(al2) 0.85900, tacc(ob0) 0.98600, tacc(ob1) 0.96100, tacc(ob2) 0.94200, tacc(ob3) 0.96800, tacc(ob4) 0.94600, tacc(ob5) 0.78900, tacc(ob6) 0.95200, tacc(ob7) 0.91200, nli_acc 0.64579, entcon_acc 0.94627, 21.94 secs\n",
      "\u001b[1m---- Epoch 33/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.54070, triplet_loss 0.42954, nli_loss 0.81598, nli_acc 0.82862, entcon_loss 0.29451, entcon_acc 0.95280, 75.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92300, tacc(al2) 0.85900, tacc(ob0) 0.98600, tacc(ob1) 0.96100, tacc(ob2) 0.94300, tacc(ob3) 0.96700, tacc(ob4) 0.94500, tacc(ob5) 0.79400, tacc(ob6) 0.95100, tacc(ob7) 0.90400, nli_acc 0.65443, entcon_acc 0.96119, 22.35 secs\n",
      "\u001b[1m---- Epoch 34/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "Current run is terminating due to exception: CUDA out of memory. Tried to allocate 424.00 MiB (GPU 0; 10.76 GiB total capacity; 8.53 GiB already allocated; 356.56 MiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 424.00 MiB (GPU 0; 10.76 GiB total capacity; 8.53 GiB already allocated; 356.56 MiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 676, in <module>\n",
      "    collate_batch_fn_kwargs=collate_batch_fn_kwargs,\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 587, in train_from_scratch\n",
      "    # loss weights\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 364, in train_model\n",
      "    # Run common boilerplate code and start training\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1087, in _run_once_on_dataset_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 312, in step_fn_wrapper\n",
      "    output = step_fn__entcon(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 273, in step_fn__entcon\n",
      "    con_p_embeddings = model(input_ids=con_p_input_ids, attention_mask=con_p_attention_mask)['text_embeddings']\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/fact_encoder.py\", line 207, in forward\n",
      "    elementwise_product = p_embeddings * h_embeddings\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1373, in forward\n",
      "    prediction_scores = self.cls(sequence_output)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 710, in forward\n",
      "    prediction_scores = self.predictions(sequence_output)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 700, in forward\n",
      "    hidden_states = self.decoder(hidden_states)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 424.00 MiB (GPU 0; 10.76 GiB total capacity; 8.53 GiB already allocated; 356.56 MiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230928_025244_MIMIC-CXR(triplets+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 80 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 25 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 8 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,1e-4,8,1e-6,1e-4,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 2., 1.5, 2.]}\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--gpt4_radnli_labels_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "--triplets_weight 1.5 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 1.5 \\\n",
    "--entcon_weight 1.0 \\\n",
    "--triplet_loss_weight 1.0 \\\n",
    "--nli_loss_weight 2.0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets+entcon+nli)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--nli_hidden_layer_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9dad69e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 80\n",
      "   batches_per_epoch: 600\n",
      "   batch_size: 21\n",
      "   val_batch_size: 80\n",
      "   radgraph_spert_batch_size: None\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: 128\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210310_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\n",
      "   freeze_huggingface_model: False\n",
      "   spert_size_embedding: 25\n",
      "   spert_max_pairs: 1000\n",
      "   spert_prop_drop: 0.1\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,2e-6,8e-5,8,2e-6\n",
      "   iters_to_accumulate: 10\n",
      "   override_lr: False\n",
      "   max_grad_norm: None\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 2.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.5, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\n",
      "   integrated_sentence_facts_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\n",
      "   gpt4_radnli_labels_jsonl_filepath: /home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\n",
      "   dataset_name: MIMIC-CXR(triplets+entcon+nli)\n",
      "   triplets_weight: 1.0\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 2.0\n",
      "   entcon_weight: 1.0\n",
      "   radgraph_ner_re_weight: 0.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: True\n",
      "  nli_hidden_layer_size: 128\n",
      "  use_spert: False\n",
      "  spert_size_embedding: None\n",
      "  spert_relation_types: None\n",
      "  spert_entity_types: None\n",
      "  spert_max_pairs: None\n",
      "  spert_prop_drop: None\n",
      "  spert_cls_token: None\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210310_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,2e-6,8e-5,8,2e-6\n",
      "1e-06 3 8e-05 8 2e-06 8e-05 8 2e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 10, max_grad_norm = None\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 2.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.5\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "Loading integrated NLI from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl...\n",
      "Number of samples: 162036\n",
      "Loading integrated sentence facts from /home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl...\n",
      "Number of integrated sentence facts: 677694\n",
      "Number of entailment samples added: 1323679\n",
      "----\n",
      "\u001b[1mBuilding train NLI dataset and dataloader...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 0 | Label: 0 -> 21667 (2987.98)\n",
      "Source: 1 | Label: 0 -> 3744 (1672.60)\n",
      "Source: 2 | Label: 0 -> 465 (695.76)\n",
      "Source: 3 | Label: 0 -> 473 (701.58)\n",
      "Source: 4 | Label: 0 -> 93 (279.62)\n",
      "Source: 5 | Label: 0 -> 1323679 (8410.16)\n",
      "Source: 0 | Label: 1 -> 34854 (3435.46)\n",
      "Source: 1 | Label: 1 -> 3743 (1672.44)\n",
      "Source: 2 | Label: 1 -> 465 (695.76)\n",
      "Source: 3 | Label: 1 -> 474 (702.30)\n",
      "Source: 4 | Label: 1 -> 281 (538.25)\n",
      "Source: 0 | Label: 2 -> 90988 (4470.43)\n",
      "Source: 1 | Label: 2 -> 3744 (1672.60)\n",
      "Source: 2 | Label: 2 -> 465 (695.76)\n",
      "Source: 3 | Label: 2 -> 474 (702.30)\n",
      "Source: 4 | Label: 2 -> 106 (304.54)\n",
      "----\n",
      "\u001b[1mBuilding train entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 21667\n",
      "Number of contradiction samples: 95777\n",
      "Source: 0 -> (len(ent_premises), len(cont_premises)): (21667, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 3744\n",
      "Number of contradiction samples: 95777\n",
      "Source: 1 -> (len(ent_premises), len(cont_premises)): (3744, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 465\n",
      "Number of contradiction samples: 95777\n",
      "Source: 2 -> (len(ent_premises), len(cont_premises)): (465, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 473\n",
      "Number of contradiction samples: 95777\n",
      "Source: 3 -> (len(ent_premises), len(cont_premises)): (473, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 93\n",
      "Number of contradiction samples: 95777\n",
      "Source: 4 -> (len(ent_premises), len(cont_premises)): (93, 95777)\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 1323679\n",
      "Number of contradiction samples: 95777\n",
      "Source: 5 -> (len(ent_premises), len(cont_premises)): (1323679, 95777)\n",
      "Number of datasets: 6\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'PA and lateral chest radiographs demonstrate consolidation of the left lower lobe.', 'ent_h': 'Atelectasis or consolidation in the left lower lobe.', 'con_p': 'There is no increase in left perihilar consolidation.', 'con_h': 'Left perihilar consolidation has increased, due to increasing edema or infection.'}\n",
      "{'ent_p': 'An initial EGD showed [**Country 1372**] and other food from 20 to 40 cm in the esophagus.', 'ent_h': ' The patient had an upper endoscopy. ', 'con_p': '44 yo male presents with multiple stab wounds over left chest and flank with diaphragmatic injury and liver laceration.', 'con_h': ' Patient presents with no wounds'}\n",
      "{'ent_p': 'During admission, patient was noted to have acute renal failure on top of CRI secondary to multiple myeloma.', 'ent_h': ' the patient has acute renal failure', 'con_p': 'The right lower lobe collapse has worsened over time.', 'con_h': 'Interval improvement of right lower lobe collapse.'}\n",
      "{'ent_p': 'He had a 500 cc blood loss, after which he was transferred 2 units of packed red blood cells and 2 units of fresh frozen plasma.', 'ent_h': ' The patient was given blood because of a low hematocrit. ', 'con_p': 'Compression deformities of multiple lower thoracic vertebral bodies.', 'con_h': \"The patient's lower thoracic vertebral bodies are in perfect condition.\"}\n",
      "----\n",
      "Number of RadNLI samples: 480\n",
      "Number of GPT-4 RadNLI labels: 960\n",
      "Number of MS_CXR_T samples: 361\n",
      "Number of total samples: 926\n",
      "----\n",
      "\u001b[1mBuilding val NLI dataset and dataloader...\u001b[0m\n",
      "----\n",
      "\u001b[1mBuilding val entailment/contradiction dataset and dataloader...\u001b[0m\n",
      "EntailmentContradictionDataset: __init__\n",
      "Number of entailment samples: 290\n",
      "Number of contradiction samples: 335\n",
      "Example entailment/contradiction samples:\n",
      "{'ent_p': 'Interval decrease in the opacities at the right lung base.', 'ent_h': 'Interval improvement of right lung base opacities.', 'con_p': 'Heart size cannot be assessed.', 'con_h': 'Heart size is normal.'}\n",
      "{'ent_p': 'No acute osseous abnormalities are visualized.', 'ent_h': 'No acute osseous abnormality seen.', 'con_p': 'Heart size is normal.', 'con_h': 'Heart size cannot be assessed.'}\n",
      "{'ent_p': 'status post right thoracentesis with interval decrease in right pleural effusion, which is now trace in size.', 'ent_h': 'status post right thoracentesis with interval improvement in right pleural effusion, which is now trace in size.', 'con_p': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.', 'con_h': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.'}\n",
      "{'ent_p': 'The lungs are otherwise clear without focal consolidation.', 'ent_h': 'The lungs however are clear.', 'con_p': 'PA and lateral chest radiographs demonstrate a normal cardiomediastinal silhouette.', 'con_h': 'Cardiomediastinal and hilar contours are unremarkable with the exception of a tortuous aorta.'}\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n",
      "len(_train_dataloaders) = 3\n",
      "len(_train_weights) = 3\n",
      "_train_weights = [1.0, 2.0, 1.0]\n",
      "len(_val_dataloaders) = 3\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets+entcon+nli)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_25_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8973.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210310_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_25_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8973.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_220207_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.48151, triplet_loss 0.44174, nli_loss 0.59921, nli_acc 0.88460, entcon_loss 0.28587, entcon_acc 0.96032, 93.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.94300, tacc(al2) 0.87000, tacc(ob0) 0.97200, tacc(ob1) 0.93200, tacc(ob2) 0.92900, tacc(ob3) 0.96000, tacc(ob4) 0.96100, tacc(ob5) 0.77000, tacc(ob6) 0.94900, tacc(ob7) 0.90900, nli_acc 0.65443, entcon_acc 0.96716, 9.98 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8972.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.49195, triplet_loss 0.44799, nli_loss 0.61308, nli_acc 0.88302, entcon_loss 0.29365, entcon_acc 0.94825, 85.61 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.94100, tacc(al2) 0.87200, tacc(ob0) 0.97200, tacc(ob1) 0.92700, tacc(ob2) 0.92800, tacc(ob3) 0.95900, tacc(ob4) 0.95900, tacc(ob5) 0.76900, tacc(ob6) 0.95100, tacc(ob7) 0.90600, nli_acc 0.65875, entcon_acc 0.96716, 10.11 secs\n",
      "\u001b[1m---- Epoch 3/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.50115, triplet_loss 0.44951, nli_loss 0.62770, nli_acc 0.87619, entcon_loss 0.29970, entcon_acc 0.94603, 86.93 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.94000, tacc(al2) 0.86600, tacc(ob0) 0.97100, tacc(ob1) 0.92800, tacc(ob2) 0.92600, tacc(ob3) 0.96300, tacc(ob4) 0.96200, tacc(ob5) 0.76800, tacc(ob6) 0.95000, tacc(ob7) 0.90400, nli_acc 0.67063, entcon_acc 0.96418, 10.20 secs\n",
      "\u001b[1m---- Epoch 4/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.51973, triplet_loss 0.45889, nli_loss 0.66176, nli_acc 0.86635, entcon_loss 0.29649, entcon_acc 0.94667, 88.38 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93100, tacc(al2) 0.86700, tacc(ob0) 0.97100, tacc(ob1) 0.93400, tacc(ob2) 0.91400, tacc(ob3) 0.95400, tacc(ob4) 0.95800, tacc(ob5) 0.75300, tacc(ob6) 0.93200, tacc(ob7) 0.90400, nli_acc 0.66307, entcon_acc 0.95224, 10.36 secs\n",
      "\u001b[1m---- Epoch 5/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000050) ...\n",
      "loss 0.50353, triplet_loss 0.45628, nli_loss 0.62842, nli_acc 0.87333, entcon_loss 0.30099, entcon_acc 0.93968, 86.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86900, tacc(ob0) 0.97200, tacc(ob1) 0.92800, tacc(ob2) 0.92100, tacc(ob3) 0.95400, tacc(ob4) 0.95900, tacc(ob5) 0.75800, tacc(ob6) 0.95100, tacc(ob7) 0.91400, nli_acc 0.68251, entcon_acc 0.95821, 10.27 secs\n",
      "\u001b[1m---- Epoch 6/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000032) ...\n",
      "loss 0.50065, triplet_loss 0.45723, nli_loss 0.62630, nli_acc 0.86984, entcon_loss 0.29279, entcon_acc 0.94921, 86.40 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92600, tacc(al2) 0.87000, tacc(ob0) 0.97700, tacc(ob1) 0.94000, tacc(ob2) 0.92600, tacc(ob3) 0.95900, tacc(ob4) 0.95500, tacc(ob5) 0.76100, tacc(ob6) 0.95600, tacc(ob7) 0.91000, nli_acc 0.65875, entcon_acc 0.97015, 10.30 secs\n",
      "\u001b[1m---- Epoch 7/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000020) ...\n",
      "loss 0.48626, triplet_loss 0.45626, nli_loss 0.59699, nli_acc 0.88206, entcon_loss 0.29481, entcon_acc 0.95270, 80.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92700, tacc(al2) 0.86300, tacc(ob0) 0.97500, tacc(ob1) 0.93300, tacc(ob2) 0.91900, tacc(ob3) 0.95500, tacc(ob4) 0.95800, tacc(ob5) 0.75900, tacc(ob6) 0.95000, tacc(ob7) 0.91600, nli_acc 0.65983, entcon_acc 0.96418, 10.14 secs\n",
      "\u001b[1m---- Epoch 8/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000013) ...\n",
      "loss 0.48812, triplet_loss 0.45410, nli_loss 0.59954, nli_acc 0.88254, entcon_loss 0.29929, entcon_acc 0.94794, 88.37 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92800, tacc(al2) 0.86300, tacc(ob0) 0.97400, tacc(ob1) 0.93300, tacc(ob2) 0.92100, tacc(ob3) 0.95800, tacc(ob4) 0.95900, tacc(ob5) 0.76600, tacc(ob6) 0.95300, tacc(ob7) 0.91200, nli_acc 0.66631, entcon_acc 0.94627, 10.23 secs\n",
      "\u001b[1m---- Epoch 9/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000008) ...\n",
      "loss 0.48213, triplet_loss 0.45683, nli_loss 0.58821, nli_acc 0.88429, entcon_loss 0.29529, entcon_acc 0.95016, 87.18 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93100, tacc(al2) 0.86500, tacc(ob0) 0.97300, tacc(ob1) 0.94200, tacc(ob2) 0.92300, tacc(ob3) 0.95900, tacc(ob4) 0.96200, tacc(ob5) 0.76700, tacc(ob6) 0.95200, tacc(ob7) 0.91000, nli_acc 0.66847, entcon_acc 0.95821, 10.20 secs\n",
      "\u001b[1m---- Epoch 10/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.48147, triplet_loss 0.45191, nli_loss 0.58891, nli_acc 0.88349, entcon_loss 0.29615, entcon_acc 0.95175, 86.02 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92900, tacc(al2) 0.86700, tacc(ob0) 0.97300, tacc(ob1) 0.94000, tacc(ob2) 0.92000, tacc(ob3) 0.96200, tacc(ob4) 0.96000, tacc(ob5) 0.76600, tacc(ob6) 0.95600, tacc(ob7) 0.91200, nli_acc 0.67603, entcon_acc 0.96119, 10.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8974.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.46775, triplet_loss 0.44884, nli_loss 0.56351, nli_acc 0.89016, entcon_loss 0.29515, entcon_acc 0.95714, 86.88 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86600, tacc(ob0) 0.97400, tacc(ob1) 0.93700, tacc(ob2) 0.92100, tacc(ob3) 0.96200, tacc(ob4) 0.96100, tacc(ob5) 0.76500, tacc(ob6) 0.95500, tacc(ob7) 0.91000, nli_acc 0.67171, entcon_acc 0.97910, 10.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8988.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.47019, triplet_loss 0.44995, nli_loss 0.56925, nli_acc 0.88968, entcon_loss 0.29233, entcon_acc 0.95619, 86.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.86500, tacc(ob0) 0.97200, tacc(ob1) 0.93700, tacc(ob2) 0.92100, tacc(ob3) 0.96200, tacc(ob4) 0.95800, tacc(ob5) 0.76700, tacc(ob6) 0.95200, tacc(ob7) 0.91200, nli_acc 0.67063, entcon_acc 0.95224, 10.31 secs\n",
      "\u001b[1m---- Epoch 13/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.48297, triplet_loss 0.45888, nli_loss 0.58639, nli_acc 0.88587, entcon_loss 0.30021, entcon_acc 0.94540, 85.65 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93100, tacc(al2) 0.86200, tacc(ob0) 0.97600, tacc(ob1) 0.93600, tacc(ob2) 0.92400, tacc(ob3) 0.95800, tacc(ob4) 0.95800, tacc(ob5) 0.76200, tacc(ob6) 0.95200, tacc(ob7) 0.91000, nli_acc 0.68143, entcon_acc 0.96716, 10.28 secs\n",
      "\u001b[1m---- Epoch 14/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.48739, triplet_loss 0.45828, nli_loss 0.59741, nli_acc 0.88206, entcon_loss 0.29647, entcon_acc 0.94952, 86.14 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92800, tacc(al2) 0.88100, tacc(ob0) 0.97800, tacc(ob1) 0.93200, tacc(ob2) 0.92900, tacc(ob3) 0.95900, tacc(ob4) 0.95500, tacc(ob5) 0.75700, tacc(ob6) 0.95100, tacc(ob7) 0.91100, nli_acc 0.64579, entcon_acc 0.95522, 10.12 secs\n",
      "\u001b[1m---- Epoch 15/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.48231, triplet_loss 0.45558, nli_loss 0.58966, nli_acc 0.88190, entcon_loss 0.29433, entcon_acc 0.95143, 88.95 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93400, tacc(al2) 0.86400, tacc(ob0) 0.97500, tacc(ob1) 0.93100, tacc(ob2) 0.92000, tacc(ob3) 0.96000, tacc(ob4) 0.95500, tacc(ob5) 0.75800, tacc(ob6) 0.95100, tacc(ob7) 0.90800, nli_acc 0.66739, entcon_acc 0.97612, 10.26 secs\n",
      "\u001b[1m---- Epoch 16/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.46791, triplet_loss 0.45640, nli_loss 0.56505, nli_acc 0.89270, entcon_loss 0.28514, entcon_acc 0.95810, 86.15 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.93500, tacc(al2) 0.86800, tacc(ob0) 0.97200, tacc(ob1) 0.93800, tacc(ob2) 0.91800, tacc(ob3) 0.95800, tacc(ob4) 0.95300, tacc(ob5) 0.75300, tacc(ob6) 0.95000, tacc(ob7) 0.91100, nli_acc 0.65227, entcon_acc 0.95522, 10.14 secs\n",
      "\u001b[1m---- Epoch 17/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.47730, triplet_loss 0.45190, nli_loss 0.58286, nli_acc 0.88857, entcon_loss 0.29161, entcon_acc 0.95460, 85.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.93600, tacc(al2) 0.85900, tacc(ob0) 0.97300, tacc(ob1) 0.93600, tacc(ob2) 0.92100, tacc(ob3) 0.95900, tacc(ob4) 0.95400, tacc(ob5) 0.75800, tacc(ob6) 0.95200, tacc(ob7) 0.90800, nli_acc 0.67279, entcon_acc 0.96119, 10.24 secs\n",
      "\u001b[1m---- Epoch 18/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.46113, triplet_loss 0.44854, nli_loss 0.55381, nli_acc 0.89444, entcon_loss 0.28835, entcon_acc 0.95492, 86.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93300, tacc(al2) 0.86800, tacc(ob0) 0.97200, tacc(ob1) 0.93700, tacc(ob2) 0.92300, tacc(ob3) 0.95500, tacc(ob4) 0.95400, tacc(ob5) 0.75800, tacc(ob6) 0.95300, tacc(ob7) 0.90700, nli_acc 0.67063, entcon_acc 0.94030, 10.07 secs\n",
      "\u001b[1m---- Epoch 19/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.46655, triplet_loss 0.45470, nli_loss 0.55875, nli_acc 0.89302, entcon_loss 0.29398, entcon_acc 0.95143, 85.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93500, tacc(al2) 0.86700, tacc(ob0) 0.97200, tacc(ob1) 0.94000, tacc(ob2) 0.92300, tacc(ob3) 0.95600, tacc(ob4) 0.95400, tacc(ob5) 0.75800, tacc(ob6) 0.94900, tacc(ob7) 0.90800, nli_acc 0.67279, entcon_acc 0.96119, 10.41 secs\n",
      "\u001b[1m---- Epoch 20/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.46221, triplet_loss 0.45541, nli_loss 0.55243, nli_acc 0.89603, entcon_loss 0.28856, entcon_acc 0.95206, 83.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93400, tacc(al2) 0.86500, tacc(ob0) 0.97200, tacc(ob1) 0.94100, tacc(ob2) 0.92500, tacc(ob3) 0.95600, tacc(ob4) 0.95600, tacc(ob5) 0.75400, tacc(ob6) 0.95200, tacc(ob7) 0.90800, nli_acc 0.66415, entcon_acc 0.92836, 10.13 secs\n",
      "\u001b[1m---- Epoch 21/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.46611, triplet_loss 0.46080, nli_loss 0.55306, nli_acc 0.89079, entcon_loss 0.29754, entcon_acc 0.95810, 83.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.87300, tacc(ob0) 0.97000, tacc(ob1) 0.93300, tacc(ob2) 0.92000, tacc(ob3) 0.95300, tacc(ob4) 0.95000, tacc(ob5) 0.75300, tacc(ob6) 0.95100, tacc(ob7) 0.89700, nli_acc 0.67819, entcon_acc 0.95821, 10.21 secs\n",
      "\u001b[1m---- Epoch 22/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.47546, triplet_loss 0.45482, nli_loss 0.57797, nli_acc 0.88429, entcon_loss 0.29109, entcon_acc 0.95460, 84.04 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92800, tacc(al2) 0.86600, tacc(ob0) 0.96900, tacc(ob1) 0.92400, tacc(ob2) 0.92100, tacc(ob3) 0.94900, tacc(ob4) 0.95100, tacc(ob5) 0.75700, tacc(ob6) 0.95000, tacc(ob7) 0.90400, nli_acc 0.69006, entcon_acc 0.96119, 10.24 secs\n",
      "\u001b[1m---- Epoch 23/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.46227, triplet_loss 0.45422, nli_loss 0.55012, nli_acc 0.89254, entcon_loss 0.29460, entcon_acc 0.95302, 83.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.93000, tacc(al2) 0.87000, tacc(ob0) 0.97000, tacc(ob1) 0.93000, tacc(ob2) 0.92300, tacc(ob3) 0.95100, tacc(ob4) 0.95400, tacc(ob5) 0.75600, tacc(ob6) 0.94400, tacc(ob7) 0.90600, nli_acc 0.65875, entcon_acc 0.95821, 10.20 secs\n",
      "\u001b[1m---- Epoch 24/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.47725, triplet_loss 0.45423, nli_loss 0.58019, nli_acc 0.88429, entcon_loss 0.29440, entcon_acc 0.95333, 82.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93200, tacc(al2) 0.86100, tacc(ob0) 0.97400, tacc(ob1) 0.92800, tacc(ob2) 0.92200, tacc(ob3) 0.95300, tacc(ob4) 0.95600, tacc(ob5) 0.76900, tacc(ob6) 0.94800, tacc(ob7) 0.91500, nli_acc 0.66523, entcon_acc 0.95522, 10.21 secs\n",
      "\u001b[1m---- Epoch 25/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.46543, triplet_loss 0.45500, nli_loss 0.55917, nli_acc 0.89508, entcon_loss 0.28837, entcon_acc 0.95683, 84.62 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93200, tacc(al2) 0.86200, tacc(ob0) 0.97000, tacc(ob1) 0.92700, tacc(ob2) 0.92400, tacc(ob3) 0.95400, tacc(ob4) 0.95500, tacc(ob5) 0.76200, tacc(ob6) 0.94200, tacc(ob7) 0.91400, nli_acc 0.66955, entcon_acc 0.96418, 10.18 secs\n",
      "\u001b[1m---- Epoch 26/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.47049, triplet_loss 0.44827, nli_loss 0.57196, nli_acc 0.89127, entcon_loss 0.28975, entcon_acc 0.95206, 82.85 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93300, tacc(al2) 0.86400, tacc(ob0) 0.97200, tacc(ob1) 0.93300, tacc(ob2) 0.92300, tacc(ob3) 0.95300, tacc(ob4) 0.95300, tacc(ob5) 0.77200, tacc(ob6) 0.94400, tacc(ob7) 0.91500, nli_acc 0.66523, entcon_acc 0.95522, 10.17 secs\n",
      "\u001b[1m---- Epoch 27/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.46468, triplet_loss 0.45514, nli_loss 0.55774, nli_acc 0.89127, entcon_loss 0.28812, entcon_acc 0.95429, 83.68 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93000, tacc(al2) 0.86400, tacc(ob0) 0.97300, tacc(ob1) 0.93500, tacc(ob2) 0.92000, tacc(ob3) 0.95200, tacc(ob4) 0.95400, tacc(ob5) 0.76500, tacc(ob6) 0.94300, tacc(ob7) 0.91400, nli_acc 0.67171, entcon_acc 0.96119, 9.91 secs\n",
      "\u001b[1m---- Epoch 28/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.44444, triplet_loss 0.44902, nli_loss 0.51927, nli_acc 0.90587, entcon_loss 0.29021, entcon_acc 0.95270, 84.86 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93200, tacc(al2) 0.86300, tacc(ob0) 0.97200, tacc(ob1) 0.93500, tacc(ob2) 0.92000, tacc(ob3) 0.95400, tacc(ob4) 0.95300, tacc(ob5) 0.76100, tacc(ob6) 0.94300, tacc(ob7) 0.91500, nli_acc 0.67171, entcon_acc 0.95522, 10.26 secs\n",
      "\u001b[1m---- Epoch 29/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.45661, triplet_loss 0.45863, nli_loss 0.53482, nli_acc 0.89524, entcon_loss 0.29817, entcon_acc 0.95302, 83.29 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92500, tacc(al2) 0.86300, tacc(ob0) 0.97300, tacc(ob1) 0.92700, tacc(ob2) 0.93100, tacc(ob3) 0.95200, tacc(ob4) 0.95500, tacc(ob5) 0.76300, tacc(ob6) 0.94300, tacc(ob7) 0.89800, nli_acc 0.65659, entcon_acc 0.96418, 10.12 secs\n",
      "\u001b[1m---- Epoch 30/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.46995, triplet_loss 0.46033, nli_loss 0.56413, nli_acc 0.88635, entcon_loss 0.29121, entcon_acc 0.95492, 83.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92800, tacc(al2) 0.87300, tacc(ob0) 0.96900, tacc(ob1) 0.92700, tacc(ob2) 0.92800, tacc(ob3) 0.95200, tacc(ob4) 0.95800, tacc(ob5) 0.76700, tacc(ob6) 0.93700, tacc(ob7) 0.90500, nli_acc 0.67927, entcon_acc 0.94627, 10.17 secs\n",
      "\u001b[1m---- Epoch 31/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.45700, triplet_loss 0.44961, nli_loss 0.54393, nli_acc 0.88937, entcon_loss 0.29051, entcon_acc 0.95143, 83.38 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92500, tacc(al2) 0.86900, tacc(ob0) 0.97300, tacc(ob1) 0.92700, tacc(ob2) 0.92600, tacc(ob3) 0.95000, tacc(ob4) 0.95400, tacc(ob5) 0.76400, tacc(ob6) 0.94200, tacc(ob7) 0.90000, nli_acc 0.67603, entcon_acc 0.96716, 10.29 secs\n",
      "\u001b[1m---- Epoch 32/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.46455, triplet_loss 0.45536, nli_loss 0.55602, nli_acc 0.89175, entcon_loss 0.29081, entcon_acc 0.95460, 83.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92500, tacc(al2) 0.86900, tacc(ob0) 0.97200, tacc(ob1) 0.92800, tacc(ob2) 0.92500, tacc(ob3) 0.95300, tacc(ob4) 0.95600, tacc(ob5) 0.75100, tacc(ob6) 0.94300, tacc(ob7) 0.90000, nli_acc 0.66631, entcon_acc 0.96418, 10.10 secs\n",
      "\u001b[1m---- Epoch 33/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.45506, triplet_loss 0.45420, nli_loss 0.53914, nli_acc 0.89476, entcon_loss 0.28777, entcon_acc 0.95111, 83.77 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92700, tacc(al2) 0.87000, tacc(ob0) 0.97000, tacc(ob1) 0.92900, tacc(ob2) 0.92800, tacc(ob3) 0.95000, tacc(ob4) 0.95500, tacc(ob5) 0.75800, tacc(ob6) 0.93900, tacc(ob7) 0.90200, nli_acc 0.69222, entcon_acc 0.95522, 10.32 secs\n",
      "\u001b[1m---- Epoch 34/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.46685, triplet_loss 0.45463, nli_loss 0.56469, nli_acc 0.88873, entcon_loss 0.28338, entcon_acc 0.96000, 84.11 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.87100, tacc(ob0) 0.97500, tacc(ob1) 0.93100, tacc(ob2) 0.92300, tacc(ob3) 0.95200, tacc(ob4) 0.95800, tacc(ob5) 0.76200, tacc(ob6) 0.94600, tacc(ob7) 0.90400, nli_acc 0.68575, entcon_acc 0.95224, 10.39 secs\n",
      "\u001b[1m---- Epoch 35/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.44856, triplet_loss 0.45605, nli_loss 0.52187, nli_acc 0.89952, entcon_loss 0.29444, entcon_acc 0.95111, 85.07 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86800, tacc(ob0) 0.97300, tacc(ob1) 0.92900, tacc(ob2) 0.92500, tacc(ob3) 0.95200, tacc(ob4) 0.95900, tacc(ob5) 0.76200, tacc(ob6) 0.94600, tacc(ob7) 0.90300, nli_acc 0.68683, entcon_acc 0.94328, 10.21 secs\n",
      "\u001b[1m---- Epoch 36/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.43064, triplet_loss 0.44679, nli_loss 0.49654, nli_acc 0.90635, entcon_loss 0.28268, entcon_acc 0.95619, 83.37 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.87100, tacc(ob0) 0.97300, tacc(ob1) 0.93000, tacc(ob2) 0.92500, tacc(ob3) 0.95400, tacc(ob4) 0.96100, tacc(ob5) 0.76300, tacc(ob6) 0.94800, tacc(ob7) 0.90400, nli_acc 0.68683, entcon_acc 0.96418, 10.17 secs\n",
      "\u001b[1m---- Epoch 37/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.46625, triplet_loss 0.45933, nli_loss 0.55441, nli_acc 0.89238, entcon_loss 0.29684, entcon_acc 0.94952, 84.42 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.94000, tacc(al2) 0.86700, tacc(ob0) 0.97300, tacc(ob1) 0.93100, tacc(ob2) 0.92400, tacc(ob3) 0.94800, tacc(ob4) 0.96000, tacc(ob5) 0.78000, tacc(ob6) 0.94000, tacc(ob7) 0.90100, nli_acc 0.68790, entcon_acc 0.93134, 10.22 secs\n",
      "\u001b[1m---- Epoch 38/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.45449, triplet_loss 0.45383, nli_loss 0.53635, nli_acc 0.89333, entcon_loss 0.29142, entcon_acc 0.95778, 83.83 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.93600, tacc(al2) 0.87300, tacc(ob0) 0.97000, tacc(ob1) 0.92800, tacc(ob2) 0.92400, tacc(ob3) 0.95500, tacc(ob4) 0.95800, tacc(ob5) 0.76800, tacc(ob6) 0.94400, tacc(ob7) 0.90000, nli_acc 0.67495, entcon_acc 0.95522, 10.14 secs\n",
      "\u001b[1m---- Epoch 39/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.44077, triplet_loss 0.45184, nli_loss 0.50819, nli_acc 0.90175, entcon_loss 0.29485, entcon_acc 0.95175, 83.95 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93400, tacc(al2) 0.87100, tacc(ob0) 0.97400, tacc(ob1) 0.93400, tacc(ob2) 0.92400, tacc(ob3) 0.96000, tacc(ob4) 0.95600, tacc(ob5) 0.76600, tacc(ob6) 0.94500, tacc(ob7) 0.91100, nli_acc 0.67063, entcon_acc 0.92537, 10.31 secs\n",
      "\u001b[1m---- Epoch 40/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.45298, triplet_loss 0.45633, nli_loss 0.53531, nli_acc 0.89206, entcon_loss 0.28499, entcon_acc 0.95587, 85.01 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93200, tacc(al2) 0.87100, tacc(ob0) 0.97400, tacc(ob1) 0.93600, tacc(ob2) 0.92700, tacc(ob3) 0.95600, tacc(ob4) 0.96300, tacc(ob5) 0.76500, tacc(ob6) 0.94000, tacc(ob7) 0.91100, nli_acc 0.65767, entcon_acc 0.96119, 10.17 secs\n",
      "\u001b[1m---- Epoch 41/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.45074, triplet_loss 0.45446, nli_loss 0.52907, nli_acc 0.89635, entcon_loss 0.29036, entcon_acc 0.95460, 83.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93400, tacc(al2) 0.87400, tacc(ob0) 0.97500, tacc(ob1) 0.93500, tacc(ob2) 0.92600, tacc(ob3) 0.95800, tacc(ob4) 0.96200, tacc(ob5) 0.77300, tacc(ob6) 0.94200, tacc(ob7) 0.91200, nli_acc 0.65659, entcon_acc 0.94925, 10.14 secs\n",
      "\u001b[1m---- Epoch 42/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.45329, triplet_loss 0.45387, nli_loss 0.53718, nli_acc 0.89968, entcon_loss 0.28493, entcon_acc 0.95683, 84.86 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93400, tacc(al2) 0.87200, tacc(ob0) 0.97800, tacc(ob1) 0.93600, tacc(ob2) 0.92800, tacc(ob3) 0.95900, tacc(ob4) 0.96000, tacc(ob5) 0.76900, tacc(ob6) 0.94300, tacc(ob7) 0.91000, nli_acc 0.67819, entcon_acc 0.96418, 10.10 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_42_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8989.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 43/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.44656, triplet_loss 0.45742, nli_loss 0.52103, nli_acc 0.90032, entcon_loss 0.28677, entcon_acc 0.95619, 85.16 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93400, tacc(al2) 0.87400, tacc(ob0) 0.97800, tacc(ob1) 0.93900, tacc(ob2) 0.92800, tacc(ob3) 0.96000, tacc(ob4) 0.96000, tacc(ob5) 0.77000, tacc(ob6) 0.94400, tacc(ob7) 0.90800, nli_acc 0.67279, entcon_acc 0.97612, 10.17 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_43_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8999.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 44/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.42796, triplet_loss 0.44650, nli_loss 0.49032, nli_acc 0.90714, entcon_loss 0.28472, entcon_acc 0.95810, 83.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93300, tacc(al2) 0.87500, tacc(ob0) 0.97800, tacc(ob1) 0.93900, tacc(ob2) 0.92800, tacc(ob3) 0.96100, tacc(ob4) 0.96000, tacc(ob5) 0.77100, tacc(ob6) 0.94300, tacc(ob7) 0.90800, nli_acc 0.67279, entcon_acc 0.96119, 10.21 secs\n",
      "\u001b[1m---- Epoch 45/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.45869, triplet_loss 0.44948, nli_loss 0.54788, nli_acc 0.89413, entcon_loss 0.28954, entcon_acc 0.95111, 84.32 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92900, tacc(al2) 0.88200, tacc(ob0) 0.97300, tacc(ob1) 0.93800, tacc(ob2) 0.92700, tacc(ob3) 0.95500, tacc(ob4) 0.96100, tacc(ob5) 0.76400, tacc(ob6) 0.93600, tacc(ob7) 0.89800, nli_acc 0.66307, entcon_acc 0.95821, 10.25 secs\n",
      "\u001b[1m---- Epoch 46/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.45104, triplet_loss 0.45862, nli_loss 0.52884, nli_acc 0.89460, entcon_loss 0.28785, entcon_acc 0.95937, 84.12 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.93200, tacc(al2) 0.87300, tacc(ob0) 0.97100, tacc(ob1) 0.93400, tacc(ob2) 0.92200, tacc(ob3) 0.95400, tacc(ob4) 0.96100, tacc(ob5) 0.79000, tacc(ob6) 0.94500, tacc(ob7) 0.90300, nli_acc 0.67711, entcon_acc 0.96716, 10.12 secs\n",
      "\u001b[1m---- Epoch 47/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.44232, triplet_loss 0.45530, nli_loss 0.51345, nli_acc 0.90540, entcon_loss 0.28707, entcon_acc 0.95429, 83.59 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92600, tacc(al2) 0.87200, tacc(ob0) 0.96500, tacc(ob1) 0.93000, tacc(ob2) 0.92400, tacc(ob3) 0.95300, tacc(ob4) 0.96200, tacc(ob5) 0.75800, tacc(ob6) 0.94000, tacc(ob7) 0.90100, nli_acc 0.66955, entcon_acc 0.96418, 10.19 secs\n",
      "\u001b[1m---- Epoch 48/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.44768, triplet_loss 0.45940, nli_loss 0.52311, nli_acc 0.90127, entcon_loss 0.28511, entcon_acc 0.95841, 84.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92900, tacc(al2) 0.87800, tacc(ob0) 0.97600, tacc(ob1) 0.93900, tacc(ob2) 0.92600, tacc(ob3) 0.95300, tacc(ob4) 0.95800, tacc(ob5) 0.77300, tacc(ob6) 0.94700, tacc(ob7) 0.91300, nli_acc 0.68035, entcon_acc 0.95821, 10.03 secs\n",
      "\u001b[1m---- Epoch 49/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.44384, triplet_loss 0.45441, nli_loss 0.51781, nli_acc 0.89587, entcon_loss 0.28536, entcon_acc 0.95270, 86.25 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92900, tacc(al2) 0.87500, tacc(ob0) 0.97500, tacc(ob1) 0.93800, tacc(ob2) 0.92900, tacc(ob3) 0.94900, tacc(ob4) 0.95900, tacc(ob5) 0.77400, tacc(ob6) 0.94600, tacc(ob7) 0.91000, nli_acc 0.67495, entcon_acc 0.95522, 10.19 secs\n",
      "\u001b[1m---- Epoch 50/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.42489, triplet_loss 0.45607, nli_loss 0.48140, nli_acc 0.91016, entcon_loss 0.28070, entcon_acc 0.96222, 84.02 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93100, tacc(al2) 0.87400, tacc(ob0) 0.97700, tacc(ob1) 0.93900, tacc(ob2) 0.93400, tacc(ob3) 0.94900, tacc(ob4) 0.96000, tacc(ob5) 0.77800, tacc(ob6) 0.94800, tacc(ob7) 0.91000, nli_acc 0.67063, entcon_acc 0.94627, 10.18 secs\n",
      "\u001b[1m---- Epoch 51/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.43683, triplet_loss 0.45357, nli_loss 0.50496, nli_acc 0.90524, entcon_loss 0.28385, entcon_acc 0.95905, 83.56 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92900, tacc(al2) 0.87500, tacc(ob0) 0.97600, tacc(ob1) 0.93900, tacc(ob2) 0.93200, tacc(ob3) 0.95100, tacc(ob4) 0.96100, tacc(ob5) 0.77400, tacc(ob6) 0.94900, tacc(ob7) 0.90800, nli_acc 0.67387, entcon_acc 0.97313, 10.40 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_51_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9001.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 52/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.42539, triplet_loss 0.45372, nli_loss 0.48375, nli_acc 0.91127, entcon_loss 0.28035, entcon_acc 0.96317, 84.44 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.93000, tacc(al2) 0.87600, tacc(ob0) 0.97500, tacc(ob1) 0.94000, tacc(ob2) 0.93200, tacc(ob3) 0.95100, tacc(ob4) 0.96200, tacc(ob5) 0.77600, tacc(ob6) 0.94700, tacc(ob7) 0.90900, nli_acc 0.67495, entcon_acc 0.96418, 10.19 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_52_encc+nlcc+ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9002.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 53/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.44076, triplet_loss 0.45318, nli_loss 0.50968, nli_acc 0.90556, entcon_loss 0.29050, entcon_acc 0.95619, 83.86 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92200, tacc(al2) 0.88300, tacc(ob0) 0.97500, tacc(ob1) 0.93100, tacc(ob2) 0.93800, tacc(ob3) 0.94200, tacc(ob4) 0.96800, tacc(ob5) 0.77100, tacc(ob6) 0.94900, tacc(ob7) 0.90300, nli_acc 0.65767, entcon_acc 0.94627, 10.20 secs\n",
      "\u001b[1m---- Epoch 54/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.45846, triplet_loss 0.45835, nli_loss 0.54248, nli_acc 0.89444, entcon_loss 0.29054, entcon_acc 0.95619, 78.22 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92800, tacc(al2) 0.87700, tacc(ob0) 0.97800, tacc(ob1) 0.94000, tacc(ob2) 0.92600, tacc(ob3) 0.95500, tacc(ob4) 0.96100, tacc(ob5) 0.76900, tacc(ob6) 0.93900, tacc(ob7) 0.91200, nli_acc 0.68467, entcon_acc 0.93433, 10.19 secs\n",
      "\u001b[1m---- Epoch 55/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.42613, triplet_loss 0.45608, nli_loss 0.48254, nli_acc 0.90698, entcon_loss 0.28335, entcon_acc 0.96063, 67.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92500, tacc(al2) 0.87700, tacc(ob0) 0.97300, tacc(ob1) 0.93500, tacc(ob2) 0.92700, tacc(ob3) 0.95600, tacc(ob4) 0.96500, tacc(ob5) 0.77400, tacc(ob6) 0.94400, tacc(ob7) 0.90400, nli_acc 0.69006, entcon_acc 0.93134, 10.11 secs\n",
      "\u001b[1m---- Epoch 56/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.42275, triplet_loss 0.44378, nli_loss 0.47892, nli_acc 0.90587, entcon_loss 0.28939, entcon_acc 0.95206, 83.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92800, tacc(al2) 0.86900, tacc(ob0) 0.97300, tacc(ob1) 0.93300, tacc(ob2) 0.92500, tacc(ob3) 0.96200, tacc(ob4) 0.96600, tacc(ob5) 0.76800, tacc(ob6) 0.94700, tacc(ob7) 0.90500, nli_acc 0.68467, entcon_acc 0.93433, 10.21 secs\n",
      "\u001b[1m---- Epoch 57/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.44257, triplet_loss 0.45299, nli_loss 0.51633, nli_acc 0.89952, entcon_loss 0.28464, entcon_acc 0.95810, 83.82 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92900, tacc(al2) 0.86600, tacc(ob0) 0.97300, tacc(ob1) 0.93500, tacc(ob2) 0.93100, tacc(ob3) 0.96100, tacc(ob4) 0.96600, tacc(ob5) 0.77300, tacc(ob6) 0.95000, tacc(ob7) 0.90800, nli_acc 0.68467, entcon_acc 0.95821, 10.12 secs\n",
      "\u001b[1m---- Epoch 58/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.42317, triplet_loss 0.44746, nli_loss 0.47981, nli_acc 0.90857, entcon_loss 0.28562, entcon_acc 0.95714, 84.31 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92800, tacc(al2) 0.87000, tacc(ob0) 0.97500, tacc(ob1) 0.93600, tacc(ob2) 0.92800, tacc(ob3) 0.96200, tacc(ob4) 0.96300, tacc(ob5) 0.76700, tacc(ob6) 0.94800, tacc(ob7) 0.90700, nli_acc 0.67711, entcon_acc 0.95224, 10.10 secs\n",
      "\u001b[1m---- Epoch 59/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.43181, triplet_loss 0.44632, nli_loss 0.49792, nli_acc 0.90683, entcon_loss 0.28507, entcon_acc 0.95683, 83.14 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92800, tacc(al2) 0.87200, tacc(ob0) 0.97600, tacc(ob1) 0.93400, tacc(ob2) 0.93000, tacc(ob3) 0.96200, tacc(ob4) 0.96300, tacc(ob5) 0.76700, tacc(ob6) 0.95000, tacc(ob7) 0.90700, nli_acc 0.68467, entcon_acc 0.96716, 10.22 secs\n",
      "\u001b[1m---- Epoch 60/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41682, triplet_loss 0.45069, nli_loss 0.46674, nli_acc 0.91492, entcon_loss 0.28311, entcon_acc 0.95937, 82.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92900, tacc(al2) 0.87000, tacc(ob0) 0.97700, tacc(ob1) 0.93300, tacc(ob2) 0.92800, tacc(ob3) 0.96000, tacc(ob4) 0.96300, tacc(ob5) 0.76700, tacc(ob6) 0.95000, tacc(ob7) 0.90700, nli_acc 0.68575, entcon_acc 0.95224, 10.26 secs\n",
      "\u001b[1m---- Epoch 61/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.45797, triplet_loss 0.45065, nli_loss 0.54466, nli_acc 0.89429, entcon_loss 0.29192, entcon_acc 0.94603, 83.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92900, tacc(al2) 0.86900, tacc(ob0) 0.97900, tacc(ob1) 0.93700, tacc(ob2) 0.93600, tacc(ob3) 0.96300, tacc(ob4) 0.96300, tacc(ob5) 0.75700, tacc(ob6) 0.94000, tacc(ob7) 0.91400, nli_acc 0.66091, entcon_acc 0.97015, 10.16 secs\n",
      "\u001b[1m---- Epoch 62/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.44418, triplet_loss 0.45074, nli_loss 0.51744, nli_acc 0.89762, entcon_loss 0.29110, entcon_acc 0.95429, 83.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92500, tacc(al2) 0.87500, tacc(ob0) 0.97700, tacc(ob1) 0.93700, tacc(ob2) 0.93600, tacc(ob3) 0.96000, tacc(ob4) 0.96000, tacc(ob5) 0.77600, tacc(ob6) 0.94300, tacc(ob7) 0.90900, nli_acc 0.64579, entcon_acc 0.92836, 10.21 secs\n",
      "\u001b[1m---- Epoch 63/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.42853, triplet_loss 0.45381, nli_loss 0.49021, nli_acc 0.90587, entcon_loss 0.27988, entcon_acc 0.96286, 82.64 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92700, tacc(al2) 0.87400, tacc(ob0) 0.97700, tacc(ob1) 0.94400, tacc(ob2) 0.93500, tacc(ob3) 0.96600, tacc(ob4) 0.95000, tacc(ob5) 0.77300, tacc(ob6) 0.94800, tacc(ob7) 0.90900, nli_acc 0.65335, entcon_acc 0.94627, 10.19 secs\n",
      "\u001b[1m---- Epoch 64/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.43800, triplet_loss 0.45105, nli_loss 0.50733, nli_acc 0.90429, entcon_loss 0.28631, entcon_acc 0.95429, 83.49 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92700, tacc(al2) 0.87300, tacc(ob0) 0.97800, tacc(ob1) 0.93900, tacc(ob2) 0.93200, tacc(ob3) 0.96200, tacc(ob4) 0.95300, tacc(ob5) 0.76500, tacc(ob6) 0.94900, tacc(ob7) 0.90900, nli_acc 0.67387, entcon_acc 0.93731, 10.27 secs\n",
      "\u001b[1m---- Epoch 65/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.42350, triplet_loss 0.45245, nli_loss 0.47625, nli_acc 0.90698, entcon_loss 0.28905, entcon_acc 0.95651, 83.46 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92500, tacc(al2) 0.87500, tacc(ob0) 0.97600, tacc(ob1) 0.94600, tacc(ob2) 0.93200, tacc(ob3) 0.96300, tacc(ob4) 0.95500, tacc(ob5) 0.77100, tacc(ob6) 0.94500, tacc(ob7) 0.91000, nli_acc 0.66955, entcon_acc 0.93731, 10.20 secs\n",
      "\u001b[1m---- Epoch 66/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.41705, triplet_loss 0.44569, nli_loss 0.47052, nli_acc 0.90905, entcon_loss 0.28146, entcon_acc 0.95556, 84.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92400, tacc(al2) 0.87600, tacc(ob0) 0.97900, tacc(ob1) 0.94200, tacc(ob2) 0.93400, tacc(ob3) 0.96200, tacc(ob4) 0.95800, tacc(ob5) 0.76900, tacc(ob6) 0.94300, tacc(ob7) 0.91100, nli_acc 0.66091, entcon_acc 0.94925, 10.10 secs\n",
      "\u001b[1m---- Epoch 67/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41514, triplet_loss 0.45251, nli_loss 0.46194, nli_acc 0.91302, entcon_loss 0.28419, entcon_acc 0.95270, 84.24 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92500, tacc(al2) 0.87600, tacc(ob0) 0.97800, tacc(ob1) 0.94300, tacc(ob2) 0.93300, tacc(ob3) 0.96100, tacc(ob4) 0.95900, tacc(ob5) 0.77300, tacc(ob6) 0.94300, tacc(ob7) 0.91200, nli_acc 0.66415, entcon_acc 0.95821, 10.23 secs\n",
      "\u001b[1m---- Epoch 68/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41586, triplet_loss 0.44765, nli_loss 0.46842, nli_acc 0.91397, entcon_loss 0.27893, entcon_acc 0.96190, 84.87 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92500, tacc(al2) 0.87400, tacc(ob0) 0.97800, tacc(ob1) 0.94200, tacc(ob2) 0.93300, tacc(ob3) 0.96000, tacc(ob4) 0.96200, tacc(ob5) 0.76500, tacc(ob6) 0.94400, tacc(ob7) 0.91000, nli_acc 0.66199, entcon_acc 0.95821, 10.14 secs\n",
      "\u001b[1m---- Epoch 69/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.43055, triplet_loss 0.45322, nli_loss 0.49366, nli_acc 0.90730, entcon_loss 0.28166, entcon_acc 0.95651, 83.09 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92900, tacc(al2) 0.86200, tacc(ob0) 0.97500, tacc(ob1) 0.93800, tacc(ob2) 0.93100, tacc(ob3) 0.96300, tacc(ob4) 0.95700, tacc(ob5) 0.75500, tacc(ob6) 0.94600, tacc(ob7) 0.90900, nli_acc 0.66523, entcon_acc 0.94030, 10.28 secs\n",
      "\u001b[1m---- Epoch 70/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.43972, triplet_loss 0.45665, nli_loss 0.50622, nli_acc 0.89968, entcon_loss 0.28979, entcon_acc 0.95143, 81.94 secs\n",
      "(2) Validation stage ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tacc(al0) 0.97000, tacc(al1) 0.93100, tacc(al2) 0.87500, tacc(ob0) 0.97200, tacc(ob1) 0.93100, tacc(ob2) 0.93400, tacc(ob3) 0.95600, tacc(ob4) 0.96300, tacc(ob5) 0.78400, tacc(ob6) 0.95100, tacc(ob7) 0.91000, nli_acc 0.68143, entcon_acc 0.94925, 10.18 secs\n",
      "\u001b[1m---- Epoch 71/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.43332, triplet_loss 0.44481, nli_loss 0.50115, nli_acc 0.90698, entcon_loss 0.28618, entcon_acc 0.95873, 84.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93000, tacc(al2) 0.87300, tacc(ob0) 0.97600, tacc(ob1) 0.93600, tacc(ob2) 0.93400, tacc(ob3) 0.95900, tacc(ob4) 0.95800, tacc(ob5) 0.76600, tacc(ob6) 0.94800, tacc(ob7) 0.91000, nli_acc 0.67819, entcon_acc 0.95821, 10.17 secs\n",
      "\u001b[1m---- Epoch 72/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.41383, triplet_loss 0.45547, nli_loss 0.46038, nli_acc 0.91444, entcon_loss 0.27908, entcon_acc 0.96254, 83.73 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93200, tacc(al2) 0.87800, tacc(ob0) 0.97700, tacc(ob1) 0.93700, tacc(ob2) 0.93700, tacc(ob3) 0.96200, tacc(ob4) 0.96000, tacc(ob5) 0.76900, tacc(ob6) 0.94800, tacc(ob7) 0.90800, nli_acc 0.68898, entcon_acc 0.94030, 10.23 secs\n",
      "\u001b[1m---- Epoch 73/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.40867, triplet_loss 0.44905, nli_loss 0.45310, nli_acc 0.91365, entcon_loss 0.27942, entcon_acc 0.95714, 83.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93200, tacc(al2) 0.86900, tacc(ob0) 0.97700, tacc(ob1) 0.93700, tacc(ob2) 0.93300, tacc(ob3) 0.96100, tacc(ob4) 0.96100, tacc(ob5) 0.76600, tacc(ob6) 0.94800, tacc(ob7) 0.91100, nli_acc 0.67927, entcon_acc 0.95224, 10.24 secs\n",
      "\u001b[1m---- Epoch 74/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.41908, triplet_loss 0.44776, nli_loss 0.47334, nli_acc 0.91063, entcon_loss 0.28186, entcon_acc 0.96889, 84.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.93300, tacc(al2) 0.87300, tacc(ob0) 0.97700, tacc(ob1) 0.93900, tacc(ob2) 0.93200, tacc(ob3) 0.96400, tacc(ob4) 0.96100, tacc(ob5) 0.76500, tacc(ob6) 0.94500, tacc(ob7) 0.90900, nli_acc 0.67387, entcon_acc 0.95821, 10.20 secs\n",
      "\u001b[1m---- Epoch 75/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42269, triplet_loss 0.44949, nli_loss 0.47899, nli_acc 0.90873, entcon_loss 0.28331, entcon_acc 0.95492, 84.69 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.93200, tacc(al2) 0.87300, tacc(ob0) 0.97600, tacc(ob1) 0.94000, tacc(ob2) 0.93300, tacc(ob3) 0.96300, tacc(ob4) 0.96100, tacc(ob5) 0.76700, tacc(ob6) 0.94700, tacc(ob7) 0.90800, nli_acc 0.66739, entcon_acc 0.96119, 10.00 secs\n",
      "\u001b[1m---- Epoch 76/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41489, triplet_loss 0.44948, nli_loss 0.46810, nli_acc 0.91460, entcon_loss 0.27391, entcon_acc 0.96508, 84.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93400, tacc(al2) 0.87200, tacc(ob0) 0.97500, tacc(ob1) 0.94200, tacc(ob2) 0.93200, tacc(ob3) 0.96400, tacc(ob4) 0.95900, tacc(ob5) 0.76300, tacc(ob6) 0.94500, tacc(ob7) 0.90800, nli_acc 0.67171, entcon_acc 0.96119, 10.12 secs\n",
      "\u001b[1m---- Epoch 77/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.43275, triplet_loss 0.44919, nli_loss 0.49930, nli_acc 0.90444, entcon_loss 0.28323, entcon_acc 0.96254, 84.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.93000, tacc(al2) 0.86000, tacc(ob0) 0.96900, tacc(ob1) 0.93100, tacc(ob2) 0.91700, tacc(ob3) 0.95400, tacc(ob4) 0.94600, tacc(ob5) 0.75300, tacc(ob6) 0.92500, tacc(ob7) 0.89500, nli_acc 0.64147, entcon_acc 0.96418, 10.16 secs\n",
      "\u001b[1m---- Epoch 78/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000047) ...\n",
      "loss 0.45597, triplet_loss 0.46183, nli_loss 0.53478, nli_acc 0.89444, entcon_loss 0.29251, entcon_acc 0.95556, 83.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92500, tacc(al2) 0.86300, tacc(ob0) 0.97000, tacc(ob1) 0.94400, tacc(ob2) 0.92500, tacc(ob3) 0.95800, tacc(ob4) 0.94900, tacc(ob5) 0.75300, tacc(ob6) 0.93500, tacc(ob7) 0.91000, nli_acc 0.66631, entcon_acc 0.97612, 10.17 secs\n",
      "\u001b[1m---- Epoch 79/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000028) ...\n",
      "loss 0.42317, triplet_loss 0.45553, nli_loss 0.47710, nli_acc 0.90556, entcon_loss 0.28293, entcon_acc 0.96349, 83.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.92600, tacc(al2) 0.86800, tacc(ob0) 0.97500, tacc(ob1) 0.93400, tacc(ob2) 0.93500, tacc(ob3) 0.95900, tacc(ob4) 0.95000, tacc(ob5) 0.76200, tacc(ob6) 0.94400, tacc(ob7) 0.90700, nli_acc 0.68467, entcon_acc 0.94328, 10.17 secs\n",
      "\u001b[1m---- Epoch 80/80\u001b[0m\n",
      "(1) Training stage (lr = 0.000016) ...\n",
      "loss 0.40586, triplet_loss 0.45305, nli_loss 0.44348, nli_acc 0.91476, entcon_loss 0.28345, entcon_acc 0.96127, 84.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92900, tacc(al2) 0.87200, tacc(ob0) 0.97700, tacc(ob1) 0.94100, tacc(ob2) 0.93500, tacc(ob3) 0.95800, tacc(ob4) 0.95700, tacc(ob5) 0.77800, tacc(ob6) 0.94000, tacc(ob7) 0.91000, nli_acc 0.66523, entcon_acc 0.96418, 10.19 secs\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20231002_210310_MIMIC-CXR(triplets+entcon+nli)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\" \\\n",
    "--epochs 80 \\\n",
    "--batches_per_epoch 600 \\\n",
    "--batch_size 21 \\\n",
    "--val_batch_size 80 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 10 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,2e-6,8e-5,8,2e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 2., 1.5, 2.]}\" \\\n",
    "--integrated_nli_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "--integrated_sentence_facts_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_sentence_facts(58655550,50135857).jsonl\" \\\n",
    "--gpt4_radnli_labels_jsonl_filepath \\\n",
    "\"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "--triplets_weight 1.0 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 2.0 \\\n",
    "--entcon_weight 1.0 \\\n",
    "--radgraph_ner_re_weight 0 \\\n",
    "--triplet_loss_weight 1.0 \\\n",
    "--nli_loss_weight 2.0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets+entcon+nli)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--nli_hidden_layer_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
