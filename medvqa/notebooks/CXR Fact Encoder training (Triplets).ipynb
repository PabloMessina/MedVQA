{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ../train_fact_embedding.py \\\n",
    "# --pretrained_checkpoint_folder_path \\\n",
    "# \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230924_175553_MIMIC-CXR(GPT3.5,GPT4,CXR-BERT,triplet_loss,ChestImaGenome,RadGraph,NLI)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "# --epochs 300 \\\n",
    "# --batches_per_epoch 500 \\\n",
    "# --batch_size 45 \\\n",
    "# --num_workers 3 \\\n",
    "# --iters_to_accumulate 8 \\\n",
    "# --optimizer_name \"adamw\" \\\n",
    "# --scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "# --lr 1e-6 \\\n",
    "# --warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "# --triplets_filepath \\\n",
    "# \"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000).pkl\" \\\n",
    "# --triplet_rule_weights \\\n",
    "# \"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 1., 1., 2.]}\" \\\n",
    "# --integrated_facts_metadata_jsonl_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_fact_metadata(595880,60579117).improved_comparison(6741113).jsonl\" \\\n",
    "# --paraphrases_jsonl_filepaths \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part1.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_anatomical_locations__part2.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_anatomical_locations__part3.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__single-words.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part1.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part2.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words__part3.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-16k-0613_paraphrased_observations__two-or-more-words__part4.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_paraphrased_observations__two-or-more-words_cluster-balanced.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_1of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_2of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_3of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_paraphrased_observations__two-or-more-words_cluster-balanced_4of5.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part1.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part2.jsonl\" \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-3.5-turbo-0613_hard_triplets_from_facts(cluster-balanced,hardest)-part3.jsonl\" \\\n",
    "# --integrated_chest_imagenome_observations_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_observations(9).pkl\" \\\n",
    "# --integrated_chest_imagenome_anatomical_locations_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_chest_imagenome_anatomical_locations(5).pkl\" \\\n",
    "# --integrated_nli_jsonl_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/integrated_nli_examples(162036,21465751).jsonl\"  \\\n",
    "# --gpt4_radnli_labels_jsonl_filepath \\\n",
    "# \"/home/pamessina/medvqa-workspace/cache/mimiccxr/openai/gpt-4-0613_radnli_queries.jsonl\" \\\n",
    "# --n_chest_imagenome_observations 74 \\\n",
    "# --n_chest_imagenome_anatomical_locations 38 \\\n",
    "# --triplets_weight 1.0 \\\n",
    "# --metadata_classification_weight 0 \\\n",
    "# --chest_imagenome_observations_classification_weight 0 \\\n",
    "# --chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "# --nli_weight 0 \\\n",
    "# --entcon_weight 0 \\\n",
    "# --dataset_name \"MIMIC-CXR(triplets-only)\" \\\n",
    "# --huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "# --embedding_size 128 \\\n",
    "# --use_amp \\\n",
    "# --no_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c052a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 500\n",
      "   batch_size: 100\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: None\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "   iters_to_accumulate: 4\n",
      "   override_lr: False\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   gpt4_radnli_labels_jsonl_filepath: None\n",
      "   dataset_name: MIMIC-CXR(triplets-only)\n",
      "   triplets_weight: 1.0\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 0.0\n",
      "   entcon_weight: 0.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': None}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,2e-4,8,1e-6,2e-4,8,1e-6\n",
      "1e-06 3 0.0002 8 1e-06 0.0002 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 0.0002\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 1.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.0\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n",
      "len(_train_dataloaders) = 1\n",
      "len(_train_weights) = 1\n",
      "_train_weights = [1.0]\n",
      "len(_val_dataloaders) = 1\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets-only)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "NOTE: Using only validation metrics\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_185515_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_185515_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_185515_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.50143, triplet_loss 0.50143, 83.17 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.95000, tacc(al1) 0.79500, tacc(al2) 0.98300, tacc(ob0) 0.97100, tacc(ob1) 0.91900, tacc(ob2) 0.99000, tacc(ob3) 0.91100, tacc(ob4) 0.91200, tacc(ob5) 0.89700, tacc(ob6) 0.93400, tacc(ob7) 0.75800, 6.13 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.8923.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000006) ...\n",
      "loss 0.48259, triplet_loss 0.48259, 80.97 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.83400, tacc(al2) 0.98900, tacc(ob0) 0.97800, tacc(ob1) 0.93200, tacc(ob2) 0.98700, tacc(ob3) 0.93400, tacc(ob4) 0.92100, tacc(ob5) 0.88200, tacc(ob6) 0.94900, tacc(ob7) 0.80900, 6.28 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9115.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000034) ...\n",
      "loss 0.46576, triplet_loss 0.46576, 81.92 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98000, tacc(al1) 0.88300, tacc(al2) 0.97200, tacc(ob0) 0.98300, tacc(ob1) 0.94100, tacc(ob2) 0.97300, tacc(ob3) 0.94100, tacc(ob4) 0.93700, tacc(ob5) 0.84500, tacc(ob6) 0.95700, tacc(ob7) 0.86700, 6.23 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_3_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9264.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.46144, triplet_loss 0.46144, 81.84 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.88900, tacc(al2) 0.93000, tacc(ob0) 0.98100, tacc(ob1) 0.92800, tacc(ob2) 0.94300, tacc(ob3) 0.94300, tacc(ob4) 0.92800, tacc(ob5) 0.79700, tacc(ob6) 0.94700, tacc(ob7) 0.87400, 6.46 secs\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000103) ...\n",
      "loss 0.44817, triplet_loss 0.44817, 81.05 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98000, tacc(al1) 0.90700, tacc(al2) 0.92600, tacc(ob0) 0.98300, tacc(ob1) 0.95200, tacc(ob2) 0.94000, tacc(ob3) 0.95500, tacc(ob4) 0.95200, tacc(ob5) 0.79300, tacc(ob6) 0.93600, tacc(ob7) 0.88000, 6.50 secs\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000053) ...\n",
      "loss 0.44070, triplet_loss 0.44070, 81.36 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.90000, tacc(al2) 0.90500, tacc(ob0) 0.98600, tacc(ob1) 0.95100, tacc(ob2) 0.95200, tacc(ob3) 0.95100, tacc(ob4) 0.94500, tacc(ob5) 0.81900, tacc(ob6) 0.93500, tacc(ob7) 0.89300, 6.29 secs\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.43704, triplet_loss 0.43704, 81.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.90800, tacc(al2) 0.90500, tacc(ob0) 0.98700, tacc(ob1) 0.95700, tacc(ob2) 0.95400, tacc(ob3) 0.95900, tacc(ob4) 0.94500, tacc(ob5) 0.79400, tacc(ob6) 0.93600, tacc(ob7) 0.89100, 6.56 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9265.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000014) ...\n",
      "loss 0.43503, triplet_loss 0.43503, 82.34 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.90500, tacc(al2) 0.90200, tacc(ob0) 0.98800, tacc(ob1) 0.95100, tacc(ob2) 0.95300, tacc(ob3) 0.96000, tacc(ob4) 0.94700, tacc(ob5) 0.80600, tacc(ob6) 0.94000, tacc(ob7) 0.89800, 6.50 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9274.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.43420, triplet_loss 0.43420, 83.66 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.91000, tacc(al2) 0.90700, tacc(ob0) 0.98700, tacc(ob1) 0.95200, tacc(ob2) 0.94600, tacc(ob3) 0.96100, tacc(ob4) 0.94800, tacc(ob5) 0.80200, tacc(ob6) 0.94500, tacc(ob7) 0.89300, 6.73 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_9_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9274.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.43347, triplet_loss 0.43347, 82.26 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.91200, tacc(al2) 0.90100, tacc(ob0) 0.98700, tacc(ob1) 0.95100, tacc(ob2) 0.94800, tacc(ob3) 0.96000, tacc(ob4) 0.95000, tacc(ob5) 0.80200, tacc(ob6) 0.94000, tacc(ob7) 0.89300, 6.64 secs\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.43327, triplet_loss 0.43327, 83.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.91200, tacc(al2) 0.90200, tacc(ob0) 0.98800, tacc(ob1) 0.95200, tacc(ob2) 0.94800, tacc(ob3) 0.96100, tacc(ob4) 0.95000, tacc(ob5) 0.80400, tacc(ob6) 0.94200, tacc(ob7) 0.89400, 6.77 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9278.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.43345, triplet_loss 0.43345, 80.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.91300, tacc(al2) 0.90100, tacc(ob0) 0.98700, tacc(ob1) 0.95300, tacc(ob2) 0.94800, tacc(ob3) 0.96000, tacc(ob4) 0.94900, tacc(ob5) 0.80000, tacc(ob6) 0.94200, tacc(ob7) 0.89200, 6.73 secs\n",
      "\u001b[1m---- Epoch 13/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.44369, triplet_loss 0.44369, 83.70 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96900, tacc(al1) 0.89100, tacc(al2) 0.89100, tacc(ob0) 0.98600, tacc(ob1) 0.94300, tacc(ob2) 0.92100, tacc(ob3) 0.96200, tacc(ob4) 0.93500, tacc(ob5) 0.77400, tacc(ob6) 0.93300, tacc(ob7) 0.89600, 7.50 secs\n",
      "\u001b[1m---- Epoch 14/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.43714, triplet_loss 0.43714, 78.50 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96400, tacc(al1) 0.90100, tacc(al2) 0.88600, tacc(ob0) 0.98600, tacc(ob1) 0.95300, tacc(ob2) 0.91800, tacc(ob3) 0.96300, tacc(ob4) 0.94500, tacc(ob5) 0.77700, tacc(ob6) 0.93100, tacc(ob7) 0.90600, 7.75 secs\n",
      "\u001b[1m---- Epoch 15/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.43177, triplet_loss 0.43177, 84.30 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96600, tacc(al1) 0.89800, tacc(al2) 0.88200, tacc(ob0) 0.98900, tacc(ob1) 0.95900, tacc(ob2) 0.93200, tacc(ob3) 0.96700, tacc(ob4) 0.95500, tacc(ob5) 0.80700, tacc(ob6) 0.94800, tacc(ob7) 0.90400, 7.74 secs\n",
      "\u001b[1m---- Epoch 16/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.42955, triplet_loss 0.42955, 83.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.90200, tacc(al2) 0.88400, tacc(ob0) 0.99300, tacc(ob1) 0.95700, tacc(ob2) 0.93700, tacc(ob3) 0.96600, tacc(ob4) 0.94700, tacc(ob5) 0.80600, tacc(ob6) 0.93900, tacc(ob7) 0.90500, 8.11 secs\n",
      "\u001b[1m---- Epoch 17/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.42806, triplet_loss 0.42806, 85.91 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.89900, tacc(al2) 0.88300, tacc(ob0) 0.99100, tacc(ob1) 0.96100, tacc(ob2) 0.94000, tacc(ob3) 0.96400, tacc(ob4) 0.95300, tacc(ob5) 0.80600, tacc(ob6) 0.94100, tacc(ob7) 0.90600, 8.17 secs\n",
      "\u001b[1m---- Epoch 18/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.42858, triplet_loss 0.42858, 86.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.90700, tacc(al2) 0.88500, tacc(ob0) 0.99100, tacc(ob1) 0.95800, tacc(ob2) 0.93900, tacc(ob3) 0.96600, tacc(ob4) 0.95400, tacc(ob5) 0.80100, tacc(ob6) 0.94400, tacc(ob7) 0.90400, 8.90 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_18_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9280.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 19/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.42629, triplet_loss 0.42629, 88.46 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.90800, tacc(al2) 0.88500, tacc(ob0) 0.98900, tacc(ob1) 0.96000, tacc(ob2) 0.93700, tacc(ob3) 0.96400, tacc(ob4) 0.95100, tacc(ob5) 0.80200, tacc(ob6) 0.94400, tacc(ob7) 0.90400, 8.31 secs\n",
      "\u001b[1m---- Epoch 20/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42714, triplet_loss 0.42714, 93.33 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.90600, tacc(al2) 0.88500, tacc(ob0) 0.98900, tacc(ob1) 0.96200, tacc(ob2) 0.93900, tacc(ob3) 0.96400, tacc(ob4) 0.95300, tacc(ob5) 0.80000, tacc(ob6) 0.94500, tacc(ob7) 0.90500, 9.98 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_20_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9283.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 21/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.44095, triplet_loss 0.44095, 93.45 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.95900, tacc(al1) 0.90800, tacc(al2) 0.89300, tacc(ob0) 0.98500, tacc(ob1) 0.95000, tacc(ob2) 0.92700, tacc(ob3) 0.95500, tacc(ob4) 0.93500, tacc(ob5) 0.78900, tacc(ob6) 0.94000, tacc(ob7) 0.89900, 10.26 secs\n",
      "\u001b[1m---- Epoch 22/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.43442, triplet_loss 0.43442, 92.21 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.91000, tacc(al2) 0.89300, tacc(ob0) 0.98400, tacc(ob1) 0.95200, tacc(ob2) 0.93100, tacc(ob3) 0.96300, tacc(ob4) 0.95300, tacc(ob5) 0.80300, tacc(ob6) 0.94200, tacc(ob7) 0.91600, 9.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_22_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9283.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 23/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.42896, triplet_loss 0.42896, 90.47 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97100, tacc(al1) 0.90900, tacc(al2) 0.89400, tacc(ob0) 0.98300, tacc(ob1) 0.96100, tacc(ob2) 0.92600, tacc(ob3) 0.96400, tacc(ob4) 0.95300, tacc(ob5) 0.79000, tacc(ob6) 0.94300, tacc(ob7) 0.91700, 9.85 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_23_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9284.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 24/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.42640, triplet_loss 0.42640, 93.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96700, tacc(al1) 0.91800, tacc(al2) 0.88000, tacc(ob0) 0.98500, tacc(ob1) 0.96100, tacc(ob2) 0.92200, tacc(ob3) 0.96300, tacc(ob4) 0.96000, tacc(ob5) 0.80400, tacc(ob6) 0.94600, tacc(ob7) 0.91700, 10.15 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_24_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9299.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 25/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "loss 0.42526, triplet_loss 0.42526, 92.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.91600, tacc(al2) 0.87900, tacc(ob0) 0.98500, tacc(ob1) 0.95900, tacc(ob2) 0.92700, tacc(ob3) 0.96300, tacc(ob4) 0.95800, tacc(ob5) 0.80100, tacc(ob6) 0.94700, tacc(ob7) 0.91500, 9.04 secs\n",
      "\u001b[1m---- Epoch 26/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.42615, triplet_loss 0.42615, 94.25 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.91700, tacc(al2) 0.88000, tacc(ob0) 0.98500, tacc(ob1) 0.96400, tacc(ob2) 0.92600, tacc(ob3) 0.96200, tacc(ob4) 0.96000, tacc(ob5) 0.80000, tacc(ob6) 0.94800, tacc(ob7) 0.91400, 10.36 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_26_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9299.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 27/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.42519, triplet_loss 0.42519, 94.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96700, tacc(al1) 0.91800, tacc(al2) 0.88200, tacc(ob0) 0.98500, tacc(ob1) 0.96300, tacc(ob2) 0.92600, tacc(ob3) 0.96200, tacc(ob4) 0.96100, tacc(ob5) 0.80100, tacc(ob6) 0.95000, tacc(ob7) 0.91500, 9.25 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_27_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9304.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 28/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42423, triplet_loss 0.42423, 93.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.91900, tacc(al2) 0.88100, tacc(ob0) 0.98500, tacc(ob1) 0.96400, tacc(ob2) 0.92700, tacc(ob3) 0.96300, tacc(ob4) 0.96200, tacc(ob5) 0.80600, tacc(ob6) 0.95000, tacc(ob7) 0.91300, 10.07 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_28_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9310.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 29/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000200) ...\n",
      "loss 0.61394, triplet_loss 0.61394, 93.41 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.30200, tacc(al1) 0.36600, tacc(al2) 0.33100, tacc(ob0) 0.32600, tacc(ob1) 0.33600, tacc(ob2) 0.30900, tacc(ob3) 0.32800, tacc(ob4) 0.32400, tacc(ob5) 0.31800, tacc(ob6) 0.30900, tacc(ob7) 0.30600, 9.99 secs\n",
      "\u001b[1m---- Epoch 30/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000094) ...\n",
      "loss 0.69312, triplet_loss 0.69312, 92.98 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.40100, tacc(al1) 0.37200, tacc(al2) 0.36200, tacc(ob0) 0.35800, tacc(ob1) 0.35700, tacc(ob2) 0.37800, tacc(ob3) 0.36900, tacc(ob4) 0.35700, tacc(ob5) 0.37200, tacc(ob6) 0.35700, tacc(ob7) 0.37300, 9.11 secs\n",
      "\u001b[1m---- Epoch 31/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000044) ...\n",
      "loss 0.69318, triplet_loss 0.69318, 92.68 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.31500, tacc(al1) 0.32600, tacc(al2) 0.30300, tacc(ob0) 0.31800, tacc(ob1) 0.30600, tacc(ob2) 0.31300, tacc(ob3) 0.32900, tacc(ob4) 0.33200, tacc(ob5) 0.26800, tacc(ob6) 0.30600, tacc(ob7) 0.29400, 10.18 secs\n",
      "\u001b[1m---- Epoch 32/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000021) ...\n",
      "loss 0.69316, triplet_loss 0.69316, 94.75 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.35300, tacc(al1) 0.33200, tacc(al2) 0.31200, tacc(ob0) 0.31900, tacc(ob1) 0.33800, tacc(ob2) 0.33900, tacc(ob3) 0.36600, tacc(ob4) 0.35200, tacc(ob5) 0.32100, tacc(ob6) 0.32700, tacc(ob7) 0.37500, 11.41 secs\n",
      "\u001b[1m---- Epoch 33/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000010) ...\n",
      "^C iteration 16100\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 675, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 586, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 364, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 302, in step_fn_wrapper\n",
      "    output = step_fn__triplet_ranking(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 72, in step_fn__triplet_ranking\n",
      "    gradient_accumulator.step(batch_loss)\n",
      "  File \"/home/pamessina/medvqa/medvqa/losses/optimizers.py\", line 26, in step\n",
      "    self.scaler.scale(batch_loss).backward()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 500 \\\n",
    "--batch_size 100 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 4 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 1., 1., 2.]}\" \\\n",
    "--triplets_weight 1.0 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 0 \\\n",
    "--entcon_weight 0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets-only)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4785ab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   epochs: 200\n",
      "   batches_per_epoch: 500\n",
      "   batch_size: 100\n",
      "   checkpoint_folder: None\n",
      "   huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "   embedding_size: 128\n",
      "   n_chest_imagenome_observations: None\n",
      "   n_chest_imagenome_anatomical_locations: None\n",
      "   use_aux_task_hidden_layer: False\n",
      "   aux_task_hidden_layer_size: None\n",
      "   nli_hidden_layer_size: None\n",
      "   pretrained_checkpoint_folder_path: /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_185515_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\n",
      "   freeze_huggingface_model: False\n",
      "   optimizer_name: adamw\n",
      "   lr: 1e-06\n",
      "   scheduler: exp-warmup+decay+cyclicdecay\n",
      "   lr_decay: 0.76\n",
      "   lr_decay_patience: 2\n",
      "   warmup_and_decay_args: None\n",
      "   warmup_and_cosine_args: None\n",
      "   warmup_decay_and_cyclic_decay_args: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "   iters_to_accumulate: 4\n",
      "   override_lr: False\n",
      "   triplet_loss_weight: 1.0\n",
      "   category_classif_loss_weight: 1.0\n",
      "   health_status_classif_loss_weight: 1.0\n",
      "   comparison_status_classif_loss_weight: 1.0\n",
      "   chest_imagenome_obs_classif_loss_weight: 1.0\n",
      "   chest_imagenome_anatloc_classif_loss_weight: 1.0\n",
      "   nli_loss_weight: 1.0\n",
      "   entcon_loss_weight: 1.0\n",
      "   triplets_filepath: /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\n",
      "   triplet_rule_weights: {'anatomical_locations': [1.0, 2.0, 1.0], 'observations': [1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0]}\n",
      "   integrated_facts_metadata_jsonl_filepath: None\n",
      "   paraphrases_jsonl_filepaths: None\n",
      "   integrated_chest_imagenome_observations_filepath: None\n",
      "   integrated_chest_imagenome_anatomical_locations_filepath: None\n",
      "   integrated_nli_jsonl_filepath: None\n",
      "   gpt4_radnli_labels_jsonl_filepath: None\n",
      "   dataset_name: MIMIC-CXR(triplets-only)\n",
      "   triplets_weight: 1.0\n",
      "   metadata_classification_weight: 0.0\n",
      "   chest_imagenome_observations_classification_weight: 0.0\n",
      "   chest_imagenome_anatomical_locations_classification_weight: 0.0\n",
      "   nli_weight: 0.0\n",
      "   entcon_weight: 0.0\n",
      "   num_workers: 3\n",
      "   device: GPU\n",
      "   use_amp: True\n",
      "   save: True\n",
      "\u001b[1m\u001b[34m----- Training model from scratch ------\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m1) \u001b[0m\u001b[1m\u001b[34mdevice =\u001b[0m \u001b[1m\u001b[34mcuda\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m2) \u001b[0m\u001b[1m\u001b[34mCreating instance of FactEncoder ...\u001b[0m\n",
      "Fact encoder\n",
      "  huggingface_model_name: microsoft/BiomedVLP-CXR-BERT-specialized\n",
      "  embedding_size: 128\n",
      "  classify_category: False\n",
      "  n_categories: 6\n",
      "  classify_health_status: False\n",
      "  n_health_statuses: 5\n",
      "  classify_comparison_status: False\n",
      "  n_comparison_statuses: 15\n",
      "  classify_chest_imagenome_obs: False\n",
      "  n_chest_imagenome_observations: None\n",
      "  classify_chest_imagenome_anatloc: False\n",
      "  n_chest_imagenome_anatomical_locations: None\n",
      "  use_aux_task_hidden_layer: False\n",
      "  aux_task_hidden_layer_size: None\n",
      "  do_nli: False\n",
      "  nli_hidden_layer_size: None\n",
      "\u001b[93m\u001b[1mWARNING: unused_kwargs: {'pretrained_checkpoint_folder_path': '/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_185515_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/'}\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m3) \u001b[0m\u001b[1m\u001b[34mDefining optimizer ...\u001b[0m\n",
      "create_optimizer(): name = adamw\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m4) \u001b[0m\u001b[1m\u001b[34mDefining scheduler ...\u001b[0m\n",
      "Using exp-warmup+decay+cyclicdecay scheduler: 1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\n",
      "1e-06 3 8e-05 8 1e-06 8e-05 8 1e-06\n",
      "self.steps_to_restart = 8\n",
      "self.steps = -1\n",
      "self.initial_lr = 8e-05\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m5) \u001b[0m\u001b[1m\u001b[34mCreating trainer and validator engines ...\u001b[0m\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "GradientAccumulator.__init__(): num_accumulation_steps = 4\n",
      "Focal_BCE_WBCE_Loss(): focal_weight = 0.3333333333333333 bce_weight = 0.3333333333333333 wbce_weight = 0.3333333333333333\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m6) \u001b[0m\u001b[1m\u001b[34mDefining collate batch functions ...\u001b[0m\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading triplets from /mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl...\n",
      "----\n",
      "\u001b[1mBuilding train triplet ranking dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 3440187\n",
      "\tWeight: 1.0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 3437513\n",
      "\tWeight: 2.0\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 3183923\n",
      "\tWeight: 1.0\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 4057081\n",
      "\tWeight: 1.0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 4056101\n",
      "\tWeight: 2.0\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 4427650\n",
      "\tWeight: 1.0\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 2602178\n",
      "\tWeight: 1.0\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 6247192\n",
      "\tWeight: 1.0\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 3462077\n",
      "\tWeight: 1.0\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 3172719\n",
      "\tWeight: 1.0\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 3044781\n",
      "\tWeight: 2.0\n",
      "----\n",
      "\u001b[1mBuilding val triplets dataset and dataloader...\u001b[0m\n",
      "anatomical_locations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al0\n",
      "anatomical_locations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al1\n",
      "anatomical_locations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: al2\n",
      "observations -> rule 0: \"Rank paraphrases very highly\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob0\n",
      "observations -> rule 1: \"Rank paraphrases very highly - Hard negative\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob1\n",
      "observations -> rule 2: \"Rank some triplets according to CXR-BERT and Leveinshtein consensus, while anchor and positive share the same health status\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob2\n",
      "observations -> rule 3: \"Hard health-status-vs-others triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob3\n",
      "observations -> rule 4: \"Short observation, detailed observation, and fact must be close to each other\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob4\n",
      "observations -> rule 5: \"Rank triplets according to Chest ImaGenome labels\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob5\n",
      "observations -> rule 6: \"RadGraph-based triplets\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob6\n",
      "observations -> rule 7: \"Hard negative triplets generated by ChatGPT\"\n",
      "\tNumber of triplets: 1000\n",
      "\tRule ID: ob7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(_train_dataloaders) = 1\n",
      "len(_train_weights) = 1\n",
      "_train_weights = [1.0]\n",
      "len(_val_dataloaders) = 1\n",
      "fact_embedding_trainer.name =  MIMIC-CXR(triplets-only)\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m7) \u001b[0m\u001b[1m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "NOTE: Using only validation metrics\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m8) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m9) \u001b[0m\u001b[1m\u001b[34mDefining learning rate scheduler handler ...\u001b[0m\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m10) \u001b[0m\u001b[1m\u001b[34mDefining checkpoint folder path ...\u001b[0m\n",
      "\u001b[1m\u001b[31mcheckpoint_folder_path =\u001b[0m \u001b[1m\u001b[31m/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_194923_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)\u001b[0m\n",
      "metadata saved to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_194923_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metadata.json\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m11) \u001b[0m\u001b[1m\u001b[34mLoading pretrained weights ...\u001b[0m\n",
      "checkpoint_names = ['checkpoint_28_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9310.pt']\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_185515_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/checkpoint_28_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9310.pt\n",
      "Checkpoint successfully loaded!\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m12) \u001b[0m\u001b[1m\u001b[34mDefining log_metrics_handler ...\u001b[0m\n",
      "MetricsLogger :: we'll be logging to /mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_194923_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/metrics_logs.csv\n",
      "\u001b[1m\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[1m\u001b[34m13) \u001b[0m\u001b[1m\u001b[34mRunning trainer engine ...\u001b[0m\n",
      "\u001b[1m---- Epoch 1/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.42402, triplet_loss 0.42402, 83.92 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96700, tacc(al1) 0.91800, tacc(al2) 0.88300, tacc(ob0) 0.98500, tacc(ob1) 0.96400, tacc(ob2) 0.92800, tacc(ob3) 0.96300, tacc(ob4) 0.96200, tacc(ob5) 0.80700, tacc(ob6) 0.95100, tacc(ob7) 0.91500, 6.75 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_1_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9314.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 2/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000004) ...\n",
      "loss 0.42412, triplet_loss 0.42412, 82.01 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96500, tacc(al1) 0.91900, tacc(al2) 0.88400, tacc(ob0) 0.98500, tacc(ob1) 0.96300, tacc(ob2) 0.92900, tacc(ob3) 0.96200, tacc(ob4) 0.96000, tacc(ob5) 0.80400, tacc(ob6) 0.95200, tacc(ob7) 0.91900, 6.65 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_2_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9316.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 3/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000019) ...\n",
      "loss 0.42509, triplet_loss 0.42509, 83.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96800, tacc(al1) 0.91400, tacc(al2) 0.88200, tacc(ob0) 0.98800, tacc(ob1) 0.96100, tacc(ob2) 0.92300, tacc(ob3) 0.96300, tacc(ob4) 0.95900, tacc(ob5) 0.79600, tacc(ob6) 0.95000, tacc(ob7) 0.91700, 6.81 secs\n",
      "\u001b[1m---- Epoch 4/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42698, triplet_loss 0.42698, 83.43 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97000, tacc(al1) 0.92000, tacc(al2) 0.87400, tacc(ob0) 0.98800, tacc(ob1) 0.95500, tacc(ob2) 0.92800, tacc(ob3) 0.96100, tacc(ob4) 0.95200, tacc(ob5) 0.80200, tacc(ob6) 0.95100, tacc(ob7) 0.91800, 6.76 secs\n",
      "\u001b[1m---- Epoch 5/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000046) ...\n",
      "loss 0.42426, triplet_loss 0.42426, 96.99 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92200, tacc(al2) 0.88200, tacc(ob0) 0.98700, tacc(ob1) 0.96600, tacc(ob2) 0.94000, tacc(ob3) 0.95900, tacc(ob4) 0.95200, tacc(ob5) 0.80000, tacc(ob6) 0.95200, tacc(ob7) 0.91600, 8.33 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_5_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9323.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 6/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000027) ...\n",
      "loss 0.42184, triplet_loss 0.42184, 84.10 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.91600, tacc(al2) 0.87700, tacc(ob0) 0.98800, tacc(ob1) 0.96400, tacc(ob2) 0.92800, tacc(ob3) 0.96200, tacc(ob4) 0.95300, tacc(ob5) 0.80100, tacc(ob6) 0.95800, tacc(ob7) 0.91800, 7.69 secs\n",
      "\u001b[1m---- Epoch 7/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000015) ...\n",
      "loss 0.42118, triplet_loss 0.42118, 102.63 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92100, tacc(al2) 0.87400, tacc(ob0) 0.98900, tacc(ob1) 0.96300, tacc(ob2) 0.92600, tacc(ob3) 0.96700, tacc(ob4) 0.95400, tacc(ob5) 0.80400, tacc(ob6) 0.95200, tacc(ob7) 0.92200, 9.00 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_7_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9323.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 8/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000009) ...\n",
      "loss 0.42092, triplet_loss 0.42092, 95.39 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92000, tacc(al2) 0.88100, tacc(ob0) 0.98700, tacc(ob1) 0.96100, tacc(ob2) 0.93300, tacc(ob3) 0.96500, tacc(ob4) 0.95200, tacc(ob5) 0.80900, tacc(ob6) 0.95500, tacc(ob7) 0.92400, 16.06 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_8_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9332.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 9/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000005) ...\n",
      "loss 0.41950, triplet_loss 0.41950, 134.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.91800, tacc(al2) 0.88200, tacc(ob0) 0.99100, tacc(ob1) 0.96400, tacc(ob2) 0.92500, tacc(ob3) 0.96300, tacc(ob4) 0.95100, tacc(ob5) 0.80800, tacc(ob6) 0.95400, tacc(ob7) 0.92200, 16.49 secs\n",
      "\u001b[1m---- Epoch 10/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.42014, triplet_loss 0.42014, 136.00 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92000, tacc(al2) 0.88300, tacc(ob0) 0.99100, tacc(ob1) 0.96400, tacc(ob2) 0.92400, tacc(ob3) 0.96500, tacc(ob4) 0.95300, tacc(ob5) 0.80700, tacc(ob6) 0.95600, tacc(ob7) 0.92200, 8.87 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_10_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9333.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 11/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41974, triplet_loss 0.41974, 86.76 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92100, tacc(al2) 0.88100, tacc(ob0) 0.99100, tacc(ob1) 0.96400, tacc(ob2) 0.92500, tacc(ob3) 0.96500, tacc(ob4) 0.95400, tacc(ob5) 0.81200, tacc(ob6) 0.95900, tacc(ob7) 0.92300, 8.45 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_11_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9341.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 12/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41901, triplet_loss 0.41901, 79.52 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.92200, tacc(al2) 0.87900, tacc(ob0) 0.99000, tacc(ob1) 0.96400, tacc(ob2) 0.92500, tacc(ob3) 0.96400, tacc(ob4) 0.95200, tacc(ob5) 0.80800, tacc(ob6) 0.95800, tacc(ob7) 0.92100, 9.75 secs\n",
      "\u001b[1m---- Epoch 13/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42143, triplet_loss 0.42143, 87.04 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97400, tacc(al1) 0.92100, tacc(al2) 0.87700, tacc(ob0) 0.98800, tacc(ob1) 0.96000, tacc(ob2) 0.94000, tacc(ob3) 0.95900, tacc(ob4) 0.95700, tacc(ob5) 0.81400, tacc(ob6) 0.95700, tacc(ob7) 0.92100, 9.72 secs\n",
      "\u001b[1m---- Epoch 14/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.42090, triplet_loss 0.42090, 89.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.91600, tacc(al2) 0.87400, tacc(ob0) 0.98900, tacc(ob1) 0.96900, tacc(ob2) 0.93200, tacc(ob3) 0.96300, tacc(ob4) 0.95300, tacc(ob5) 0.80300, tacc(ob6) 0.95300, tacc(ob7) 0.92100, 9.28 secs\n",
      "\u001b[1m---- Epoch 15/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41864, triplet_loss 0.41864, 87.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98000, tacc(al1) 0.91900, tacc(al2) 0.88900, tacc(ob0) 0.99200, tacc(ob1) 0.96600, tacc(ob2) 0.93300, tacc(ob3) 0.96300, tacc(ob4) 0.95100, tacc(ob5) 0.81100, tacc(ob6) 0.95600, tacc(ob7) 0.92500, 10.26 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_15_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9354.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 16/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41823, triplet_loss 0.41823, 91.71 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92100, tacc(al2) 0.87700, tacc(ob0) 0.99200, tacc(ob1) 0.96300, tacc(ob2) 0.93100, tacc(ob3) 0.96300, tacc(ob4) 0.95100, tacc(ob5) 0.80700, tacc(ob6) 0.95300, tacc(ob7) 0.92600, 10.18 secs\n",
      "\u001b[1m---- Epoch 17/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41771, triplet_loss 0.41771, 92.19 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92800, tacc(al2) 0.87800, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.92800, tacc(ob3) 0.96300, tacc(ob4) 0.95100, tacc(ob5) 0.81000, tacc(ob6) 0.95400, tacc(ob7) 0.92200, 9.55 secs\n",
      "\u001b[1m---- Epoch 18/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41731, triplet_loss 0.41731, 92.06 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92500, tacc(al2) 0.87500, tacc(ob0) 0.99200, tacc(ob1) 0.96500, tacc(ob2) 0.92700, tacc(ob3) 0.96400, tacc(ob4) 0.94800, tacc(ob5) 0.79800, tacc(ob6) 0.95500, tacc(ob7) 0.92100, 10.25 secs\n",
      "\u001b[1m---- Epoch 19/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41726, triplet_loss 0.41726, 90.45 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.92700, tacc(al2) 0.87600, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.92600, tacc(ob3) 0.96400, tacc(ob4) 0.94900, tacc(ob5) 0.79400, tacc(ob6) 0.95400, tacc(ob7) 0.92500, 9.62 secs\n",
      "\u001b[1m---- Epoch 20/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41674, triplet_loss 0.41674, 92.80 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92600, tacc(al2) 0.87600, tacc(ob0) 0.99100, tacc(ob1) 0.96600, tacc(ob2) 0.92500, tacc(ob3) 0.96600, tacc(ob4) 0.95000, tacc(ob5) 0.79600, tacc(ob6) 0.95300, tacc(ob7) 0.92000, 9.91 secs\n",
      "\u001b[1m---- Epoch 21/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.42023, triplet_loss 0.42023, 92.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.96600, tacc(al1) 0.92500, tacc(al2) 0.86600, tacc(ob0) 0.99400, tacc(ob1) 0.95500, tacc(ob2) 0.91600, tacc(ob3) 0.96500, tacc(ob4) 0.94700, tacc(ob5) 0.79900, tacc(ob6) 0.95200, tacc(ob7) 0.91300, 9.75 secs\n",
      "\u001b[1m---- Epoch 22/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.41798, triplet_loss 0.41798, 93.11 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97200, tacc(al1) 0.92700, tacc(al2) 0.85600, tacc(ob0) 0.99300, tacc(ob1) 0.95800, tacc(ob2) 0.93100, tacc(ob3) 0.96200, tacc(ob4) 0.95100, tacc(ob5) 0.80500, tacc(ob6) 0.95000, tacc(ob7) 0.92200, 10.03 secs\n",
      "\u001b[1m---- Epoch 23/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41694, triplet_loss 0.41694, 91.74 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92700, tacc(al2) 0.86500, tacc(ob0) 0.99100, tacc(ob1) 0.96200, tacc(ob2) 0.93300, tacc(ob3) 0.96200, tacc(ob4) 0.95000, tacc(ob5) 0.80400, tacc(ob6) 0.95400, tacc(ob7) 0.91800, 10.28 secs\n",
      "\u001b[1m---- Epoch 24/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41528, triplet_loss 0.41528, 95.07 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93400, tacc(al2) 0.86900, tacc(ob0) 0.99400, tacc(ob1) 0.96200, tacc(ob2) 0.92400, tacc(ob3) 0.96100, tacc(ob4) 0.94800, tacc(ob5) 0.80100, tacc(ob6) 0.95200, tacc(ob7) 0.92500, 9.86 secs\n",
      "\u001b[1m---- Epoch 25/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41500, triplet_loss 0.41500, 110.38 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93500, tacc(al2) 0.86700, tacc(ob0) 0.99400, tacc(ob1) 0.96200, tacc(ob2) 0.92600, tacc(ob3) 0.96200, tacc(ob4) 0.94700, tacc(ob5) 0.80500, tacc(ob6) 0.95300, tacc(ob7) 0.92300, 9.15 secs\n",
      "\u001b[1m---- Epoch 26/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41524, triplet_loss 0.41524, 92.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93200, tacc(al2) 0.86700, tacc(ob0) 0.99400, tacc(ob1) 0.96300, tacc(ob2) 0.93100, tacc(ob3) 0.96200, tacc(ob4) 0.94600, tacc(ob5) 0.80500, tacc(ob6) 0.95300, tacc(ob7) 0.92600, 9.87 secs\n",
      "\u001b[1m---- Epoch 27/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41518, triplet_loss 0.41518, 98.14 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93300, tacc(al2) 0.86800, tacc(ob0) 0.99400, tacc(ob1) 0.96200, tacc(ob2) 0.93200, tacc(ob3) 0.96200, tacc(ob4) 0.94900, tacc(ob5) 0.80700, tacc(ob6) 0.95200, tacc(ob7) 0.92600, 10.22 secs\n",
      "\u001b[1m---- Epoch 28/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41462, triplet_loss 0.41462, 93.16 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97500, tacc(al1) 0.93300, tacc(al2) 0.86700, tacc(ob0) 0.99400, tacc(ob1) 0.96200, tacc(ob2) 0.93200, tacc(ob3) 0.96300, tacc(ob4) 0.94900, tacc(ob5) 0.81200, tacc(ob6) 0.95400, tacc(ob7) 0.92600, 10.58 secs\n",
      "\u001b[1m---- Epoch 29/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41877, triplet_loss 0.41877, 94.25 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98100, tacc(al1) 0.92200, tacc(al2) 0.87300, tacc(ob0) 0.99200, tacc(ob1) 0.96900, tacc(ob2) 0.93400, tacc(ob3) 0.96500, tacc(ob4) 0.94400, tacc(ob5) 0.79200, tacc(ob6) 0.95100, tacc(ob7) 0.91500, 10.31 secs\n",
      "\u001b[1m---- Epoch 30/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.41738, triplet_loss 0.41738, 97.27 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.91800, tacc(al2) 0.86600, tacc(ob0) 0.99200, tacc(ob1) 0.97100, tacc(ob2) 0.93200, tacc(ob3) 0.96700, tacc(ob4) 0.94000, tacc(ob5) 0.80500, tacc(ob6) 0.95300, tacc(ob7) 0.91600, 9.63 secs\n",
      "\u001b[1m---- Epoch 31/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n",
      "loss 0.41591, triplet_loss 0.41591, 90.90 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97300, tacc(al1) 0.92700, tacc(al2) 0.86600, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.92500, tacc(ob3) 0.96600, tacc(ob4) 0.94100, tacc(ob5) 0.80400, tacc(ob6) 0.95400, tacc(ob7) 0.92000, 9.27 secs\n",
      "\u001b[1m---- Epoch 32/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41514, triplet_loss 0.41514, 91.96 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92800, tacc(al2) 0.86800, tacc(ob0) 0.99300, tacc(ob1) 0.96800, tacc(ob2) 0.92600, tacc(ob3) 0.96500, tacc(ob4) 0.94200, tacc(ob5) 0.81100, tacc(ob6) 0.95400, tacc(ob7) 0.91600, 9.50 secs\n",
      "\u001b[1m---- Epoch 33/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41367, triplet_loss 0.41367, 90.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.92500, tacc(al2) 0.87000, tacc(ob0) 0.99100, tacc(ob1) 0.96600, tacc(ob2) 0.92300, tacc(ob3) 0.96700, tacc(ob4) 0.95000, tacc(ob5) 0.80700, tacc(ob6) 0.95900, tacc(ob7) 0.91800, 10.12 secs\n",
      "\u001b[1m---- Epoch 34/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41299, triplet_loss 0.41299, 93.24 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92500, tacc(al2) 0.86900, tacc(ob0) 0.99100, tacc(ob1) 0.96700, tacc(ob2) 0.92400, tacc(ob3) 0.96600, tacc(ob4) 0.94500, tacc(ob5) 0.81000, tacc(ob6) 0.95700, tacc(ob7) 0.91700, 10.45 secs\n",
      "\u001b[1m---- Epoch 35/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41339, triplet_loss 0.41339, 102.05 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92600, tacc(al2) 0.87200, tacc(ob0) 0.99100, tacc(ob1) 0.96600, tacc(ob2) 0.92700, tacc(ob3) 0.96500, tacc(ob4) 0.94700, tacc(ob5) 0.80900, tacc(ob6) 0.95800, tacc(ob7) 0.91700, 18.95 secs\n",
      "\u001b[1m---- Epoch 36/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41326, triplet_loss 0.41326, 158.54 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92600, tacc(al2) 0.87000, tacc(ob0) 0.99100, tacc(ob1) 0.96500, tacc(ob2) 0.92800, tacc(ob3) 0.96500, tacc(ob4) 0.94700, tacc(ob5) 0.81100, tacc(ob6) 0.95800, tacc(ob7) 0.91500, 8.42 secs\n",
      "\u001b[1m---- Epoch 37/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "loss 0.41818, triplet_loss 0.41818, 86.90 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.92700, tacc(al2) 0.86300, tacc(ob0) 0.99300, tacc(ob1) 0.96800, tacc(ob2) 0.92200, tacc(ob3) 0.96700, tacc(ob4) 0.94700, tacc(ob5) 0.79900, tacc(ob6) 0.96100, tacc(ob7) 0.91800, 8.65 secs\n",
      "\u001b[1m---- Epoch 38/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000043) ...\n",
      "loss 0.41611, triplet_loss 0.41611, 84.80 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.92600, tacc(al2) 0.86600, tacc(ob0) 0.99100, tacc(ob1) 0.96800, tacc(ob2) 0.92200, tacc(ob3) 0.96500, tacc(ob4) 0.95000, tacc(ob5) 0.79300, tacc(ob6) 0.94800, tacc(ob7) 0.92200, 7.61 secs\n",
      "\u001b[1m---- Epoch 39/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000023) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.41449, triplet_loss 0.41449, 87.79 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.98000, tacc(al1) 0.93200, tacc(al2) 0.86700, tacc(ob0) 0.99000, tacc(ob1) 0.96500, tacc(ob2) 0.91900, tacc(ob3) 0.97000, tacc(ob4) 0.95600, tacc(ob5) 0.79400, tacc(ob6) 0.95300, tacc(ob7) 0.92300, 9.15 secs\n",
      "\u001b[1m---- Epoch 40/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000012) ...\n",
      "loss 0.41354, triplet_loss 0.41354, 93.89 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97600, tacc(al1) 0.93400, tacc(al2) 0.86200, tacc(ob0) 0.99000, tacc(ob1) 0.96900, tacc(ob2) 0.93000, tacc(ob3) 0.97300, tacc(ob4) 0.95400, tacc(ob5) 0.81500, tacc(ob6) 0.95500, tacc(ob7) 0.91800, 18.82 secs\n",
      "\u001b[1m\u001b[31mNew checkpoint saved: checkpoint_40_ta0)+ta1)+ta2)+ta0)+ta1)+ta2)+ta3)+ta4)+ta5)+ta6)+ta7)=0.9355.pt\u001b[0m\n",
      "\u001b[1m---- Epoch 41/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000007) ...\n",
      "loss 0.41229, triplet_loss 0.41229, 135.57 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93100, tacc(al2) 0.86100, tacc(ob0) 0.99000, tacc(ob1) 0.96800, tacc(ob2) 0.92600, tacc(ob3) 0.96800, tacc(ob4) 0.95000, tacc(ob5) 0.79900, tacc(ob6) 0.95200, tacc(ob7) 0.92000, 8.96 secs\n",
      "\u001b[1m---- Epoch 42/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000003) ...\n",
      "loss 0.41131, triplet_loss 0.41131, 95.03 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97900, tacc(al1) 0.93100, tacc(al2) 0.86400, tacc(ob0) 0.99200, tacc(ob1) 0.96900, tacc(ob2) 0.92900, tacc(ob3) 0.96800, tacc(ob4) 0.95200, tacc(ob5) 0.80900, tacc(ob6) 0.95400, tacc(ob7) 0.92200, 7.62 secs\n",
      "\u001b[1m---- Epoch 43/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000002) ...\n",
      "loss 0.41031, triplet_loss 0.41031, 84.82 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97800, tacc(al1) 0.93100, tacc(al2) 0.86600, tacc(ob0) 0.99200, tacc(ob1) 0.97000, tacc(ob2) 0.92800, tacc(ob3) 0.96700, tacc(ob4) 0.95400, tacc(ob5) 0.80900, tacc(ob6) 0.95300, tacc(ob7) 0.92100, 9.04 secs\n",
      "\u001b[1m---- Epoch 44/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000001) ...\n",
      "loss 0.41236, triplet_loss 0.41236, 85.60 secs\n",
      "(2) Validation stage ...\n",
      "tacc(al0) 0.97700, tacc(al1) 0.93200, tacc(al2) 0.86500, tacc(ob0) 0.99200, tacc(ob1) 0.96900, tacc(ob2) 0.92700, tacc(ob3) 0.96900, tacc(ob4) 0.95300, tacc(ob5) 0.81000, tacc(ob6) 0.95400, tacc(ob7) 0.91900, 8.84 secs\n",
      "\u001b[1m---- Epoch 45/200\u001b[0m\n",
      "(1) Training stage (lr = 0.000080) ...\n",
      "^C iteration 22350\n",
      "Engine run is terminating due to exception: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 675, in <module>\n",
      "    train_from_scratch(**args)\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 586, in train_from_scratch\n",
      "    return train_model(\n",
      "  File \"/home/pamessina/medvqa/medvqa/notebooks/../train_fact_embedding.py\", line 364, in train_model\n",
      "    run_common_boilerplate_code_and_start_training(\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/utils.py\", line 121, in run_common_boilerplate_code_and_start_training\n",
      "    trainer_engine.run(train_dataloader, max_epochs=epochs, epoch_length=batches_per_epoch)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 892, in run\n",
      "    return self._internal_run()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 935, in _internal_run\n",
      "    return next(self._internal_run_generator)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n",
      "    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n",
      "    self.state.output = self._process_function(self, self.state.batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 302, in step_fn_wrapper\n",
      "    output = step_fn__triplet_ranking(batch)\n",
      "  File \"/home/pamessina/medvqa/medvqa/training/fact_embedding.py\", line 56, in step_fn__triplet_ranking\n",
      "    a_embeddings = model(input_ids=a_input_ids, attention_mask=a_attention_mask)['text_embeddings']\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/medvqa/medvqa/models/nlp/fact_encoder.py\", line 124, in forward\n",
      "    text_embeddings = self.model.get_projected_text_embeddings(input_ids=input_ids, attention_mask=attention_mask)\n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 123, in get_projected_text_embeddings\n",
      "    outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
      "  File \"/home/pamessina/.cache/huggingface/modules/transformers_modules/microsoft/BiomedVLP-CXR-BERT-specialized/b59c09e51ab2410b24f4be214bbb49043fe63fc2/modeling_cxrbert.py\", line 84, in forward\n",
      "    bert_for_masked_lm_output = super().forward(input_ids=input_ids,\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1358, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1020, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 610, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 495, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 425, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 284, in forward\n",
      "    mixed_query_layer = self.query(hidden_states)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/pamessina/venv2/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 113, in forward\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../train_fact_embedding.py \\\n",
    "--pretrained_checkpoint_folder_path \\\n",
    "\"/mnt/data/pamessina/workspaces/medvqa-workspace/models/fact_embedding/20230927_185515_MIMIC-CXR(triplets-only)_FactEncoder(microsoft-BiomedVLP-CXR-BERT-specialized)/\" \\\n",
    "--epochs 200 \\\n",
    "--batches_per_epoch 500 \\\n",
    "--batch_size 100 \\\n",
    "--num_workers 3 \\\n",
    "--iters_to_accumulate 4 \\\n",
    "--optimizer_name \"adamw\" \\\n",
    "--scheduler \"exp-warmup+decay+cyclicdecay\" \\\n",
    "--lr 1e-6 \\\n",
    "--warmup_decay_and_cyclic_decay_args \"1e-6,3,8e-5,8,1e-6,8e-5,8,1e-6\" \\\n",
    "--triplets_filepath \\\n",
    "\"/mnt/workspace/pamessina/medvqa-workspace/cache/mimiccxr/triplets(4374074,318620,2395255,3002000,1000,1000,274,1091231605151220188).pkl\" \\\n",
    "--triplet_rule_weights \\\n",
    "\"{'anatomical_locations': [1., 2., 1.], 'observations': [1., 2., 1., 1., 1., 1., 1., 2.]}\" \\\n",
    "--triplets_weight 1.0 \\\n",
    "--metadata_classification_weight 0 \\\n",
    "--chest_imagenome_observations_classification_weight 0 \\\n",
    "--chest_imagenome_anatomical_locations_classification_weight 0 \\\n",
    "--nli_weight 0 \\\n",
    "--entcon_weight 0 \\\n",
    "--dataset_name \"MIMIC-CXR(triplets-only)\" \\\n",
    "--huggingface_model_name \"microsoft/BiomedVLP-CXR-BERT-specialized\" \\\n",
    "--embedding_size 128 \\\n",
    "--use_amp \\\n",
    "--save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
