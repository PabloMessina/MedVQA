{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: ground-truth\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 5\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=0\n",
      "checkpoint_names = ['checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4885.pt', 'checkpoint_27_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4564.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4885.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4885.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = ground-truth\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=ground-truth).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 32697, len(set(self.report_ids)) = 3258\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 32697, len(set(self.report_ids)) = 3258\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.mask_token', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.key.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 32697\n",
      "len(dataloader) = 205\n",
      "Evaluating model ...\n",
      "oracc 0.97961, chxlmicf1 0.48655, chxlmacf1 0.40516, chxlacc 0.59688, chxlrocaucmic 0.69589, chxlrocaucmac 0.63301, qlmacf1 0.16492, 109.16 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3258, len(gt_reports)=3258\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3258 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3258 texts ...\n",
      "Chexpert labeler: running a maximum of 5 concurrent processes over 5 chunks\n",
      "chunk: i=0, b=0, e=647, chunk_size=647\n",
      "chunk: i=1, b=647, e=1294, chunk_size=647\n",
      "chunk: i=2, b=1294, e=1941, chunk_size=647\n",
      "chunk: i=3, b=1941, e=2588, chunk_size=647\n",
      "chunk: i=4, b=2588, e=3235, chunk_size=644\n",
      "\t#### process 1: running chexpert labeler over 647 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114322_0.7153357258680196_0.csv --output_path /data/labeler-output_20230108_114322_0.7153357258680196_0.csv\n",
      "\t#### process 2: running chexpert labeler over 647 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114322_0.7153357258680196_1.csv --output_path /data/labeler-output_20230108_114322_0.7153357258680196_1.csv\n",
      "\t#### process 3: running chexpert labeler over 647 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114322_0.7153357258680196_2.csv --output_path /data/labeler-output_20230108_114322_0.7153357258680196_2.csv\n",
      "\t#### process 4: running chexpert labeler over 647 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114322_0.7153357258680196_3.csv --output_path /data/labeler-output_20230108_114322_0.7153357258680196_3.csv\n",
      "\t#### process 5: running chexpert labeler over 644 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114322_0.7153357258680196_4.csv --output_path /data/labeler-output_20230108_114322_0.7153357258680196_4.csv\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 596.945540189743\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 2 finished, elapsed time = 600.6596949100494\n",
      "\t**** process 3 finished, elapsed time = 600.6597564220428\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 4 finished, elapsed time = 609.0710296630859\n",
      "\t**** process 5 finished, elapsed time = 609.0710940361023\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=ground-truth).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"ground-truth\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: ground-truth\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 5\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using default transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [224, 224], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=0\n",
      "checkpoint_names = ['checkpoint_48_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4796.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_48_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4796.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_48_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4796.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = ground-truth\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=ground-truth).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 32697, len(set(self.report_ids)) = 3258\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 32697, len(set(self.report_ids)) = 3258\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-large were not used when initializing ViTModel: ['decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.mask_token', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-large and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 1024\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-large+onehot+transf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 32697\n",
      "len(dataloader) = 205\n",
      "Evaluating model ...\n",
      "oracc 0.98481, chxlmicf1 0.49119, chxlmacf1 0.41089, chxlacc 0.60412, chxlrocaucmic 0.70105, chxlrocaucmac 0.64330, qlmacf1 0.16852, 208.92 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3258, len(gt_reports)=3258\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3258 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3258 texts ...\n",
      "Chexpert labeler: running a maximum of 5 concurrent processes over 5 chunks\n",
      "chunk: i=0, b=0, e=649, chunk_size=649\n",
      "chunk: i=1, b=649, e=1298, chunk_size=649\n",
      "chunk: i=2, b=1298, e=1947, chunk_size=649\n",
      "chunk: i=3, b=1947, e=2596, chunk_size=649\n",
      "chunk: i=4, b=2596, e=3245, chunk_size=645\n",
      "\t#### process 1: running chexpert labeler over 649 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_115808_0.5123784305041952_0.csv --output_path /data/labeler-output_20230108_115808_0.5123784305041952_0.csv\n",
      "\t#### process 2: running chexpert labeler over 649 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_115808_0.5123784305041952_1.csv --output_path /data/labeler-output_20230108_115808_0.5123784305041952_1.csv\n",
      "\t#### process 3: running chexpert labeler over 649 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_115808_0.5123784305041952_2.csv --output_path /data/labeler-output_20230108_115808_0.5123784305041952_2.csv\n",
      "\t#### process 4: running chexpert labeler over 649 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_115808_0.5123784305041952_3.csv --output_path /data/labeler-output_20230108_115808_0.5123784305041952_3.csv\n",
      "\t#### process 5: running chexpert labeler over 645 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_115808_0.5123784305041952_4.csv --output_path /data/labeler-output_20230108_115808_0.5123784305041952_4.csv\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 452.9035415649414\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 2 finished, elapsed time = 470.53508496284485\n",
      "\t**** process 3 finished, elapsed time = 470.53516960144043\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 4 finished, elapsed time = 475.5699119567871\n",
      "\t**** process 5 finished, elapsed time = 475.5700044631958\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=ground-truth).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"ground-truth\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp\n",
      "   eval_mode: ground-truth\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 5\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=0\n",
      "checkpoint_names = ['checkpoint_58_chf1+chf1+cD+ema+gacc+orcc+qlf1+qlf1+wmmp=0.5791.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/checkpoint_58_chf1+chf1+cD+ema+gacc+orcc+qlf1+qlf1+wmmp=0.5791.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/checkpoint_58_chf1+chf1+cD+ema+gacc+orcc+qlf1+qlf1+wmmp=0.5791.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = ground-truth\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=ground-truth).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 32697, len(set(self.report_ids)) = 3258\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 32697, len(set(self.report_ids)) = 3258\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.mask_token', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.query.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  self.global_feat_size = 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 32697\n",
      "len(dataloader) = 205\n",
      "Evaluating model ...\n",
      "oracc 0.99683, chxlmicf1 0.58966, chxlmacf1 0.49034, chxlacc 0.71951, chxlrocaucmic 0.81089, chxlrocaucmac 0.76090, qlmacf1 0.22098, 93.88 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3258, len(gt_reports)=3258\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3258 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3258 texts ...\n",
      "Chexpert labeler: running a maximum of 5 concurrent processes over 5 chunks\n",
      "chunk: i=0, b=0, e=651, chunk_size=651\n",
      "chunk: i=1, b=651, e=1302, chunk_size=651\n",
      "chunk: i=2, b=1302, e=1953, chunk_size=651\n",
      "chunk: i=3, b=1953, e=2604, chunk_size=651\n",
      "chunk: i=4, b=2604, e=3255, chunk_size=650\n",
      "\t#### process 1: running chexpert labeler over 651 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_214605_0.8741520599030584_0.csv --output_path /data/labeler-output_20230108_214605_0.8741520599030584_0.csv\n",
      "\t#### process 2: running chexpert labeler over 651 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_214605_0.8741520599030584_1.csv --output_path /data/labeler-output_20230108_214605_0.8741520599030584_1.csv\n",
      "\t#### process 3: running chexpert labeler over 651 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_214605_0.8741520599030584_2.csv --output_path /data/labeler-output_20230108_214605_0.8741520599030584_2.csv\n",
      "\t#### process 4: running chexpert labeler over 651 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_214605_0.8741520599030584_3.csv --output_path /data/labeler-output_20230108_214605_0.8741520599030584_3.csv\n",
      "\t#### process 5: running chexpert labeler over 650 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_214605_0.8741520599030584_4.csv --output_path /data/labeler-output_20230108_214605_0.8741520599030584_4.csv\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 626.2658088207245\n",
      "\t**** process 2 finished, elapsed time = 626.2659540176392\n",
      "\t**** process 3 finished, elapsed time = 626.2659940719604\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 4 finished, elapsed time = 641.2872252464294\n",
      "\t**** process 5 finished, elapsed time = 641.28728723526\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=ground-truth).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"ground-truth\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp\n",
      "   eval_mode: chexpert-labels\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 4\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=97\n",
      "checkpoint_names = ['checkpoint_58_chf1+chf1+cD+ema+gacc+orcc+qlf1+qlf1+wmmp=0.5791.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/checkpoint_58_chf1+chf1+cD+ema+gacc+orcc+qlf1+qlf1+wmmp=0.5791.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/checkpoint_58_chf1+chf1+cD+ema+gacc+orcc+qlf1+qlf1+wmmp=0.5791.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = chexpert-labels\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 45766, len(set(self.report_ids)) = 3269\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 45766, len(set(self.report_ids)) = 3269\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.mask_token', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight']\r\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  self.global_feat_size = 768\r\n",
      "  n_questions = 350\r\n",
      "  n_questions_aux_task = 97\r\n",
      "  question_encoding = one-hot\r\n",
      "  answer_decoding = transformer\r\n",
      "  visual_input_mode = raw-image\r\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 45766\n",
      "len(dataloader) = 287\n",
      "Evaluating model ...\n",
      "oracc 0.99638, chxlmicf1 0.56838, chxlmacf1 0.47212, chxlacc 0.71573, chxlrocaucmic 0.80948, chxlrocaucmac 0.75720, qlmacf1 0.20391, 129.38 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3269, len(gt_reports)=3269\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "Chexpert labeler: running a maximum of 4 concurrent processes over 4 chunks\n",
      "chunk: i=0, b=0, e=176, chunk_size=176\n",
      "chunk: i=1, b=176, e=352, chunk_size=176\n",
      "chunk: i=2, b=352, e=528, chunk_size=176\n",
      "chunk: i=3, b=528, e=704, chunk_size=176\n",
      "\t#### process 1: running chexpert labeler over 176 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_215946_0.6101929474009878_0.csv --output_path /data/labeler-output_20230108_215946_0.6101929474009878_0.csv\n",
      "\t#### process 2: running chexpert labeler over 176 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_215946_0.6101929474009878_1.csv --output_path /data/labeler-output_20230108_215946_0.6101929474009878_1.csv\n",
      "\t#### process 3: running chexpert labeler over 176 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_215946_0.6101929474009878_2.csv --output_path /data/labeler-output_20230108_215946_0.6101929474009878_2.csv\n",
      "\t#### process 4: running chexpert labeler over 176 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_215946_0.6101929474009878_3.csv --output_path /data/labeler-output_20230108_215946_0.6101929474009878_3.csv\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 84.75285959243774\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "\t**** process 2 finished, elapsed time = 86.5193383693695\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "\t**** process 3 finished, elapsed time = 86.72585964202881\n",
      "\t**** process 4 finished, elapsed time = 87.15587019920349\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=chexpert-labels).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230108_144019_mim+mim(chex)+iu+iu(chex)+chexp(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.6_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"chexpert-labels\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
