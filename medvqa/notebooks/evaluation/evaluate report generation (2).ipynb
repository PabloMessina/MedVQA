{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: models/vqa/20221231_010452_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: chexpert-labels\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 4\n",
      "   save_for_error_analysis: True\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_010452_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using default transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [256, 256], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=97\n",
      "checkpoint_names = ['checkpoint_81_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5723.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_010452_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_81_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5723.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_010452_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_81_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5723.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = chexpert-labels\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl ...\n",
      "\tNo, it isn't :(\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/qa_adapted_reports__20220904_095810.json\n",
      "Obtaining basic data ...\n",
      "   basic data loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_basic_data(dataset=qa_adapted_reports__20220904_095810.json).pkl\n",
      "_precompute_questions_per_report():\n",
      "   len(data) = 3269\n",
      "   questions per report data saved to /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/questions_per_report(report_eval_mode=chexpert-labels;dataset=qa_adapted_reports__20220904_095810.json).pkl\n",
      "Preprocessing MIMIC-CXR vqa dataset ...\n",
      "100%|████████████████████████████████████| 3269/3269 [00:00<00:00, 38707.11it/s]\n",
      "saving data to /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl\n",
      "\tdone!\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 45766, len(set(self.report_ids)) = 3269\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 45766, len(set(self.report_ids)) = 3269\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "create_densenet121_feature_extractor()\n",
      "   drop_rate: 0.0\n",
      "DenseNet121's pretrained weights loaded from ImageNet\n",
      "  self.global_feat_size = 2048\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(dense121+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 45766\n",
      "len(dataloader) = 287\n",
      "Evaluating model ...\n",
      "oracc 0.99704, chxlmicf1 0.56875, chxlmacf1 0.47228, chxlacc 0.71182, chxlrocaucmic 0.81297, chxlrocaucmac 0.75859, qlmacf1 0.20616, 120.95 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3269, len(gt_reports)=3269\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "Chexpert labeler: running a maximum of 4 concurrent processes over 4 chunks\n",
      "chunk: i=0, b=0, e=172, chunk_size=172\n",
      "chunk: i=1, b=172, e=344, chunk_size=172\n",
      "chunk: i=2, b=344, e=516, chunk_size=172\n",
      "chunk: i=3, b=516, e=688, chunk_size=169\n",
      "\t#### process 1: running chexpert labeler over 172 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_102435_0.665393730348625_0.csv --output_path /data/labeler-output_20230108_102435_0.665393730348625_0.csv\n",
      "\t#### process 2: running chexpert labeler over 172 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_102435_0.665393730348625_1.csv --output_path /data/labeler-output_20230108_102435_0.665393730348625_1.csv\n",
      "\t#### process 3: running chexpert labeler over 172 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_102435_0.665393730348625_2.csv --output_path /data/labeler-output_20230108_102435_0.665393730348625_2.csv\n",
      "\t#### process 4: running chexpert labeler over 169 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_102435_0.665393730348625_3.csv --output_path /data/labeler-output_20230108_102435_0.665393730348625_3.csv\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 90.73855638504028\n",
      "\t**** process 2 finished, elapsed time = 90.73870968818665\n",
      "\t**** process 3 finished, elapsed time = 90.73877215385437\n",
      "\t**** process 4 finished, elapsed time = 90.96960973739624\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20221231_010452_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=chexpert-labels).pkl\n",
      "Report-level results for error analysis successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20221231_010452_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_results_for_error_analysis(eval_mode=chexpert-labels).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"models/vqa/20221231_010452_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(dense121+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"chexpert-labels\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 4 \\\n",
    "        --save-for-error-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_012821_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: chexpert-labels\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 4\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_012821_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=97\n",
      "checkpoint_names = ['checkpoint_96_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5551.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_012821_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_96_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5551.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_012821_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_96_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5551.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = chexpert-labels\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 45766, len(set(self.report_ids)) = 3269\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 45766, len(set(self.report_ids)) = 3269\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.mask_token', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.5.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 45766\n",
      "len(dataloader) = 287\n",
      "Evaluating model ...\n",
      "oracc 0.99638, chxlmicf1 0.54895, chxlmacf1 0.46205, chxlacc 0.69027, chxlrocaucmic 0.79280, chxlrocaucmac 0.75163, qlmacf1 0.19580, 231.10 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3269, len(gt_reports)=3269\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "Chexpert labeler: running a maximum of 4 concurrent processes over 4 chunks\n",
      "chunk: i=0, b=0, e=110, chunk_size=110\n",
      "chunk: i=1, b=110, e=220, chunk_size=110\n",
      "chunk: i=2, b=220, e=330, chunk_size=110\n",
      "chunk: i=3, b=330, e=440, chunk_size=107\n",
      "\t#### process 1: running chexpert labeler over 110 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_112933_0.9981613513686108_0.csv --output_path /data/labeler-output_20230108_112933_0.9981613513686108_0.csv\n",
      "\t#### process 2: running chexpert labeler over 110 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_112933_0.9981613513686108_1.csv --output_path /data/labeler-output_20230108_112933_0.9981613513686108_1.csv\n",
      "\t#### process 3: running chexpert labeler over 110 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_112933_0.9981613513686108_2.csv --output_path /data/labeler-output_20230108_112933_0.9981613513686108_2.csv\n",
      "\t#### process 4: running chexpert labeler over 107 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_112933_0.9981613513686108_3.csv --output_path /data/labeler-output_20230108_112933_0.9981613513686108_3.csv\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 60.84557104110718\n",
      "\t**** process 2 finished, elapsed time = 60.84575033187866\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 3 finished, elapsed time = 62.69754958152771\n",
      "\t**** process 4 finished, elapsed time = 62.69766139984131\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20221231_012821_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=chexpert-labels).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20221231_012821_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"chexpert-labels\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: chexpert-labels\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 4\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=97\n",
      "checkpoint_names = ['checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4782.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4782.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4782.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = chexpert-labels\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 45766, len(set(self.report_ids)) = 3269\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 45766, len(set(self.report_ids)) = 3269\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.mask_token', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 45766\n",
      "len(dataloader) = 287\n",
      "Evaluating model ...\n",
      "oracc 0.97599, chxlmicf1 0.45949, chxlmacf1 0.38155, chxlacc 0.58629, chxlrocaucmic 0.68713, chxlrocaucmac 0.61497, qlmacf1 0.15014, 232.18 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3269, len(gt_reports)=3269\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "Chexpert labeler: running a maximum of 4 concurrent processes over 4 chunks\n",
      "chunk: i=0, b=0, e=43, chunk_size=43\n",
      "chunk: i=1, b=43, e=86, chunk_size=43\n",
      "chunk: i=2, b=86, e=129, chunk_size=43\n",
      "chunk: i=3, b=129, e=172, chunk_size=40\n",
      "\t#### process 1: running chexpert labeler over 43 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_113522_0.3568415384970426_0.csv --output_path /data/labeler-output_20230108_113522_0.3568415384970426_0.csv\n",
      "\t#### process 2: running chexpert labeler over 43 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_113522_0.3568415384970426_1.csv --output_path /data/labeler-output_20230108_113522_0.3568415384970426_1.csv\n",
      "\t#### process 3: running chexpert labeler over 43 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_113522_0.3568415384970426_2.csv --output_path /data/labeler-output_20230108_113522_0.3568415384970426_2.csv\n",
      "\t#### process 4: running chexpert labeler over 40 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_113522_0.3568415384970426_3.csv --output_path /data/labeler-output_20230108_113522_0.3568415384970426_3.csv\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "\t**** process 1 finished, elapsed time = 29.15361762046814\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 2 finished, elapsed time = 30.745598554611206\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "\t**** process 3 finished, elapsed time = 31.448429822921753\n",
      "\t**** process 4 finished, elapsed time = 31.448541164398193\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=chexpert-labels).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_184840_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"chexpert-labels\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_172050_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: chexpert-labels\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 4\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_172050_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=97\n",
      "checkpoint_names = ['checkpoint_100_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5555.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_172050_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_100_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5555.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_172050_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_100_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.5555.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = chexpert-labels\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 45766, len(set(self.report_ids)) = 3269\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 45766, len(set(self.report_ids)) = 3269\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.mask_token', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.0.layernorm_before.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 45766\n",
      "len(dataloader) = 287\n",
      "Evaluating model ...\n",
      "oracc 0.99638, chxlmicf1 0.55066, chxlmacf1 0.46086, chxlacc 0.69023, chxlrocaucmic 0.79427, chxlrocaucmac 0.75115, qlmacf1 0.19453, 244.03 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3269, len(gt_reports)=3269\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "Chexpert labeler: running a maximum of 4 concurrent processes over 4 chunks\n",
      "chunk: i=0, b=0, e=102, chunk_size=102\n",
      "chunk: i=1, b=102, e=204, chunk_size=102\n",
      "chunk: i=2, b=204, e=306, chunk_size=102\n",
      "chunk: i=3, b=306, e=408, chunk_size=99\n",
      "\t#### process 1: running chexpert labeler over 102 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114115_0.11157334046161593_0.csv --output_path /data/labeler-output_20230108_114115_0.11157334046161593_0.csv\n",
      "\t#### process 2: running chexpert labeler over 102 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114115_0.11157334046161593_1.csv --output_path /data/labeler-output_20230108_114115_0.11157334046161593_1.csv\n",
      "\t#### process 3: running chexpert labeler over 102 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114115_0.11157334046161593_2.csv --output_path /data/labeler-output_20230108_114115_0.11157334046161593_2.csv\n",
      "\t#### process 4: running chexpert labeler over 99 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114115_0.11157334046161593_3.csv --output_path /data/labeler-output_20230108_114115_0.11157334046161593_3.csv\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "\t**** process 1 finished, elapsed time = 75.34779500961304\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 2 finished, elapsed time = 78.14362263679504\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 3 finished, elapsed time = 80.02374911308289\n",
      "\t**** process 4 finished, elapsed time = 80.02385902404785\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230106_172050_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=chexpert-labels).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_172050_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"chexpert-labels\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: chexpert-labels\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 4\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using Huggingface ViT model transform for facebook/vit-mae-base\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = (224, 224), use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=97\n",
      "checkpoint_names = ['checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4885.pt', 'checkpoint_27_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4564.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4885.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_39_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4885.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = chexpert-labels\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 45766, len(set(self.report_ids)) = 3269\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 45766, len(set(self.report_ids)) = 3269\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTModel: ['decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.mask_token', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.5.intermediate.dense.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/mae/20230106_124822_mim+iux+chx+cxr14+vinbig+padchest_VitMAE_dws=1.0,0.05,0.6,0.4,0.4,0.5/checkpoint_40_loss=0.9663.pt\n",
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 768\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-base+onehot+transf)\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 45766\n",
      "len(dataloader) = 287\n",
      "Evaluating model ...\n",
      "oracc 0.98060, chxlmicf1 0.46292, chxlmacf1 0.38512, chxlacc 0.58948, chxlrocaucmic 0.69280, chxlrocaucmac 0.62394, qlmacf1 0.15139, 254.46 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3269, len(gt_reports)=3269\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "Chexpert labeler: running a maximum of 4 concurrent processes over 4 chunks\n",
      "chunk: i=0, b=0, e=81, chunk_size=81\n",
      "chunk: i=1, b=81, e=162, chunk_size=81\n",
      "chunk: i=2, b=162, e=243, chunk_size=81\n",
      "chunk: i=3, b=243, e=324, chunk_size=78\n",
      "\t#### process 1: running chexpert labeler over 81 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114751_0.15160574909070057_0.csv --output_path /data/labeler-output_20230108_114751_0.15160574909070057_0.csv\n",
      "\t#### process 2: running chexpert labeler over 81 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114751_0.15160574909070057_1.csv --output_path /data/labeler-output_20230108_114751_0.15160574909070057_1.csv\n",
      "\t#### process 3: running chexpert labeler over 81 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114751_0.15160574909070057_2.csv --output_path /data/labeler-output_20230108_114751_0.15160574909070057_2.csv\n",
      "\t#### process 4: running chexpert labeler over 78 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_114751_0.15160574909070057_3.csv --output_path /data/labeler-output_20230108_114751_0.15160574909070057_3.csv\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 67.43089199066162\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 2 finished, elapsed time = 70.22675347328186\n",
      "\t**** process 3 finished, elapsed time = 70.22684979438782\n",
      "\t**** process 4 finished, elapsed time = 70.2269036769867\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=chexpert-labels).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230106_185156_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-base+onehot+transf)_visenc-pretr=1_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"chexpert-labels\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script's arguments:\n",
      "   checkpoint_folder: /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\n",
      "   eval_mode: chexpert-labels\n",
      "   n_questions_per_report: None\n",
      "   qclass_threshold: None\n",
      "   batch_size: 160\n",
      "   device: GPU\n",
      "   num_workers: 3\n",
      "   answer_decoding: greedy-search\n",
      "   eval_checkpoint_folder: None\n",
      "   precomputed_question_probs_path: None\n",
      "   precomputed_question_thresholds_path: None\n",
      "   use_random_image: False\n",
      "   eval_iuxray: False\n",
      "   eval_mimiccxr: True\n",
      "   use_amp: False\n",
      "   max_processes_for_chexpert_labeler: 4\n",
      "   save_for_error_analysis: False\n",
      "----- Evaluating model ------\n",
      "metadata loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/metadata.json\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m1) \u001b[0m\u001b[34mdevice =\u001b[0m \u001b[34mcuda\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m2) \u001b[0m\u001b[34mLoading iuxray and mimiccxr QA adapted reports ...\u001b[0m\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m3) \u001b[0m\u001b[34mInitializing tokenizer ...\u001b[0m\n",
      "Loading /mnt/data/pamessina/workspaces/medvqa-workspace/cache/vocab(min_freq=10;mode=report;qa_adapted_reports__20220904_091601.json;qa_adapted_reports__20220904_095810.json;_mnt_data_padchest_PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv).pkl ...\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m4) \u001b[0m\u001b[34mEstimating maximum answer length ...\u001b[0m\n",
      "max_answer_length = 15\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m5) \u001b[0m\u001b[34mDefining image transform ...\u001b[0m\n",
      "Using default transform\n",
      "mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225), image_size = [224, 224], use_center_crop = False\n",
      "Returning transform without augmentation\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m6) \u001b[0m\u001b[34mDefining collate_batch_fn ...\u001b[0m\n",
      "get_vqa_collate_batch_fn(): dataset_id=1, one_hot_question_offset=97\n",
      "checkpoint_names = ['checkpoint_48_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4796.pt']\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m7) \u001b[0m\u001b[34mLoading model from checkpoint ...\u001b[0m\n",
      "checkpoint_path =  /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_48_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4796.pt\n",
      "pretrained_checkpoint_path = /mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/checkpoint_48_b+chf1+chf1+cD+cxf1+cxf1+ema+gacc+orcc+paf1+paf1+paf1+paf1+qlf1+qlf1+vnf1+vnf1+wmmp=0.4796.pt\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m8) \u001b[0m\u001b[34mCreating MIMIC-CXR vqa evaluator ...\u001b[0m\n",
      "report_eval_mode = chexpert-labels\n",
      "Checking if data is already cached in path /mnt/data/pamessina/workspaces/medvqa-workspace/cache/mimiccxr/mimiccxr_preprocessed_test_data__(dataset=qa_adapted_reports__20220904_095810.json;tokenizer=4871,39534,3343246773703667053;report_eval_mode=chexpert-labels).pkl ...\n",
      "\tYes, it is, data successfully loaded :)\n",
      "batch_size = 160\n",
      "len(self.report_ids) = 45766, len(set(self.report_ids)) = 3269\n",
      "VQA_Evaluator():\n",
      "  len(self.test_indices) = 45766, len(set(self.report_ids)) = 3269\n",
      "generating test dataset ...\n",
      "generating test dataloader ...\n",
      "done!\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m9) \u001b[0m\u001b[34mCreating instance of OpenEndedVQA model ...\u001b[0m\n",
      "OpenEndedVQA():\n",
      "  image_encoder_pretrained_weights_path None\n",
      "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at facebook/vit-mae-large were not used when initializing ViTModel: ['decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.mask_token', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.key.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-mae-large and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Ignore freezing parameter: pooler.dense.weight\n",
      "Ignore freezing parameter: pooler.dense.bias\n",
      "  self.global_feat_size = 1024\n",
      "  n_questions = 350\n",
      "  n_questions_aux_task = 97\n",
      "  question_encoding = one-hot\n",
      "  answer_decoding = transformer\n",
      "  visual_input_mode = raw-image\n",
      "  name = oevqa(facebook/vit-mae-large+onehot+transf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m10) \u001b[0m\u001b[34mCreating evaluator engine ...\u001b[0m\n",
      "get_engine(): shift_answer=False\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m11) \u001b[0m\u001b[34mAttaching metrics, losses, timer and events to engines ...\u001b[0m\n",
      "\n",
      "========================\n",
      "\u001b[34m--------------------------------------------------\u001b[0m\n",
      "\u001b[34m12) \u001b[0m\u001b[34mRunning evaluator engine on MIMIC-CXR test split ...\u001b[0m\n",
      "len(dataset) = 45766\n",
      "len(dataloader) = 287\n",
      "Evaluating model ...\n",
      "oracc 0.98487, chxlmicf1 0.46742, chxlmacf1 0.39039, chxlacc 0.59643, chxlrocaucmic 0.69803, chxlrocaucmac 0.63354, qlmacf1 0.15460, 650.91 secs\n",
      "\u001b[34mComputing metrics ...\u001b[0m\n",
      "recovered reports: len(gen_reports)=3269, len(gt_reports)=3269\n",
      "Cache successfully loaded from /mnt/data/pamessina/workspaces/medvqa-workspace/cache/chexpert_labeler_cache.pkl\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "All labels found in cache, no need to invoke chexpert labeler\n",
      "(*) Chexpert: labeling 3269 texts ...\n",
      "Chexpert labeler: running a maximum of 4 concurrent processes over 4 chunks\n",
      "chunk: i=0, b=0, e=58, chunk_size=58\n",
      "chunk: i=1, b=58, e=116, chunk_size=58\n",
      "chunk: i=2, b=116, e=174, chunk_size=58\n",
      "chunk: i=3, b=174, e=232, chunk_size=58\n",
      "\t#### process 1: running chexpert labeler over 58 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_120055_0.5961548963342449_0.csv --output_path /data/labeler-output_20230108_120055_0.5961548963342449_0.csv\n",
      "\t#### process 2: running chexpert labeler over 58 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_120055_0.5961548963342449_1.csv --output_path /data/labeler-output_20230108_120055_0.5961548963342449_1.csv\n",
      "\t#### process 3: running chexpert labeler over 58 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_120055_0.5961548963342449_2.csv --output_path /data/labeler-output_20230108_120055_0.5961548963342449_2.csv\n",
      "\t#### process 4: running chexpert labeler over 58 texts ...\n",
      "\tCommand = docker run -v /mnt/data/pamessina/workspaces/medvqa-workspace/tmp/chexpert-labeler:/data chexpert-labeler:latest python label.py --reports_path /data/labeler-input_20230108_120055_0.5961548963342449_3.csv --output_path /data/labeler-output_20230108_120055_0.5961548963342449_3.csv\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 1 finished, elapsed time = 46.51719641685486\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 2 finished, elapsed time = 50.160555601119995\n",
      "Generating LALR tables\n",
      "Downloading 'http://search.maven.org/remotecontent?filepath=edu/stanford/nlp/stanford-corenlp/3.5.2/stanford-corenlp-3.5.2.jar' -> '/root/.local/share/pystanforddeps/stanford-corenlp-3.5.2.jar'\n",
      "\t**** process 3 finished, elapsed time = 50.882811069488525\n",
      "\t**** process 4 finished, elapsed time = 50.88290190696716\n",
      "Report-level metrics successfully saved to /mnt/data/pamessina/workspaces/medvqa-workspace/results/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp/mimiccxr_report_level_metrics(eval_mode=chexpert-labels).pkl\n"
     ]
    }
   ],
   "source": [
    "!python ../eval_report_generation.py \\\n",
    "        --checkpoint-folder \"/mnt/data/pamessina/workspaces/medvqa-workspace/models/vqa/20230107_205007_mim+mim(chex)+iu+iu(chex)+chexp(vqa)+cxr14(vqa)+vinbig(vqa)+padchest(vqa)_oevqa(facebook-vit-mae-large+onehot+transf)_visenc-pretr=0_dws=1.0,0.8,0.2,0.16,0.5,0.4,0.4,0.5_medtok_orien_chx_ql_amp\" \\\n",
    "        --eval-mode \"chexpert-labels\" \\\n",
    "        --no-iuxray \\\n",
    "        --batch-size 160 \\\n",
    "        --num-workers 3 \\\n",
    "        --max-processes-for-chexpert-labeler 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
